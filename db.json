{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"themes/hexo-theme-matery/source/favicon.png","path":"favicon.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/css/gitment.css","path":"css/gitment.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/css/my-gitalk.css","path":"css/my-gitalk.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/css/matery.css","path":"css/matery.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/css/my.css","path":"css/my.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/js/matery.js","path":"js/matery.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/js/search.js","path":"js/search.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/avatar.jpg","path":"medias/avatar.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/comment_bg.png","path":"medias/comment_bg.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/cover.jpg","path":"medias/cover.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/icp.png","path":"medias/icp.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/logo.png","path":"medias/logo.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/animate/animate.min.css","path":"libs/animate/animate.min.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/aos/aos.css","path":"libs/aos/aos.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/aos/aos.js","path":"libs/aos/aos.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/aplayer/APlayer.min.css","path":"libs/aplayer/APlayer.min.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/aplayer/APlayer.min.js","path":"libs/aplayer/APlayer.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/codeBlock/codeBlockFuction.js","path":"libs/codeBlock/codeBlockFuction.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/codeBlock/codeCopy.js","path":"libs/codeBlock/codeCopy.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/codeBlock/codeLang.js","path":"libs/codeBlock/codeLang.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/codeBlock/codeShrink.js","path":"libs/codeBlock/codeShrink.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/background/canvas-nest.js","path":"libs/background/canvas-nest.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/background/ribbon-dynamic.js","path":"libs/background/ribbon-dynamic.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/background/ribbon-refresh.min.js","path":"libs/background/ribbon-refresh.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/background/ribbon.min.js","path":"libs/background/ribbon.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/cryptojs/crypto-js.min.js","path":"libs/cryptojs/crypto-js.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/dplayer/DPlayer.min.css","path":"libs/dplayer/DPlayer.min.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/dplayer/DPlayer.min.js","path":"libs/dplayer/DPlayer.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/echarts/echarts.min.js","path":"libs/echarts/echarts.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/jqcloud/jqcloud-1.0.4.min.js","path":"libs/jqcloud/jqcloud-1.0.4.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/jqcloud/jqcloud.css","path":"libs/jqcloud/jqcloud.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/gitalk/gitalk.css","path":"libs/gitalk/gitalk.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/gitalk/gitalk.min.js","path":"libs/gitalk/gitalk.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/gitment/gitment-default.css","path":"libs/gitment/gitment-default.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/gitment/gitment.js","path":"libs/gitment/gitment.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/instantpage/instantpage.js","path":"libs/instantpage/instantpage.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/jquery/jquery.min.js","path":"libs/jquery/jquery.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/masonry/masonry.pkgd.min.js","path":"libs/masonry/masonry.pkgd.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/materialize/materialize.min.css","path":"libs/materialize/materialize.min.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/materialize/materialize.min.js","path":"libs/materialize/materialize.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/others/busuanzi.pure.mini.js","path":"libs/others/busuanzi.pure.mini.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/others/clicklove.js","path":"libs/others/clicklove.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/scrollprogress/scrollProgress.min.js","path":"libs/scrollprogress/scrollProgress.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/tocbot/tocbot.css","path":"libs/tocbot/tocbot.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/tocbot/tocbot.min.js","path":"libs/tocbot/tocbot.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/valine/Valine.min.js","path":"libs/valine/Valine.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/valine/av-min.js","path":"libs/valine/av-min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/banner/0.jpg","path":"medias/banner/0.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/banner/1.jpg","path":"medias/banner/1.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/banner/2.jpg","path":"medias/banner/2.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/banner/3.jpg","path":"medias/banner/3.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/banner/4.jpg","path":"medias/banner/4.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/banner/5.jpg","path":"medias/banner/5.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/banner/6.jpg","path":"medias/banner/6.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/0.jpg","path":"medias/featureimages/0.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/1.jpg","path":"medias/featureimages/1.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/10.jpg","path":"medias/featureimages/10.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/11.jpg","path":"medias/featureimages/11.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/12.jpg","path":"medias/featureimages/12.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/13.jpg","path":"medias/featureimages/13.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/14.jpg","path":"medias/featureimages/14.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/15.jpg","path":"medias/featureimages/15.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/16.jpg","path":"medias/featureimages/16.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/17.jpg","path":"medias/featureimages/17.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/18.jpg","path":"medias/featureimages/18.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/19.jpg","path":"medias/featureimages/19.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/2.jpg","path":"medias/featureimages/2.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/20.jpg","path":"medias/featureimages/20.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/21.jpg","path":"medias/featureimages/21.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/22.jpg","path":"medias/featureimages/22.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/23.jpg","path":"medias/featureimages/23.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/3.jpg","path":"medias/featureimages/3.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/4.jpg","path":"medias/featureimages/4.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/5.jpg","path":"medias/featureimages/5.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/6.jpg","path":"medias/featureimages/6.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/7.jpg","path":"medias/featureimages/7.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/8.jpg","path":"medias/featureimages/8.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/9.jpg","path":"medias/featureimages/9.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/reward/wechat.png","path":"medias/reward/wechat.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/css/all.css","path":"libs/awesome/css/all.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-brands-400.eot","path":"libs/awesome/webfonts/fa-brands-400.eot","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-brands-400.svg","path":"libs/awesome/webfonts/fa-brands-400.svg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-brands-400.ttf","path":"libs/awesome/webfonts/fa-brands-400.ttf","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-brands-400.woff","path":"libs/awesome/webfonts/fa-brands-400.woff","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-brands-400.woff2","path":"libs/awesome/webfonts/fa-brands-400.woff2","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-regular-400.svg","path":"libs/awesome/webfonts/fa-regular-400.svg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-regular-400.eot","path":"libs/awesome/webfonts/fa-regular-400.eot","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-regular-400.woff","path":"libs/awesome/webfonts/fa-regular-400.woff","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-regular-400.ttf","path":"libs/awesome/webfonts/fa-regular-400.ttf","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-regular-400.woff2","path":"libs/awesome/webfonts/fa-regular-400.woff2","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-solid-900.svg","path":"libs/awesome/webfonts/fa-solid-900.svg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-solid-900.ttf","path":"libs/awesome/webfonts/fa-solid-900.ttf","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-solid-900.eot","path":"libs/awesome/webfonts/fa-solid-900.eot","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-solid-900.woff","path":"libs/awesome/webfonts/fa-solid-900.woff","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-solid-900.woff2","path":"libs/awesome/webfonts/fa-solid-900.woff2","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/css/lightgallery.min.css","path":"libs/lightGallery/css/lightgallery.min.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/fonts/lg.eot","path":"libs/lightGallery/fonts/lg.eot","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/fonts/lg.svg","path":"libs/lightGallery/fonts/lg.svg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/fonts/lg.ttf","path":"libs/lightGallery/fonts/lg.ttf","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/fonts/lg.woff","path":"libs/lightGallery/fonts/lg.woff","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/img/loading.gif","path":"libs/lightGallery/img/loading.gif","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/img/video-play.png","path":"libs/lightGallery/img/video-play.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/img/vimeo-play.png","path":"libs/lightGallery/img/vimeo-play.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/img/youtube-play.png","path":"libs/lightGallery/img/youtube-play.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/js/lightgallery-all.min.js","path":"libs/lightGallery/js/lightgallery-all.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/share/css/share.min.css","path":"libs/share/css/share.min.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/share/fonts/iconfont.svg","path":"libs/share/fonts/iconfont.svg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/share/fonts/iconfont.eot","path":"libs/share/fonts/iconfont.eot","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/share/fonts/iconfont.woff","path":"libs/share/fonts/iconfont.woff","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/share/fonts/iconfont.ttf","path":"libs/share/fonts/iconfont.ttf","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/share/js/social-share.min.js","path":"libs/share/js/social-share.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/share/js/jquery.share.min.js","path":"libs/share/js/jquery.share.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/wisesky.jpg","path":"medias/wisesky.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/reward/alipay.png","path":"medias/reward/alipay.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/ACG/1.jpg","path":"medias/ACG/1.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/ACG/2.jpg","path":"medias/ACG/2.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/ACG/3.jpg","path":"medias/ACG/3.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/ACG/4.jpg","path":"medias/ACG/4.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/ACG/5.jpg","path":"medias/ACG/5.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/ACG/6.jpg","path":"medias/ACG/6.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/ACG/7.jpg","path":"medias/ACG/7.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/ACG/8.jpg","path":"medias/ACG/8.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/ACG/9.jpg","path":"medias/ACG/9.jpg","modified":0,"renderable":1}],"Cache":[{"_id":"source/_posts/hello-world.md","hash":"8b7386961fcf8a46dc6cc34fa9d7422c423b08aa","modified":1602577934454},{"_id":"source/tags/index.md","hash":"6c223043ba32b7c76c33eafceeff9ed879520bfb","modified":1602576798016},{"_id":"source/about/index.md","hash":"48622c880dbd4a56f3346b15c7dc5c36d5e4239b","modified":1602570937056},{"_id":"source/categories/index.md","hash":"03ecf6ae4723c42f6eef84663eb50841513e4b62","modified":1602576755198},{"_id":"themes/hexo-theme-matery/.gitignore","hash":"727607929a51db7ea10968f547c26041eee9cfff","modified":1602570653689},{"_id":"themes/hexo-theme-matery/LICENSE","hash":"7df059597099bb7dcf25d2a9aedfaf4465f72d8d","modified":1602570653689},{"_id":"themes/hexo-theme-matery/README.md","hash":"56299cf1fe60a11fef61b3948fe148f995df747e","modified":1602570653690},{"_id":"themes/hexo-theme-matery/layout/404.ejs","hash":"9c8ca67377211e5d60fdde272a975faa9a91a22a","modified":1602570653691},{"_id":"themes/hexo-theme-matery/README_CN.md","hash":"0fdf818476a444663cc8ffa2f194199d9fd93508","modified":1602570653690},{"_id":"themes/hexo-theme-matery/_config.yml","hash":"9440d60860d7823cb6193934528caedb8ace2939","modified":1606210030449},{"_id":"themes/hexo-theme-matery/layout/archive.ejs","hash":"cdac701de8370f9f3794a0eed4165983993a1ca7","modified":1602570653697},{"_id":"themes/hexo-theme-matery/layout/about.ejs","hash":"41849f9300b8dc47048333fcf4a897dd8a2a13ca","modified":1602570653697},{"_id":"themes/hexo-theme-matery/layout/contact.ejs","hash":"72fb5af3fc2f8955e2eb10926bbe4532a04ccd1b","modified":1602570653698},{"_id":"themes/hexo-theme-matery/layout/category.ejs","hash":"00019bca11fb46477f22017cb1f5ad8444da0580","modified":1602570653698},{"_id":"themes/hexo-theme-matery/layout/categories.ejs","hash":"8e54665cc25d7c333da7d9f312987190be6215da","modified":1602570653697},{"_id":"themes/hexo-theme-matery/layout/friends.ejs","hash":"f5d6459bed0f4ecb214f2dbff5b2207a80c44f66","modified":1602570653698},{"_id":"themes/hexo-theme-matery/layout/index.ejs","hash":"4dc6f08e7709cc04e886be72dbf0d06469f0effc","modified":1602570653698},{"_id":"themes/hexo-theme-matery/layout/tag.ejs","hash":"85a4b05bd8a6ad0f17ff2e97dae56949b379c204","modified":1602570653699},{"_id":"themes/hexo-theme-matery/layout/post.ejs","hash":"90b5a4c1f70e4756db569c15a7c6cad0c77c4500","modified":1602570653699},{"_id":"themes/hexo-theme-matery/layout/tags.ejs","hash":"cf9517aa6a0111355121f44615d6923e312283c7","modified":1602570653699},{"_id":"themes/hexo-theme-matery/languages/default.yml","hash":"54ccc01b097c5bf6820f0edfcece1a87b78ab32d","modified":1602570653690},{"_id":"themes/hexo-theme-matery/layout/layout.ejs","hash":"746abd6bec5ed42bfeb54575fa613d38fb19fe96","modified":1602570653698},{"_id":"themes/hexo-theme-matery/languages/zh-CN.yml","hash":"ec0c18fb0e3ab3ee44268dc2b44fc832cffe3c1b","modified":1602580129662},{"_id":"themes/hexo-theme-matery/languages/zh-HK.yml","hash":"ae34ac0e175c3037675722e436637efbceea32f0","modified":1602570653691},{"_id":"themes/hexo-theme-matery/layout/_partial/back-top.ejs","hash":"47ee36a042bb6d52bbe1d0f329637e8ffcf1d0aa","modified":1602570653691},{"_id":"themes/hexo-theme-matery/source/favicon.png","hash":"774fee8c6d0be9dbb010b20f36c06848d06e3da0","modified":1602570653700},{"_id":"themes/hexo-theme-matery/layout/_partial/background.ejs","hash":"aef6edeeb11209831a11d8c7f5d59992e2573335","modified":1602570653691},{"_id":"themes/hexo-theme-matery/layout/_partial/baidu-push.ejs","hash":"2cebcc5ea3614d7f76ec36670e68050cbe611202","modified":1602570653691},{"_id":"themes/hexo-theme-matery/layout/_partial/bg-cover.ejs","hash":"02191109712f61c0e487b8f0b8466597181a9004","modified":1602570653692},{"_id":"themes/hexo-theme-matery/layout/_partial/bg-cover-content.ejs","hash":"28617bf2a35a4269eba6df466acd174e416d2d1e","modified":1602570653691},{"_id":"themes/hexo-theme-matery/layout/_partial/baidu-analytics.ejs","hash":"3bbcdb474ca1dcad514bdc4b7763e17c55df04fd","modified":1602570653691},{"_id":"themes/hexo-theme-matery/layout/_partial/disqus.ejs","hash":"b2dc2c8b5ed56815e55cc2ea54a6dc4eeba2375d","modified":1602570653692},{"_id":"themes/hexo-theme-matery/layout/_partial/gitalk.ejs","hash":"2aa8fbb04b046fa7679092a48372d7e036835dff","modified":1602570653692},{"_id":"themes/hexo-theme-matery/layout/_partial/github-link.ejs","hash":"3aeb581bd78ab8e15b858e4c44c03bcf92f20b9e","modified":1602570653692},{"_id":"themes/hexo-theme-matery/layout/_partial/gitment.ejs","hash":"90f6218512ef2eab63ada7ad2fc766ae635a2297","modified":1602570653692},{"_id":"themes/hexo-theme-matery/layout/_partial/footer.ejs","hash":"4b5476478ba12183b7c97a33d5545fc53be362a8","modified":1602570653692},{"_id":"themes/hexo-theme-matery/layout/_partial/google-analytics.ejs","hash":"5f4992205617da5f8cc5863c62b5ec46e414e2fb","modified":1602570653692},{"_id":"themes/hexo-theme-matery/layout/_partial/head.ejs","hash":"8d263ebccccd0f9e69539f402955296de6f24a62","modified":1602570653692},{"_id":"themes/hexo-theme-matery/layout/_partial/header.ejs","hash":"59e38c70f3d8e7165e686e5e84a627835f4321b0","modified":1602570653693},{"_id":"themes/hexo-theme-matery/layout/_partial/index-cover.ejs","hash":"76b4a37e0364380b143fdf94bf1a5e6941564414","modified":1602570653693},{"_id":"themes/hexo-theme-matery/layout/_partial/livere.ejs","hash":"9c3401b42ea7f26410a5593bae93ada7e57b43be","modified":1602570653693},{"_id":"themes/hexo-theme-matery/layout/_partial/minivaline.ejs","hash":"5f09386aece8f9cf31f6059bbde79cd6c5171493","modified":1602570653693},{"_id":"themes/hexo-theme-matery/layout/_partial/navigation.ejs","hash":"78b70ff24b3039c871331ebec114b936c1756cc8","modified":1602570653693},{"_id":"themes/hexo-theme-matery/layout/_partial/paging.ejs","hash":"e2df12cf92a82b1a7a7add2eac1db1d954bc5511","modified":1602570653693},{"_id":"themes/hexo-theme-matery/layout/_partial/post-cover.ejs","hash":"d1c873c5de54498c722e155aadb8c0ec39485dfa","modified":1602570653693},{"_id":"themes/hexo-theme-matery/layout/_partial/post-detail-toc.ejs","hash":"a8c9abd8cf806235cadb087a5acca3f9182b76ea","modified":1602570653694},{"_id":"themes/hexo-theme-matery/layout/_partial/post-statis.ejs","hash":"04889f9031743c6b081d02fa4027b0dbfcc45ecf","modified":1602570653694},{"_id":"themes/hexo-theme-matery/layout/_partial/mobile-nav.ejs","hash":"cb0cb452be1cd1857ba600f04025b506f3b6fc79","modified":1602570653693},{"_id":"themes/hexo-theme-matery/layout/_partial/prev-next.ejs","hash":"c76b78782ea82340104fccc089417572e0adece4","modified":1602570653694},{"_id":"themes/hexo-theme-matery/layout/_partial/reprint-statement.ejs","hash":"0ce3f9361f558b99cc2f059c5e50b0e2a152ae38","modified":1602570653694},{"_id":"themes/hexo-theme-matery/layout/_partial/post-detail.ejs","hash":"d05926e79aa6dfc235193b9d8c6aa03118b0eade","modified":1602570653694},{"_id":"themes/hexo-theme-matery/layout/_partial/reward.ejs","hash":"ffc55bc7e73bc698bfc58d8e3780c336b83282cf","modified":1602570653694},{"_id":"themes/hexo-theme-matery/layout/_partial/share.ejs","hash":"c941730a2471d6aab367cbb6e09ed08b56c83143","modified":1602570653695},{"_id":"themes/hexo-theme-matery/layout/_partial/search.ejs","hash":"b09872f69c962cb6dd9d4050a322fdea94903f84","modified":1602570653694},{"_id":"themes/hexo-theme-matery/layout/_partial/social-link.ejs","hash":"6f871bd3a70f720e4e451f1f4f625cbc6d8994a4","modified":1602570653695},{"_id":"themes/hexo-theme-matery/layout/_widget/artitalk.ejs","hash":"b14e486f12b9ac42a273b80e4d785fcb94cf04b2","modified":1602570653695},{"_id":"themes/hexo-theme-matery/layout/_partial/valine.ejs","hash":"0e4c0a6154aa34007849928ca88f05b6185b256e","modified":1602570653695},{"_id":"themes/hexo-theme-matery/layout/_widget/category-radar.ejs","hash":"1d8747fda89a0b2ca3c7008867cbfeecad0578a6","modified":1602570653696},{"_id":"themes/hexo-theme-matery/layout/_widget/music.ejs","hash":"e9e3e327d5de9d7aeadbde32e1d558652d9e9195","modified":1602570653696},{"_id":"themes/hexo-theme-matery/layout/_widget/category-cloud.ejs","hash":"1b3df1009234c0112424b497b18b4ad8240b3bc7","modified":1602570653696},{"_id":"themes/hexo-theme-matery/layout/_widget/my-gallery.ejs","hash":"65a2d2f9722f84c7fd98f6bdf79087a14848ebd8","modified":1602570653696},{"_id":"themes/hexo-theme-matery/layout/_widget/dream.ejs","hash":"9a472ad5591100cdb65d0df9d01034163bd6dd9d","modified":1602570653696},{"_id":"themes/hexo-theme-matery/layout/_widget/my-projects.ejs","hash":"ef60b64021fa349b0048425d858dfcf6c906fede","modified":1602570653696},{"_id":"themes/hexo-theme-matery/layout/_widget/my-skills.ejs","hash":"89a0092df72d23093128f2fbbdc8ca7f83ebcfd9","modified":1602570653696},{"_id":"themes/hexo-theme-matery/layout/_widget/post-calendar.ejs","hash":"48821e644bc73553d7c5c56d2e8ee111a70cd776","modified":1602570653696},{"_id":"themes/hexo-theme-matery/layout/_widget/post-charts.ejs","hash":"ab5f986f428215941aeaa0c88aefd440c47d3bcf","modified":1602570653697},{"_id":"themes/hexo-theme-matery/layout/_widget/recommend.ejs","hash":"8551137e94ca4e2e3b8b63d5626255884cb60cb5","modified":1602570653697},{"_id":"themes/hexo-theme-matery/layout/_widget/tag-cloud.ejs","hash":"fc42b72cddc231f7485cdc1fd6852b66be6add26","modified":1602570653697},{"_id":"themes/hexo-theme-matery/layout/_widget/tag-wordcloud.ejs","hash":"487aacb2454d6bf0d21cdb07ddd1fd5ddbca9038","modified":1602570653697},{"_id":"themes/hexo-theme-matery/layout/_widget/video.ejs","hash":"a0e002377af2a7f7e4da6d9a644de97adb035925","modified":1602570653697},{"_id":"themes/hexo-theme-matery/source/css/gitment.css","hash":"2bd15cc17dca35ac3ecc0acf167a23a1dd362acd","modified":1602570653699},{"_id":"themes/hexo-theme-matery/source/css/my-gitalk.css","hash":"eeda46a83d0db1cc239a9cd27d544faf663f9883","modified":1602570653700},{"_id":"themes/hexo-theme-matery/source/css/matery.css","hash":"87bd1dacf48c9daab7ea43466368247f1e4107d1","modified":1602570653700},{"_id":"themes/hexo-theme-matery/source/css/my.css","hash":"10577fbc30f241b126d1b51b1f56136ecba86b19","modified":1602570653700},{"_id":"themes/hexo-theme-matery/source/js/matery.js","hash":"b86de5fe3e9766b7ff80df12ea41c3a9e30825f7","modified":1602570653700},{"_id":"themes/hexo-theme-matery/source/js/search.js","hash":"d559d402b4d4a0931821fe6e22a8831fc43a953d","modified":1602570653700},{"_id":"themes/hexo-theme-matery/source/medias/avatar.jpg","hash":"2a6287308628881ce27b9a7de53ba15c2be00d02","modified":1602570653748},{"_id":"themes/hexo-theme-matery/source/medias/icp.png","hash":"27a96f31f7d0413c6ade6f40e06f021f501151c7","modified":1602570653776},{"_id":"themes/hexo-theme-matery/source/medias/comment_bg.png","hash":"dfc93d24081884fbc58cab0f8fd19e77d31d6123","modified":1602570653758},{"_id":"themes/hexo-theme-matery/source/medias/logo.png","hash":"d9095f5ea8719374d9d1ff020279426f5b2a1396","modified":1602570653776},{"_id":"themes/hexo-theme-matery/source/libs/aos/aos.css","hash":"191a3705a8f63e589a50a0ff2f2c5559f1a1b6b2","modified":1602570653701},{"_id":"themes/hexo-theme-matery/source/libs/animate/animate.min.css","hash":"97afa151569f046b2e01f27c1871646e9cd87caf","modified":1602570653701},{"_id":"themes/hexo-theme-matery/source/libs/aos/aos.js","hash":"02bfb40b0c4b6e9b0b4081218357145cbb327d74","modified":1602570653701},{"_id":"themes/hexo-theme-matery/source/libs/aplayer/APlayer.min.css","hash":"07372a2ba507388d0fed166d761b1c2c2a659dce","modified":1602570653702},{"_id":"themes/hexo-theme-matery/source/libs/aplayer/APlayer.min.js","hash":"22caa28ff6b41a16ff40f15d38f1739e22359478","modified":1602570653702},{"_id":"themes/hexo-theme-matery/source/libs/codeBlock/codeBlockFuction.js","hash":"c7ab06d27a525b15b1eb69027135269e9b9132fb","modified":1602570653727},{"_id":"themes/hexo-theme-matery/source/libs/codeBlock/codeCopy.js","hash":"6d39a766af62e625f177c4d5cf3adc35eed71e61","modified":1602570653727},{"_id":"themes/hexo-theme-matery/source/libs/codeBlock/codeLang.js","hash":"bac88b4d4e3679732d29bd037c34f089cf27cf05","modified":1602570653727},{"_id":"themes/hexo-theme-matery/source/libs/codeBlock/codeShrink.js","hash":"201e8cd761b4be557247bdaf1ebc7c11c83194f6","modified":1602570653727},{"_id":"themes/hexo-theme-matery/source/libs/background/canvas-nest.js","hash":"65333d0dbb9c1173a1b13031b230161fc42c8b2f","modified":1602570653726},{"_id":"themes/hexo-theme-matery/source/libs/background/ribbon-dynamic.js","hash":"052b80c29e6bc585aa28d4504b743bdbac220a88","modified":1602570653726},{"_id":"themes/hexo-theme-matery/source/libs/background/ribbon-refresh.min.js","hash":"6d98692b2cad8c746a562db18b170b35c24402f4","modified":1602570653726},{"_id":"themes/hexo-theme-matery/source/libs/background/ribbon.min.js","hash":"6a99d494c030388f96f6086a7aaa0f03f3fe532e","modified":1602570653727},{"_id":"themes/hexo-theme-matery/source/libs/cryptojs/crypto-js.min.js","hash":"5989527a378b55011a59522f41eeb3981518325c","modified":1602570653728},{"_id":"themes/hexo-theme-matery/source/libs/jqcloud/jqcloud-1.0.4.min.js","hash":"257eaae3020599e4939f50d5008a743827f25b8c","modified":1602570653738},{"_id":"themes/hexo-theme-matery/source/libs/dplayer/DPlayer.min.css","hash":"f7d19655f873b813ffba5d1a17145c91f82631b8","modified":1602570653728},{"_id":"themes/hexo-theme-matery/source/libs/jqcloud/jqcloud.css","hash":"20d9f11a19d95c70e27cb922e0d6dccbec4eae89","modified":1602570653738},{"_id":"themes/hexo-theme-matery/source/libs/gitment/gitment-default.css","hash":"2903c59ee06b965bef32e937bd69f5b0b2190717","modified":1602570653737},{"_id":"themes/hexo-theme-matery/source/libs/gitalk/gitalk.css","hash":"940ded3ea12c2fe1ab0820d2831ec405f3f1fe9f","modified":1602570653735},{"_id":"themes/hexo-theme-matery/source/libs/instantpage/instantpage.js","hash":"83ce8919b1a69b2f1809ffaf99b52a8627e650e9","modified":1602570653738},{"_id":"themes/hexo-theme-matery/source/libs/masonry/masonry.pkgd.min.js","hash":"ff940b4ea68368ca0e4d5560cbb79fb147dfc3c5","modified":1602570653741},{"_id":"themes/hexo-theme-matery/source/libs/others/busuanzi.pure.mini.js","hash":"6e41f31100ae7eb3a6f23f2c168f6dd56e7f7a9a","modified":1602570653743},{"_id":"themes/hexo-theme-matery/source/libs/others/clicklove.js","hash":"6a39b8c683ba5dcd92f70c6ab45d1cfac3213e8e","modified":1602570653743},{"_id":"themes/hexo-theme-matery/source/libs/scrollprogress/scrollProgress.min.js","hash":"777ffe5d07e85a14fbe97d846f45ffc0087251cc","modified":1602570653743},{"_id":"themes/hexo-theme-matery/source/libs/tocbot/tocbot.css","hash":"9ab8ef576c9a57115194152e79cca79b0a41dd70","modified":1602570653746},{"_id":"themes/hexo-theme-matery/source/libs/tocbot/tocbot.min.js","hash":"5ec27317f0270b8cf6b884c6f12025700b9a565c","modified":1602570653746},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/10.jpg","hash":"98e7f6fd9c97d4de9044b6871ca08ebf14db11b9","modified":1602570653761},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/13.jpg","hash":"35a320174f8e316e3eadaec658024276b651c6e9","modified":1602570653765},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/15.jpg","hash":"da0fbee3b7bde1607eace377ddf834c0be99edfe","modified":1602570653766},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/16.jpg","hash":"97a829c4bc94f9d2929b20a1a9b798c57b9f7205","modified":1602570653766},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/2.jpg","hash":"4bba691cf71a517ecaeaf42afd3e8f8b31e346c1","modified":1602570653768},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/21.jpg","hash":"b26edb128bb0bf58b23fd2f014e9555e89a2ca3b","modified":1602570653770},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/22.jpg","hash":"754579747a3e99747d890fca3162f370b96a7941","modified":1602570653770},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/23.jpg","hash":"7d7f37da3fa7128343adac23866449eb2c6a549a","modified":1602570653771},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/3.jpg","hash":"6ec646c2a70f5f11edacf225c1477f2200a37a96","modified":1602570653771},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/5.jpg","hash":"41ca20129a37fedc573eec28dd7d7b9e5b09228a","modified":1602570653773},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/7.jpg","hash":"7975141cd64e875122c0ea33daaca1a06bf00b8e","modified":1602570653774},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/8.jpg","hash":"8e4b7186352085483ca1174c7c0800114c48df8b","modified":1602570653775},{"_id":"themes/hexo-theme-matery/source/medias/reward/alipay.jpg","hash":"1abc719b95d1b26f1f898e6b0a9b7609146e332f","modified":1602570653777},{"_id":"themes/hexo-theme-matery/source/medias/reward/wechat.png","hash":"f1c84ceb948d0876b5ec7d4d55b654ef3f5132e1","modified":1602577429437},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-regular-400.woff","hash":"59439d3ad31d856d78ec3e2bd9f1eafa2c7a581c","modified":1602570653716},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-regular-400.eot","hash":"439c8afd3373acb4a73135a34e220464a89cd5e2","modified":1602570653713},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-regular-400.ttf","hash":"0f4bd02942a54a6b3200d9078adff88c2812e751","modified":1602570653716},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-regular-400.woff2","hash":"f6f653b4ea8fc487bdb590d39d5a726258a55f40","modified":1602570653717},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/fonts/lg.eot","hash":"54caf05a81e33d7bf04f2e420736ce6f1de5f936","modified":1602570653739},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/css/lightgallery.min.css","hash":"1b7227237f9785c66062a4811508916518e4132c","modified":1602570653739},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/fonts/lg.ttf","hash":"f6421c0c397311ae09f9257aa58bcd5e9720f493","modified":1602570653740},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/fonts/lg.svg","hash":"9c6632aeec67d3e84a1434884aa801514ff8103b","modified":1602570653739},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/fonts/lg.woff","hash":"3048de344dd5cad4624e0127e58eaae4b576f574","modified":1602570653740},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/img/loading.gif","hash":"15a76af2739482d8de7354abc6d8dc4fca8d145e","modified":1602570653740},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/img/video-play.png","hash":"2962e03ddbe04d7e201a5acccac531a2bbccddfc","modified":1602570653740},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/img/youtube-play.png","hash":"f8d11384d33b7a79ee2ba8d522844f14d5067a80","modified":1602570653740},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/img/vimeo-play.png","hash":"9b72fc0f86a01467ed0b68c9cc4d604ec316d517","modified":1602570653740},{"_id":"themes/hexo-theme-matery/source/libs/share/css/share.min.css","hash":"8a778a86f3ce9a042df6be63a9f1039631e351a5","modified":1602570653744},{"_id":"themes/hexo-theme-matery/source/libs/share/fonts/iconfont.svg","hash":"1d56c9d5db0273f07c43cc1397e440f98ba7827a","modified":1602570653744},{"_id":"themes/hexo-theme-matery/source/libs/share/fonts/iconfont.eot","hash":"00ff749c8e202401190cc98d56087cdda716abe4","modified":1602570653744},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/js/lightgallery-all.min.js","hash":"9f5ef4bc8a0a3c746ca4f3c3e6d64493b1a977d8","modified":1602570653741},{"_id":"themes/hexo-theme-matery/source/libs/share/js/social-share.min.js","hash":"a3090a02786dcd4efc6355c1c1dc978add8d6827","modified":1602570653746},{"_id":"themes/hexo-theme-matery/source/libs/share/fonts/iconfont.woff","hash":"2e3fce1dcfbd6e2114e7bfbeaf72d3c62e15a1bd","modified":1602570653745},{"_id":"themes/hexo-theme-matery/source/libs/share/fonts/iconfont.ttf","hash":"afd898f59d363887418669520b24d175f966a083","modified":1602570653745},{"_id":"themes/hexo-theme-matery/source/libs/share/js/jquery.share.min.js","hash":"41367dcb857e02e3c417ebe68a554ce1d4430806","modified":1602570653745},{"_id":"themes/hexo-theme-matery/source/libs/gitment/gitment.js","hash":"28c02c45ce568e084cd1041dc493f83f9c6c88c6","modified":1602570653737},{"_id":"themes/hexo-theme-matery/source/libs/dplayer/DPlayer.min.js","hash":"c3bad7b265574fab0ae4d45867422ea1cb9d6599","modified":1602570653729},{"_id":"themes/hexo-theme-matery/source/libs/jquery/jquery.min.js","hash":"2115753ca5fb7032aec498db7bb5dca624dbe6be","modified":1602570653739},{"_id":"themes/hexo-theme-matery/source/libs/valine/Valine.min.js","hash":"6cbdbf91e1f046dd41267a5ff0691a1fccba99df","modified":1602570653747},{"_id":"themes/hexo-theme-matery/source/medias/banner/0.jpg","hash":"69ec96cd9b4bc3aa631adc9da61353f50c39f031","modified":1602570653749},{"_id":"themes/hexo-theme-matery/source/medias/banner/2.jpg","hash":"39fb2535460ce66cc0b34e07ffb9411db1405f09","modified":1602570653752},{"_id":"themes/hexo-theme-matery/source/medias/banner/3.jpg","hash":"4ac047e92d0363b1a61ab756aca6dac13fb77494","modified":1602570653754},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/1.jpg","hash":"684ae89de8cb7acefae19f5aee6c612037c46393","modified":1602570653761},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/0.jpg","hash":"1c3300f029fc85d6dda6fa4f1d699551034cdaf7","modified":1602570653760},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/11.jpg","hash":"f55972ce7175684f2b11c3c9fc2b5b14bccbfae8","modified":1602570653762},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/12.jpg","hash":"8a4b2e7d92ae95c3b0c921db23c35aa9a41a7d58","modified":1602570653764},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/17.jpg","hash":"42d47903551ee81885c1386022982cae165841c5","modified":1602570653767},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/14.jpg","hash":"38e11221406785bcd93aa9cd23e568e164630ef1","modified":1602570653765},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/20.jpg","hash":"3b11f9b461168d907073f793190865fe621a8573","modified":1602570653769},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/19.jpg","hash":"eb250906fdbc0c408f42ae9933725bc1a05d79fb","modified":1602570653768},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/6.jpg","hash":"c8f2aa4bbb041158b4e73733a341e6a77c8583f7","modified":1602570653774},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/18.jpg","hash":"64829272ec85bb819d55ff89e5b5fd6f64aa436b","modified":1602570653767},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/4.jpg","hash":"e06c47de27619984be9d5d02947f8370a432dfea","modified":1602570653772},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/9.jpg","hash":"b956a2291a04b2132366b53666cf34858b8bdb1f","modified":1602570653776},{"_id":"themes/hexo-theme-matery/source/libs/awesome/css/all.css","hash":"ecc41e32ad2696877a1656749841f3b5543bbe3d","modified":1602570653703},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-brands-400.eot","hash":"22f9e7d5226408eb2d0a11e118257a3ca22b8670","modified":1602570653705},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-brands-400.ttf","hash":"91cbeeaceb644a971241c08362898599d6d968ce","modified":1602570653711},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-brands-400.woff2","hash":"a46bd47ff0a90b812aafafda587d095cdb844271","modified":1602570653713},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-brands-400.woff","hash":"18838f5260317da3c5ed29bf844ac8a4f7ad0529","modified":1602570653712},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-solid-900.woff","hash":"92803b8753ceda573c6906774677c5a7081d2fbb","modified":1602570653725},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-solid-900.woff2","hash":"9c081b88b106c6c04ecb895ba7ba7d3dcb3b55ac","modified":1602570653726},{"_id":"themes/hexo-theme-matery/source/medias/cover.jpg","hash":"d4957ff7cc5e88555cd840f2956ab0561e6f1ccf","modified":1602570653759},{"_id":"themes/hexo-theme-matery/source/libs/gitalk/gitalk.min.js","hash":"8fefe38f28804f90116bdcb74a0875c9de9f3b7d","modified":1602570653736},{"_id":"themes/hexo-theme-matery/source/libs/materialize/materialize.min.css","hash":"a69d456e3345e7f59cd0d47d1b3e70fd4a496a05","modified":1602570653742},{"_id":"themes/hexo-theme-matery/source/libs/materialize/materialize.min.js","hash":"c8b4c65651921d888cf5f27430dfe2ad190d35bf","modified":1602570653743},{"_id":"themes/hexo-theme-matery/source/libs/valine/av-min.js","hash":"541efb9edc1ce425cbe3897cfc25803211fe6a05","modified":1602570653747},{"_id":"themes/hexo-theme-matery/source/medias/banner/1.jpg","hash":"ab122a36998a4f62a61e61a4fc5e00248113413b","modified":1602570653751},{"_id":"themes/hexo-theme-matery/source/medias/banner/5.jpg","hash":"852418f4f09e796e12bc3bab7a1488d3f37d6486","modified":1602570653757},{"_id":"themes/hexo-theme-matery/source/medias/banner/6.jpg","hash":"ed7282cc129c4ff9f322d2f2897fb4aac5c48589","modified":1602570653758},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-regular-400.svg","hash":"3d3a49445343d80f3b553e3e3425b9a7bd49acaf","modified":1602570653714},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-solid-900.ttf","hash":"9521ed12274c2cbc910cea77657116fcf6545da3","modified":1602570653724},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-solid-900.eot","hash":"cab8e84ae5682d1d556e234df9c790985888def8","modified":1602570653718},{"_id":"themes/hexo-theme-matery/source/medias/banner/4.jpg","hash":"e5ac5033678afa9d69edffe9a61004f836cb5734","modified":1602570653756},{"_id":"themes/hexo-theme-matery/source/libs/echarts/echarts.min.js","hash":"9496f386a0da4601cad22c479cc5543913a4d67f","modified":1602570653735},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-brands-400.svg","hash":"5e2d2a159294576bea69cc3360efb5ffe110ab2d","modified":1602570653708},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-solid-900.svg","hash":"7da88b19e1486f8c968d3cf5ab3f194f01ea17fd","modified":1602570653722},{"_id":"public/atom.xml","hash":"425e803f9fe680b4cc793209c8066626cfc8473d","modified":1621496966152},{"_id":"public/search.xml","hash":"760c2a0f43e671ff2c1f24b94cc69c7f728a2070","modified":1621496966152},{"_id":"public/categories/index.html","hash":"ed7a98350f18395b33e0cdde9639a782c1684d03","modified":1621496966152},{"_id":"public/about/index.html","hash":"9ab943ddb2e42484143920869ce569868a9df888","modified":1621496966152},{"_id":"public/tags/index.html","hash":"337f02a4f1c32110b7591d55af117aab1dac6d66","modified":1621496966152},{"_id":"public/2020/10/13/hello-world/index.html","hash":"cd274fbb6171d22d206412e3c2ed506998a9444c","modified":1602576511910},{"_id":"public/archives/index.html","hash":"2d94a19279727710c5346ccfc7f27a1a00320660","modified":1621496966152},{"_id":"public/archives/2020/index.html","hash":"7f5db357bfea3607439873945cf28f5b4b7ae2e8","modified":1621496966152},{"_id":"public/archives/2020/10/index.html","hash":"99ec4bbed7d29c41bcdf00910c04e21fdd9d417f","modified":1621496966152},{"_id":"public/index.html","hash":"7319508bbf652c446652e56db67598f096f67be9","modified":1621496966152},{"_id":"public/favicon.png","hash":"774fee8c6d0be9dbb010b20f36c06848d06e3da0","modified":1602571742461},{"_id":"public/medias/comment_bg.png","hash":"dfc93d24081884fbc58cab0f8fd19e77d31d6123","modified":1602571742461},{"_id":"public/medias/icp.png","hash":"27a96f31f7d0413c6ade6f40e06f021f501151c7","modified":1602571742461},{"_id":"public/medias/logo.png","hash":"d9095f5ea8719374d9d1ff020279426f5b2a1396","modified":1602571742461},{"_id":"public/medias/featureimages/2.jpg","hash":"4bba691cf71a517ecaeaf42afd3e8f8b31e346c1","modified":1602571742461},{"_id":"public/medias/featureimages/22.jpg","hash":"754579747a3e99747d890fca3162f370b96a7941","modified":1602571742461},{"_id":"public/medias/featureimages/23.jpg","hash":"7d7f37da3fa7128343adac23866449eb2c6a549a","modified":1602571742461},{"_id":"public/medias/featureimages/5.jpg","hash":"41ca20129a37fedc573eec28dd7d7b9e5b09228a","modified":1602571742461},{"_id":"public/medias/featureimages/3.jpg","hash":"6ec646c2a70f5f11edacf225c1477f2200a37a96","modified":1602571742461},{"_id":"public/medias/featureimages/7.jpg","hash":"7975141cd64e875122c0ea33daaca1a06bf00b8e","modified":1602571742461},{"_id":"public/medias/reward/alipay.jpg","hash":"1abc719b95d1b26f1f898e6b0a9b7609146e332f","modified":1602571742461},{"_id":"public/medias/reward/wechat.png","hash":"f1c84ceb948d0876b5ec7d4d55b654ef3f5132e1","modified":1602577498584},{"_id":"public/libs/awesome/webfonts/fa-regular-400.woff","hash":"59439d3ad31d856d78ec3e2bd9f1eafa2c7a581c","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-regular-400.woff2","hash":"f6f653b4ea8fc487bdb590d39d5a726258a55f40","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-regular-400.ttf","hash":"0f4bd02942a54a6b3200d9078adff88c2812e751","modified":1602571742461},{"_id":"public/libs/lightGallery/fonts/lg.eot","hash":"54caf05a81e33d7bf04f2e420736ce6f1de5f936","modified":1602571742461},{"_id":"public/libs/lightGallery/fonts/lg.woff","hash":"3048de344dd5cad4624e0127e58eaae4b576f574","modified":1602571742461},{"_id":"public/libs/lightGallery/fonts/lg.ttf","hash":"f6421c0c397311ae09f9257aa58bcd5e9720f493","modified":1602571742461},{"_id":"public/libs/lightGallery/fonts/lg.svg","hash":"9c6632aeec67d3e84a1434884aa801514ff8103b","modified":1602571742461},{"_id":"public/libs/lightGallery/img/video-play.png","hash":"2962e03ddbe04d7e201a5acccac531a2bbccddfc","modified":1602571742461},{"_id":"public/libs/lightGallery/img/vimeo-play.png","hash":"9b72fc0f86a01467ed0b68c9cc4d604ec316d517","modified":1602571742461},{"_id":"public/libs/lightGallery/img/loading.gif","hash":"15a76af2739482d8de7354abc6d8dc4fca8d145e","modified":1602571742461},{"_id":"public/libs/lightGallery/img/youtube-play.png","hash":"f8d11384d33b7a79ee2ba8d522844f14d5067a80","modified":1602571742461},{"_id":"public/libs/share/fonts/iconfont.woff","hash":"2e3fce1dcfbd6e2114e7bfbeaf72d3c62e15a1bd","modified":1602571742461},{"_id":"public/libs/share/fonts/iconfont.eot","hash":"00ff749c8e202401190cc98d56087cdda716abe4","modified":1602571742461},{"_id":"public/libs/share/fonts/iconfont.svg","hash":"1d56c9d5db0273f07c43cc1397e440f98ba7827a","modified":1602571742461},{"_id":"public/libs/share/fonts/iconfont.ttf","hash":"afd898f59d363887418669520b24d175f966a083","modified":1602571742461},{"_id":"public/medias/avatar.jpg","hash":"2a6287308628881ce27b9a7de53ba15c2be00d02","modified":1602571742461},{"_id":"public/medias/banner/0.jpg","hash":"69ec96cd9b4bc3aa631adc9da61353f50c39f031","modified":1602571742461},{"_id":"public/medias/featureimages/10.jpg","hash":"98e7f6fd9c97d4de9044b6871ca08ebf14db11b9","modified":1602571742461},{"_id":"public/medias/featureimages/13.jpg","hash":"35a320174f8e316e3eadaec658024276b651c6e9","modified":1602571742461},{"_id":"public/medias/featureimages/15.jpg","hash":"da0fbee3b7bde1607eace377ddf834c0be99edfe","modified":1602571742461},{"_id":"public/medias/featureimages/16.jpg","hash":"97a829c4bc94f9d2929b20a1a9b798c57b9f7205","modified":1602571742461},{"_id":"public/medias/featureimages/17.jpg","hash":"42d47903551ee81885c1386022982cae165841c5","modified":1602571742461},{"_id":"public/medias/featureimages/19.jpg","hash":"eb250906fdbc0c408f42ae9933725bc1a05d79fb","modified":1602571742461},{"_id":"public/medias/featureimages/18.jpg","hash":"64829272ec85bb819d55ff89e5b5fd6f64aa436b","modified":1602571742461},{"_id":"public/medias/featureimages/20.jpg","hash":"3b11f9b461168d907073f793190865fe621a8573","modified":1602571742461},{"_id":"public/medias/featureimages/21.jpg","hash":"b26edb128bb0bf58b23fd2f014e9555e89a2ca3b","modified":1602571742461},{"_id":"public/medias/featureimages/4.jpg","hash":"e06c47de27619984be9d5d02947f8370a432dfea","modified":1602571742461},{"_id":"public/medias/featureimages/9.jpg","hash":"b956a2291a04b2132366b53666cf34858b8bdb1f","modified":1602571742461},{"_id":"public/medias/featureimages/8.jpg","hash":"8e4b7186352085483ca1174c7c0800114c48df8b","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-brands-400.woff","hash":"18838f5260317da3c5ed29bf844ac8a4f7ad0529","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-brands-400.woff2","hash":"a46bd47ff0a90b812aafafda587d095cdb844271","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-regular-400.eot","hash":"439c8afd3373acb4a73135a34e220464a89cd5e2","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-solid-900.woff","hash":"92803b8753ceda573c6906774677c5a7081d2fbb","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-solid-900.woff2","hash":"9c081b88b106c6c04ecb895ba7ba7d3dcb3b55ac","modified":1602571742461},{"_id":"public/medias/cover.jpg","hash":"d4957ff7cc5e88555cd840f2956ab0561e6f1ccf","modified":1602571742461},{"_id":"public/medias/banner/2.jpg","hash":"39fb2535460ce66cc0b34e07ffb9411db1405f09","modified":1602571742461},{"_id":"public/medias/banner/5.jpg","hash":"852418f4f09e796e12bc3bab7a1488d3f37d6486","modified":1602571742461},{"_id":"public/medias/featureimages/0.jpg","hash":"1c3300f029fc85d6dda6fa4f1d699551034cdaf7","modified":1602571742461},{"_id":"public/medias/featureimages/1.jpg","hash":"684ae89de8cb7acefae19f5aee6c612037c46393","modified":1602571742461},{"_id":"public/medias/featureimages/12.jpg","hash":"8a4b2e7d92ae95c3b0c921db23c35aa9a41a7d58","modified":1602571742461},{"_id":"public/medias/featureimages/11.jpg","hash":"f55972ce7175684f2b11c3c9fc2b5b14bccbfae8","modified":1602571742461},{"_id":"public/medias/featureimages/14.jpg","hash":"38e11221406785bcd93aa9cd23e568e164630ef1","modified":1602571742461},{"_id":"public/medias/featureimages/6.jpg","hash":"c8f2aa4bbb041158b4e73733a341e6a77c8583f7","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-brands-400.eot","hash":"22f9e7d5226408eb2d0a11e118257a3ca22b8670","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-regular-400.svg","hash":"3d3a49445343d80f3b553e3e3425b9a7bd49acaf","modified":1602571742461},{"_id":"public/css/gitment.css","hash":"2bd15cc17dca35ac3ecc0acf167a23a1dd362acd","modified":1602571742461},{"_id":"public/css/my-gitalk.css","hash":"eeda46a83d0db1cc239a9cd27d544faf663f9883","modified":1602571742461},{"_id":"public/css/my.css","hash":"10577fbc30f241b126d1b51b1f56136ecba86b19","modified":1602571742461},{"_id":"public/js/matery.js","hash":"b86de5fe3e9766b7ff80df12ea41c3a9e30825f7","modified":1602571742461},{"_id":"public/js/search.js","hash":"d559d402b4d4a0931821fe6e22a8831fc43a953d","modified":1602571742461},{"_id":"public/libs/aos/aos.js","hash":"02bfb40b0c4b6e9b0b4081218357145cbb327d74","modified":1602571742461},{"_id":"public/libs/aplayer/APlayer.min.css","hash":"07372a2ba507388d0fed166d761b1c2c2a659dce","modified":1602571742461},{"_id":"public/libs/codeBlock/codeBlockFuction.js","hash":"c7ab06d27a525b15b1eb69027135269e9b9132fb","modified":1602571742461},{"_id":"public/libs/codeBlock/codeCopy.js","hash":"6d39a766af62e625f177c4d5cf3adc35eed71e61","modified":1602571742461},{"_id":"public/libs/codeBlock/codeLang.js","hash":"bac88b4d4e3679732d29bd037c34f089cf27cf05","modified":1602571742461},{"_id":"public/libs/codeBlock/codeShrink.js","hash":"201e8cd761b4be557247bdaf1ebc7c11c83194f6","modified":1602571742461},{"_id":"public/libs/background/canvas-nest.js","hash":"65333d0dbb9c1173a1b13031b230161fc42c8b2f","modified":1602571742461},{"_id":"public/libs/background/ribbon-dynamic.js","hash":"052b80c29e6bc585aa28d4504b743bdbac220a88","modified":1602571742461},{"_id":"public/libs/background/ribbon-refresh.min.js","hash":"6d98692b2cad8c746a562db18b170b35c24402f4","modified":1602571742461},{"_id":"public/libs/background/ribbon.min.js","hash":"6a99d494c030388f96f6086a7aaa0f03f3fe532e","modified":1602571742461},{"_id":"public/libs/jqcloud/jqcloud-1.0.4.min.js","hash":"257eaae3020599e4939f50d5008a743827f25b8c","modified":1602571742461},{"_id":"public/libs/jqcloud/jqcloud.css","hash":"20d9f11a19d95c70e27cb922e0d6dccbec4eae89","modified":1602571742461},{"_id":"public/libs/instantpage/instantpage.js","hash":"83ce8919b1a69b2f1809ffaf99b52a8627e650e9","modified":1602571742461},{"_id":"public/libs/others/clicklove.js","hash":"6a39b8c683ba5dcd92f70c6ab45d1cfac3213e8e","modified":1602571742461},{"_id":"public/libs/others/busuanzi.pure.mini.js","hash":"6e41f31100ae7eb3a6f23f2c168f6dd56e7f7a9a","modified":1602571742461},{"_id":"public/libs/scrollprogress/scrollProgress.min.js","hash":"777ffe5d07e85a14fbe97d846f45ffc0087251cc","modified":1602571742461},{"_id":"public/libs/tocbot/tocbot.css","hash":"9ab8ef576c9a57115194152e79cca79b0a41dd70","modified":1602571742461},{"_id":"public/libs/tocbot/tocbot.min.js","hash":"5ec27317f0270b8cf6b884c6f12025700b9a565c","modified":1602571742461},{"_id":"public/libs/share/css/share.min.css","hash":"8a778a86f3ce9a042df6be63a9f1039631e351a5","modified":1602571742461},{"_id":"public/medias/banner/3.jpg","hash":"4ac047e92d0363b1a61ab756aca6dac13fb77494","modified":1602571742461},{"_id":"public/medias/banner/6.jpg","hash":"ed7282cc129c4ff9f322d2f2897fb4aac5c48589","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-brands-400.ttf","hash":"91cbeeaceb644a971241c08362898599d6d968ce","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-solid-900.ttf","hash":"9521ed12274c2cbc910cea77657116fcf6545da3","modified":1602571742461},{"_id":"public/libs/aos/aos.css","hash":"191a3705a8f63e589a50a0ff2f2c5559f1a1b6b2","modified":1602571742461},{"_id":"public/libs/gitalk/gitalk.css","hash":"940ded3ea12c2fe1ab0820d2831ec405f3f1fe9f","modified":1602571742461},{"_id":"public/libs/gitment/gitment-default.css","hash":"2903c59ee06b965bef32e937bd69f5b0b2190717","modified":1602571742461},{"_id":"public/libs/masonry/masonry.pkgd.min.js","hash":"ff940b4ea68368ca0e4d5560cbb79fb147dfc3c5","modified":1602571742461},{"_id":"public/libs/lightGallery/css/lightgallery.min.css","hash":"1b7227237f9785c66062a4811508916518e4132c","modified":1602571742461},{"_id":"public/libs/share/js/social-share.min.js","hash":"a3090a02786dcd4efc6355c1c1dc978add8d6827","modified":1602571742461},{"_id":"public/libs/share/js/jquery.share.min.js","hash":"41367dcb857e02e3c417ebe68a554ce1d4430806","modified":1602571742461},{"_id":"public/medias/banner/1.jpg","hash":"ab122a36998a4f62a61e61a4fc5e00248113413b","modified":1602571742461},{"_id":"public/css/matery.css","hash":"87bd1dacf48c9daab7ea43466368247f1e4107d1","modified":1602571742461},{"_id":"public/libs/cryptojs/crypto-js.min.js","hash":"5989527a378b55011a59522f41eeb3981518325c","modified":1602571742461},{"_id":"public/libs/dplayer/DPlayer.min.css","hash":"f7d19655f873b813ffba5d1a17145c91f82631b8","modified":1602571742461},{"_id":"public/libs/animate/animate.min.css","hash":"97afa151569f046b2e01f27c1871646e9cd87caf","modified":1602571742461},{"_id":"public/libs/aplayer/APlayer.min.js","hash":"22caa28ff6b41a16ff40f15d38f1739e22359478","modified":1602571742461},{"_id":"public/libs/lightGallery/js/lightgallery-all.min.js","hash":"9f5ef4bc8a0a3c746ca4f3c3e6d64493b1a977d8","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-solid-900.eot","hash":"cab8e84ae5682d1d556e234df9c790985888def8","modified":1602571742461},{"_id":"public/libs/gitment/gitment.js","hash":"28c02c45ce568e084cd1041dc493f83f9c6c88c6","modified":1602571742461},{"_id":"public/libs/valine/Valine.min.js","hash":"6cbdbf91e1f046dd41267a5ff0691a1fccba99df","modified":1602571742461},{"_id":"public/libs/awesome/css/all.css","hash":"ecc41e32ad2696877a1656749841f3b5543bbe3d","modified":1602571742461},{"_id":"public/medias/banner/4.jpg","hash":"e5ac5033678afa9d69edffe9a61004f836cb5734","modified":1602571742461},{"_id":"public/libs/jquery/jquery.min.js","hash":"2115753ca5fb7032aec498db7bb5dca624dbe6be","modified":1602571742461},{"_id":"public/libs/dplayer/DPlayer.min.js","hash":"c3bad7b265574fab0ae4d45867422ea1cb9d6599","modified":1602571742461},{"_id":"public/libs/materialize/materialize.min.css","hash":"a69d456e3345e7f59cd0d47d1b3e70fd4a496a05","modified":1602571742461},{"_id":"public/libs/gitalk/gitalk.min.js","hash":"8fefe38f28804f90116bdcb74a0875c9de9f3b7d","modified":1602571742461},{"_id":"public/libs/valine/av-min.js","hash":"541efb9edc1ce425cbe3897cfc25803211fe6a05","modified":1602571742461},{"_id":"public/libs/materialize/materialize.min.js","hash":"c8b4c65651921d888cf5f27430dfe2ad190d35bf","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-brands-400.svg","hash":"5e2d2a159294576bea69cc3360efb5ffe110ab2d","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-solid-900.svg","hash":"7da88b19e1486f8c968d3cf5ab3f194f01ea17fd","modified":1602571742461},{"_id":"public/libs/echarts/echarts.min.js","hash":"9496f386a0da4601cad22c479cc5543913a4d67f","modified":1602571742461},{"_id":"themes/hexo-theme-matery/source/medias/wisesky.jpg","hash":"ca0e4de76b63d2782d8618aeea148467258ae14f","modified":1602572521751},{"_id":"public/medias/wisesky.jpg","hash":"ca0e4de76b63d2782d8618aeea148467258ae14f","modified":1602574164658},{"_id":"themes/hexo-theme-matery/source/medias/reward/.DS_Store","hash":"fe9ec4436feaf1a9fedf0f2a2938c80df09fa8fa","modified":1602577812069},{"_id":"themes/hexo-theme-matery/source/medias/.DS_Store","hash":"49a8c2d15a0f57fe40bb502e337a296afdb1ef83","modified":1602577383050},{"_id":"themes/hexo-theme-matery/source/medias/reward/alipay.png","hash":"7c1cd3be7825da29c2aca0d8c0a77f6f791b6d3a","modified":1602577351418},{"_id":"public/uncategorized/hello-world/index.html","hash":"38d91d91a28d189bc1f36f490d64f1cab54bb5dd","modified":1602577909635},{"_id":"public/medias/reward/alipay.png","hash":"7c1cd3be7825da29c2aca0d8c0a77f6f791b6d3a","modified":1602577498584},{"_id":"public/tags/test/index.html","hash":"5613085029c80e1e0c0ea348cea0ee0e817fced3","modified":1621496966152},{"_id":"public/test/hello-world/index.html","hash":"258096a2581e401331d1f13925ab96d8154cfa38","modified":1607587332408},{"_id":"public/categories/test/index.html","hash":"805e2633d5f445b48c01cca8b39f87fc791e6bca","modified":1621496966152},{"_id":"source/_posts/my-first-blog.md","hash":"c7e5400935afb8289e78a08d62bfe907fba89d36","modified":1606285615417},{"_id":"source/.DS_Store","hash":"e99cc6920f98e5d694d97a1f601d60ebea90cc40","modified":1621495672290},{"_id":"themes/hexo-theme-matery/source/medias/ACG/9.jpg","hash":"10b2ddfbe7f03c4513ace7a2855a47f907478d55","modified":1602578209287},{"_id":"themes/hexo-theme-matery/source/medias/ACG/4.jpg","hash":"f06a264751553ff6b2e3b158c21154ce0aacda16","modified":1602578203288},{"_id":"themes/hexo-theme-matery/source/medias/ACG/3.jpg","hash":"f779883e34f277950961f5f1ab62a433dcd91567","modified":1602578201822},{"_id":"themes/hexo-theme-matery/source/medias/ACG/8.jpg","hash":"63c5ff3afa220b73031891a08cf56146f7fd9fee","modified":1602578208159},{"_id":"themes/hexo-theme-matery/source/medias/ACG/2.jpg","hash":"28792c2f8c55005db8c0e6aeae70b394c7dd0fff","modified":1602578200558},{"_id":"themes/hexo-theme-matery/source/medias/ACG/7.jpg","hash":"f80b51d620ca4fd2878295b00c68659c7990cfa0","modified":1602578206996},{"_id":"themes/hexo-theme-matery/source/medias/ACG/6.jpg","hash":"f82a96d6e74947be00c049d57d4e49a102344774","modified":1602578205731},{"_id":"themes/hexo-theme-matery/source/medias/ACG/5.jpg","hash":"5dfa664bb0bcf5ed1480e44bbeb92291a59b3326","modified":1602578204467},{"_id":"themes/hexo-theme-matery/source/medias/ACG/1.jpg","hash":"33c9e2585ba65798847eb7d10009db4089593350","modified":1602578198957},{"_id":"public/sui-bi/my-first-blog/index.html","hash":"f083da341fc1d909ef9996332c20945f036443cb","modified":1607587146319},{"_id":"public/categories/随笔/index.html","hash":"20d8e82df68a2e331b38e5e34a3f813de0b59c80","modified":1621496966152},{"_id":"public/tags/随笔/index.html","hash":"b1a7caaf45dcde02baa540bb137d055f9a3795bd","modified":1621496966152},{"_id":"public/css/prism-tomorrow.css","hash":"3b99487dfc9b4e51e9105a93743b92a761840e34","modified":1602665020148},{"_id":"public/medias/ACG/9.jpg","hash":"10b2ddfbe7f03c4513ace7a2855a47f907478d55","modified":1602665020148},{"_id":"public/medias/ACG/3.jpg","hash":"f779883e34f277950961f5f1ab62a433dcd91567","modified":1602665020148},{"_id":"public/medias/ACG/4.jpg","hash":"f06a264751553ff6b2e3b158c21154ce0aacda16","modified":1602665020148},{"_id":"public/medias/ACG/8.jpg","hash":"63c5ff3afa220b73031891a08cf56146f7fd9fee","modified":1602665020148},{"_id":"public/medias/ACG/2.jpg","hash":"28792c2f8c55005db8c0e6aeae70b394c7dd0fff","modified":1602665020148},{"_id":"public/medias/ACG/7.jpg","hash":"f80b51d620ca4fd2878295b00c68659c7990cfa0","modified":1602665020148},{"_id":"public/medias/ACG/6.jpg","hash":"f82a96d6e74947be00c049d57d4e49a102344774","modified":1602665020148},{"_id":"public/medias/ACG/5.jpg","hash":"5dfa664bb0bcf5ed1480e44bbeb92291a59b3326","modified":1602665020148},{"_id":"public/medias/ACG/1.jpg","hash":"33c9e2585ba65798847eb7d10009db4089593350","modified":1602665020148},{"_id":"source/_posts/.DS_Store","hash":"15e613ffb85d4c1c2580803f9a341ecb8eb340d1","modified":1607335687233},{"_id":"source/_posts/my-first-blog/Euphonium_Movie_2nd_KV.jpg","hash":"ca061b9e7f84622a9786b51979258304ed68f2ab","modified":1602665209583},{"_id":"source/_posts/my-first-blog/p2409965093.jpg","hash":"3e95adce661298a77553ffb74451bf8d8f94dbd3","modified":1602665506316},{"_id":"source/_posts/my-first-blog/p2382958621.png","hash":"8f905289010e20f1ec91485c06fefb9aa3969210","modified":1602665506289},{"_id":"source/_posts/my-first-blog/Euphonium_Movie_Finale_KV2.jpg","hash":"4bc062c352ba272482c27be24c373e3aa0dd99db","modified":1602665209643},{"_id":"public/sui-bi/my-first-blog/Euphonium_Movie_2nd_KV.jpg","hash":"ca061b9e7f84622a9786b51979258304ed68f2ab","modified":1602665561247},{"_id":"public/sui-bi/my-first-blog/p2409965093.jpg","hash":"3e95adce661298a77553ffb74451bf8d8f94dbd3","modified":1602665561247},{"_id":"public/sui-bi/my-first-blog/p2382958621.png","hash":"8f905289010e20f1ec91485c06fefb9aa3969210","modified":1602665561247},{"_id":"public/sui-bi/my-first-blog/Euphonium_Movie_Finale_KV2.jpg","hash":"4bc062c352ba272482c27be24c373e3aa0dd99db","modified":1602665561247},{"_id":"source/_posts/my-first-blog/relife-2.jpg","hash":"3e95adce661298a77553ffb74451bf8d8f94dbd3","modified":1602665691497},{"_id":"source/_posts/my-first-blog/relife-1.png","hash":"8f905289010e20f1ec91485c06fefb9aa3969210","modified":1602665687425},{"_id":"public/sui-bi/my-first-blog/relife-2.jpg","hash":"3e95adce661298a77553ffb74451bf8d8f94dbd3","modified":1602665761392},{"_id":"public/sui-bi/my-first-blog/relife-1.png","hash":"8f905289010e20f1ec91485c06fefb9aa3969210","modified":1602665761392},{"_id":"source/_posts/改变与懒惰.md","hash":"a1309fc7ecd208af09188de98e92de9dd6fbe719","modified":1603879037910},{"_id":"source/_posts/sth-change-myself/2020-07-31.jpg","hash":"1e70ffa9161923f5a7b25c5f9da60f925ade49cf","modified":1602319160796},{"_id":"public/sui-bi/gai-bian-yu-lan-duo/index.html","hash":"21bb755d93994a7f7da145416bef2867cb72bd5c","modified":1607587146319},{"_id":"source/_posts/改变与懒惰/2020-07-31.jpg","hash":"1e70ffa9161923f5a7b25c5f9da60f925ade49cf","modified":1602319160796},{"_id":"public/sui-bi/gai-bian-yu-lan-duo/2020-07-31.jpg","hash":"1e70ffa9161923f5a7b25c5f9da60f925ade49cf","modified":1603877001247},{"_id":"source/_posts/改变与懒惰/IMG_9373.jpeg","hash":"3abf97bca9a9719e6c3797cb69fbaf3e3b82f183","modified":1603878562340},{"_id":"public/sui-bi/gai-bian-yu-lan-duo/IMG_9373.jpeg","hash":"3abf97bca9a9719e6c3797cb69fbaf3e3b82f183","modified":1603878590251},{"_id":"source/_posts/记ubuntu重启引起的故障排查.md","hash":"77ae20e01b2eda05026ae6c56d48638eb72e059c","modified":1607587287729},{"_id":"public/tags/ubuntu/index.html","hash":"c7366ebd33ccea8650bbb9220fe6edd7c42b4014","modified":1621496966152},{"_id":"public/tags/故障排查/index.html","hash":"6d3d47ca2b42745691c75170983024d029728283","modified":1621496966152},{"_id":"public/yun-wei/ji-ubuntu-chong-qi-yin-qi-de-gu-zhang-pai-cha/index.html","hash":"e1258cbdaf70fb557e6b32d11f8facebfa0689b4","modified":1607587146319},{"_id":"public/archives/2020/11/index.html","hash":"e0c4cad604024564c5577422659205afe08606a4","modified":1621496966152},{"_id":"public/tags/grub/index.html","hash":"428089ccef3c83340eb2fffaf54ee91cc8e2596d","modified":1621496966152},{"_id":"public/tags/linux内核/index.html","hash":"c7d15206decf356780cceac59962c8a141723c31","modified":1621496966152},{"_id":"public/categories/运维/index.html","hash":"9d4871d1c062f945c646470ea901a692ba7c8f83","modified":1621496966152},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array.md","hash":"38c137779b3225cbe8640623089ecd5e7d8d2519","modified":1607587296687},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img2.jpg","hash":"8839af294b39128cc5ebac6602b4432c78f1f702","modified":1605604665263},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img3.jpg","hash":"91dc5aa98d0e31eea2c8a7e06aa02a93c2fed481","modified":1605604660564},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img6.jpg","hash":"8839af294b39128cc5ebac6602b4432c78f1f702","modified":1605605676330},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img7.jpg","hash":"99da6e3b4c7064128818cd54cf48f54464a4b379","modified":1605605679078},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img8.jpg","hash":"02e2ce27e3ec791a9f78b746b321e90def906ca3","modified":1605605684311},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img1.png","hash":"1561362bfc8b07eeca9e43b997431177a3b7601a","modified":1605601397721},{"_id":"public/algorithm-leetcode/leetcode-33-search-in-rotated-sorted-array/index.html","hash":"14e755a215967c9e99d9d29bd48df693b977acb3","modified":1605609271672},{"_id":"public/categories/Algorithm-LeetCode/index.html","hash":"9d46e00e60588a274764937e1e653a06083659f8","modified":1605609271672},{"_id":"public/tags/Algorithm/index.html","hash":"ff8c93ed596d4497fe4b335993915ef1fab4d541","modified":1621496966152},{"_id":"public/algorithm-leetcode/leetcode-33-search-in-rotated-sorted-array/img3.jpg","hash":"91dc5aa98d0e31eea2c8a7e06aa02a93c2fed481","modified":1605609207295},{"_id":"public/algorithm-leetcode/leetcode-33-search-in-rotated-sorted-array/img7.jpg","hash":"99da6e3b4c7064128818cd54cf48f54464a4b379","modified":1605609207295},{"_id":"public/algorithm-leetcode/leetcode-33-search-in-rotated-sorted-array/img6.jpg","hash":"8839af294b39128cc5ebac6602b4432c78f1f702","modified":1605609207295},{"_id":"public/algorithm-leetcode/leetcode-33-search-in-rotated-sorted-array/img8.jpg","hash":"02e2ce27e3ec791a9f78b746b321e90def906ca3","modified":1605609207295},{"_id":"public/algorithm-leetcode/leetcode-33-search-in-rotated-sorted-array/img2.jpg","hash":"8839af294b39128cc5ebac6602b4432c78f1f702","modified":1605609207295},{"_id":"public/algorithm-leetcode/leetcode-33-search-in-rotated-sorted-array/img1.png","hash":"1561362bfc8b07eeca9e43b997431177a3b7601a","modified":1605609207295},{"_id":"public/algorithm/leetcode/leetcode-33-search-in-rotated-sorted-array/index.html","hash":"d51f222777c9e812ba0f8b9aad05cf96023265f8","modified":1605609346756},{"_id":"public/categories/Algorithm/index.html","hash":"36e712a01279cae76b215053b176764bcc45c514","modified":1621496966152},{"_id":"public/categories/Algorithm/LeetCode/index.html","hash":"d46713a23432b4b6230b9a873316706befaf8bfd","modified":1605609346756},{"_id":"public/algorithm/leetcode/leetcode-33-search-in-rotated-sorted-array/img2.jpg","hash":"8839af294b39128cc5ebac6602b4432c78f1f702","modified":1605609346756},{"_id":"public/algorithm/leetcode/leetcode-33-search-in-rotated-sorted-array/img3.jpg","hash":"91dc5aa98d0e31eea2c8a7e06aa02a93c2fed481","modified":1605609346756},{"_id":"public/algorithm/leetcode/leetcode-33-search-in-rotated-sorted-array/img6.jpg","hash":"8839af294b39128cc5ebac6602b4432c78f1f702","modified":1605609346756},{"_id":"public/algorithm/leetcode/leetcode-33-search-in-rotated-sorted-array/img7.jpg","hash":"99da6e3b4c7064128818cd54cf48f54464a4b379","modified":1605609346756},{"_id":"public/algorithm/leetcode/leetcode-33-search-in-rotated-sorted-array/img8.jpg","hash":"02e2ce27e3ec791a9f78b746b321e90def906ca3","modified":1605609346756},{"_id":"public/algorithm/leetcode/leetcode-33-search-in-rotated-sorted-array/img1.png","hash":"1561362bfc8b07eeca9e43b997431177a3b7601a","modified":1605609346756},{"_id":"public/algorithm/leetcode-33-search-in-rotated-sorted-array/index.html","hash":"4e1c329df7da6db9e1cc8eaa15e2159986c681e7","modified":1607587146319},{"_id":"public/tags/LeetCode/index.html","hash":"e260b5ae0694008a845bb6b44fb6c311965d3511","modified":1621496966152},{"_id":"public/algorithm/leetcode-33-search-in-rotated-sorted-array/img2.jpg","hash":"8839af294b39128cc5ebac6602b4432c78f1f702","modified":1605609419212},{"_id":"public/algorithm/leetcode-33-search-in-rotated-sorted-array/img6.jpg","hash":"8839af294b39128cc5ebac6602b4432c78f1f702","modified":1605609419212},{"_id":"public/algorithm/leetcode-33-search-in-rotated-sorted-array/img3.jpg","hash":"91dc5aa98d0e31eea2c8a7e06aa02a93c2fed481","modified":1605609419212},{"_id":"public/algorithm/leetcode-33-search-in-rotated-sorted-array/img7.jpg","hash":"99da6e3b4c7064128818cd54cf48f54464a4b379","modified":1605609419212},{"_id":"public/algorithm/leetcode-33-search-in-rotated-sorted-array/img8.jpg","hash":"02e2ce27e3ec791a9f78b746b321e90def906ca3","modified":1605609419212},{"_id":"public/algorithm/leetcode-33-search-in-rotated-sorted-array/img1.png","hash":"1561362bfc8b07eeca9e43b997431177a3b7601a","modified":1605609419212},{"_id":"source/_posts/同步和异步编程-如何并行写同一个日志文件实践.md","hash":"e8d482206efd10d16cc8d6dea31c43267e2427ab","modified":1609321383929},{"_id":"source/_posts/同步和异步编程-如何并行写同一个日志文件实践/1.jpeg","hash":"7ab7e4f532a7ceca75c6b9dbeffe4512883483f6","modified":1606125898681},{"_id":"source/_posts/同步和异步编程-如何并行写同一个日志文件实践/2.jpeg","hash":"e8afe7e8b17d1ee02551c2c6956f24fe802ad147","modified":1606125894245},{"_id":"public/algorithms/tong-bu-he-yi-bu-bian-cheng-ru-he-bing-xing-xie-tong-yi-ge-ri-zhi-wen-jian-shi-jian/index.html","hash":"cd978c6fe4afa24745ca8dfe9fac45b763a33604","modified":1609321386855},{"_id":"public/categories/Algorithms/index.html","hash":"ec6c175db47e3bf46eb7844922e630d6429cd5a2","modified":1621496966152},{"_id":"public/tags/Algorithms/index.html","hash":"1738b0d264adc8d59a70cc7733020947ecd53be7","modified":1621496966152},{"_id":"public/tags/OS/index.html","hash":"47607b88ea60c568cad10ded051f0ee45a121cec","modified":1621496966152},{"_id":"public/algorithms/tong-bu-he-yi-bu-bian-cheng-ru-he-bing-xing-xie-tong-yi-ge-ri-zhi-wen-jian-shi-jian/1.jpeg","hash":"7ab7e4f532a7ceca75c6b9dbeffe4512883483f6","modified":1606208305818},{"_id":"public/algorithms/tong-bu-he-yi-bu-bian-cheng-ru-he-bing-xing-xie-tong-yi-ge-ri-zhi-wen-jian-shi-jian/2.jpeg","hash":"e8afe7e8b17d1ee02551c2c6956f24fe802ad147","modified":1606208305818},{"_id":"source/_posts/GPU-in-Pytorch-并行和分布式实践.md","hash":"db5570eb8b4bd75b700b78aca156ae768433d4d7","modified":1610694623263},{"_id":"source/_posts/并行和分布式训练-in-Pytorch.md","hash":"359fac6de0a159566dfeb31a9200758eb18eb270","modified":1606287881380},{"_id":"public/pytorch/gpu-in-pytorch-bing-xing-he-fen-bu-shi-shi-jian/index.html","hash":"eebcf3beb5a8201f3f89177942daf58d29fd34e8","modified":1621496966152},{"_id":"public/pytorch/bing-xing-he-fen-bu-shi-xun-lian-in-pytorch/index.html","hash":"737e7305cf12eab004519847a2160137caa2ee0c","modified":1606386112132},{"_id":"public/categories/Pytorch/index.html","hash":"5ec3f29b8eba47e01bdd7d68937fb04e3abdfd28","modified":1621496966152},{"_id":"public/tags/Pytorch/index.html","hash":"bc125a8a3b996ea92c6419ef1e53592dc54c029b","modified":1621496966152},{"_id":"public/tags/CUDA/index.html","hash":"b65bd52debf3d1665137fece679a2392ed94614d","modified":1621496966152},{"_id":"public/tags/GPU/index.html","hash":"63aa4fa437426990ddbef402c91ee9781eb72d08","modified":1621496966152},{"_id":"public/tags/NLP/index.html","hash":"9602a400454f51e6ff0f75f0cd168ed803d5200b","modified":1607334899261},{"_id":"source/_posts/LeetCode-1-100-刷题有感.md","hash":"77eea51910993543a022d21d671abebb38afe513","modified":1607587307037},{"_id":"public/sui-bi/leetcode-1-100-shua-ti-you-gan/index.html","hash":"d6b7c25ab9e014df1d395a5e167f0862531aaaae","modified":1607595087858},{"_id":"public/archives/2020/12/index.html","hash":"faf11c3dd956fe257fa51d0e9c0274610cd7321f","modified":1621496966152},{"_id":"source/_posts/LeetCode-115-Distinct-Subsequences.md","hash":"05a024d8dbfe1de678dd477ba116a32fe4225a9d","modified":1607595083217},{"_id":"source/_posts/GPU-in-Pytorch-并行和分布式实践/1.png","hash":"8a8b4c079a90aaa0773d0e98f8fdbfcc422ff4c8","modified":1607581522220},{"_id":"public/uncategorized/leetcode-115-distinct-subsequences/index.html","hash":"df6d923879f595b76f6fd49ca3876768866534d7","modified":1607594612520},{"_id":"public/pytorch/gpu-in-pytorch-bing-xing-he-fen-bu-shi-shi-jian/1.png","hash":"8a8b4c079a90aaa0773d0e98f8fdbfcc422ff4c8","modified":1607587146319},{"_id":"public/algorithms/leetcode-115-distinct-subsequences/index.html","hash":"4929a9aec73445f3e4a303fd28815be66b3912c3","modified":1608197436991},{"_id":"source/_posts/LeetCode-126-Word-Ladder-II.md","hash":"6f34292345a185b9f366321d21d3493999193250","modified":1608174426076},{"_id":"public/uncategorized/leetcode-126-word-ladder-ii/index.html","hash":"599cc916b1ae47809f51239228dde86bc7698722","modified":1608018948407},{"_id":"source/_posts/132-Palindrome-Partitioning-II.md","hash":"1ef8b00332737a661a80de88df2c339602cfc45c","modified":1608197580529},{"_id":"source/_posts/132-Palindrome-Partitioning-II/1.png","hash":"096aa5522771cf3aed36316ca0d12c74c72e134b","modified":1608195929481},{"_id":"source/_posts/132-Palindrome-Partitioning-II/2.png","hash":"422e51f2570ab261c5032d94209adbbd61332015","modified":1608195894436},{"_id":"source/_posts/132-Palindrome-Partitioning-II/3.png","hash":"f4e046c35a648f60fc4de379f7f968ec775fd084","modified":1608195909990},{"_id":"source/_posts/132-Palindrome-Partitioning-II/4.png","hash":"062fb6dd474e26028b6f1657ca562702cd823313","modified":1608195955378},{"_id":"public/algorithms/132-palindrome-partitioning-ii/index.html","hash":"6581cd78f71163263301c46ca5ffd4e930f746e0","modified":1609320899501},{"_id":"public/algorithms/leetcode-126-word-ladder-ii/index.html","hash":"dba2d6fa991c8a956e67d5867b5eca4c3c707d39","modified":1608197436991},{"_id":"public/archives/page/2/index.html","hash":"86293e4ead733d4d743bd495b3d7ea1a55f80199","modified":1621496966152},{"_id":"public/archives/2020/page/2/index.html","hash":"fcdf974c06d408a834ce336e2be58d23f6a55d3d","modified":1621496966152},{"_id":"public/algorithms/132-palindrome-partitioning-ii/4.png","hash":"062fb6dd474e26028b6f1657ca562702cd823313","modified":1608197436991},{"_id":"public/algorithms/132-palindrome-partitioning-ii/3.png","hash":"f4e046c35a648f60fc4de379f7f968ec775fd084","modified":1608197436991},{"_id":"public/algorithms/132-palindrome-partitioning-ii/1.png","hash":"096aa5522771cf3aed36316ca0d12c74c72e134b","modified":1608197436991},{"_id":"public/algorithms/132-palindrome-partitioning-ii/2.png","hash":"422e51f2570ab261c5032d94209adbbd61332015","modified":1608197436991},{"_id":"source/_posts/Python-下的多进程和多线程编程.md","hash":"3512c57b605ca584beb714ba774c7575a6625c2a","modified":1609730880190},{"_id":"public/algorithms/python-xia-de-duo-jin-cheng-he-duo-xian-cheng-bian-cheng/index.html","hash":"aad2af0c0dd6ce9aeb5c3eb5a81a43919531cfd1","modified":1621496966152},{"_id":"source/_drafts/2020-失望与重生.md","hash":"5db353c48c6dee1134a333413fc4505c1fed9702","modified":1609747693230},{"_id":"source/_drafts/.DS_Store","hash":"f885f7f1583a566ec7fd6082465d368806de6966","modified":1621495724038},{"_id":"source/_drafts/高校教师996-关于内卷的反思.md","hash":"1be2218bdfa1655dc0f471b5598caba92d5d7e75","modified":1616410779687},{"_id":"source/_drafts/业余时间的创造力和执行力.md","hash":"cc0fa1391388afaa03e496c6104e5211b25093c3","modified":1620615195929},{"_id":"source/_posts/146-LRU-Cache.md","hash":"3a938e11592d3de0ea23681c7ad9c24ccd56d71b","modified":1611213797970},{"_id":"source/_posts/135-Candy.md","hash":"f811acb7e23ef66471da618209d57c12a86e0e36","modified":1610532233128},{"_id":"source/_posts/139-Word-Break.md","hash":"27667764b5bd85290cb665fb503cf45861b09cc0","modified":1610612969193},{"_id":"source/_posts/147-148-merge-sort.md","hash":"3e47d6a2b33f225caa219e3abce26b67e4db3dd1","modified":1611217685905},{"_id":"source/_posts/权力的意志如何影响我的生活.md","hash":"771db4bbf4bd1dffb0d9303dc0d12e48d8ae701b","modified":1621496893153},{"_id":"source/_posts/关于未来的计划.md","hash":"c31f7492b2aeb105643539767d5677df1f4d6879","modified":1621495924510},{"_id":"source/_posts/双重思想（转载）.md","hash":"ef81f31a80934a63ca7e4098b4c6da249a3b1773","modified":1616123210320},{"_id":"source/_posts/跳出刷题的自我怀疑-转载.md","hash":"ac04416c6d7430e77c3c6f96ed81c5ba3c780867","modified":1615345856692},{"_id":"source/_posts/时断时续的执行力反思.md","hash":"abc2af4d6c78e59e3b5826d4651aa67427444dc3","modified":1617332001397},{"_id":"public/sui-bi/shuang-chong-si-xiang-zhuan-zai/index.html","hash":"282a485b9bec738fe7183aca032c83a9b2a5b6da","modified":1621496966152},{"_id":"public/sui-bi/guan-yu-wei-lai-de-ji-hua/index.html","hash":"fa27048b79226f8f381691c6f5d0a9117ed0d798","modified":1621496966152},{"_id":"public/sui-bi/quan-li-de-yi-zhi-ru-he-ying-xiang-wo-de-sheng-huo/index.html","hash":"478e377d19012a94253114c5e84c8704096546cf","modified":1621496966152},{"_id":"public/algorithms/tiao-chu-shua-ti-de-zi-wo-huai-yi-zhuan-zai/index.html","hash":"b20e8ec99a19377db1dc2964273d6bcec4a529c9","modified":1621496966152},{"_id":"public/sui-bi/shi-duan-shi-xu-de-zhi-xing-li-fan-si/index.html","hash":"b28e3a24dd0f04b72e637f5f9a99fa5b98559ac8","modified":1621496966152},{"_id":"public/algorithms/147-148-merge-sort/index.html","hash":"a431f1c5f22a5cf764c14a95f5ee2d4571021e71","modified":1621496966152},{"_id":"public/uncategorized/146-lru-cache/index.html","hash":"f41c3366eb11fc4ec9c143e7be726f28991643ee","modified":1621496966152},{"_id":"public/algorithms/139-word-break/index.html","hash":"e8135dc3f579b7d0187570e5dcdc00a849183704","modified":1621496966152},{"_id":"public/algorithms/135-candy/index.html","hash":"3d31a9f2f573519433987de6e4c1ffd47b55091e","modified":1621496966152},{"_id":"public/page/2/index.html","hash":"3775d69cda9c402a7d7e3988e8f85c640659d115","modified":1621496966152},{"_id":"public/archives/page/3/index.html","hash":"d398f7e533fde2787eb450a7a485038c8f84c6c6","modified":1621496966152},{"_id":"public/archives/2021/index.html","hash":"1e825c45963d192b8b9a79d0361eaa5b835b1a26","modified":1621496966152},{"_id":"public/archives/2021/01/index.html","hash":"1c9bd05e56e0af7596463363595a8ae7cdaa4283","modified":1621496966152},{"_id":"public/archives/2021/03/index.html","hash":"97e0096b275b2723cdde04f1ab57a4c1656c1a09","modified":1621496966152},{"_id":"public/tags/总结/index.html","hash":"24c86b626b8748f71be00cdd386714ecc06c4858","modified":1621496966152},{"_id":"public/tags/反思/index.html","hash":"e10af5e7b0c4dcb5cd1be7019464a85f0270128d","modified":1621496966152},{"_id":"public/tags/1984/index.html","hash":"d0afe8567e103452db1ad1f29c48928396920593","modified":1621496966152},{"_id":"public/tags/哲学/index.html","hash":"f5c232ab4f4ae84eba0d45896c73766487762f41","modified":1621496966152},{"_id":"public/tags/王小波/index.html","hash":"a7f58e4ca2813fbe76620ae28aff058b74b57d1a","modified":1621496966152}],"Category":[{"name":"test","_id":"ckg7piwa500006a2813an1zrb"},{"name":"随笔","_id":"ckg7qrrd000047j280hei7e4u"},{"name":"运维","_id":"ckh35zvby0001u72804px1qo7"},{"name":"-[Algorithm] -[LeetCode]","_id":"ckhlu9lvi0001w3289jga9zva"},{"name":"-Algorithm -LeetCode","_id":"ckhluawp60000x728ehjadxzq"},{"name":"Algorithm","_id":"ckhlubjrc0000y9282so58qf8"},{"name":"LeetCode","parent":"ckhlubjrc0000y9282so58qf8","_id":"ckhlubjrd0003y928345fh68r"},{"name":"Algorithms","_id":"ckhvqydbp00016z281mnxdhf3"},{"name":"Pytorch","_id":"ckhyotdje0002l928gyqa3gxs"}],"Data":[],"Page":[{"title":"about","date":"2020-10-13T06:35:21.000Z","type":"about","layout":"about","_content":"","source":"about/index.md","raw":"---\ntitle: about\ndate: 2020-10-13 14:35:21\ntype: 'about'\nlayout: 'about'\n---\n","updated":"2020-10-13T06:35:37.056Z","path":"about/index.html","comments":1,"_id":"ckg7lu78t0000bz288lkhb9ig","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"categories","date":"2020-10-13T06:33:52.000Z","type":"categories","layout":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2020-10-13 14:33:52\ntype: 'categories'\nlayout: 'categories'\n---","updated":"2020-10-13T08:12:35.198Z","path":"categories/index.html","_id":"ckg7lu78z0002bz281zevhen5","comments":1,"content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"tags","date":"2018-09-30T10:23:38.000Z","type":"tags","layout":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2018-09-30 18:23:38\ntype: \"tags\"\nlayout: \"tags\"\n---","updated":"2020-10-13T08:13:18.016Z","path":"tags/index.html","_id":"ckg7lu7900003bz282emkhkys","comments":1,"content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"Hello World","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\ntags: test\ncategories: test\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","slug":"hello-world","published":1,"date":"2020-10-13T08:32:14.454Z","updated":"2020-10-13T08:32:14.454Z","_id":"ckg7lu78w0001bz2813ba5wh9","comments":1,"layout":"post","photos":[],"link":"","content":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><pre class=\" language-bash\"><code class=\"language-bash\">$ hexo new <span class=\"token string\">\"My New Post\"</span></code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><pre class=\" language-bash\"><code class=\"language-bash\">$ hexo server</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><pre class=\" language-bash\"><code class=\"language-bash\">$ hexo generate</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><pre class=\" language-bash\"><code class=\"language-bash\">$ hexo deploy</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><pre><code class=\"bash\">$ hexo new &quot;My New Post&quot;</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><pre><code class=\"bash\">$ hexo server</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><pre><code class=\"bash\">$ hexo generate</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><pre><code class=\"bash\">$ hexo deploy</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n"},{"title":"为什么要写博客？","date":"2020-10-13T09:00:43.000Z","summary":"姗姗来迟的博客","toc":false,"mathjax":true,"top":true,"cover":true,"_content":"\n　　我有一个很奇特的习性，就是无论做任何事情，喜欢起个大早，却赶了个晚集；究其原因，想来应该是一直以来困扰我的慵懒毛病，做什么事情都是三分钟热度，热情消退之后，很多设想都被抛诸脑后，结果留下一地鸡毛。曾经几度自我挣扎过，甚至几度接近成功，但是每次达到一定高度之后，很快就如弹簧般反弹到最初的起点。从这个角度分析，我想应该是每次跟自我的斗争时候，都没有树立一个明显的目标，结果在对付完心中的对自己的愧疚和不甘之后，很快的便沉溺于自我满足的情绪中，直到意识到自己是不是该做点什么来拯救一下自己。写博客的设想也是如此，曾经看到过很多博主在序言中论述自己为何写博客，很多文字和想法都对自己很有感触，觉得自己也完全可以学习这种做法，努力提升自己，改变自己。这就有了第一次想写blog的想法，来自于模仿，然而遗憾的是，行动力太差，这个设想也仅仅停留在设想而已；随后无论是写日记，记录此时自己的所思所想，学习笔记，工作笔记，算法刷题的小结，看书感想，电影鉴赏，动漫观后感，旅游记录等，其实都零零碎碎都产生了很多碎片化的记录，只是没有按照合理的结构组织成一份文章而已。真正意识到这种记录的整理是非常非常有必要的是在看自己的相册的时候，恍惚间自己居然已经做过这么多事情，留下了这么多图片记录，自己居然完全不记得了，而且看自己相册的过程是一个跟理想中的美好回忆完全不同的艰难历程，其中充斥着废图，毫不相干的图片，甚至毫无意义，不明所以的照片，严重干扰了自己的回忆活动，尽管iOS很贴心的用算法整理出了一些有主题的照片合辑，但是仍然杯水车薪，难以从根本上解决这个问题。每次想到，曾经经历过如此重要的经历，尽管有文字，图片甚至视频记录下来了，但是却散落在这些毫不相干的地方，自己只是一味的寻求记录，仿佛有着疯狂的收集癖的人一样，疯狂的记录着自己周遭的一切，却完全忘记了要去用心去感受和回忆经历的那些重要时刻。等到自己忘记的差不多的时候，才偶然发现有这么一个记录存在，然而这个时候再回忆的时候，发现自己心里毫无涟漪，似乎在看一个跟自己无关的人的过去那样欣赏着这些破碎的记录，每次想到这里，感觉异常可惜。\n\n<img src=\"Euphonium_Movie_2nd_KV.jpg\" width=\"50%\" height=\"50%\">\n\n　　所以便有了想要通过写blog的方式来记录下自己的经历的想法，遗憾的是，这个虽然不算很复杂的事情，自己也一直没有提上日程，直到近一年来，由于疫情的原因，开始有了足够的时间来思考这个事情，这期间也培养了自己做规划，做总结并不断完善的习惯，这多亏了非常厉害的博主[pluskid/freemind](http://freemind.pluskid.org) 的一篇[关于知识整理，积累与记忆](http://freemind.pluskid.org/misc/knowledge-accumulate/) 的优秀文章，恰逢其时的解决了我关于知识和记忆的一些困扰，所以很顺利的便开始学习和实践起来，收到了很不错的效果，如果不是由于自己间歇性的懒惰，应该还能做的更好。（关于这位厉害的博主，之前也写过不少文章，活成自己希望的样子，每次都能恰到好处的激励自己，以后一定要把这些文章都挪到这里来）\n\n　　在囤积了足够多的文章之后，也到了要准备把这些记录整理输出到页面上的时候了，没想到这个进程被我从7-8月份生生拖到了十月份，才开始着手技术选型和部署相关的事宜，虽然从定制角度，wordpress是最初的首选，但是起数据库的管理方式对于文章为主的组织方式还是觉得有点不习惯，再则很早以前小试过Hexo，上手起来还是很舒适，奇怪的是，熟悉的概念和框架，也还是花了整个下午的时间去测试，磕磕碰碰的，虽然勉强完成了，但是潜意识里对自己的高要求和鄙夷，多少还是让这个成果来的不是那么爽快。我觉得是背负了之前曾经学过这个负担，所以测试途中的任何波折都会成为一个自我否定的理由。之前也讨论过，自己总是潜意识的用否定和打压来给自己前进的动力，但是为什么一直是负反馈的逼迫，而不是正反馈的鼓励，我不得而知，仿佛给压力是前进的唯一动力，而现在我意识到这种方法并不总是有效，反而很影响心情和状态的时候，决心想要改变这个势态。从部署Blog的过程中观察到的这个小心翼翼和不自信的事实，自己这方面还有很大的进步空间。此处就不多展开了，总之，大体的框架都布置好了，只剩下文章了。在选择技术框架的时候，有个回答很好，无论是什么blog框架，坚持写作才是最重要的。提醒了我，不断的优化和改进才是完美进化的捷径，一步到位只是完美主义者的美妙幻想而已。尽管只完成了hexo和主题的最基础的修改，但是起码最重要的文章已经准备就绪，其他的细节以后一步一步的改进和优化。\n\n![](relife-1.png)\n\n　　絮絮叨叨的说了很多，好像都没有触及到自己的blog想要记录的关键，想起不久前看的一部番Relife 中的一个问题，既然认定了青春会结束，恋爱会终结，毕业之后，这一切都烟消云散，为什么此刻要执着于对方呢？这一切的意义是什么？ 我和主人公想的是一样的，就是这些对黄金时代美好事物的追求，会成为生命中最珍贵的回忆，就算最后可能一无所获，但是曾经那份雄心壮志，觉得自己无所不能的勇气，以后和最珍惜的伙伴的度过那些美好岁月，都是未来路上自己所能依仗的一切了。自己早已不复黄金岁月，每每想起，总是心酸无奈，五味杂陈。很多事情都渐渐遗忘，我无法忍受自己面对这种消逝，却选择无动于衷，总是想方设法的要留住。最终，发现还是用文字记录下来，才是对自己最好的一种交代，因为用心在写，用心在看。幸运的是，自己并没有忘记自己的梦想和追求，有很多事很多计划都亟待自己去实现，戒骄戒躁，更要戒怠惰，享受前进途中的种种风景，毕竟人生路上还有很多美好等着自己去探索，怎能因为这些微不足道的原因妨碍自己去追求和享受呢？\n\n![](relife-2.jpg)","source":"_posts/my-first-blog.md","raw":"---\ntitle: 为什么要写博客？\ndate: 2020-10-13 17:00:43\ncategories: 随笔\ntags: 随笔\nsummary: 姗姗来迟的博客\ntoc: false\nmathjax: true\n#password: \n\ntop: true\ncover: true\n\n#img: #feature image\n#coverImg: # cover roll image\n---\n\n　　我有一个很奇特的习性，就是无论做任何事情，喜欢起个大早，却赶了个晚集；究其原因，想来应该是一直以来困扰我的慵懒毛病，做什么事情都是三分钟热度，热情消退之后，很多设想都被抛诸脑后，结果留下一地鸡毛。曾经几度自我挣扎过，甚至几度接近成功，但是每次达到一定高度之后，很快就如弹簧般反弹到最初的起点。从这个角度分析，我想应该是每次跟自我的斗争时候，都没有树立一个明显的目标，结果在对付完心中的对自己的愧疚和不甘之后，很快的便沉溺于自我满足的情绪中，直到意识到自己是不是该做点什么来拯救一下自己。写博客的设想也是如此，曾经看到过很多博主在序言中论述自己为何写博客，很多文字和想法都对自己很有感触，觉得自己也完全可以学习这种做法，努力提升自己，改变自己。这就有了第一次想写blog的想法，来自于模仿，然而遗憾的是，行动力太差，这个设想也仅仅停留在设想而已；随后无论是写日记，记录此时自己的所思所想，学习笔记，工作笔记，算法刷题的小结，看书感想，电影鉴赏，动漫观后感，旅游记录等，其实都零零碎碎都产生了很多碎片化的记录，只是没有按照合理的结构组织成一份文章而已。真正意识到这种记录的整理是非常非常有必要的是在看自己的相册的时候，恍惚间自己居然已经做过这么多事情，留下了这么多图片记录，自己居然完全不记得了，而且看自己相册的过程是一个跟理想中的美好回忆完全不同的艰难历程，其中充斥着废图，毫不相干的图片，甚至毫无意义，不明所以的照片，严重干扰了自己的回忆活动，尽管iOS很贴心的用算法整理出了一些有主题的照片合辑，但是仍然杯水车薪，难以从根本上解决这个问题。每次想到，曾经经历过如此重要的经历，尽管有文字，图片甚至视频记录下来了，但是却散落在这些毫不相干的地方，自己只是一味的寻求记录，仿佛有着疯狂的收集癖的人一样，疯狂的记录着自己周遭的一切，却完全忘记了要去用心去感受和回忆经历的那些重要时刻。等到自己忘记的差不多的时候，才偶然发现有这么一个记录存在，然而这个时候再回忆的时候，发现自己心里毫无涟漪，似乎在看一个跟自己无关的人的过去那样欣赏着这些破碎的记录，每次想到这里，感觉异常可惜。\n\n<img src=\"Euphonium_Movie_2nd_KV.jpg\" width=\"50%\" height=\"50%\">\n\n　　所以便有了想要通过写blog的方式来记录下自己的经历的想法，遗憾的是，这个虽然不算很复杂的事情，自己也一直没有提上日程，直到近一年来，由于疫情的原因，开始有了足够的时间来思考这个事情，这期间也培养了自己做规划，做总结并不断完善的习惯，这多亏了非常厉害的博主[pluskid/freemind](http://freemind.pluskid.org) 的一篇[关于知识整理，积累与记忆](http://freemind.pluskid.org/misc/knowledge-accumulate/) 的优秀文章，恰逢其时的解决了我关于知识和记忆的一些困扰，所以很顺利的便开始学习和实践起来，收到了很不错的效果，如果不是由于自己间歇性的懒惰，应该还能做的更好。（关于这位厉害的博主，之前也写过不少文章，活成自己希望的样子，每次都能恰到好处的激励自己，以后一定要把这些文章都挪到这里来）\n\n　　在囤积了足够多的文章之后，也到了要准备把这些记录整理输出到页面上的时候了，没想到这个进程被我从7-8月份生生拖到了十月份，才开始着手技术选型和部署相关的事宜，虽然从定制角度，wordpress是最初的首选，但是起数据库的管理方式对于文章为主的组织方式还是觉得有点不习惯，再则很早以前小试过Hexo，上手起来还是很舒适，奇怪的是，熟悉的概念和框架，也还是花了整个下午的时间去测试，磕磕碰碰的，虽然勉强完成了，但是潜意识里对自己的高要求和鄙夷，多少还是让这个成果来的不是那么爽快。我觉得是背负了之前曾经学过这个负担，所以测试途中的任何波折都会成为一个自我否定的理由。之前也讨论过，自己总是潜意识的用否定和打压来给自己前进的动力，但是为什么一直是负反馈的逼迫，而不是正反馈的鼓励，我不得而知，仿佛给压力是前进的唯一动力，而现在我意识到这种方法并不总是有效，反而很影响心情和状态的时候，决心想要改变这个势态。从部署Blog的过程中观察到的这个小心翼翼和不自信的事实，自己这方面还有很大的进步空间。此处就不多展开了，总之，大体的框架都布置好了，只剩下文章了。在选择技术框架的时候，有个回答很好，无论是什么blog框架，坚持写作才是最重要的。提醒了我，不断的优化和改进才是完美进化的捷径，一步到位只是完美主义者的美妙幻想而已。尽管只完成了hexo和主题的最基础的修改，但是起码最重要的文章已经准备就绪，其他的细节以后一步一步的改进和优化。\n\n![](relife-1.png)\n\n　　絮絮叨叨的说了很多，好像都没有触及到自己的blog想要记录的关键，想起不久前看的一部番Relife 中的一个问题，既然认定了青春会结束，恋爱会终结，毕业之后，这一切都烟消云散，为什么此刻要执着于对方呢？这一切的意义是什么？ 我和主人公想的是一样的，就是这些对黄金时代美好事物的追求，会成为生命中最珍贵的回忆，就算最后可能一无所获，但是曾经那份雄心壮志，觉得自己无所不能的勇气，以后和最珍惜的伙伴的度过那些美好岁月，都是未来路上自己所能依仗的一切了。自己早已不复黄金岁月，每每想起，总是心酸无奈，五味杂陈。很多事情都渐渐遗忘，我无法忍受自己面对这种消逝，却选择无动于衷，总是想方设法的要留住。最终，发现还是用文字记录下来，才是对自己最好的一种交代，因为用心在写，用心在看。幸运的是，自己并没有忘记自己的梦想和追求，有很多事很多计划都亟待自己去实现，戒骄戒躁，更要戒怠惰，享受前进途中的种种风景，毕竟人生路上还有很多美好等着自己去探索，怎能因为这些微不足道的原因妨碍自己去追求和享受呢？\n\n![](relife-2.jpg)","slug":"my-first-blog","published":1,"updated":"2020-11-25T06:26:55.417Z","_id":"ckg7qjivj00037j28eqt5h8o6","comments":1,"layout":"post","photos":[],"link":"","content":"<p>　　我有一个很奇特的习性，就是无论做任何事情，喜欢起个大早，却赶了个晚集；究其原因，想来应该是一直以来困扰我的慵懒毛病，做什么事情都是三分钟热度，热情消退之后，很多设想都被抛诸脑后，结果留下一地鸡毛。曾经几度自我挣扎过，甚至几度接近成功，但是每次达到一定高度之后，很快就如弹簧般反弹到最初的起点。从这个角度分析，我想应该是每次跟自我的斗争时候，都没有树立一个明显的目标，结果在对付完心中的对自己的愧疚和不甘之后，很快的便沉溺于自我满足的情绪中，直到意识到自己是不是该做点什么来拯救一下自己。写博客的设想也是如此，曾经看到过很多博主在序言中论述自己为何写博客，很多文字和想法都对自己很有感触，觉得自己也完全可以学习这种做法，努力提升自己，改变自己。这就有了第一次想写blog的想法，来自于模仿，然而遗憾的是，行动力太差，这个设想也仅仅停留在设想而已；随后无论是写日记，记录此时自己的所思所想，学习笔记，工作笔记，算法刷题的小结，看书感想，电影鉴赏，动漫观后感，旅游记录等，其实都零零碎碎都产生了很多碎片化的记录，只是没有按照合理的结构组织成一份文章而已。真正意识到这种记录的整理是非常非常有必要的是在看自己的相册的时候，恍惚间自己居然已经做过这么多事情，留下了这么多图片记录，自己居然完全不记得了，而且看自己相册的过程是一个跟理想中的美好回忆完全不同的艰难历程，其中充斥着废图，毫不相干的图片，甚至毫无意义，不明所以的照片，严重干扰了自己的回忆活动，尽管iOS很贴心的用算法整理出了一些有主题的照片合辑，但是仍然杯水车薪，难以从根本上解决这个问题。每次想到，曾经经历过如此重要的经历，尽管有文字，图片甚至视频记录下来了，但是却散落在这些毫不相干的地方，自己只是一味的寻求记录，仿佛有着疯狂的收集癖的人一样，疯狂的记录着自己周遭的一切，却完全忘记了要去用心去感受和回忆经历的那些重要时刻。等到自己忘记的差不多的时候，才偶然发现有这么一个记录存在，然而这个时候再回忆的时候，发现自己心里毫无涟漪，似乎在看一个跟自己无关的人的过去那样欣赏着这些破碎的记录，每次想到这里，感觉异常可惜。</p>\n<img src=\"Euphonium_Movie_2nd_KV.jpg\" width=\"50%\" height=\"50%\">\n\n<p>　　所以便有了想要通过写blog的方式来记录下自己的经历的想法，遗憾的是，这个虽然不算很复杂的事情，自己也一直没有提上日程，直到近一年来，由于疫情的原因，开始有了足够的时间来思考这个事情，这期间也培养了自己做规划，做总结并不断完善的习惯，这多亏了非常厉害的博主<a href=\"http://freemind.pluskid.org/\">pluskid/freemind</a> 的一篇<a href=\"http://freemind.pluskid.org/misc/knowledge-accumulate/\">关于知识整理，积累与记忆</a> 的优秀文章，恰逢其时的解决了我关于知识和记忆的一些困扰，所以很顺利的便开始学习和实践起来，收到了很不错的效果，如果不是由于自己间歇性的懒惰，应该还能做的更好。（关于这位厉害的博主，之前也写过不少文章，活成自己希望的样子，每次都能恰到好处的激励自己，以后一定要把这些文章都挪到这里来）</p>\n<p>　　在囤积了足够多的文章之后，也到了要准备把这些记录整理输出到页面上的时候了，没想到这个进程被我从7-8月份生生拖到了十月份，才开始着手技术选型和部署相关的事宜，虽然从定制角度，wordpress是最初的首选，但是起数据库的管理方式对于文章为主的组织方式还是觉得有点不习惯，再则很早以前小试过Hexo，上手起来还是很舒适，奇怪的是，熟悉的概念和框架，也还是花了整个下午的时间去测试，磕磕碰碰的，虽然勉强完成了，但是潜意识里对自己的高要求和鄙夷，多少还是让这个成果来的不是那么爽快。我觉得是背负了之前曾经学过这个负担，所以测试途中的任何波折都会成为一个自我否定的理由。之前也讨论过，自己总是潜意识的用否定和打压来给自己前进的动力，但是为什么一直是负反馈的逼迫，而不是正反馈的鼓励，我不得而知，仿佛给压力是前进的唯一动力，而现在我意识到这种方法并不总是有效，反而很影响心情和状态的时候，决心想要改变这个势态。从部署Blog的过程中观察到的这个小心翼翼和不自信的事实，自己这方面还有很大的进步空间。此处就不多展开了，总之，大体的框架都布置好了，只剩下文章了。在选择技术框架的时候，有个回答很好，无论是什么blog框架，坚持写作才是最重要的。提醒了我，不断的优化和改进才是完美进化的捷径，一步到位只是完美主义者的美妙幻想而已。尽管只完成了hexo和主题的最基础的修改，但是起码最重要的文章已经准备就绪，其他的细节以后一步一步的改进和优化。</p>\n<p><img src=\"relife-1.png\"></p>\n<p>　　絮絮叨叨的说了很多，好像都没有触及到自己的blog想要记录的关键，想起不久前看的一部番Relife 中的一个问题，既然认定了青春会结束，恋爱会终结，毕业之后，这一切都烟消云散，为什么此刻要执着于对方呢？这一切的意义是什么？ 我和主人公想的是一样的，就是这些对黄金时代美好事物的追求，会成为生命中最珍贵的回忆，就算最后可能一无所获，但是曾经那份雄心壮志，觉得自己无所不能的勇气，以后和最珍惜的伙伴的度过那些美好岁月，都是未来路上自己所能依仗的一切了。自己早已不复黄金岁月，每每想起，总是心酸无奈，五味杂陈。很多事情都渐渐遗忘，我无法忍受自己面对这种消逝，却选择无动于衷，总是想方设法的要留住。最终，发现还是用文字记录下来，才是对自己最好的一种交代，因为用心在写，用心在看。幸运的是，自己并没有忘记自己的梦想和追求，有很多事很多计划都亟待自己去实现，戒骄戒躁，更要戒怠惰，享受前进途中的种种风景，毕竟人生路上还有很多美好等着自己去探索，怎能因为这些微不足道的原因妨碍自己去追求和享受呢？</p>\n<p><img src=\"relife-2.jpg\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p>　　我有一个很奇特的习性，就是无论做任何事情，喜欢起个大早，却赶了个晚集；究其原因，想来应该是一直以来困扰我的慵懒毛病，做什么事情都是三分钟热度，热情消退之后，很多设想都被抛诸脑后，结果留下一地鸡毛。曾经几度自我挣扎过，甚至几度接近成功，但是每次达到一定高度之后，很快就如弹簧般反弹到最初的起点。从这个角度分析，我想应该是每次跟自我的斗争时候，都没有树立一个明显的目标，结果在对付完心中的对自己的愧疚和不甘之后，很快的便沉溺于自我满足的情绪中，直到意识到自己是不是该做点什么来拯救一下自己。写博客的设想也是如此，曾经看到过很多博主在序言中论述自己为何写博客，很多文字和想法都对自己很有感触，觉得自己也完全可以学习这种做法，努力提升自己，改变自己。这就有了第一次想写blog的想法，来自于模仿，然而遗憾的是，行动力太差，这个设想也仅仅停留在设想而已；随后无论是写日记，记录此时自己的所思所想，学习笔记，工作笔记，算法刷题的小结，看书感想，电影鉴赏，动漫观后感，旅游记录等，其实都零零碎碎都产生了很多碎片化的记录，只是没有按照合理的结构组织成一份文章而已。真正意识到这种记录的整理是非常非常有必要的是在看自己的相册的时候，恍惚间自己居然已经做过这么多事情，留下了这么多图片记录，自己居然完全不记得了，而且看自己相册的过程是一个跟理想中的美好回忆完全不同的艰难历程，其中充斥着废图，毫不相干的图片，甚至毫无意义，不明所以的照片，严重干扰了自己的回忆活动，尽管iOS很贴心的用算法整理出了一些有主题的照片合辑，但是仍然杯水车薪，难以从根本上解决这个问题。每次想到，曾经经历过如此重要的经历，尽管有文字，图片甚至视频记录下来了，但是却散落在这些毫不相干的地方，自己只是一味的寻求记录，仿佛有着疯狂的收集癖的人一样，疯狂的记录着自己周遭的一切，却完全忘记了要去用心去感受和回忆经历的那些重要时刻。等到自己忘记的差不多的时候，才偶然发现有这么一个记录存在，然而这个时候再回忆的时候，发现自己心里毫无涟漪，似乎在看一个跟自己无关的人的过去那样欣赏着这些破碎的记录，每次想到这里，感觉异常可惜。</p>\n<img src=\"Euphonium_Movie_2nd_KV.jpg\" width=\"50%\" height=\"50%\">\n\n<p>　　所以便有了想要通过写blog的方式来记录下自己的经历的想法，遗憾的是，这个虽然不算很复杂的事情，自己也一直没有提上日程，直到近一年来，由于疫情的原因，开始有了足够的时间来思考这个事情，这期间也培养了自己做规划，做总结并不断完善的习惯，这多亏了非常厉害的博主<a href=\"http://freemind.pluskid.org/\">pluskid/freemind</a> 的一篇<a href=\"http://freemind.pluskid.org/misc/knowledge-accumulate/\">关于知识整理，积累与记忆</a> 的优秀文章，恰逢其时的解决了我关于知识和记忆的一些困扰，所以很顺利的便开始学习和实践起来，收到了很不错的效果，如果不是由于自己间歇性的懒惰，应该还能做的更好。（关于这位厉害的博主，之前也写过不少文章，活成自己希望的样子，每次都能恰到好处的激励自己，以后一定要把这些文章都挪到这里来）</p>\n<p>　　在囤积了足够多的文章之后，也到了要准备把这些记录整理输出到页面上的时候了，没想到这个进程被我从7-8月份生生拖到了十月份，才开始着手技术选型和部署相关的事宜，虽然从定制角度，wordpress是最初的首选，但是起数据库的管理方式对于文章为主的组织方式还是觉得有点不习惯，再则很早以前小试过Hexo，上手起来还是很舒适，奇怪的是，熟悉的概念和框架，也还是花了整个下午的时间去测试，磕磕碰碰的，虽然勉强完成了，但是潜意识里对自己的高要求和鄙夷，多少还是让这个成果来的不是那么爽快。我觉得是背负了之前曾经学过这个负担，所以测试途中的任何波折都会成为一个自我否定的理由。之前也讨论过，自己总是潜意识的用否定和打压来给自己前进的动力，但是为什么一直是负反馈的逼迫，而不是正反馈的鼓励，我不得而知，仿佛给压力是前进的唯一动力，而现在我意识到这种方法并不总是有效，反而很影响心情和状态的时候，决心想要改变这个势态。从部署Blog的过程中观察到的这个小心翼翼和不自信的事实，自己这方面还有很大的进步空间。此处就不多展开了，总之，大体的框架都布置好了，只剩下文章了。在选择技术框架的时候，有个回答很好，无论是什么blog框架，坚持写作才是最重要的。提醒了我，不断的优化和改进才是完美进化的捷径，一步到位只是完美主义者的美妙幻想而已。尽管只完成了hexo和主题的最基础的修改，但是起码最重要的文章已经准备就绪，其他的细节以后一步一步的改进和优化。</p>\n<p><img src=\"relife-1.png\"></p>\n<p>　　絮絮叨叨的说了很多，好像都没有触及到自己的blog想要记录的关键，想起不久前看的一部番Relife 中的一个问题，既然认定了青春会结束，恋爱会终结，毕业之后，这一切都烟消云散，为什么此刻要执着于对方呢？这一切的意义是什么？ 我和主人公想的是一样的，就是这些对黄金时代美好事物的追求，会成为生命中最珍贵的回忆，就算最后可能一无所获，但是曾经那份雄心壮志，觉得自己无所不能的勇气，以后和最珍惜的伙伴的度过那些美好岁月，都是未来路上自己所能依仗的一切了。自己早已不复黄金岁月，每每想起，总是心酸无奈，五味杂陈。很多事情都渐渐遗忘，我无法忍受自己面对这种消逝，却选择无动于衷，总是想方设法的要留住。最终，发现还是用文字记录下来，才是对自己最好的一种交代，因为用心在写，用心在看。幸运的是，自己并没有忘记自己的梦想和追求，有很多事很多计划都亟待自己去实现，戒骄戒躁，更要戒怠惰，享受前进途中的种种风景，毕竟人生路上还有很多美好等着自己去探索，怎能因为这些微不足道的原因妨碍自己去追求和享受呢？</p>\n<p><img src=\"relife-2.jpg\"></p>\n"},{"title":"改变与懒惰：主观能动性讨论","toc":true,"mathjax":true,"top":true,"cover":true,"date":"2020-10-27T06:52:29.000Z","_content":"\n　　前段时间，大概是在暑假期间吧，似乎因为看了很多动漫，仍然沉迷在学生时代的火热的夏天气息中。是呀！美好的夏天，如梦似幻。然而最难过的是，尽管现在的我无论想要如何去追寻这种夏日的味道，哪怕只是夏天的尾气而已，我却仍然无比清晰的认识到，自己的暑假早已经一去不复返了。这让自己陷入到一个两难的境地，一面渴望夏日的美好，一面却又警醒自己去接受事实，既无法肆意去做自我，也没办法彻底的放弃，去追寻所谓的成熟，就特别的拧巴。至于为什么总是要自己接受自己不断长大的事实，我也不甚明了，仿佛跟自己说的多了，就真的成长了一样。可能唯一让自己神伤的就只是不断变老的事实吧，或许告诉自己去年的自己跟现在的自己其实没什么太多变化和进步，那么多少有点对不起自己不断老去的时间，所以滋生出这样心理暗示，说是聊以慰己也好，说是对自己的失望继而产生的愧疚也好，总之，都是一份对自己自甘堕落，原地踏步的一种惩罚吧！所以，带着这份不甘，不明就里的开始了这份夏日的无聊幻想。\n\n![2020-07-31](IMG_9373.jpeg)\n\n　　清楚记得是在7月31日，下班从公司回家， 开车走了一条平时从来没有走过的路，天色湛蓝，久违的看到了雾状的云絮，随着落日余晖，呈现出一幅不断变幻的黄昏美景，在这个陌生的却干净的路上走着，有一种异域他乡的漂泊感油然而生。不得不感叹，人有时候就是这么的任性，假如面对突然变化的外部环境，心情不免充满了惊慌和不安；但是一旦在舒适区呆的足够久，却又开始渴望起陌生的神秘起来。所以，在那一刻，我有点爱丽丝梦游仙境的不真实感，才从二次元的幻想出来，又来到这里，心中突然诞生起一股强烈的想要去亲眼见证 命运石之门，青春猪头少年，灌篮高手，你的名字 里面二次元名场景的三次元场景的冲动。这其中当然有自己近段时间一直懒于出游，疫情原因，甚至直接放弃远行的原因，已经快有一年没有认真旅行了，去年这个时候，还沉浸在五月天鸟巢演唱会的期待中。没想到一年后，都快要忘记旅行是什么感觉了，心中只有漠然和麻木。我熟悉这种感觉，当我十年前无法面对自己的时候，我就是这样一步一步麻痹自我，无视自己的感觉，任由时间和偶然的未来来试图冲淡这份难过，虽然几次试图反抗，但是最后还是来到了自己预想中的，失去了爱和共情能力的终点。等到意识到，这样堕落下去，并不能真正解决这份难过的时候，又过去了好久好久。想要改变的时候，心中的那份激情早已经烟消云散了，当我想要去喜欢的时候，内心却做到真正意义上的毫无波澜，直到再次遇见的二次元，这种状况才好起来。所以此时，我又重新遭遇了这份想要改变，却无法调动自己的内心的尴尬，似乎在这十年间，我无意识间培养出一股冷眼旁观的特质，无论是跟自己是否相关的人和事，习惯性的把自己置身事外，试图冷静分析利弊得失，却完全丧失了**意识主观能动性**，这股自己窃以为最核心和宝贵的动力，我明白，我所尊崇的理性，作为绝对意义上的中立，统治了自己的是非观，但是一旦要去自己去亲自上阵的时候，光有理性，总是感觉差点什么东西，实践起来也并不是那么得心应手，甚至还有点举步维艰。总之，无论做什么，我缺乏一种，“当我在做这件事，我在做什么的” 的自觉性。记得不久前，自己还有强烈的这种动力想要实现梦想，却在错失良机之后，陷入长久的沉寂。当工作从长远来看不受自己控制的变得愈加明朗起来的时候，自己陷入了对未来的强烈不安之中。造成如今的局面，都是有点自己自暴自弃的成分在里面，屏蔽自己感情，活在当下，试图忘掉自己的梦想，成为了如今自己生活的主题。自我屏蔽，前面已经聊过，被证实只是一种自我麻醉而已，跟逃避没什么两样，对于解决问题，没什么卵用；活在当下，虽然并没有什么不好，但是作为浑浑噩噩度日的借口，时间久了，自己也有点不好意思起来；试图忘记梦想，真的是逃避之集大成者。兜兜转转，最后还是来到了需要认真面对自己的这个路口，时间只不过让自己忘却了些许忧伤，积攒了一点可怜的勇气而已。如果自己没有根本性的转变和思想觉悟，我并不认为自己能有所突破。\n\n　　所以，回到，发现问题，认识问题，解决问题 的模式 ！\n\n发现问题：\t\n\n​\t***生活缺乏 目标，激情和动力！***\n\n认识问题：\n\n　　按照 身边人面对这个问题的惯常思路，没有动力，给压力就好了。我并不否认这种方法的有效性，但是我一方面不认可这种方法，另一方面我本身也不是很习惯这种被迫做事的急促感，我认为扼杀了创意和生活的乐趣。曾经我也讨论过这个问题，得出的结论是，我并不适合这种压迫产生的动力；但另一方面，自己却由于过分懒散，反而落入了完全忘记自己的目标是什么的另一个极端。这样的后果就是，摸鱼到一定程度，自己甚至都开始厌恶这样无所事事的日子，想要找点事情来分散下多余的脑力，这个时候甚至反而觉得有点儿事做还挺有意思的。很遗憾的是，目前自己正处于这种状态，由于工作内容对自己来说比较容易，自己也没有适时的给自己足够的挑战，10月份以来，自己一直处于这种慵懒的状态，所以整个月的都处于基本停止的状态。仔细想来，应该是上次的面试结束之后，由于非技术原因没拿到offer，尽管情绪上来说没什么影响，但是准备算法OJ的积极性有点下降，努力想要认真刷算法题的这个弦突然就松下来了，这才导致后续很多事情都开始丧失积极性直到彻底懒得去做了。从这个角度来说，自己在战略上，还是需要足够的压力来推进自己去实践，否则很容易落入偷懒的陷阱之中。譬如，每天的总结，每周的反思，都在那之后渐渐都没有了；细想之下，我都是用，不做也没什么，晚点做也一样这种轻描淡写的借口一带而过。想来，自己也足够鸡贼，在缺乏意识驱动的事情上，是没有感情的理性机器，在这种需要理性监督的日常惯例上，却贪图一时的便利，自欺欺人起来。说到这里，猛然发现自己有一个很坑爹的问题就是，每当某些事做的很好的时候，心里洋溢着开心和满足，发自内心的对自己说，原来认真一把，也很有意思很好玩的，信誓旦旦的对自己说，记住这种感觉，明天也要这样加油努力。 转头就把这些警醒忘得一干二净，反而在满足感的驱使下开始完全放松的日子。从根源上来讲，自己这个问题应该是，目标太小，易于满足；然而另一方面，为了从长远角度保证计划的实施同时保持生活的乐趣，避免陷入某天计划没完成，直接导致后续计划的崩盘，本来就没有给每天安排很多任务，所以这样就陷入了一种矛盾的状态。 面对这个事情，我想起来之前 看到的**延迟满足**这个词，当时看的就觉得虽然意思明白，但是实践起来是什么样子，没有什么概念。现在或许能有点明白，应该是，满足感留在完成那一天，但是再接再厉的警醒，还是需要时刻提醒自己，在整个规划中，自己走到哪一步了，有了今天 的成果，明天该怎么更进一步，这种不断进步的渴求，才是敦促自己前进的最原始的动力。\n\n解决问题：\n\n　　用认真的态度过好每一天，而不是得过且过的自我敷衍。这其实是自己一直没太关注的，总是试图用全局的计划和美好的理想来敦促自己，发现仍然过不好每一天的生活。渴望进步的决心，是绝对不会变的，但是认真的态度，特别是每天都坚持保持这种精神，实际是不容易办到的，而且偶尔的放松，实际上对自己有很大影响，因为自己特别容易自暴自弃，接受那些不受控制或者并非属于自身原因的变化，享受当下认真带来的满足和乐趣，放眼明日需要改变和努力的方向，过好自己的每一天，我想也不会有那种缺乏目标，激情和动力的乏味感觉了吧！\n\n　　说到这里，这篇关于懒惰和改变的讨论应该已经得到答案了。但是彼时对夏日的怀恋和异域的沉迷并没有得到解决，似乎一场旅行是非常有必要的，虽然真的很想去日本旅游，遗憾的是年末疫情复发可能对出国旅行产生很大的影响，不过究竟是什么程度的影响，自己也并没有做过多考察，剩下几个月的时间，自己还是应该鼓起干劲，多做一些信息上的搜集和准备，努力规划一场美好的动漫朝圣之旅。\n\n![](2020-07-31.jpg)","source":"_posts/改变与懒惰.md","raw":"---\ntitle: 改变与懒惰：主观能动性讨论\ntoc: true\nmathjax: true\ntop: true\ncover: true\ndate: 2020-10-27 14:52:29\ncategories: 随笔\ntags: 随笔\n---\n\n　　前段时间，大概是在暑假期间吧，似乎因为看了很多动漫，仍然沉迷在学生时代的火热的夏天气息中。是呀！美好的夏天，如梦似幻。然而最难过的是，尽管现在的我无论想要如何去追寻这种夏日的味道，哪怕只是夏天的尾气而已，我却仍然无比清晰的认识到，自己的暑假早已经一去不复返了。这让自己陷入到一个两难的境地，一面渴望夏日的美好，一面却又警醒自己去接受事实，既无法肆意去做自我，也没办法彻底的放弃，去追寻所谓的成熟，就特别的拧巴。至于为什么总是要自己接受自己不断长大的事实，我也不甚明了，仿佛跟自己说的多了，就真的成长了一样。可能唯一让自己神伤的就只是不断变老的事实吧，或许告诉自己去年的自己跟现在的自己其实没什么太多变化和进步，那么多少有点对不起自己不断老去的时间，所以滋生出这样心理暗示，说是聊以慰己也好，说是对自己的失望继而产生的愧疚也好，总之，都是一份对自己自甘堕落，原地踏步的一种惩罚吧！所以，带着这份不甘，不明就里的开始了这份夏日的无聊幻想。\n\n![2020-07-31](IMG_9373.jpeg)\n\n　　清楚记得是在7月31日，下班从公司回家， 开车走了一条平时从来没有走过的路，天色湛蓝，久违的看到了雾状的云絮，随着落日余晖，呈现出一幅不断变幻的黄昏美景，在这个陌生的却干净的路上走着，有一种异域他乡的漂泊感油然而生。不得不感叹，人有时候就是这么的任性，假如面对突然变化的外部环境，心情不免充满了惊慌和不安；但是一旦在舒适区呆的足够久，却又开始渴望起陌生的神秘起来。所以，在那一刻，我有点爱丽丝梦游仙境的不真实感，才从二次元的幻想出来，又来到这里，心中突然诞生起一股强烈的想要去亲眼见证 命运石之门，青春猪头少年，灌篮高手，你的名字 里面二次元名场景的三次元场景的冲动。这其中当然有自己近段时间一直懒于出游，疫情原因，甚至直接放弃远行的原因，已经快有一年没有认真旅行了，去年这个时候，还沉浸在五月天鸟巢演唱会的期待中。没想到一年后，都快要忘记旅行是什么感觉了，心中只有漠然和麻木。我熟悉这种感觉，当我十年前无法面对自己的时候，我就是这样一步一步麻痹自我，无视自己的感觉，任由时间和偶然的未来来试图冲淡这份难过，虽然几次试图反抗，但是最后还是来到了自己预想中的，失去了爱和共情能力的终点。等到意识到，这样堕落下去，并不能真正解决这份难过的时候，又过去了好久好久。想要改变的时候，心中的那份激情早已经烟消云散了，当我想要去喜欢的时候，内心却做到真正意义上的毫无波澜，直到再次遇见的二次元，这种状况才好起来。所以此时，我又重新遭遇了这份想要改变，却无法调动自己的内心的尴尬，似乎在这十年间，我无意识间培养出一股冷眼旁观的特质，无论是跟自己是否相关的人和事，习惯性的把自己置身事外，试图冷静分析利弊得失，却完全丧失了**意识主观能动性**，这股自己窃以为最核心和宝贵的动力，我明白，我所尊崇的理性，作为绝对意义上的中立，统治了自己的是非观，但是一旦要去自己去亲自上阵的时候，光有理性，总是感觉差点什么东西，实践起来也并不是那么得心应手，甚至还有点举步维艰。总之，无论做什么，我缺乏一种，“当我在做这件事，我在做什么的” 的自觉性。记得不久前，自己还有强烈的这种动力想要实现梦想，却在错失良机之后，陷入长久的沉寂。当工作从长远来看不受自己控制的变得愈加明朗起来的时候，自己陷入了对未来的强烈不安之中。造成如今的局面，都是有点自己自暴自弃的成分在里面，屏蔽自己感情，活在当下，试图忘掉自己的梦想，成为了如今自己生活的主题。自我屏蔽，前面已经聊过，被证实只是一种自我麻醉而已，跟逃避没什么两样，对于解决问题，没什么卵用；活在当下，虽然并没有什么不好，但是作为浑浑噩噩度日的借口，时间久了，自己也有点不好意思起来；试图忘记梦想，真的是逃避之集大成者。兜兜转转，最后还是来到了需要认真面对自己的这个路口，时间只不过让自己忘却了些许忧伤，积攒了一点可怜的勇气而已。如果自己没有根本性的转变和思想觉悟，我并不认为自己能有所突破。\n\n　　所以，回到，发现问题，认识问题，解决问题 的模式 ！\n\n发现问题：\t\n\n​\t***生活缺乏 目标，激情和动力！***\n\n认识问题：\n\n　　按照 身边人面对这个问题的惯常思路，没有动力，给压力就好了。我并不否认这种方法的有效性，但是我一方面不认可这种方法，另一方面我本身也不是很习惯这种被迫做事的急促感，我认为扼杀了创意和生活的乐趣。曾经我也讨论过这个问题，得出的结论是，我并不适合这种压迫产生的动力；但另一方面，自己却由于过分懒散，反而落入了完全忘记自己的目标是什么的另一个极端。这样的后果就是，摸鱼到一定程度，自己甚至都开始厌恶这样无所事事的日子，想要找点事情来分散下多余的脑力，这个时候甚至反而觉得有点儿事做还挺有意思的。很遗憾的是，目前自己正处于这种状态，由于工作内容对自己来说比较容易，自己也没有适时的给自己足够的挑战，10月份以来，自己一直处于这种慵懒的状态，所以整个月的都处于基本停止的状态。仔细想来，应该是上次的面试结束之后，由于非技术原因没拿到offer，尽管情绪上来说没什么影响，但是准备算法OJ的积极性有点下降，努力想要认真刷算法题的这个弦突然就松下来了，这才导致后续很多事情都开始丧失积极性直到彻底懒得去做了。从这个角度来说，自己在战略上，还是需要足够的压力来推进自己去实践，否则很容易落入偷懒的陷阱之中。譬如，每天的总结，每周的反思，都在那之后渐渐都没有了；细想之下，我都是用，不做也没什么，晚点做也一样这种轻描淡写的借口一带而过。想来，自己也足够鸡贼，在缺乏意识驱动的事情上，是没有感情的理性机器，在这种需要理性监督的日常惯例上，却贪图一时的便利，自欺欺人起来。说到这里，猛然发现自己有一个很坑爹的问题就是，每当某些事做的很好的时候，心里洋溢着开心和满足，发自内心的对自己说，原来认真一把，也很有意思很好玩的，信誓旦旦的对自己说，记住这种感觉，明天也要这样加油努力。 转头就把这些警醒忘得一干二净，反而在满足感的驱使下开始完全放松的日子。从根源上来讲，自己这个问题应该是，目标太小，易于满足；然而另一方面，为了从长远角度保证计划的实施同时保持生活的乐趣，避免陷入某天计划没完成，直接导致后续计划的崩盘，本来就没有给每天安排很多任务，所以这样就陷入了一种矛盾的状态。 面对这个事情，我想起来之前 看到的**延迟满足**这个词，当时看的就觉得虽然意思明白，但是实践起来是什么样子，没有什么概念。现在或许能有点明白，应该是，满足感留在完成那一天，但是再接再厉的警醒，还是需要时刻提醒自己，在整个规划中，自己走到哪一步了，有了今天 的成果，明天该怎么更进一步，这种不断进步的渴求，才是敦促自己前进的最原始的动力。\n\n解决问题：\n\n　　用认真的态度过好每一天，而不是得过且过的自我敷衍。这其实是自己一直没太关注的，总是试图用全局的计划和美好的理想来敦促自己，发现仍然过不好每一天的生活。渴望进步的决心，是绝对不会变的，但是认真的态度，特别是每天都坚持保持这种精神，实际是不容易办到的，而且偶尔的放松，实际上对自己有很大影响，因为自己特别容易自暴自弃，接受那些不受控制或者并非属于自身原因的变化，享受当下认真带来的满足和乐趣，放眼明日需要改变和努力的方向，过好自己的每一天，我想也不会有那种缺乏目标，激情和动力的乏味感觉了吧！\n\n　　说到这里，这篇关于懒惰和改变的讨论应该已经得到答案了。但是彼时对夏日的怀恋和异域的沉迷并没有得到解决，似乎一场旅行是非常有必要的，虽然真的很想去日本旅游，遗憾的是年末疫情复发可能对出国旅行产生很大的影响，不过究竟是什么程度的影响，自己也并没有做过多考察，剩下几个月的时间，自己还是应该鼓起干劲，多做一些信息上的搜集和准备，努力规划一场美好的动漫朝圣之旅。\n\n![](2020-07-31.jpg)","slug":"改变与懒惰","published":1,"updated":"2020-10-28T09:57:17.910Z","_id":"ckgt6w3by0000wx280fap2hbl","comments":1,"layout":"post","photos":[],"link":"","content":"<p>　　前段时间，大概是在暑假期间吧，似乎因为看了很多动漫，仍然沉迷在学生时代的火热的夏天气息中。是呀！美好的夏天，如梦似幻。然而最难过的是，尽管现在的我无论想要如何去追寻这种夏日的味道，哪怕只是夏天的尾气而已，我却仍然无比清晰的认识到，自己的暑假早已经一去不复返了。这让自己陷入到一个两难的境地，一面渴望夏日的美好，一面却又警醒自己去接受事实，既无法肆意去做自我，也没办法彻底的放弃，去追寻所谓的成熟，就特别的拧巴。至于为什么总是要自己接受自己不断长大的事实，我也不甚明了，仿佛跟自己说的多了，就真的成长了一样。可能唯一让自己神伤的就只是不断变老的事实吧，或许告诉自己去年的自己跟现在的自己其实没什么太多变化和进步，那么多少有点对不起自己不断老去的时间，所以滋生出这样心理暗示，说是聊以慰己也好，说是对自己的失望继而产生的愧疚也好，总之，都是一份对自己自甘堕落，原地踏步的一种惩罚吧！所以，带着这份不甘，不明就里的开始了这份夏日的无聊幻想。</p>\n<p><img src=\"IMG_9373.jpeg\" alt=\"2020-07-31\"></p>\n<p>　　清楚记得是在7月31日，下班从公司回家， 开车走了一条平时从来没有走过的路，天色湛蓝，久违的看到了雾状的云絮，随着落日余晖，呈现出一幅不断变幻的黄昏美景，在这个陌生的却干净的路上走着，有一种异域他乡的漂泊感油然而生。不得不感叹，人有时候就是这么的任性，假如面对突然变化的外部环境，心情不免充满了惊慌和不安；但是一旦在舒适区呆的足够久，却又开始渴望起陌生的神秘起来。所以，在那一刻，我有点爱丽丝梦游仙境的不真实感，才从二次元的幻想出来，又来到这里，心中突然诞生起一股强烈的想要去亲眼见证 命运石之门，青春猪头少年，灌篮高手，你的名字 里面二次元名场景的三次元场景的冲动。这其中当然有自己近段时间一直懒于出游，疫情原因，甚至直接放弃远行的原因，已经快有一年没有认真旅行了，去年这个时候，还沉浸在五月天鸟巢演唱会的期待中。没想到一年后，都快要忘记旅行是什么感觉了，心中只有漠然和麻木。我熟悉这种感觉，当我十年前无法面对自己的时候，我就是这样一步一步麻痹自我，无视自己的感觉，任由时间和偶然的未来来试图冲淡这份难过，虽然几次试图反抗，但是最后还是来到了自己预想中的，失去了爱和共情能力的终点。等到意识到，这样堕落下去，并不能真正解决这份难过的时候，又过去了好久好久。想要改变的时候，心中的那份激情早已经烟消云散了，当我想要去喜欢的时候，内心却做到真正意义上的毫无波澜，直到再次遇见的二次元，这种状况才好起来。所以此时，我又重新遭遇了这份想要改变，却无法调动自己的内心的尴尬，似乎在这十年间，我无意识间培养出一股冷眼旁观的特质，无论是跟自己是否相关的人和事，习惯性的把自己置身事外，试图冷静分析利弊得失，却完全丧失了<strong>意识主观能动性</strong>，这股自己窃以为最核心和宝贵的动力，我明白，我所尊崇的理性，作为绝对意义上的中立，统治了自己的是非观，但是一旦要去自己去亲自上阵的时候，光有理性，总是感觉差点什么东西，实践起来也并不是那么得心应手，甚至还有点举步维艰。总之，无论做什么，我缺乏一种，“当我在做这件事，我在做什么的” 的自觉性。记得不久前，自己还有强烈的这种动力想要实现梦想，却在错失良机之后，陷入长久的沉寂。当工作从长远来看不受自己控制的变得愈加明朗起来的时候，自己陷入了对未来的强烈不安之中。造成如今的局面，都是有点自己自暴自弃的成分在里面，屏蔽自己感情，活在当下，试图忘掉自己的梦想，成为了如今自己生活的主题。自我屏蔽，前面已经聊过，被证实只是一种自我麻醉而已，跟逃避没什么两样，对于解决问题，没什么卵用；活在当下，虽然并没有什么不好，但是作为浑浑噩噩度日的借口，时间久了，自己也有点不好意思起来；试图忘记梦想，真的是逃避之集大成者。兜兜转转，最后还是来到了需要认真面对自己的这个路口，时间只不过让自己忘却了些许忧伤，积攒了一点可怜的勇气而已。如果自己没有根本性的转变和思想觉悟，我并不认为自己能有所突破。</p>\n<p>　　所以，回到，发现问题，认识问题，解决问题 的模式 ！</p>\n<p>发现问题：    </p>\n<p>​    <strong><em>生活缺乏 目标，激情和动力！</em></strong></p>\n<p>认识问题：</p>\n<p>　　按照 身边人面对这个问题的惯常思路，没有动力，给压力就好了。我并不否认这种方法的有效性，但是我一方面不认可这种方法，另一方面我本身也不是很习惯这种被迫做事的急促感，我认为扼杀了创意和生活的乐趣。曾经我也讨论过这个问题，得出的结论是，我并不适合这种压迫产生的动力；但另一方面，自己却由于过分懒散，反而落入了完全忘记自己的目标是什么的另一个极端。这样的后果就是，摸鱼到一定程度，自己甚至都开始厌恶这样无所事事的日子，想要找点事情来分散下多余的脑力，这个时候甚至反而觉得有点儿事做还挺有意思的。很遗憾的是，目前自己正处于这种状态，由于工作内容对自己来说比较容易，自己也没有适时的给自己足够的挑战，10月份以来，自己一直处于这种慵懒的状态，所以整个月的都处于基本停止的状态。仔细想来，应该是上次的面试结束之后，由于非技术原因没拿到offer，尽管情绪上来说没什么影响，但是准备算法OJ的积极性有点下降，努力想要认真刷算法题的这个弦突然就松下来了，这才导致后续很多事情都开始丧失积极性直到彻底懒得去做了。从这个角度来说，自己在战略上，还是需要足够的压力来推进自己去实践，否则很容易落入偷懒的陷阱之中。譬如，每天的总结，每周的反思，都在那之后渐渐都没有了；细想之下，我都是用，不做也没什么，晚点做也一样这种轻描淡写的借口一带而过。想来，自己也足够鸡贼，在缺乏意识驱动的事情上，是没有感情的理性机器，在这种需要理性监督的日常惯例上，却贪图一时的便利，自欺欺人起来。说到这里，猛然发现自己有一个很坑爹的问题就是，每当某些事做的很好的时候，心里洋溢着开心和满足，发自内心的对自己说，原来认真一把，也很有意思很好玩的，信誓旦旦的对自己说，记住这种感觉，明天也要这样加油努力。 转头就把这些警醒忘得一干二净，反而在满足感的驱使下开始完全放松的日子。从根源上来讲，自己这个问题应该是，目标太小，易于满足；然而另一方面，为了从长远角度保证计划的实施同时保持生活的乐趣，避免陷入某天计划没完成，直接导致后续计划的崩盘，本来就没有给每天安排很多任务，所以这样就陷入了一种矛盾的状态。 面对这个事情，我想起来之前 看到的<strong>延迟满足</strong>这个词，当时看的就觉得虽然意思明白，但是实践起来是什么样子，没有什么概念。现在或许能有点明白，应该是，满足感留在完成那一天，但是再接再厉的警醒，还是需要时刻提醒自己，在整个规划中，自己走到哪一步了，有了今天 的成果，明天该怎么更进一步，这种不断进步的渴求，才是敦促自己前进的最原始的动力。</p>\n<p>解决问题：</p>\n<p>　　用认真的态度过好每一天，而不是得过且过的自我敷衍。这其实是自己一直没太关注的，总是试图用全局的计划和美好的理想来敦促自己，发现仍然过不好每一天的生活。渴望进步的决心，是绝对不会变的，但是认真的态度，特别是每天都坚持保持这种精神，实际是不容易办到的，而且偶尔的放松，实际上对自己有很大影响，因为自己特别容易自暴自弃，接受那些不受控制或者并非属于自身原因的变化，享受当下认真带来的满足和乐趣，放眼明日需要改变和努力的方向，过好自己的每一天，我想也不会有那种缺乏目标，激情和动力的乏味感觉了吧！</p>\n<p>　　说到这里，这篇关于懒惰和改变的讨论应该已经得到答案了。但是彼时对夏日的怀恋和异域的沉迷并没有得到解决，似乎一场旅行是非常有必要的，虽然真的很想去日本旅游，遗憾的是年末疫情复发可能对出国旅行产生很大的影响，不过究竟是什么程度的影响，自己也并没有做过多考察，剩下几个月的时间，自己还是应该鼓起干劲，多做一些信息上的搜集和准备，努力规划一场美好的动漫朝圣之旅。</p>\n<p><img src=\"2020-07-31.jpg\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p>　　前段时间，大概是在暑假期间吧，似乎因为看了很多动漫，仍然沉迷在学生时代的火热的夏天气息中。是呀！美好的夏天，如梦似幻。然而最难过的是，尽管现在的我无论想要如何去追寻这种夏日的味道，哪怕只是夏天的尾气而已，我却仍然无比清晰的认识到，自己的暑假早已经一去不复返了。这让自己陷入到一个两难的境地，一面渴望夏日的美好，一面却又警醒自己去接受事实，既无法肆意去做自我，也没办法彻底的放弃，去追寻所谓的成熟，就特别的拧巴。至于为什么总是要自己接受自己不断长大的事实，我也不甚明了，仿佛跟自己说的多了，就真的成长了一样。可能唯一让自己神伤的就只是不断变老的事实吧，或许告诉自己去年的自己跟现在的自己其实没什么太多变化和进步，那么多少有点对不起自己不断老去的时间，所以滋生出这样心理暗示，说是聊以慰己也好，说是对自己的失望继而产生的愧疚也好，总之，都是一份对自己自甘堕落，原地踏步的一种惩罚吧！所以，带着这份不甘，不明就里的开始了这份夏日的无聊幻想。</p>\n<p><img src=\"IMG_9373.jpeg\" alt=\"2020-07-31\"></p>\n<p>　　清楚记得是在7月31日，下班从公司回家， 开车走了一条平时从来没有走过的路，天色湛蓝，久违的看到了雾状的云絮，随着落日余晖，呈现出一幅不断变幻的黄昏美景，在这个陌生的却干净的路上走着，有一种异域他乡的漂泊感油然而生。不得不感叹，人有时候就是这么的任性，假如面对突然变化的外部环境，心情不免充满了惊慌和不安；但是一旦在舒适区呆的足够久，却又开始渴望起陌生的神秘起来。所以，在那一刻，我有点爱丽丝梦游仙境的不真实感，才从二次元的幻想出来，又来到这里，心中突然诞生起一股强烈的想要去亲眼见证 命运石之门，青春猪头少年，灌篮高手，你的名字 里面二次元名场景的三次元场景的冲动。这其中当然有自己近段时间一直懒于出游，疫情原因，甚至直接放弃远行的原因，已经快有一年没有认真旅行了，去年这个时候，还沉浸在五月天鸟巢演唱会的期待中。没想到一年后，都快要忘记旅行是什么感觉了，心中只有漠然和麻木。我熟悉这种感觉，当我十年前无法面对自己的时候，我就是这样一步一步麻痹自我，无视自己的感觉，任由时间和偶然的未来来试图冲淡这份难过，虽然几次试图反抗，但是最后还是来到了自己预想中的，失去了爱和共情能力的终点。等到意识到，这样堕落下去，并不能真正解决这份难过的时候，又过去了好久好久。想要改变的时候，心中的那份激情早已经烟消云散了，当我想要去喜欢的时候，内心却做到真正意义上的毫无波澜，直到再次遇见的二次元，这种状况才好起来。所以此时，我又重新遭遇了这份想要改变，却无法调动自己的内心的尴尬，似乎在这十年间，我无意识间培养出一股冷眼旁观的特质，无论是跟自己是否相关的人和事，习惯性的把自己置身事外，试图冷静分析利弊得失，却完全丧失了<strong>意识主观能动性</strong>，这股自己窃以为最核心和宝贵的动力，我明白，我所尊崇的理性，作为绝对意义上的中立，统治了自己的是非观，但是一旦要去自己去亲自上阵的时候，光有理性，总是感觉差点什么东西，实践起来也并不是那么得心应手，甚至还有点举步维艰。总之，无论做什么，我缺乏一种，“当我在做这件事，我在做什么的” 的自觉性。记得不久前，自己还有强烈的这种动力想要实现梦想，却在错失良机之后，陷入长久的沉寂。当工作从长远来看不受自己控制的变得愈加明朗起来的时候，自己陷入了对未来的强烈不安之中。造成如今的局面，都是有点自己自暴自弃的成分在里面，屏蔽自己感情，活在当下，试图忘掉自己的梦想，成为了如今自己生活的主题。自我屏蔽，前面已经聊过，被证实只是一种自我麻醉而已，跟逃避没什么两样，对于解决问题，没什么卵用；活在当下，虽然并没有什么不好，但是作为浑浑噩噩度日的借口，时间久了，自己也有点不好意思起来；试图忘记梦想，真的是逃避之集大成者。兜兜转转，最后还是来到了需要认真面对自己的这个路口，时间只不过让自己忘却了些许忧伤，积攒了一点可怜的勇气而已。如果自己没有根本性的转变和思想觉悟，我并不认为自己能有所突破。</p>\n<p>　　所以，回到，发现问题，认识问题，解决问题 的模式 ！</p>\n<p>发现问题：    </p>\n<p>​    <strong><em>生活缺乏 目标，激情和动力！</em></strong></p>\n<p>认识问题：</p>\n<p>　　按照 身边人面对这个问题的惯常思路，没有动力，给压力就好了。我并不否认这种方法的有效性，但是我一方面不认可这种方法，另一方面我本身也不是很习惯这种被迫做事的急促感，我认为扼杀了创意和生活的乐趣。曾经我也讨论过这个问题，得出的结论是，我并不适合这种压迫产生的动力；但另一方面，自己却由于过分懒散，反而落入了完全忘记自己的目标是什么的另一个极端。这样的后果就是，摸鱼到一定程度，自己甚至都开始厌恶这样无所事事的日子，想要找点事情来分散下多余的脑力，这个时候甚至反而觉得有点儿事做还挺有意思的。很遗憾的是，目前自己正处于这种状态，由于工作内容对自己来说比较容易，自己也没有适时的给自己足够的挑战，10月份以来，自己一直处于这种慵懒的状态，所以整个月的都处于基本停止的状态。仔细想来，应该是上次的面试结束之后，由于非技术原因没拿到offer，尽管情绪上来说没什么影响，但是准备算法OJ的积极性有点下降，努力想要认真刷算法题的这个弦突然就松下来了，这才导致后续很多事情都开始丧失积极性直到彻底懒得去做了。从这个角度来说，自己在战略上，还是需要足够的压力来推进自己去实践，否则很容易落入偷懒的陷阱之中。譬如，每天的总结，每周的反思，都在那之后渐渐都没有了；细想之下，我都是用，不做也没什么，晚点做也一样这种轻描淡写的借口一带而过。想来，自己也足够鸡贼，在缺乏意识驱动的事情上，是没有感情的理性机器，在这种需要理性监督的日常惯例上，却贪图一时的便利，自欺欺人起来。说到这里，猛然发现自己有一个很坑爹的问题就是，每当某些事做的很好的时候，心里洋溢着开心和满足，发自内心的对自己说，原来认真一把，也很有意思很好玩的，信誓旦旦的对自己说，记住这种感觉，明天也要这样加油努力。 转头就把这些警醒忘得一干二净，反而在满足感的驱使下开始完全放松的日子。从根源上来讲，自己这个问题应该是，目标太小，易于满足；然而另一方面，为了从长远角度保证计划的实施同时保持生活的乐趣，避免陷入某天计划没完成，直接导致后续计划的崩盘，本来就没有给每天安排很多任务，所以这样就陷入了一种矛盾的状态。 面对这个事情，我想起来之前 看到的<strong>延迟满足</strong>这个词，当时看的就觉得虽然意思明白，但是实践起来是什么样子，没有什么概念。现在或许能有点明白，应该是，满足感留在完成那一天，但是再接再厉的警醒，还是需要时刻提醒自己，在整个规划中，自己走到哪一步了，有了今天 的成果，明天该怎么更进一步，这种不断进步的渴求，才是敦促自己前进的最原始的动力。</p>\n<p>解决问题：</p>\n<p>　　用认真的态度过好每一天，而不是得过且过的自我敷衍。这其实是自己一直没太关注的，总是试图用全局的计划和美好的理想来敦促自己，发现仍然过不好每一天的生活。渴望进步的决心，是绝对不会变的，但是认真的态度，特别是每天都坚持保持这种精神，实际是不容易办到的，而且偶尔的放松，实际上对自己有很大影响，因为自己特别容易自暴自弃，接受那些不受控制或者并非属于自身原因的变化，享受当下认真带来的满足和乐趣，放眼明日需要改变和努力的方向，过好自己的每一天，我想也不会有那种缺乏目标，激情和动力的乏味感觉了吧！</p>\n<p>　　说到这里，这篇关于懒惰和改变的讨论应该已经得到答案了。但是彼时对夏日的怀恋和异域的沉迷并没有得到解决，似乎一场旅行是非常有必要的，虽然真的很想去日本旅游，遗憾的是年末疫情复发可能对出国旅行产生很大的影响，不过究竟是什么程度的影响，自己也并没有做过多考察，剩下几个月的时间，自己还是应该鼓起干劲，多做一些信息上的搜集和准备，努力规划一场美好的动漫朝圣之旅。</p>\n<p><img src=\"2020-07-31.jpg\"></p>\n"},{"title":"记ubuntu重启引起的故障排查","toc":true,"mathjax":true,"top":false,"cover":true,"date":"2020-11-04T07:25:29.000Z","_content":"\n　　前几天，由于自己服务所在的服务器需要停机维护，运行已有近半年的ubuntu 18.04 LTS终于迎来首次重启，尽管由于预料到长时间的服务器不停机，在关机或者重启阶段会有不可预知的事件发生。但是当服务器真的出现网络无法连接的状况的时候，长时间没有安装系统的我那一刻竟然还有点懵B了，此文主要关注此次故障问题排查和解决记录，作为以后服务器维护 参考之用。\n\n### 网络故障\n\n　　重启之后，首先出现的问题就是网络接口灯直接熄灭，几番周折接上显示器和键盘之后，发现是网卡没有ipv4的地址，反而是ipv6的地址是有的。\n\n```bash\nifconfig -a # 显示所有的网卡，如果网卡无ip，单纯的用ifconfig 是无法显示的所有网卡设备的\n# or\nip a\n```\n\n\n\n　　考虑现今的网络设备ipv6基本属于摆设，所以首先定位的问题是静态ip配置没有生效（此刻持续懵B 3mins，完全忘记如何手工配置 静态ip，某些最基础的操作，还是相当依赖UI），搜索一番得到结果是\n\n```bash\nvi /etc/network/interfaces\n# change\nauto enp1s0 # enp5s0 根据ifconfig 或许实际的网卡编号\niface enp1s0 inet static\n\t\taddress x.x.x.x\n\t\tnetmask 255.255.255.0\n\t\tgateway x.x.x.x\n\t\tdns-nameservers 114.114.114.114 8.8.8.8\n\t\t\nsudo ip a flush enp1s0\nsudo systemctl restart networking.service\n```\n\n　　悲剧的是，最后重启 networking.service 并没有发现这个服务，一度陷入僵局，最后发现 Ubuntu 18.04 LTS 开始启用 netplan 组建作为网络管理器，所以这里应该使用netplan来配置静态ip，又是搜索一番\n\n```bash\nvi /etc/netplan/50-cloud-init.yaml # 对于yaml文件可是深恶痛绝，奇怪冒号后必须接空格 和 不允许 缩进\\t的设定\n# change\n# This file describes the network interfaces available on your system\n# For more information, see netplan(5).\nnetwork:\n  version: 2\n  renderer: networkd\n  ethernets:\n    enp1s0:\n     dhcp4: no\n     addresses: [192.168.1.222/24]\n     gateway4: 192.168.1.1\n     nameservers:\n       addresses: [8.8.8.8,8.8.4.4]\n       \nsudo netplan apply\n# or\nsudo netplan --dubug apply\n```\n\n　　坑爹的是，居然没有 netplan 这个命令，然而却有/etc/netplan/50-cloud-init.yaml这个配置网络的文件，而且里面都是曾经配置完好的文件，应该是上次安装就已经确定下来生成的配置文件，这个就属于很小众的问题了，徜徉 StackOverFlow 和 StackExchange 数小时之后得到答案是：从 Ubuntu 16.04  Upgrade Ubuntu 18.04 会出现 netplan 配置已经安装，但是仍然使用 /etc/network/interfaces 配置ip生效的情况。\n\n```bash\n# netplan 安装 \nsudo apt install netplan.io # 坑爹的软件包命名\n```\n\n\n\n但是在这两个文件都存在，同时都配置的情况下，仍然无法使静态ip的配置生效，最主要的是，这个服务器就是原生安装的Ubuntu 18.04 ，并不存在Upgrade 导致这个问题存在的原因。最后无奈，只能通过命令临时生效的静态ip配置，来勉强达到可以上网的目的，想来毕竟服务器24h不关机，临时配置也算可用。唯一的缺点就是，万一重启，就又不得不去机房，现场维护，颇为不便。\n\n```bash\n# 临时生效的 ip 网关 dns 配置方法\n# 以下所有配置 重启失效\nifconfig enp1s0 x.x.x.x netmask 255.255.255.0\nroute add default gw x.x.x.x\nvi /etc/resolf.conf\n\n```\n\n\n\n### nvidia-smi 以及 docker 故障\n\n　　更坑爹是，总算以为完事了的时候，发现nvidia-smi挂了，docker也挂了\n\n```bash\nnvidia-smi\nNVIDIA-SMI has failed because it couldn’t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n```\n\n### 原因分析　　\n\n　　结合之前网络问题，推测应该是重启导致的系统层面的问题。考虑到此次维护硬件只有内存的变动，通过命令查看内存是正常运作的，判断应该是 Ubuntu 系统在重启的时候发生的变化，导致这个问题发生。以ubuntu重启，nvidia 失效，作为关键词，终于找到本次故障的最终原因：Ubuntu 18.04 会自动更新linux内核，并在重启的时候自动启动最新的内核\n\n\n\n### 故障解决，切换成旧版本内核\n\n　　首先需要确认旧版本内核是否可以解决上述所有问题，最简单的切换就内核的方法是，在 grub 启动菜单里面选择,Advanced options for Ubuntu -> Ubuntu , with linux x.x.x-x-generic ， 终于找到了4.15.0-60-generic 是旧版本内核，并且上述问题全部解决，而4.15.0-122-generic则是产生故障的最新内核版本。\n\n　　那么问题来了，怎么切换内核呢？ 搜索引擎救星又来了\n\n```bash\n# 查看目前系统已安装内核\ndpkg --get-selections |grep linux-image\nlinux-image-4.15.0-122-generic\t\t\tdeinstall\nlinux-image-4.15.0-60-generic\t\t\tinstall\nlinux-image-4.15.0-62-generic\t\t\tdeinstall\n\n# 查看 grub 已经生成 菜单入口名称\ngrep menuentry /boot/grub/grub.cfg\nmenuentry 'Ubuntu, with Linux 4.15.0-60-generic' \n...\n```\n\n#### Solution 1: 修改grub启动配置\n\n```bash\nvi /etc/default/grub\n# change\nGRUB_DEFAULT=“Advanced options for Ubuntu > Ubuntu, with Linux 4.15.0-60-generic”\n# 也可以 用数字标示 0作为第一个菜单\nGRUB_DEFAULT = \"1> 4\" #改成这样\n\nGRUB_TIMEOUT_STYLE=menu # default: hidden\nGRUB_TIMEOUT=3 # default: 0\n\nsudo update-grub\nsudo reboot\n```\n\n#### Solution 2: 删除新内核\n\n```bash\n# 注意 无法删除正在使用的内核\nsudo apt remove linux-image-xxx-xx-generic\n# or\nsudo dpkg --purge linux-image-x.x.x-xx-generic\n# 安装新内核\nsudo apt install linux-headers-x.x.x-x-generic linux-image-x.x.x-x-generic\n# 关闭内核自动更新\nsudo apt-mark hold linux-image-generic linux-headers-generic\n# 开启内核自动更新\nsudo apt-mark unhold linux-image-generic linux-headers-generic\n```\n\n### Conclusion\n\n　　至此，网络问题，nvidia驱动问题，docker问题都得到圆满解决。回顾过程，心态方面还是稍微不够沉着冷静，发现问题根本原因之前，过分关注表象，诸如 netplan 的配置花费的过多时间，反而问题原因没有深入思考，导致东弄西弄一下，试图用碰运气的方式来解决问题方式终究还是有瓶颈的，或许是碰运气的方式曾经取得的成果对现在行为抉择还是产生的一定的影响，其实在陷入僵局之后的思考基本已经锁定了问题，就算是没有解决问题的彼时，心里对故障的排除已经基本有底了，最后的解决也只是水到渠成而已。\n\n\n\nReferences:\n\n[How to configure static IP address on Ubuntu 18.04 ](https://linuxconfig.org/how-to-configure-static-ip-address-on-ubuntu-18-04-bionic-beaver-linux)\n\n[How to enable netplan on ubuntu server upgraded from 16.04 to 18.04](https://askubuntu.com/questions/1034711/how-to-enable-netplan-on-ubuntu-server-upgraded-from-16-04-to-18-04)\n\n[ubuntu18.04 内核自动更新导致驱动掉了](https://blog.csdn.net/qq_43222384/article/details/90314297)","source":"_posts/记ubuntu重启引起的故障排查.md","raw":"---\ntitle: 记ubuntu重启引起的故障排查\ntoc: true\nmathjax: true\ntop: false\ncover: true\ndate: 2020-11-04 15:25:29\ncategories: 运维\ntags:\n- ubuntu\n- 故障排查\n- linux内核\n- grub\n---\n\n　　前几天，由于自己服务所在的服务器需要停机维护，运行已有近半年的ubuntu 18.04 LTS终于迎来首次重启，尽管由于预料到长时间的服务器不停机，在关机或者重启阶段会有不可预知的事件发生。但是当服务器真的出现网络无法连接的状况的时候，长时间没有安装系统的我那一刻竟然还有点懵B了，此文主要关注此次故障问题排查和解决记录，作为以后服务器维护 参考之用。\n\n### 网络故障\n\n　　重启之后，首先出现的问题就是网络接口灯直接熄灭，几番周折接上显示器和键盘之后，发现是网卡没有ipv4的地址，反而是ipv6的地址是有的。\n\n```bash\nifconfig -a # 显示所有的网卡，如果网卡无ip，单纯的用ifconfig 是无法显示的所有网卡设备的\n# or\nip a\n```\n\n\n\n　　考虑现今的网络设备ipv6基本属于摆设，所以首先定位的问题是静态ip配置没有生效（此刻持续懵B 3mins，完全忘记如何手工配置 静态ip，某些最基础的操作，还是相当依赖UI），搜索一番得到结果是\n\n```bash\nvi /etc/network/interfaces\n# change\nauto enp1s0 # enp5s0 根据ifconfig 或许实际的网卡编号\niface enp1s0 inet static\n\t\taddress x.x.x.x\n\t\tnetmask 255.255.255.0\n\t\tgateway x.x.x.x\n\t\tdns-nameservers 114.114.114.114 8.8.8.8\n\t\t\nsudo ip a flush enp1s0\nsudo systemctl restart networking.service\n```\n\n　　悲剧的是，最后重启 networking.service 并没有发现这个服务，一度陷入僵局，最后发现 Ubuntu 18.04 LTS 开始启用 netplan 组建作为网络管理器，所以这里应该使用netplan来配置静态ip，又是搜索一番\n\n```bash\nvi /etc/netplan/50-cloud-init.yaml # 对于yaml文件可是深恶痛绝，奇怪冒号后必须接空格 和 不允许 缩进\\t的设定\n# change\n# This file describes the network interfaces available on your system\n# For more information, see netplan(5).\nnetwork:\n  version: 2\n  renderer: networkd\n  ethernets:\n    enp1s0:\n     dhcp4: no\n     addresses: [192.168.1.222/24]\n     gateway4: 192.168.1.1\n     nameservers:\n       addresses: [8.8.8.8,8.8.4.4]\n       \nsudo netplan apply\n# or\nsudo netplan --dubug apply\n```\n\n　　坑爹的是，居然没有 netplan 这个命令，然而却有/etc/netplan/50-cloud-init.yaml这个配置网络的文件，而且里面都是曾经配置完好的文件，应该是上次安装就已经确定下来生成的配置文件，这个就属于很小众的问题了，徜徉 StackOverFlow 和 StackExchange 数小时之后得到答案是：从 Ubuntu 16.04  Upgrade Ubuntu 18.04 会出现 netplan 配置已经安装，但是仍然使用 /etc/network/interfaces 配置ip生效的情况。\n\n```bash\n# netplan 安装 \nsudo apt install netplan.io # 坑爹的软件包命名\n```\n\n\n\n但是在这两个文件都存在，同时都配置的情况下，仍然无法使静态ip的配置生效，最主要的是，这个服务器就是原生安装的Ubuntu 18.04 ，并不存在Upgrade 导致这个问题存在的原因。最后无奈，只能通过命令临时生效的静态ip配置，来勉强达到可以上网的目的，想来毕竟服务器24h不关机，临时配置也算可用。唯一的缺点就是，万一重启，就又不得不去机房，现场维护，颇为不便。\n\n```bash\n# 临时生效的 ip 网关 dns 配置方法\n# 以下所有配置 重启失效\nifconfig enp1s0 x.x.x.x netmask 255.255.255.0\nroute add default gw x.x.x.x\nvi /etc/resolf.conf\n\n```\n\n\n\n### nvidia-smi 以及 docker 故障\n\n　　更坑爹是，总算以为完事了的时候，发现nvidia-smi挂了，docker也挂了\n\n```bash\nnvidia-smi\nNVIDIA-SMI has failed because it couldn’t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n```\n\n### 原因分析　　\n\n　　结合之前网络问题，推测应该是重启导致的系统层面的问题。考虑到此次维护硬件只有内存的变动，通过命令查看内存是正常运作的，判断应该是 Ubuntu 系统在重启的时候发生的变化，导致这个问题发生。以ubuntu重启，nvidia 失效，作为关键词，终于找到本次故障的最终原因：Ubuntu 18.04 会自动更新linux内核，并在重启的时候自动启动最新的内核\n\n\n\n### 故障解决，切换成旧版本内核\n\n　　首先需要确认旧版本内核是否可以解决上述所有问题，最简单的切换就内核的方法是，在 grub 启动菜单里面选择,Advanced options for Ubuntu -> Ubuntu , with linux x.x.x-x-generic ， 终于找到了4.15.0-60-generic 是旧版本内核，并且上述问题全部解决，而4.15.0-122-generic则是产生故障的最新内核版本。\n\n　　那么问题来了，怎么切换内核呢？ 搜索引擎救星又来了\n\n```bash\n# 查看目前系统已安装内核\ndpkg --get-selections |grep linux-image\nlinux-image-4.15.0-122-generic\t\t\tdeinstall\nlinux-image-4.15.0-60-generic\t\t\tinstall\nlinux-image-4.15.0-62-generic\t\t\tdeinstall\n\n# 查看 grub 已经生成 菜单入口名称\ngrep menuentry /boot/grub/grub.cfg\nmenuentry 'Ubuntu, with Linux 4.15.0-60-generic' \n...\n```\n\n#### Solution 1: 修改grub启动配置\n\n```bash\nvi /etc/default/grub\n# change\nGRUB_DEFAULT=“Advanced options for Ubuntu > Ubuntu, with Linux 4.15.0-60-generic”\n# 也可以 用数字标示 0作为第一个菜单\nGRUB_DEFAULT = \"1> 4\" #改成这样\n\nGRUB_TIMEOUT_STYLE=menu # default: hidden\nGRUB_TIMEOUT=3 # default: 0\n\nsudo update-grub\nsudo reboot\n```\n\n#### Solution 2: 删除新内核\n\n```bash\n# 注意 无法删除正在使用的内核\nsudo apt remove linux-image-xxx-xx-generic\n# or\nsudo dpkg --purge linux-image-x.x.x-xx-generic\n# 安装新内核\nsudo apt install linux-headers-x.x.x-x-generic linux-image-x.x.x-x-generic\n# 关闭内核自动更新\nsudo apt-mark hold linux-image-generic linux-headers-generic\n# 开启内核自动更新\nsudo apt-mark unhold linux-image-generic linux-headers-generic\n```\n\n### Conclusion\n\n　　至此，网络问题，nvidia驱动问题，docker问题都得到圆满解决。回顾过程，心态方面还是稍微不够沉着冷静，发现问题根本原因之前，过分关注表象，诸如 netplan 的配置花费的过多时间，反而问题原因没有深入思考，导致东弄西弄一下，试图用碰运气的方式来解决问题方式终究还是有瓶颈的，或许是碰运气的方式曾经取得的成果对现在行为抉择还是产生的一定的影响，其实在陷入僵局之后的思考基本已经锁定了问题，就算是没有解决问题的彼时，心里对故障的排除已经基本有底了，最后的解决也只是水到渠成而已。\n\n\n\nReferences:\n\n[How to configure static IP address on Ubuntu 18.04 ](https://linuxconfig.org/how-to-configure-static-ip-address-on-ubuntu-18-04-bionic-beaver-linux)\n\n[How to enable netplan on ubuntu server upgraded from 16.04 to 18.04](https://askubuntu.com/questions/1034711/how-to-enable-netplan-on-ubuntu-server-upgraded-from-16-04-to-18-04)\n\n[ubuntu18.04 内核自动更新导致驱动掉了](https://blog.csdn.net/qq_43222384/article/details/90314297)","slug":"记ubuntu重启引起的故障排查","published":1,"updated":"2020-12-10T08:01:27.729Z","_id":"ckh35zvbu0000u7282nma27nl","comments":1,"layout":"post","photos":[],"link":"","content":"<p>　　前几天，由于自己服务所在的服务器需要停机维护，运行已有近半年的ubuntu 18.04 LTS终于迎来首次重启，尽管由于预料到长时间的服务器不停机，在关机或者重启阶段会有不可预知的事件发生。但是当服务器真的出现网络无法连接的状况的时候，长时间没有安装系统的我那一刻竟然还有点懵B了，此文主要关注此次故障问题排查和解决记录，作为以后服务器维护 参考之用。</p>\n<h3 id=\"网络故障\"><a href=\"#网络故障\" class=\"headerlink\" title=\"网络故障\"></a>网络故障</h3><p>　　重启之后，首先出现的问题就是网络接口灯直接熄灭，几番周折接上显示器和键盘之后，发现是网卡没有ipv4的地址，反而是ipv6的地址是有的。</p>\n<pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token function\">ifconfig</span> -a <span class=\"token comment\" spellcheck=\"true\"># 显示所有的网卡，如果网卡无ip，单纯的用ifconfig 是无法显示的所有网卡设备的</span>\n<span class=\"token comment\" spellcheck=\"true\"># or</span>\nip a</code></pre>\n<p>　　考虑现今的网络设备ipv6基本属于摆设，所以首先定位的问题是静态ip配置没有生效（此刻持续懵B 3mins，完全忘记如何手工配置 静态ip，某些最基础的操作，还是相当依赖UI），搜索一番得到结果是</p>\n<pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token function\">vi</span> /etc/network/interfaces\n<span class=\"token comment\" spellcheck=\"true\"># change</span>\nauto enp1s0 <span class=\"token comment\" spellcheck=\"true\"># enp5s0 根据ifconfig 或许实际的网卡编号</span>\niface enp1s0 inet static\n        address x.x.x.x\n        netmask 255.255.255.0\n        gateway x.x.x.x\n        dns-nameservers 114.114.114.114 8.8.8.8\n\n<span class=\"token function\">sudo</span> ip a flush enp1s0\n<span class=\"token function\">sudo</span> systemctl restart networking.service</code></pre>\n<p>　　悲剧的是，最后重启 networking.service 并没有发现这个服务，一度陷入僵局，最后发现 Ubuntu 18.04 LTS 开始启用 netplan 组建作为网络管理器，所以这里应该使用netplan来配置静态ip，又是搜索一番</p>\n<pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token function\">vi</span> /etc/netplan/50-cloud-init.yaml <span class=\"token comment\" spellcheck=\"true\"># 对于yaml文件可是深恶痛绝，奇怪冒号后必须接空格 和 不允许 缩进\\t的设定</span>\n<span class=\"token comment\" spellcheck=\"true\"># change</span>\n<span class=\"token comment\" spellcheck=\"true\"># This file describes the network interfaces available on your system</span>\n<span class=\"token comment\" spellcheck=\"true\"># For more information, see netplan(5).</span>\nnetwork:\n  version: 2\n  renderer: networkd\n  ethernets:\n    enp1s0:\n     dhcp4: no\n     addresses: <span class=\"token punctuation\">[</span>192.168.1.222/24<span class=\"token punctuation\">]</span>\n     gateway4: 192.168.1.1\n     nameservers:\n       addresses: <span class=\"token punctuation\">[</span>8.8.8.8,8.8.4.4<span class=\"token punctuation\">]</span>\n\n<span class=\"token function\">sudo</span> netplan apply\n<span class=\"token comment\" spellcheck=\"true\"># or</span>\n<span class=\"token function\">sudo</span> netplan --dubug apply</code></pre>\n<p>　　坑爹的是，居然没有 netplan 这个命令，然而却有/etc/netplan/50-cloud-init.yaml这个配置网络的文件，而且里面都是曾经配置完好的文件，应该是上次安装就已经确定下来生成的配置文件，这个就属于很小众的问题了，徜徉 StackOverFlow 和 StackExchange 数小时之后得到答案是：从 Ubuntu 16.04  Upgrade Ubuntu 18.04 会出现 netplan 配置已经安装，但是仍然使用 /etc/network/interfaces 配置ip生效的情况。</p>\n<pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token comment\" spellcheck=\"true\"># netplan 安装 </span>\n<span class=\"token function\">sudo</span> apt <span class=\"token function\">install</span> netplan.io <span class=\"token comment\" spellcheck=\"true\"># 坑爹的软件包命名</span></code></pre>\n<p>但是在这两个文件都存在，同时都配置的情况下，仍然无法使静态ip的配置生效，最主要的是，这个服务器就是原生安装的Ubuntu 18.04 ，并不存在Upgrade 导致这个问题存在的原因。最后无奈，只能通过命令临时生效的静态ip配置，来勉强达到可以上网的目的，想来毕竟服务器24h不关机，临时配置也算可用。唯一的缺点就是，万一重启，就又不得不去机房，现场维护，颇为不便。</p>\n<pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token comment\" spellcheck=\"true\"># 临时生效的 ip 网关 dns 配置方法</span>\n<span class=\"token comment\" spellcheck=\"true\"># 以下所有配置 重启失效</span>\n<span class=\"token function\">ifconfig</span> enp1s0 x.x.x.x netmask 255.255.255.0\nroute add default gw x.x.x.x\n<span class=\"token function\">vi</span> /etc/resolf.conf\n</code></pre>\n<h3 id=\"nvidia-smi-以及-docker-故障\"><a href=\"#nvidia-smi-以及-docker-故障\" class=\"headerlink\" title=\"nvidia-smi 以及 docker 故障\"></a>nvidia-smi 以及 docker 故障</h3><p>　　更坑爹是，总算以为完事了的时候，发现nvidia-smi挂了，docker也挂了</p>\n<pre class=\" language-bash\"><code class=\"language-bash\">nvidia-smi\nNVIDIA-SMI has failed because it couldn’t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.</code></pre>\n<h3 id=\"原因分析\"><a href=\"#原因分析\" class=\"headerlink\" title=\"原因分析　　\"></a>原因分析　　</h3><p>　　结合之前网络问题，推测应该是重启导致的系统层面的问题。考虑到此次维护硬件只有内存的变动，通过命令查看内存是正常运作的，判断应该是 Ubuntu 系统在重启的时候发生的变化，导致这个问题发生。以ubuntu重启，nvidia 失效，作为关键词，终于找到本次故障的最终原因：Ubuntu 18.04 会自动更新linux内核，并在重启的时候自动启动最新的内核</p>\n<h3 id=\"故障解决，切换成旧版本内核\"><a href=\"#故障解决，切换成旧版本内核\" class=\"headerlink\" title=\"故障解决，切换成旧版本内核\"></a>故障解决，切换成旧版本内核</h3><p>　　首先需要确认旧版本内核是否可以解决上述所有问题，最简单的切换就内核的方法是，在 grub 启动菜单里面选择,Advanced options for Ubuntu -&gt; Ubuntu , with linux x.x.x-x-generic ， 终于找到了4.15.0-60-generic 是旧版本内核，并且上述问题全部解决，而4.15.0-122-generic则是产生故障的最新内核版本。</p>\n<p>　　那么问题来了，怎么切换内核呢？ 搜索引擎救星又来了</p>\n<pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token comment\" spellcheck=\"true\"># 查看目前系统已安装内核</span>\ndpkg --get-selections <span class=\"token operator\">|</span><span class=\"token function\">grep</span> linux-image\nlinux-image-4.15.0-122-generic            deinstall\nlinux-image-4.15.0-60-generic            <span class=\"token function\">install</span>\nlinux-image-4.15.0-62-generic            deinstall\n\n<span class=\"token comment\" spellcheck=\"true\"># 查看 grub 已经生成 菜单入口名称</span>\n<span class=\"token function\">grep</span> menuentry /boot/grub/grub.cfg\nmenuentry <span class=\"token string\">'Ubuntu, with Linux 4.15.0-60-generic'</span> \n<span class=\"token punctuation\">..</span>.</code></pre>\n<h4 id=\"Solution-1-修改grub启动配置\"><a href=\"#Solution-1-修改grub启动配置\" class=\"headerlink\" title=\"Solution 1: 修改grub启动配置\"></a>Solution 1: 修改grub启动配置</h4><pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token function\">vi</span> /etc/default/grub\n<span class=\"token comment\" spellcheck=\"true\"># change</span>\nGRUB_DEFAULT<span class=\"token operator\">=</span>“Advanced options <span class=\"token keyword\">for</span> Ubuntu <span class=\"token operator\">></span> Ubuntu, with Linux 4.15.0-60-generic”\n<span class=\"token comment\" spellcheck=\"true\"># 也可以 用数字标示 0作为第一个菜单</span>\nGRUB_DEFAULT <span class=\"token operator\">=</span> <span class=\"token string\">\"1> 4\"</span> <span class=\"token comment\" spellcheck=\"true\">#改成这样</span>\n\nGRUB_TIMEOUT_STYLE<span class=\"token operator\">=</span>menu <span class=\"token comment\" spellcheck=\"true\"># default: hidden</span>\nGRUB_TIMEOUT<span class=\"token operator\">=</span>3 <span class=\"token comment\" spellcheck=\"true\"># default: 0</span>\n\n<span class=\"token function\">sudo</span> update-grub\n<span class=\"token function\">sudo</span> <span class=\"token function\">reboot</span></code></pre>\n<h4 id=\"Solution-2-删除新内核\"><a href=\"#Solution-2-删除新内核\" class=\"headerlink\" title=\"Solution 2: 删除新内核\"></a>Solution 2: 删除新内核</h4><pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token comment\" spellcheck=\"true\"># 注意 无法删除正在使用的内核</span>\n<span class=\"token function\">sudo</span> apt remove linux-image-xxx-xx-generic\n<span class=\"token comment\" spellcheck=\"true\"># or</span>\n<span class=\"token function\">sudo</span> dpkg --purge linux-image-x.x.x-xx-generic\n<span class=\"token comment\" spellcheck=\"true\"># 安装新内核</span>\n<span class=\"token function\">sudo</span> apt <span class=\"token function\">install</span> linux-headers-x.x.x-x-generic linux-image-x.x.x-x-generic\n<span class=\"token comment\" spellcheck=\"true\"># 关闭内核自动更新</span>\n<span class=\"token function\">sudo</span> apt-mark hold linux-image-generic linux-headers-generic\n<span class=\"token comment\" spellcheck=\"true\"># 开启内核自动更新</span>\n<span class=\"token function\">sudo</span> apt-mark unhold linux-image-generic linux-headers-generic</code></pre>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>　　至此，网络问题，nvidia驱动问题，docker问题都得到圆满解决。回顾过程，心态方面还是稍微不够沉着冷静，发现问题根本原因之前，过分关注表象，诸如 netplan 的配置花费的过多时间，反而问题原因没有深入思考，导致东弄西弄一下，试图用碰运气的方式来解决问题方式终究还是有瓶颈的，或许是碰运气的方式曾经取得的成果对现在行为抉择还是产生的一定的影响，其实在陷入僵局之后的思考基本已经锁定了问题，就算是没有解决问题的彼时，心里对故障的排除已经基本有底了，最后的解决也只是水到渠成而已。</p>\n<p>References:</p>\n<p><a href=\"https://linuxconfig.org/how-to-configure-static-ip-address-on-ubuntu-18-04-bionic-beaver-linux\">How to configure static IP address on Ubuntu 18.04 </a></p>\n<p><a href=\"https://askubuntu.com/questions/1034711/how-to-enable-netplan-on-ubuntu-server-upgraded-from-16-04-to-18-04\">How to enable netplan on ubuntu server upgraded from 16.04 to 18.04</a></p>\n<p><a href=\"https://blog.csdn.net/qq_43222384/article/details/90314297\">ubuntu18.04 内核自动更新导致驱动掉了</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>　　前几天，由于自己服务所在的服务器需要停机维护，运行已有近半年的ubuntu 18.04 LTS终于迎来首次重启，尽管由于预料到长时间的服务器不停机，在关机或者重启阶段会有不可预知的事件发生。但是当服务器真的出现网络无法连接的状况的时候，长时间没有安装系统的我那一刻竟然还有点懵B了，此文主要关注此次故障问题排查和解决记录，作为以后服务器维护 参考之用。</p>\n<h3 id=\"网络故障\"><a href=\"#网络故障\" class=\"headerlink\" title=\"网络故障\"></a>网络故障</h3><p>　　重启之后，首先出现的问题就是网络接口灯直接熄灭，几番周折接上显示器和键盘之后，发现是网卡没有ipv4的地址，反而是ipv6的地址是有的。</p>\n<pre><code class=\"bash\">ifconfig -a # 显示所有的网卡，如果网卡无ip，单纯的用ifconfig 是无法显示的所有网卡设备的\n# or\nip a</code></pre>\n<p>　　考虑现今的网络设备ipv6基本属于摆设，所以首先定位的问题是静态ip配置没有生效（此刻持续懵B 3mins，完全忘记如何手工配置 静态ip，某些最基础的操作，还是相当依赖UI），搜索一番得到结果是</p>\n<pre><code class=\"bash\">vi /etc/network/interfaces\n# change\nauto enp1s0 # enp5s0 根据ifconfig 或许实际的网卡编号\niface enp1s0 inet static\n        address x.x.x.x\n        netmask 255.255.255.0\n        gateway x.x.x.x\n        dns-nameservers 114.114.114.114 8.8.8.8\n\nsudo ip a flush enp1s0\nsudo systemctl restart networking.service</code></pre>\n<p>　　悲剧的是，最后重启 networking.service 并没有发现这个服务，一度陷入僵局，最后发现 Ubuntu 18.04 LTS 开始启用 netplan 组建作为网络管理器，所以这里应该使用netplan来配置静态ip，又是搜索一番</p>\n<pre><code class=\"bash\">vi /etc/netplan/50-cloud-init.yaml # 对于yaml文件可是深恶痛绝，奇怪冒号后必须接空格 和 不允许 缩进\\t的设定\n# change\n# This file describes the network interfaces available on your system\n# For more information, see netplan(5).\nnetwork:\n  version: 2\n  renderer: networkd\n  ethernets:\n    enp1s0:\n     dhcp4: no\n     addresses: [192.168.1.222/24]\n     gateway4: 192.168.1.1\n     nameservers:\n       addresses: [8.8.8.8,8.8.4.4]\n\nsudo netplan apply\n# or\nsudo netplan --dubug apply</code></pre>\n<p>　　坑爹的是，居然没有 netplan 这个命令，然而却有/etc/netplan/50-cloud-init.yaml这个配置网络的文件，而且里面都是曾经配置完好的文件，应该是上次安装就已经确定下来生成的配置文件，这个就属于很小众的问题了，徜徉 StackOverFlow 和 StackExchange 数小时之后得到答案是：从 Ubuntu 16.04  Upgrade Ubuntu 18.04 会出现 netplan 配置已经安装，但是仍然使用 /etc/network/interfaces 配置ip生效的情况。</p>\n<pre><code class=\"bash\"># netplan 安装 \nsudo apt install netplan.io # 坑爹的软件包命名</code></pre>\n<p>但是在这两个文件都存在，同时都配置的情况下，仍然无法使静态ip的配置生效，最主要的是，这个服务器就是原生安装的Ubuntu 18.04 ，并不存在Upgrade 导致这个问题存在的原因。最后无奈，只能通过命令临时生效的静态ip配置，来勉强达到可以上网的目的，想来毕竟服务器24h不关机，临时配置也算可用。唯一的缺点就是，万一重启，就又不得不去机房，现场维护，颇为不便。</p>\n<pre><code class=\"bash\"># 临时生效的 ip 网关 dns 配置方法\n# 以下所有配置 重启失效\nifconfig enp1s0 x.x.x.x netmask 255.255.255.0\nroute add default gw x.x.x.x\nvi /etc/resolf.conf\n</code></pre>\n<h3 id=\"nvidia-smi-以及-docker-故障\"><a href=\"#nvidia-smi-以及-docker-故障\" class=\"headerlink\" title=\"nvidia-smi 以及 docker 故障\"></a>nvidia-smi 以及 docker 故障</h3><p>　　更坑爹是，总算以为完事了的时候，发现nvidia-smi挂了，docker也挂了</p>\n<pre><code class=\"bash\">nvidia-smi\nNVIDIA-SMI has failed because it couldn’t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.</code></pre>\n<h3 id=\"原因分析\"><a href=\"#原因分析\" class=\"headerlink\" title=\"原因分析　　\"></a>原因分析　　</h3><p>　　结合之前网络问题，推测应该是重启导致的系统层面的问题。考虑到此次维护硬件只有内存的变动，通过命令查看内存是正常运作的，判断应该是 Ubuntu 系统在重启的时候发生的变化，导致这个问题发生。以ubuntu重启，nvidia 失效，作为关键词，终于找到本次故障的最终原因：Ubuntu 18.04 会自动更新linux内核，并在重启的时候自动启动最新的内核</p>\n<h3 id=\"故障解决，切换成旧版本内核\"><a href=\"#故障解决，切换成旧版本内核\" class=\"headerlink\" title=\"故障解决，切换成旧版本内核\"></a>故障解决，切换成旧版本内核</h3><p>　　首先需要确认旧版本内核是否可以解决上述所有问题，最简单的切换就内核的方法是，在 grub 启动菜单里面选择,Advanced options for Ubuntu -&gt; Ubuntu , with linux x.x.x-x-generic ， 终于找到了4.15.0-60-generic 是旧版本内核，并且上述问题全部解决，而4.15.0-122-generic则是产生故障的最新内核版本。</p>\n<p>　　那么问题来了，怎么切换内核呢？ 搜索引擎救星又来了</p>\n<pre><code class=\"bash\"># 查看目前系统已安装内核\ndpkg --get-selections |grep linux-image\nlinux-image-4.15.0-122-generic            deinstall\nlinux-image-4.15.0-60-generic            install\nlinux-image-4.15.0-62-generic            deinstall\n\n# 查看 grub 已经生成 菜单入口名称\ngrep menuentry /boot/grub/grub.cfg\nmenuentry &#39;Ubuntu, with Linux 4.15.0-60-generic&#39; \n...</code></pre>\n<h4 id=\"Solution-1-修改grub启动配置\"><a href=\"#Solution-1-修改grub启动配置\" class=\"headerlink\" title=\"Solution 1: 修改grub启动配置\"></a>Solution 1: 修改grub启动配置</h4><pre><code class=\"bash\">vi /etc/default/grub\n# change\nGRUB_DEFAULT=“Advanced options for Ubuntu &gt; Ubuntu, with Linux 4.15.0-60-generic”\n# 也可以 用数字标示 0作为第一个菜单\nGRUB_DEFAULT = &quot;1&gt; 4&quot; #改成这样\n\nGRUB_TIMEOUT_STYLE=menu # default: hidden\nGRUB_TIMEOUT=3 # default: 0\n\nsudo update-grub\nsudo reboot</code></pre>\n<h4 id=\"Solution-2-删除新内核\"><a href=\"#Solution-2-删除新内核\" class=\"headerlink\" title=\"Solution 2: 删除新内核\"></a>Solution 2: 删除新内核</h4><pre><code class=\"bash\"># 注意 无法删除正在使用的内核\nsudo apt remove linux-image-xxx-xx-generic\n# or\nsudo dpkg --purge linux-image-x.x.x-xx-generic\n# 安装新内核\nsudo apt install linux-headers-x.x.x-x-generic linux-image-x.x.x-x-generic\n# 关闭内核自动更新\nsudo apt-mark hold linux-image-generic linux-headers-generic\n# 开启内核自动更新\nsudo apt-mark unhold linux-image-generic linux-headers-generic</code></pre>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>　　至此，网络问题，nvidia驱动问题，docker问题都得到圆满解决。回顾过程，心态方面还是稍微不够沉着冷静，发现问题根本原因之前，过分关注表象，诸如 netplan 的配置花费的过多时间，反而问题原因没有深入思考，导致东弄西弄一下，试图用碰运气的方式来解决问题方式终究还是有瓶颈的，或许是碰运气的方式曾经取得的成果对现在行为抉择还是产生的一定的影响，其实在陷入僵局之后的思考基本已经锁定了问题，就算是没有解决问题的彼时，心里对故障的排除已经基本有底了，最后的解决也只是水到渠成而已。</p>\n<p>References:</p>\n<p><a href=\"https://linuxconfig.org/how-to-configure-static-ip-address-on-ubuntu-18-04-bionic-beaver-linux\">How to configure static IP address on Ubuntu 18.04 </a></p>\n<p><a href=\"https://askubuntu.com/questions/1034711/how-to-enable-netplan-on-ubuntu-server-upgraded-from-16-04-to-18-04\">How to enable netplan on ubuntu server upgraded from 16.04 to 18.04</a></p>\n<p><a href=\"https://blog.csdn.net/qq_43222384/article/details/90314297\">ubuntu18.04 内核自动更新导致驱动掉了</a></p>\n"},{"title":"LeetCode 33. Search in Rotated Sorted Array","toc":true,"mathjax":true,"top":false,"cover":true,"date":"2020-11-17T08:19:58.000Z","updated":"2020-12-10T08:01:36.687Z","_content":"\n### LeetCode 33. Search in Rotated Sorted Array\n\n![](img1.png)\n\n　　应该是二分法的变种优化，但是实际分析的过程中，自己的想法被太多的if条件给弄晕了，虽然勉强解出来了，但是更多是凭借临场发挥来作出的，至于宏观的算法原理，自己仍然理解不够透彻，借助讨论区的总结出几个好玩的解法\n\n#### 解法一\n\n　　最直接的想法是找到数组的偏移量，通过还原数组为排序数组的方式，最终利用二分法搜索target ，但是自己并没有想出可以$O(log(n))$搜索偏移量的方法，所以没有深入思考。借助讨论区的提醒，其实偏移量就是最小值的下标，所以只需要找到一种在$O(log(n))$下找到最小值的方法，这里仍然是用二分法，只不过start, mid, end 之间比较关系是不同的\n\n- start 和 mid:\n\n　mid > start : 最小值 in left\n\n![](img3.jpg)\n\n　mid < start : 最小值仍然 in left:\n\n![](img2.jpg)\n\n- mid 和 end\n\n　　mid < end: 最小值 in left，end = mid\n\n![](img6.jpg)\n\n　　mid > end : 最小值 in right, start = mid\n\n![](img7.jpg)\n\n　　只需要mid 和 end就可以利用二分法搜索最小值，只不过需要注意的一点是，可能会出现死循环\n\n![](img8.jpg)\n\n　　主要原因应该是，当最小值in right 的时候，mid 原地更新，而在in right 的情况，实际上mid所在的value不可能是最小值，所以 start = mid + 1 即可\n\n```python\nwhile start < end:\n\tmid = (start + end ) //2\n\tif nums[start] > nums[end]:\n\t\tstart = mid + 1\n\telse:\n\t\tend = mid\nbias = start\n```\n\n　　最后利用偏移量，转换mid 为 偏移后的mid，二分搜索target\n\n```python\nstart = 0\nend = len(nums) - 1\nwhile start <= end:\n\tmid = (start+end) // 2\n\tmid_pos = (mid + bias) \t% len(nums)\n\tval = nums[mid_pos]\n\tif target == value:\n\t\treturn mid_pos\n\tif target < value:\n\t\tend = mid - 1\n\telse:\n\t\tstart = mid + 1\n    \nreturn -1\n```\n\n\n\nTime Complexity: $O(log(n))$\n\nSpace Complexity: $O(1)$\n\n#### 解法二\n\n　　这里还是将被切分的2段分成2个有序数组来处理，比如,[4 5 6 7 1 2 3 ]分成 [4 5 6 7 ]和[1 2 3 ]，判断target位于其中的哪一段，然后将另一段变成-inf or inf，这样做的目的是可以用正常的二分法来搜索改变后的数组。\n\n[ 4 5 6 7 1 2 3] ，如果 target = 5，那么数组可以看做 [ 4 5 6 7 inf inf inf ]。\n\n[ 4 5 6 7 1 2 3] ，如果 target = 2，那么数组可以看做 [ -inf -inf - inf -inf 1 2 3]。\n\n　　实践阶段，只需要判断nums[mid]的val，如果val跟target同一段，val不变，如果val跟target不通段，就要变成-inf or inf\n\n- nums[mid] 和 target 同一段的条件: \n\n```python\nnums[mid] > nums[0] and target > nums[0]\nor\nnums[mid] < nums[0] and target < nums[0]\n```\n\n\n\n- nums[mid] 和 target 不通段的条件：\n\n```python\nnums[mid] > nums[0] and target < nums[0]\nor\nnums[mid] < nums[0] and target > nums[0]\n```\n\n这样动态的变更nums[mid]的val，保持nums始终与target同段段那部分不变，不同段则变为-inf or inf\n\n```python\nlo = 0\nhi = len(nums) - 1\n\nwhile lo <= hi:\n\tmid = (lo+hi) // 2\n\tval = nums[mid]\n\tif (val > nums[0] ) == (target > nums[0]):\n\t\tpass\n\telse:\n\t\tval =  float('-inf') if target < nums[0] else float('inf')\n   \n\tif val < target:\n\t\tlo = mid + 1\n\telif val > target:\n\t\thi = mid - 1\n\telse:\n\t\treturn mid\n      \n  return -1\n```\n\nTime Complexity: $O(log(n))$\n\nSpace Complexity: $O(1)$\n\n#### 解法三\n\n　　基于一个事实，数组从任意位置劈开后，至少有一半是有序的\n\n　　这里只需要判断二分法的其中一半是否有序， 再根据有序的这一部分判断target是否包含其中\n\n```python\nlo = 0\nhi = len(nums) - 1\n\nwhile lo <= hi:\n\tmid = (lo+hi) // 2\n\tval = nums[mid]\n\tif target == val:\n\t\treturn mid\n\t\t# lo-mid 有序\n\tif nums[lo] <= val:\n\t\tif target >= nums[lo] and target < nums[mid]:\n\t\t\thi = mid - 1\n\t\telse:\n\t\t\tlo = mid + 1\n\telse: # mid-hi 有序\n\t\tif target > nums[mid] and target <= nums[hi]:\n\t\t\tlo = mid + 1\n\t\telse:\n\t\t\thi = mid - 1\n```\n\nTime Complexity: $O(log(n))$\n\nSpace Complexity: $O(1)$\n\n\n\n### Conclusion\n\n　　$log(n)$的时间复杂度，基本都是二分法的变体，平日自己对二分法过于忽视，导致理解不深刻，所以这个题目做起来才比较费劲。除此之外，解法二巧妙的利用了target 和 nums[mid]是否同一个分割排序段的条件，来简化判断，同时动态的更新nums[mid]值的方法很有启发性，比自己笨拙的设置一大堆if条件，结果自己都搞不清楚状况来的优雅许多。解法一二都是利用抽取出的关键信息来还原二分搜索法，解法三，则是试图找到target分段之前，获取排序信息，再以及排序信息搜索target，题目顿时变的简单了许多。\n\n\n\nReference:\n\n[LeetCode 33. Search in Rotated Sorted Array](https://leetcode.wang/leetCode-33-Search-in-Rotated-Sorted-Array.html)\n\n","source":"_posts/LeetCode-33-Search-in-Rotated-Sorted-Array.md","raw":"---\ntitle: LeetCode 33. Search in Rotated Sorted Array\ntoc: true\nmathjax: true\ntop: false\ncover: true\ndate: 2020-11-17 16:19:58\nupdated:\ncategories: Algorithm\ntags:\n\t- Algorithm\n\t- LeetCode\n---\n\n### LeetCode 33. Search in Rotated Sorted Array\n\n![](img1.png)\n\n　　应该是二分法的变种优化，但是实际分析的过程中，自己的想法被太多的if条件给弄晕了，虽然勉强解出来了，但是更多是凭借临场发挥来作出的，至于宏观的算法原理，自己仍然理解不够透彻，借助讨论区的总结出几个好玩的解法\n\n#### 解法一\n\n　　最直接的想法是找到数组的偏移量，通过还原数组为排序数组的方式，最终利用二分法搜索target ，但是自己并没有想出可以$O(log(n))$搜索偏移量的方法，所以没有深入思考。借助讨论区的提醒，其实偏移量就是最小值的下标，所以只需要找到一种在$O(log(n))$下找到最小值的方法，这里仍然是用二分法，只不过start, mid, end 之间比较关系是不同的\n\n- start 和 mid:\n\n　mid > start : 最小值 in left\n\n![](img3.jpg)\n\n　mid < start : 最小值仍然 in left:\n\n![](img2.jpg)\n\n- mid 和 end\n\n　　mid < end: 最小值 in left，end = mid\n\n![](img6.jpg)\n\n　　mid > end : 最小值 in right, start = mid\n\n![](img7.jpg)\n\n　　只需要mid 和 end就可以利用二分法搜索最小值，只不过需要注意的一点是，可能会出现死循环\n\n![](img8.jpg)\n\n　　主要原因应该是，当最小值in right 的时候，mid 原地更新，而在in right 的情况，实际上mid所在的value不可能是最小值，所以 start = mid + 1 即可\n\n```python\nwhile start < end:\n\tmid = (start + end ) //2\n\tif nums[start] > nums[end]:\n\t\tstart = mid + 1\n\telse:\n\t\tend = mid\nbias = start\n```\n\n　　最后利用偏移量，转换mid 为 偏移后的mid，二分搜索target\n\n```python\nstart = 0\nend = len(nums) - 1\nwhile start <= end:\n\tmid = (start+end) // 2\n\tmid_pos = (mid + bias) \t% len(nums)\n\tval = nums[mid_pos]\n\tif target == value:\n\t\treturn mid_pos\n\tif target < value:\n\t\tend = mid - 1\n\telse:\n\t\tstart = mid + 1\n    \nreturn -1\n```\n\n\n\nTime Complexity: $O(log(n))$\n\nSpace Complexity: $O(1)$\n\n#### 解法二\n\n　　这里还是将被切分的2段分成2个有序数组来处理，比如,[4 5 6 7 1 2 3 ]分成 [4 5 6 7 ]和[1 2 3 ]，判断target位于其中的哪一段，然后将另一段变成-inf or inf，这样做的目的是可以用正常的二分法来搜索改变后的数组。\n\n[ 4 5 6 7 1 2 3] ，如果 target = 5，那么数组可以看做 [ 4 5 6 7 inf inf inf ]。\n\n[ 4 5 6 7 1 2 3] ，如果 target = 2，那么数组可以看做 [ -inf -inf - inf -inf 1 2 3]。\n\n　　实践阶段，只需要判断nums[mid]的val，如果val跟target同一段，val不变，如果val跟target不通段，就要变成-inf or inf\n\n- nums[mid] 和 target 同一段的条件: \n\n```python\nnums[mid] > nums[0] and target > nums[0]\nor\nnums[mid] < nums[0] and target < nums[0]\n```\n\n\n\n- nums[mid] 和 target 不通段的条件：\n\n```python\nnums[mid] > nums[0] and target < nums[0]\nor\nnums[mid] < nums[0] and target > nums[0]\n```\n\n这样动态的变更nums[mid]的val，保持nums始终与target同段段那部分不变，不同段则变为-inf or inf\n\n```python\nlo = 0\nhi = len(nums) - 1\n\nwhile lo <= hi:\n\tmid = (lo+hi) // 2\n\tval = nums[mid]\n\tif (val > nums[0] ) == (target > nums[0]):\n\t\tpass\n\telse:\n\t\tval =  float('-inf') if target < nums[0] else float('inf')\n   \n\tif val < target:\n\t\tlo = mid + 1\n\telif val > target:\n\t\thi = mid - 1\n\telse:\n\t\treturn mid\n      \n  return -1\n```\n\nTime Complexity: $O(log(n))$\n\nSpace Complexity: $O(1)$\n\n#### 解法三\n\n　　基于一个事实，数组从任意位置劈开后，至少有一半是有序的\n\n　　这里只需要判断二分法的其中一半是否有序， 再根据有序的这一部分判断target是否包含其中\n\n```python\nlo = 0\nhi = len(nums) - 1\n\nwhile lo <= hi:\n\tmid = (lo+hi) // 2\n\tval = nums[mid]\n\tif target == val:\n\t\treturn mid\n\t\t# lo-mid 有序\n\tif nums[lo] <= val:\n\t\tif target >= nums[lo] and target < nums[mid]:\n\t\t\thi = mid - 1\n\t\telse:\n\t\t\tlo = mid + 1\n\telse: # mid-hi 有序\n\t\tif target > nums[mid] and target <= nums[hi]:\n\t\t\tlo = mid + 1\n\t\telse:\n\t\t\thi = mid - 1\n```\n\nTime Complexity: $O(log(n))$\n\nSpace Complexity: $O(1)$\n\n\n\n### Conclusion\n\n　　$log(n)$的时间复杂度，基本都是二分法的变体，平日自己对二分法过于忽视，导致理解不深刻，所以这个题目做起来才比较费劲。除此之外，解法二巧妙的利用了target 和 nums[mid]是否同一个分割排序段的条件，来简化判断，同时动态的更新nums[mid]值的方法很有启发性，比自己笨拙的设置一大堆if条件，结果自己都搞不清楚状况来的优雅许多。解法一二都是利用抽取出的关键信息来还原二分搜索法，解法三，则是试图找到target分段之前，获取排序信息，再以及排序信息搜索target，题目顿时变的简单了许多。\n\n\n\nReference:\n\n[LeetCode 33. Search in Rotated Sorted Array](https://leetcode.wang/leetCode-33-Search-in-Rotated-Sorted-Array.html)\n\n","slug":"LeetCode-33-Search-in-Rotated-Sorted-Array","published":1,"_id":"ckhlu9lv60000w328appuchpc","comments":1,"layout":"post","photos":[],"link":"","content":"<h3 id=\"LeetCode-33-Search-in-Rotated-Sorted-Array\"><a href=\"#LeetCode-33-Search-in-Rotated-Sorted-Array\" class=\"headerlink\" title=\"LeetCode 33. Search in Rotated Sorted Array\"></a>LeetCode 33. Search in Rotated Sorted Array</h3><p><img src=\"img1.png\"></p>\n<p>　　应该是二分法的变种优化，但是实际分析的过程中，自己的想法被太多的if条件给弄晕了，虽然勉强解出来了，但是更多是凭借临场发挥来作出的，至于宏观的算法原理，自己仍然理解不够透彻，借助讨论区的总结出几个好玩的解法</p>\n<h4 id=\"解法一\"><a href=\"#解法一\" class=\"headerlink\" title=\"解法一\"></a>解法一</h4><p>　　最直接的想法是找到数组的偏移量，通过还原数组为排序数组的方式，最终利用二分法搜索target ，但是自己并没有想出可以$O(log(n))$搜索偏移量的方法，所以没有深入思考。借助讨论区的提醒，其实偏移量就是最小值的下标，所以只需要找到一种在$O(log(n))$下找到最小值的方法，这里仍然是用二分法，只不过start, mid, end 之间比较关系是不同的</p>\n<ul>\n<li>start 和 mid:</li>\n</ul>\n<p>　mid &gt; start : 最小值 in left</p>\n<p><img src=\"img3.jpg\"></p>\n<p>　mid &lt; start : 最小值仍然 in left:</p>\n<p><img src=\"img2.jpg\"></p>\n<ul>\n<li>mid 和 end</li>\n</ul>\n<p>　　mid &lt; end: 最小值 in left，end = mid</p>\n<p><img src=\"img6.jpg\"></p>\n<p>　　mid &gt; end : 最小值 in right, start = mid</p>\n<p><img src=\"img7.jpg\"></p>\n<p>　　只需要mid 和 end就可以利用二分法搜索最小值，只不过需要注意的一点是，可能会出现死循环</p>\n<p><img src=\"img8.jpg\"></p>\n<p>　　主要原因应该是，当最小值in right 的时候，mid 原地更新，而在in right 的情况，实际上mid所在的value不可能是最小值，所以 start = mid + 1 即可</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">while</span> start <span class=\"token operator\">&lt;</span> end<span class=\"token punctuation\">:</span>\n    mid <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>start <span class=\"token operator\">+</span> end <span class=\"token punctuation\">)</span> <span class=\"token operator\">//</span><span class=\"token number\">2</span>\n    <span class=\"token keyword\">if</span> nums<span class=\"token punctuation\">[</span>start<span class=\"token punctuation\">]</span> <span class=\"token operator\">></span> nums<span class=\"token punctuation\">[</span>end<span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n        start <span class=\"token operator\">=</span> mid <span class=\"token operator\">+</span> <span class=\"token number\">1</span>\n    <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n        end <span class=\"token operator\">=</span> mid\nbias <span class=\"token operator\">=</span> start</code></pre>\n<p>　　最后利用偏移量，转换mid 为 偏移后的mid，二分搜索target</p>\n<pre class=\" language-python\"><code class=\"language-python\">start <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\nend <span class=\"token operator\">=</span> len<span class=\"token punctuation\">(</span>nums<span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span> <span class=\"token number\">1</span>\n<span class=\"token keyword\">while</span> start <span class=\"token operator\">&lt;=</span> end<span class=\"token punctuation\">:</span>\n    mid <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>start<span class=\"token operator\">+</span>end<span class=\"token punctuation\">)</span> <span class=\"token operator\">//</span> <span class=\"token number\">2</span>\n    mid_pos <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>mid <span class=\"token operator\">+</span> bias<span class=\"token punctuation\">)</span>     <span class=\"token operator\">%</span> len<span class=\"token punctuation\">(</span>nums<span class=\"token punctuation\">)</span>\n    val <span class=\"token operator\">=</span> nums<span class=\"token punctuation\">[</span>mid_pos<span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">if</span> target <span class=\"token operator\">==</span> value<span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> mid_pos\n    <span class=\"token keyword\">if</span> target <span class=\"token operator\">&lt;</span> value<span class=\"token punctuation\">:</span>\n        end <span class=\"token operator\">=</span> mid <span class=\"token operator\">-</span> <span class=\"token number\">1</span>\n    <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n        start <span class=\"token operator\">=</span> mid <span class=\"token operator\">+</span> <span class=\"token number\">1</span>\n\n<span class=\"token keyword\">return</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span></code></pre>\n<p>Time Complexity: $O(log(n))$</p>\n<p>Space Complexity: $O(1)$</p>\n<h4 id=\"解法二\"><a href=\"#解法二\" class=\"headerlink\" title=\"解法二\"></a>解法二</h4><p>　　这里还是将被切分的2段分成2个有序数组来处理，比如,[4 5 6 7 1 2 3 ]分成 [4 5 6 7 ]和[1 2 3 ]，判断target位于其中的哪一段，然后将另一段变成-inf or inf，这样做的目的是可以用正常的二分法来搜索改变后的数组。</p>\n<p>[ 4 5 6 7 1 2 3] ，如果 target = 5，那么数组可以看做 [ 4 5 6 7 inf inf inf ]。</p>\n<p>[ 4 5 6 7 1 2 3] ，如果 target = 2，那么数组可以看做 [ -inf -inf - inf -inf 1 2 3]。</p>\n<p>　　实践阶段，只需要判断nums[mid]的val，如果val跟target同一段，val不变，如果val跟target不通段，就要变成-inf or inf</p>\n<ul>\n<li>nums[mid] 和 target 同一段的条件: </li>\n</ul>\n<pre class=\" language-python\"><code class=\"language-python\">nums<span class=\"token punctuation\">[</span>mid<span class=\"token punctuation\">]</span> <span class=\"token operator\">></span> nums<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">and</span> target <span class=\"token operator\">></span> nums<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\n<span class=\"token operator\">or</span>\nnums<span class=\"token punctuation\">[</span>mid<span class=\"token punctuation\">]</span> <span class=\"token operator\">&lt;</span> nums<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">and</span> target <span class=\"token operator\">&lt;</span> nums<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span></code></pre>\n<ul>\n<li>nums[mid] 和 target 不通段的条件：</li>\n</ul>\n<pre class=\" language-python\"><code class=\"language-python\">nums<span class=\"token punctuation\">[</span>mid<span class=\"token punctuation\">]</span> <span class=\"token operator\">></span> nums<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">and</span> target <span class=\"token operator\">&lt;</span> nums<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\n<span class=\"token operator\">or</span>\nnums<span class=\"token punctuation\">[</span>mid<span class=\"token punctuation\">]</span> <span class=\"token operator\">&lt;</span> nums<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">and</span> target <span class=\"token operator\">></span> nums<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span></code></pre>\n<p>这样动态的变更nums[mid]的val，保持nums始终与target同段段那部分不变，不同段则变为-inf or inf</p>\n<pre class=\" language-python\"><code class=\"language-python\">lo <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\nhi <span class=\"token operator\">=</span> len<span class=\"token punctuation\">(</span>nums<span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span> <span class=\"token number\">1</span>\n\n<span class=\"token keyword\">while</span> lo <span class=\"token operator\">&lt;=</span> hi<span class=\"token punctuation\">:</span>\n    mid <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>lo<span class=\"token operator\">+</span>hi<span class=\"token punctuation\">)</span> <span class=\"token operator\">//</span> <span class=\"token number\">2</span>\n    val <span class=\"token operator\">=</span> nums<span class=\"token punctuation\">[</span>mid<span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>val <span class=\"token operator\">></span> nums<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token punctuation\">)</span> <span class=\"token operator\">==</span> <span class=\"token punctuation\">(</span>target <span class=\"token operator\">></span> nums<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">pass</span>\n    <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n        val <span class=\"token operator\">=</span>  float<span class=\"token punctuation\">(</span><span class=\"token string\">'-inf'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">if</span> target <span class=\"token operator\">&lt;</span> nums<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token keyword\">else</span> float<span class=\"token punctuation\">(</span><span class=\"token string\">'inf'</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">if</span> val <span class=\"token operator\">&lt;</span> target<span class=\"token punctuation\">:</span>\n        lo <span class=\"token operator\">=</span> mid <span class=\"token operator\">+</span> <span class=\"token number\">1</span>\n    <span class=\"token keyword\">elif</span> val <span class=\"token operator\">></span> target<span class=\"token punctuation\">:</span>\n        hi <span class=\"token operator\">=</span> mid <span class=\"token operator\">-</span> <span class=\"token number\">1</span>\n    <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> mid\n\n  <span class=\"token keyword\">return</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span></code></pre>\n<p>Time Complexity: $O(log(n))$</p>\n<p>Space Complexity: $O(1)$</p>\n<h4 id=\"解法三\"><a href=\"#解法三\" class=\"headerlink\" title=\"解法三\"></a>解法三</h4><p>　　基于一个事实，数组从任意位置劈开后，至少有一半是有序的</p>\n<p>　　这里只需要判断二分法的其中一半是否有序， 再根据有序的这一部分判断target是否包含其中</p>\n<pre class=\" language-python\"><code class=\"language-python\">lo <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\nhi <span class=\"token operator\">=</span> len<span class=\"token punctuation\">(</span>nums<span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span> <span class=\"token number\">1</span>\n\n<span class=\"token keyword\">while</span> lo <span class=\"token operator\">&lt;=</span> hi<span class=\"token punctuation\">:</span>\n    mid <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>lo<span class=\"token operator\">+</span>hi<span class=\"token punctuation\">)</span> <span class=\"token operator\">//</span> <span class=\"token number\">2</span>\n    val <span class=\"token operator\">=</span> nums<span class=\"token punctuation\">[</span>mid<span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">if</span> target <span class=\"token operator\">==</span> val<span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> mid\n        <span class=\"token comment\" spellcheck=\"true\"># lo-mid 有序</span>\n    <span class=\"token keyword\">if</span> nums<span class=\"token punctuation\">[</span>lo<span class=\"token punctuation\">]</span> <span class=\"token operator\">&lt;=</span> val<span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">if</span> target <span class=\"token operator\">>=</span> nums<span class=\"token punctuation\">[</span>lo<span class=\"token punctuation\">]</span> <span class=\"token operator\">and</span> target <span class=\"token operator\">&lt;</span> nums<span class=\"token punctuation\">[</span>mid<span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n            hi <span class=\"token operator\">=</span> mid <span class=\"token operator\">-</span> <span class=\"token number\">1</span>\n        <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n            lo <span class=\"token operator\">=</span> mid <span class=\"token operator\">+</span> <span class=\"token number\">1</span>\n    <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span> <span class=\"token comment\" spellcheck=\"true\"># mid-hi 有序</span>\n        <span class=\"token keyword\">if</span> target <span class=\"token operator\">></span> nums<span class=\"token punctuation\">[</span>mid<span class=\"token punctuation\">]</span> <span class=\"token operator\">and</span> target <span class=\"token operator\">&lt;=</span> nums<span class=\"token punctuation\">[</span>hi<span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n            lo <span class=\"token operator\">=</span> mid <span class=\"token operator\">+</span> <span class=\"token number\">1</span>\n        <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n            hi <span class=\"token operator\">=</span> mid <span class=\"token operator\">-</span> <span class=\"token number\">1</span></code></pre>\n<p>Time Complexity: $O(log(n))$</p>\n<p>Space Complexity: $O(1)$</p>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>　　$log(n)$的时间复杂度，基本都是二分法的变体，平日自己对二分法过于忽视，导致理解不深刻，所以这个题目做起来才比较费劲。除此之外，解法二巧妙的利用了target 和 nums[mid]是否同一个分割排序段的条件，来简化判断，同时动态的更新nums[mid]值的方法很有启发性，比自己笨拙的设置一大堆if条件，结果自己都搞不清楚状况来的优雅许多。解法一二都是利用抽取出的关键信息来还原二分搜索法，解法三，则是试图找到target分段之前，获取排序信息，再以及排序信息搜索target，题目顿时变的简单了许多。</p>\n<p>Reference:</p>\n<p><a href=\"https://leetcode.wang/leetCode-33-Search-in-Rotated-Sorted-Array.html\">LeetCode 33. Search in Rotated Sorted Array</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"LeetCode-33-Search-in-Rotated-Sorted-Array\"><a href=\"#LeetCode-33-Search-in-Rotated-Sorted-Array\" class=\"headerlink\" title=\"LeetCode 33. Search in Rotated Sorted Array\"></a>LeetCode 33. Search in Rotated Sorted Array</h3><p><img src=\"img1.png\"></p>\n<p>　　应该是二分法的变种优化，但是实际分析的过程中，自己的想法被太多的if条件给弄晕了，虽然勉强解出来了，但是更多是凭借临场发挥来作出的，至于宏观的算法原理，自己仍然理解不够透彻，借助讨论区的总结出几个好玩的解法</p>\n<h4 id=\"解法一\"><a href=\"#解法一\" class=\"headerlink\" title=\"解法一\"></a>解法一</h4><p>　　最直接的想法是找到数组的偏移量，通过还原数组为排序数组的方式，最终利用二分法搜索target ，但是自己并没有想出可以$O(log(n))$搜索偏移量的方法，所以没有深入思考。借助讨论区的提醒，其实偏移量就是最小值的下标，所以只需要找到一种在$O(log(n))$下找到最小值的方法，这里仍然是用二分法，只不过start, mid, end 之间比较关系是不同的</p>\n<ul>\n<li>start 和 mid:</li>\n</ul>\n<p>　mid &gt; start : 最小值 in left</p>\n<p><img src=\"img3.jpg\"></p>\n<p>　mid &lt; start : 最小值仍然 in left:</p>\n<p><img src=\"img2.jpg\"></p>\n<ul>\n<li>mid 和 end</li>\n</ul>\n<p>　　mid &lt; end: 最小值 in left，end = mid</p>\n<p><img src=\"img6.jpg\"></p>\n<p>　　mid &gt; end : 最小值 in right, start = mid</p>\n<p><img src=\"img7.jpg\"></p>\n<p>　　只需要mid 和 end就可以利用二分法搜索最小值，只不过需要注意的一点是，可能会出现死循环</p>\n<p><img src=\"img8.jpg\"></p>\n<p>　　主要原因应该是，当最小值in right 的时候，mid 原地更新，而在in right 的情况，实际上mid所在的value不可能是最小值，所以 start = mid + 1 即可</p>\n<pre><code class=\"python\">while start &lt; end:\n    mid = (start + end ) //2\n    if nums[start] &gt; nums[end]:\n        start = mid + 1\n    else:\n        end = mid\nbias = start</code></pre>\n<p>　　最后利用偏移量，转换mid 为 偏移后的mid，二分搜索target</p>\n<pre><code class=\"python\">start = 0\nend = len(nums) - 1\nwhile start &lt;= end:\n    mid = (start+end) // 2\n    mid_pos = (mid + bias)     % len(nums)\n    val = nums[mid_pos]\n    if target == value:\n        return mid_pos\n    if target &lt; value:\n        end = mid - 1\n    else:\n        start = mid + 1\n\nreturn -1</code></pre>\n<p>Time Complexity: $O(log(n))$</p>\n<p>Space Complexity: $O(1)$</p>\n<h4 id=\"解法二\"><a href=\"#解法二\" class=\"headerlink\" title=\"解法二\"></a>解法二</h4><p>　　这里还是将被切分的2段分成2个有序数组来处理，比如,[4 5 6 7 1 2 3 ]分成 [4 5 6 7 ]和[1 2 3 ]，判断target位于其中的哪一段，然后将另一段变成-inf or inf，这样做的目的是可以用正常的二分法来搜索改变后的数组。</p>\n<p>[ 4 5 6 7 1 2 3] ，如果 target = 5，那么数组可以看做 [ 4 5 6 7 inf inf inf ]。</p>\n<p>[ 4 5 6 7 1 2 3] ，如果 target = 2，那么数组可以看做 [ -inf -inf - inf -inf 1 2 3]。</p>\n<p>　　实践阶段，只需要判断nums[mid]的val，如果val跟target同一段，val不变，如果val跟target不通段，就要变成-inf or inf</p>\n<ul>\n<li>nums[mid] 和 target 同一段的条件: </li>\n</ul>\n<pre><code class=\"python\">nums[mid] &gt; nums[0] and target &gt; nums[0]\nor\nnums[mid] &lt; nums[0] and target &lt; nums[0]</code></pre>\n<ul>\n<li>nums[mid] 和 target 不通段的条件：</li>\n</ul>\n<pre><code class=\"python\">nums[mid] &gt; nums[0] and target &lt; nums[0]\nor\nnums[mid] &lt; nums[0] and target &gt; nums[0]</code></pre>\n<p>这样动态的变更nums[mid]的val，保持nums始终与target同段段那部分不变，不同段则变为-inf or inf</p>\n<pre><code class=\"python\">lo = 0\nhi = len(nums) - 1\n\nwhile lo &lt;= hi:\n    mid = (lo+hi) // 2\n    val = nums[mid]\n    if (val &gt; nums[0] ) == (target &gt; nums[0]):\n        pass\n    else:\n        val =  float(&#39;-inf&#39;) if target &lt; nums[0] else float(&#39;inf&#39;)\n\n    if val &lt; target:\n        lo = mid + 1\n    elif val &gt; target:\n        hi = mid - 1\n    else:\n        return mid\n\n  return -1</code></pre>\n<p>Time Complexity: $O(log(n))$</p>\n<p>Space Complexity: $O(1)$</p>\n<h4 id=\"解法三\"><a href=\"#解法三\" class=\"headerlink\" title=\"解法三\"></a>解法三</h4><p>　　基于一个事实，数组从任意位置劈开后，至少有一半是有序的</p>\n<p>　　这里只需要判断二分法的其中一半是否有序， 再根据有序的这一部分判断target是否包含其中</p>\n<pre><code class=\"python\">lo = 0\nhi = len(nums) - 1\n\nwhile lo &lt;= hi:\n    mid = (lo+hi) // 2\n    val = nums[mid]\n    if target == val:\n        return mid\n        # lo-mid 有序\n    if nums[lo] &lt;= val:\n        if target &gt;= nums[lo] and target &lt; nums[mid]:\n            hi = mid - 1\n        else:\n            lo = mid + 1\n    else: # mid-hi 有序\n        if target &gt; nums[mid] and target &lt;= nums[hi]:\n            lo = mid + 1\n        else:\n            hi = mid - 1</code></pre>\n<p>Time Complexity: $O(log(n))$</p>\n<p>Space Complexity: $O(1)$</p>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>　　$log(n)$的时间复杂度，基本都是二分法的变体，平日自己对二分法过于忽视，导致理解不深刻，所以这个题目做起来才比较费劲。除此之外，解法二巧妙的利用了target 和 nums[mid]是否同一个分割排序段的条件，来简化判断，同时动态的更新nums[mid]值的方法很有启发性，比自己笨拙的设置一大堆if条件，结果自己都搞不清楚状况来的优雅许多。解法一二都是利用抽取出的关键信息来还原二分搜索法，解法三，则是试图找到target分段之前，获取排序信息，再以及排序信息搜索target，题目顿时变的简单了许多。</p>\n<p>Reference:</p>\n<p><a href=\"https://leetcode.wang/leetCode-33-Search-in-Rotated-Sorted-Array.html\">LeetCode 33. Search in Rotated Sorted Array</a></p>\n"},{"title":"同步和异步编程(如何并行写同一个日志文件实践)","toc":true,"mathjax":true,"top":false,"cover":true,"date":"2020-11-23T08:27:20.000Z","updated":"2020-12-30T09:43:03.929Z","_content":"\n　　最近遇到如何并行写日志文件的问题，查看之前的代码，发现是利用RabbitMQ 作为消息中转，把并行的日志流通过HTTP 请求整合成串行的方式，然后通过一个单独的 Rabbit-Consumer 来写入文件，这样就成功避免了多个进程同时写一个文件造成的加锁问题，但是我觉得日志这种I/O为主的任务，动用RabbitMQ 有点大材小用的感觉，Producer-Consumer的总体思想可以不变，但是可以用其他更加轻量级的编程模型来实现，也借此重新优化一下日志输出的，毕竟之前的日志输出感觉一片混乱，在自己熟悉代码的情况下，仍然会有些理解困难，这跟当初自己写代码的时候不重视日志输出这一块有关，甚至很多情急之下写下的print打印信息都最后都没有处理。\n\n　　在开始之前，必须要回忆一些似曾相识，却又容易混淆的基本概念，因为在不区分这些基本OS名词之前，根本就完全无法下手，不知道应该如何正确的实践，具体到每个代码跟这些概念是如何结合在一起的，基本上完全不知道，学生时代留下的坏习惯作怪，理论脱离实际，这时候也才意识到书到用时方恨少。\n\n#### 阻塞与非阻塞\n\n　　这个概念有点复杂，来自OS中线程/进程的生命周期，诸如，就绪，运行，阻塞 ，但是如果仅仅按照这个方向理解，那就比较简单，仅仅就是线程/进程调度程序根据一定的策略，把线程/进程制定相应的状态，来安排和调度CPU资源；然而在OS之中，阻塞的原因很多，并非只是单纯的IO，严格上来讲应该是跟IO类似的原因被阻塞，即请求的资源无法得到，所以这里的阻塞调用，则是更加强调的是，单线程/进程在运行时，那些一定会被阻塞的编程模式，最重要的一个就是同步的编程模式，一旦请求的资源不具备，程序立刻陷入等待，这样就会特别容易被OS识别为闲置的线程/进程，并很快被阻塞。相反，异步则是一种将等待的资源交给外部程序和硬件，要么是子线程/进程，要么就是外部设备，总之试图通过改变通信模式的方式，使主程序始终维持运行态，这样就达到不被阻塞的目的。这就对编程模式有比较大的变动，因为主程序需要应对请求资源返回的各种情况需要预先有响应的程序段来处理，也就是常见的编程过程中回调函数的编写。\n\n> 阻塞调用是指调用结果返回之前，调用者会进入阻塞状态等待。只有在得到结果之后才会返回。\n>\n> 非阻塞调用是指在不能立刻得到结果之前，该函数不会阻塞当前线程，而会立刻返回。\n\n阻塞调用：比如 socket 的 recv()，调用这个函数的线程如果没有数据返回，它会一直阻塞着，也就是 recv() 后面的代码都不会执行了，程序就停在 recv() 这里等待，所以一般把 recv() 放在单独的线程里调用。\n\n非阻塞调用：比如非阻塞socket 的 send()，调用这个函数，它只是把待发送的数据复制到TCP输出缓冲区中，就立刻返回了，线程并不会阻塞，数据有没有发出去 send() 是不知道的，不会等待它发出去才返回的。\n\n#### 并发与并行\n\n基本还记得最显著的区别\n\n> 并发是指一个时间段内，有几个程序都在同一个CPU上运行，但任意一个时刻点上只有一个程序在处理机上运行。\n>\n> 并行是指一个时间段内，有几个程序都在几个CPU上运行，任意一个时刻点上，有多个程序在同时运行，并且多道程序之间互不干扰。 两者区别如下图\n\n![](1.jpeg)\n\n![](2.jpeg)\n\n#### 同步与异步\n\n这个有点基本印象，经过很多次反复理解，依然还是没有很清晰掌握\n\n> 同步：在发出一个同步调用时，在没有得到结果之前，该调用就不返回\n>\n> 异步：在发出一个异步调用后，调用者不会立刻得到结果，该调用就返回了\n\n　　同步和阻塞， 异步和非阻塞概念非常类似，但是所涉及的范围还是有所不同，总的来讲应该是2个独立的概念体系。\n\n　　同步和异步，应该是编程模型中，约定好的一种线程/进程间互相通信的方式，同步则是比较自然的一种程序运作方式，就是自顶向下，资源缺失，主程序等待资源准备，资源就绪继续运行；然而在OS系统调度的指挥下，同步很明显的缺点就是主程序无差别的等待所有资源，很容易程序会因为一些无关紧要的情况而一直处于等待，这其中就有比较大的优化空间；所以根据这种情况一种自然的解决办法是，让程序对各种资源缺失的情况能有对应的方案，最终形成了异步的通信策略，相比于定义上的，调用之后立刻返回这样不知所云的解释，我的理解是，程序是制定了一系列某些资源缺失或者不可用时，程序应该如何运作的逻辑，并且为了保持主程序运行，把程序调用之后的所有职责都交给了子程序，即子程序调用之后，应该由子程序来负责通知主程序，而不是跟同步那样，主程序持续等待子程序准备就绪，而原地踏步；这里的主要区别就是异步时候，主程序调用之后立刻转入下一步不需要这个资源的逻辑，有点类似于，乐观锁和悲观锁的性质，不加判断的断定后续逻辑分支，直接运行。缺点是，感觉异步编程为了解决临界区问题和通信问题，有很多复杂的管控逻辑需要解决，简单来说就是很多坑。好处是，如果这些坑都能解决，程序运行效率会有比较大的提升。\n\n　　如果同步异步是编程模式的话，那么阻塞和非阻塞相对来说我认为是一个并非严格的概念，如果调用必定会导致主程序闲置，进而被置入阻塞状态，那么就是阻塞，如果调用不会导致主程序闲置，那么就是非阻塞。这其中的关键点就是OS的进程/线程调度，而不是调用的问题，然而阻塞的定义中常常会加入调用是否立刻返回的解释，常常让我跟同步异步的概念分不清楚，诚然调用不返回在当今的OS下一定会引起主程序闲置并阻塞，但是主因必然是主程序闲置等待，如果异步通信，但是仍然没有后续程序逻辑，显然异步程序也会有阻塞的风险。所以，我觉得阻塞这个更应该是一个OS的结果，而不是原因。\n\n> 阻塞和非阻塞关注的是程序等待调用结果时的状态，是闲置还是持续运行\n\n线程与进程\n\n　　曾经考试的时候定义背的滚瓜烂熟，然而却完全没有代码经验，感觉彼时都到门口了，缺乏临门一脚，直到现在又千辛万苦来回到这里，重新完成当初没有做到的事情。\n\n　　这里我在实践中，线程跟并发 搞混了，以至于以为线程之间必然是并发，所以有可能分时共享cuda 的引用，认为利用多线程就可以优化GPU利用率。最后实践中，怎么都达不到预想的目的。仔细想想，尽管同一个进程的资源可以被进程内的所有线程共享，这样如果多个线程是分时共享的，那么cuda引用作为一个共享资源，就可以达到并发使用的目的；但是这个前提是，线程必须是并发的，那么线程真的是并发的吗？如果是曾经的单核单线程CPU，答案是肯定的，现在的基本都是多核多线程CPU了，所以线程级别肯定是并行的，那么cuda引用则必然作为一个临界共享资源，被多个线程抢占使用，多线程并行退化成多线程并发，虽然预想的目的似乎达到了，但是貌似并没有达到优化的目的，至少跟之前编写的单线程基本性能是一样的。\n\n　　其次是Python的GIL锁, Global Interpreter Lock，即任何Python 线程执行，只有GIL锁的线程才可以运行，也就是说一个进程下的多线程在Python下，永远只有一个线程在运行\n\n```python\nimport threading, multiprocessing\n\ndef loop():\n    x = 0\n    while True:\n        x = x^1\n\t\t\nfor i in range(multiprocessing.cpu_count()):\n    t = threading.Thread(target = loop)\n    t.start()\n```\n\n上述程序在多核CPU下占有率只有100%，但是C++/Java改写的死循环，则可以每个核心都跑完，即4核心400%\n\n更坑的是，GIL锁每个线程只有执行100条字节码的机会，那么有效的利用cuda资源则成为空谈，因为频繁的显存数据进出，比较浪费时间。所以python下一般都是利用多个进程来实现并行，因为多个进程有独立的GIL锁，互不影响。\n\n　　惭愧的是，自己写了这么久的Python，居然才第一次面对并行化问题，才知道GIL锁有这么多繁琐的坑。理论上证实这样的不可行之后，有点失望，但是也还是保留机会来完成并行写日志这个小任务的，毕竟通过HTTP接口，再怎么并发，框架还是很好的完成的这些繁琐的坑，只需要在producer-consumer 框架下，实现并行接受，串行消费就可以了，这还是比较基础的一个改写，对自己还是有信心的。\n\n　　线程安全，进程安全，意思就是在多线程或者多进程下，可以在代码中不加改变的直接使用，库和框架会自动解决并发和并行问题。其中 queue.Queue 就是线程安全的，下面就是利用Queue来实现进程间通信的producer-consumer\n\n```python\nfrom queue import Queue\nfrom threading import Thread\n\ndef producer(out_q):\n\twhile True:\n\t\tout_q.put(data)\n        \ndef consumer(in_q):\n\twhile True:\n\t\tdata = in_q.get()\n\t\t\t...\n    \t# 通过q通知 任务完成\n      in_q.task_done()\n        \nq = Queue()\nt1 = Thread(target=consumer, args=(q,))\nt2 = Thread(taget=producer, args=(q,))\nt1.start()\nt2.start()\n\n# wait for all produced items to be consumed\nq.join()\n```\n\n\n\n　　优化点，队列的流量控制，q需要指定大小，必要时候阻塞Queue，以避免连锁效应导致程序运行失常，同时阻塞的队列也会形成死锁，通过非阻塞和设定超时时间，来处理队列满或者超时的情况，避免阻塞\n\n```python\nq = Queue(100)\n\ntry:\n    data = q.get(block=False)\nexcept queue.Empty:\n    pass\n    \ntry:\n    q.put(item, timeout=5.0)\nexcept queue.Full:\n    log.warning('queued item %r discarded!', item)\n```\n\n\n\n### 最终方案\n\n　　利用tornado接受并发的http请求，解析request_body之后，直接入列队伍q.put()，这里利用了tornado 的异步接受并发的特点，直接简化producer过程。接着只需要编写consumer，取出数据，写到文件即可\n\n```python\nfrom eliot import log_call, start_action, to_file\nclass LogHandler(tornado.web.RequestHandler):\n\t# eliot 直接利用装饰器来添加日志功能，只记录input, output\n\t@log_call\n\tdef post(self, *args, **kwargs):\n\t\ttry:\n\t\t\tself.timestamp = time.ctime()\n\t\t\tself.recv_json = json.loads(self.request.body, strict=False)\n\t\t\tq.put((self.timestamp, self.recv_json ))\n\t\t\toutput_dic = {\n                'status': '1',\n                'result': 'success'\n      }\n\t\texcept Exception as e:\n\t\t\tprint('LogHandler Error: ', e)\n\t\t\treturn\n          \n\n# eliot.start_action 通过 with 来记录函数段，有点繁琐，但是对于某段代码的日志记录很方便\ndef consume_msg(queue):\n\twhile True:\n\t\twith start_action(action_type='consume_msg', lenghth = queue.qsize()):\n\t\t\ttry:\n\t\t\t\ttimestamp, msg= q.get()\n\t\t\t\tstart_time = time.time()\n\t\t\t\twith start_action(action_type='save_request_body', timestamp=timestamp,msg=msg):\n\t\t\t\t\tsave_body(msg)\n\t\t\texcept Exception as e:\n\t\t\t\twith start_action(action_type='consume_msg exceptiosn', timestamp=timestamp,e=e, msg=msg):\n\t\t\t\t\tprint('consume_mgs exception : '  , e)\n                    \nconsumer = Process(target=consume_msg, args=(q, ))\nconsumer.daemon = True # 守护进程\nconsumer.start()\n```\n\n\n\n　　至此，多进程的日志记录完成，同时也用上了之前一直想要用eliot日志系统，让日志看起来清爽了许多。\n\n### RabbiMQ\n\nRabbiMQ 的方案也写进来，毕竟原理大同小异，只是初始化和连接的部分需要重新封装一下，而且适用性更广。\n\n```python\n# rabbitmq init\ncredentials = pika.PlainCredentials(RabbitMQ.USERNAME, RabbitMQ.USERPWD)\nconnection = pika.BlockingConnection(\n                pika.ConnectionParameters(RabbitMQ.HOST, RabbitMQ.PORT,\n                                          credentials=credentials)\n    )\n# channel init and declare\nchannel = connection.channel()\nchannel.exchange_declare(exchange=RabbitMQ.EXCHANGE,\n                             exchange_type=RabbitMQ.EXCHANGE_TYPE,\n                             durable=True)\nchannel.queue_declare(queue=RabbitMQ.QUEUE_NAME_ALG, durable=True)\nchannel.queue_bind(exchange=RabbitMQ.EXCHANGE,\n                       queue=RabbitMQ.QUEUE_NAME_ALG,\n                       routing_key=RabbitMQ.ROUTING_KEY)\nchannel.basic_qos(prefetch_count=1)    # 能者多劳\nchannel.basic_consume(on_message_callback=consumer,\n                          queue=RabbitMQ.QUEUE_NAME_ALG, auto_ack=False)\nprint(' [*] Waiting for messages...')\nchannel.start_consuming()\n\ndef consumer(ch, method, properties, body):\n\tbody = json.loads(body, strict=False)\n\tflag = body['flag']\n\t# process body\n  \n\t# Ack manually\n\tch.basic_ack(delivery_tag=method.delivery_tag)\n  \n\n  \n# producer.py\ntry:\n\tcredentials = pika.PlainCredentials(\n                RabbitMQ.USERNAME, RabbitMQ.USERPWD)\n\tconnection = pika.BlockingConnection(\n                pika.ConnectionParameters(RabbitMQ.HOST, RabbitMQ.PORT,\n                                          credentials=credentials)\n            )\n\tchannel = self.connection.channel()\n\tchannel.exchange_declare(exchange=RabbitMQ.EXCHANGE,                                      \t\t\t\t\t\t\t\t\t\t\t\t\t\texchange_type=RabbitMQ.EXCHANGE_TYPE,\n                                          durable=True)\nexcept Exception as e:\n\tpass\n        \nsent_msg = {...}\nbody = json.dumps(sent_msg)\nchannel.basic_publish(exchange=RabbitMQ.EXCHANGE, \t\t\t\t\n                      routing_key=RabbitMQ.ROUTING_KEY,\n                      body=body, \n                      properties=pika.BasicProperties(delivery_mode=2))\n\nchannel.close()\nconnection.close()\n```\n\n\n\n### 计算密集型 和 IO密集型的思考\n\n　　当初写论文的时候，就在机器上跑了很多类型的负载任务，最直观的感受就是，计算密集型的负载比较难找，相对于纯粹的计算任务，很多实际的任务都会因为千奇百怪的原因被消耗在IO上，导致运行效率低下。\n\n> 计算密集型，代码运行效率为主，因为主要时间都消耗在CPU上，Python的脚本语言不适合这类任务，最好用C++/Java实现\n>\n> IO密集型，大部分时间消耗在IO上，C++/Java无法发挥优势，代码量少开发效率最高的Python具备优势\n\n　　所以，Python最好最为原型开发，保持逻辑流程通顺的前提下，可以用其他高效的语言如C++/Java改写最耗时的模块，迭代优化开发。\n\n### 协程：单线程异步\n\n　　虽然Python无法利用多线程的并行优势，但是还是有优化的手段的。如果把条件限制到单线程，那么有一个很自然的优化方式就是单线程异步，也就是只用单线程的情况下，实现程序内部的并发，这个并发是程序可控的，而不是由操作系统来根据一定策略加锁。\n\n　　Python下有协程库asyncio内置了异步IO的支持。编程模型是一个消息循环，从asyncio获取EventLoop引用，把需要执行的协程作为参数传递给EventLoop，实现异步IO,下面直接给出协程版本的producer-consumer\n\n```python\nimport asyncio\nimport random\n\nasync def producer(queue, n):\n\tfor x in range(1, n+1):\n\t\tprint('producing : ', x)    \n\t\t# simulate io job\n\t\tawait asyncio.sleep(random.random())\n\t\titem = str(x)\n\t\tawait queue.put(item)\n        \nasync def consume(queue):\n\twhile True:\n\t\titem = await queue.get()\n    # process item\n    print('consuming : ', item)\n    # simulate io \n    await asyncio.sleep(random.random())\n    # notify queue that the item has been processed\n    queue.task_done()\n        \nasync def run(n):\n    queue = asyncio.Queue()\n    # schedule consumer\n    consumer= asyncio.ensure_future(consume(queue))\n    await producer(queue, n)\n    # wait until consumer processed all items\n    await queue.join()\n    # consumer is still awaiting item, cancel it\n    consumer.cancel()\n    \nloop = asyncio.get_event_loop()\nloop.run_until_complete(run(10))\nloop.close()\n```\n\n\n\n### Conclusion\n\n　　至此，关于同步异步的一些知识点和实践问题梳理完毕，协程的实践经验还是有点欠缺，以后看框架源码的时候，需要多学习和思考。\n\n\n\nReferences:\n\n[Producer/consumer-examlpe_asyncio](https://asyncio.readthedocs.io/en/latest/producer_consumer.html)\n\n[协程-廖雪峰](https://www.liaoxuefeng.com/wiki/1016959663602400/1017968846697824)\n\n[进程-线程-廖雪峰](https://www.liaoxuefeng.com/wiki/1016959663602400/1017631469467456)\n\n[线程间通信-Python_CookBook](https://python3-cookbook.readthedocs.io/zh_CN/latest/c12/p03_communicating_between_threads.html)\n\n[Python下的多进程日志记录方案](https://juejin.cn/post/6844904039210024967#heading-3)\n\n","source":"_posts/同步和异步编程-如何并行写同一个日志文件实践.md","raw":"---\ntitle: 同步和异步编程(如何并行写同一个日志文件实践)\ntoc: true\nmathjax: true\ntop: false\ncover: true\ndate: 2020-11-23 16:27:20\nupdated:\ncategories: Algorithms\ntags: \n\t- Algorithms\n\t- OS\n---\n\n　　最近遇到如何并行写日志文件的问题，查看之前的代码，发现是利用RabbitMQ 作为消息中转，把并行的日志流通过HTTP 请求整合成串行的方式，然后通过一个单独的 Rabbit-Consumer 来写入文件，这样就成功避免了多个进程同时写一个文件造成的加锁问题，但是我觉得日志这种I/O为主的任务，动用RabbitMQ 有点大材小用的感觉，Producer-Consumer的总体思想可以不变，但是可以用其他更加轻量级的编程模型来实现，也借此重新优化一下日志输出的，毕竟之前的日志输出感觉一片混乱，在自己熟悉代码的情况下，仍然会有些理解困难，这跟当初自己写代码的时候不重视日志输出这一块有关，甚至很多情急之下写下的print打印信息都最后都没有处理。\n\n　　在开始之前，必须要回忆一些似曾相识，却又容易混淆的基本概念，因为在不区分这些基本OS名词之前，根本就完全无法下手，不知道应该如何正确的实践，具体到每个代码跟这些概念是如何结合在一起的，基本上完全不知道，学生时代留下的坏习惯作怪，理论脱离实际，这时候也才意识到书到用时方恨少。\n\n#### 阻塞与非阻塞\n\n　　这个概念有点复杂，来自OS中线程/进程的生命周期，诸如，就绪，运行，阻塞 ，但是如果仅仅按照这个方向理解，那就比较简单，仅仅就是线程/进程调度程序根据一定的策略，把线程/进程制定相应的状态，来安排和调度CPU资源；然而在OS之中，阻塞的原因很多，并非只是单纯的IO，严格上来讲应该是跟IO类似的原因被阻塞，即请求的资源无法得到，所以这里的阻塞调用，则是更加强调的是，单线程/进程在运行时，那些一定会被阻塞的编程模式，最重要的一个就是同步的编程模式，一旦请求的资源不具备，程序立刻陷入等待，这样就会特别容易被OS识别为闲置的线程/进程，并很快被阻塞。相反，异步则是一种将等待的资源交给外部程序和硬件，要么是子线程/进程，要么就是外部设备，总之试图通过改变通信模式的方式，使主程序始终维持运行态，这样就达到不被阻塞的目的。这就对编程模式有比较大的变动，因为主程序需要应对请求资源返回的各种情况需要预先有响应的程序段来处理，也就是常见的编程过程中回调函数的编写。\n\n> 阻塞调用是指调用结果返回之前，调用者会进入阻塞状态等待。只有在得到结果之后才会返回。\n>\n> 非阻塞调用是指在不能立刻得到结果之前，该函数不会阻塞当前线程，而会立刻返回。\n\n阻塞调用：比如 socket 的 recv()，调用这个函数的线程如果没有数据返回，它会一直阻塞着，也就是 recv() 后面的代码都不会执行了，程序就停在 recv() 这里等待，所以一般把 recv() 放在单独的线程里调用。\n\n非阻塞调用：比如非阻塞socket 的 send()，调用这个函数，它只是把待发送的数据复制到TCP输出缓冲区中，就立刻返回了，线程并不会阻塞，数据有没有发出去 send() 是不知道的，不会等待它发出去才返回的。\n\n#### 并发与并行\n\n基本还记得最显著的区别\n\n> 并发是指一个时间段内，有几个程序都在同一个CPU上运行，但任意一个时刻点上只有一个程序在处理机上运行。\n>\n> 并行是指一个时间段内，有几个程序都在几个CPU上运行，任意一个时刻点上，有多个程序在同时运行，并且多道程序之间互不干扰。 两者区别如下图\n\n![](1.jpeg)\n\n![](2.jpeg)\n\n#### 同步与异步\n\n这个有点基本印象，经过很多次反复理解，依然还是没有很清晰掌握\n\n> 同步：在发出一个同步调用时，在没有得到结果之前，该调用就不返回\n>\n> 异步：在发出一个异步调用后，调用者不会立刻得到结果，该调用就返回了\n\n　　同步和阻塞， 异步和非阻塞概念非常类似，但是所涉及的范围还是有所不同，总的来讲应该是2个独立的概念体系。\n\n　　同步和异步，应该是编程模型中，约定好的一种线程/进程间互相通信的方式，同步则是比较自然的一种程序运作方式，就是自顶向下，资源缺失，主程序等待资源准备，资源就绪继续运行；然而在OS系统调度的指挥下，同步很明显的缺点就是主程序无差别的等待所有资源，很容易程序会因为一些无关紧要的情况而一直处于等待，这其中就有比较大的优化空间；所以根据这种情况一种自然的解决办法是，让程序对各种资源缺失的情况能有对应的方案，最终形成了异步的通信策略，相比于定义上的，调用之后立刻返回这样不知所云的解释，我的理解是，程序是制定了一系列某些资源缺失或者不可用时，程序应该如何运作的逻辑，并且为了保持主程序运行，把程序调用之后的所有职责都交给了子程序，即子程序调用之后，应该由子程序来负责通知主程序，而不是跟同步那样，主程序持续等待子程序准备就绪，而原地踏步；这里的主要区别就是异步时候，主程序调用之后立刻转入下一步不需要这个资源的逻辑，有点类似于，乐观锁和悲观锁的性质，不加判断的断定后续逻辑分支，直接运行。缺点是，感觉异步编程为了解决临界区问题和通信问题，有很多复杂的管控逻辑需要解决，简单来说就是很多坑。好处是，如果这些坑都能解决，程序运行效率会有比较大的提升。\n\n　　如果同步异步是编程模式的话，那么阻塞和非阻塞相对来说我认为是一个并非严格的概念，如果调用必定会导致主程序闲置，进而被置入阻塞状态，那么就是阻塞，如果调用不会导致主程序闲置，那么就是非阻塞。这其中的关键点就是OS的进程/线程调度，而不是调用的问题，然而阻塞的定义中常常会加入调用是否立刻返回的解释，常常让我跟同步异步的概念分不清楚，诚然调用不返回在当今的OS下一定会引起主程序闲置并阻塞，但是主因必然是主程序闲置等待，如果异步通信，但是仍然没有后续程序逻辑，显然异步程序也会有阻塞的风险。所以，我觉得阻塞这个更应该是一个OS的结果，而不是原因。\n\n> 阻塞和非阻塞关注的是程序等待调用结果时的状态，是闲置还是持续运行\n\n线程与进程\n\n　　曾经考试的时候定义背的滚瓜烂熟，然而却完全没有代码经验，感觉彼时都到门口了，缺乏临门一脚，直到现在又千辛万苦来回到这里，重新完成当初没有做到的事情。\n\n　　这里我在实践中，线程跟并发 搞混了，以至于以为线程之间必然是并发，所以有可能分时共享cuda 的引用，认为利用多线程就可以优化GPU利用率。最后实践中，怎么都达不到预想的目的。仔细想想，尽管同一个进程的资源可以被进程内的所有线程共享，这样如果多个线程是分时共享的，那么cuda引用作为一个共享资源，就可以达到并发使用的目的；但是这个前提是，线程必须是并发的，那么线程真的是并发的吗？如果是曾经的单核单线程CPU，答案是肯定的，现在的基本都是多核多线程CPU了，所以线程级别肯定是并行的，那么cuda引用则必然作为一个临界共享资源，被多个线程抢占使用，多线程并行退化成多线程并发，虽然预想的目的似乎达到了，但是貌似并没有达到优化的目的，至少跟之前编写的单线程基本性能是一样的。\n\n　　其次是Python的GIL锁, Global Interpreter Lock，即任何Python 线程执行，只有GIL锁的线程才可以运行，也就是说一个进程下的多线程在Python下，永远只有一个线程在运行\n\n```python\nimport threading, multiprocessing\n\ndef loop():\n    x = 0\n    while True:\n        x = x^1\n\t\t\nfor i in range(multiprocessing.cpu_count()):\n    t = threading.Thread(target = loop)\n    t.start()\n```\n\n上述程序在多核CPU下占有率只有100%，但是C++/Java改写的死循环，则可以每个核心都跑完，即4核心400%\n\n更坑的是，GIL锁每个线程只有执行100条字节码的机会，那么有效的利用cuda资源则成为空谈，因为频繁的显存数据进出，比较浪费时间。所以python下一般都是利用多个进程来实现并行，因为多个进程有独立的GIL锁，互不影响。\n\n　　惭愧的是，自己写了这么久的Python，居然才第一次面对并行化问题，才知道GIL锁有这么多繁琐的坑。理论上证实这样的不可行之后，有点失望，但是也还是保留机会来完成并行写日志这个小任务的，毕竟通过HTTP接口，再怎么并发，框架还是很好的完成的这些繁琐的坑，只需要在producer-consumer 框架下，实现并行接受，串行消费就可以了，这还是比较基础的一个改写，对自己还是有信心的。\n\n　　线程安全，进程安全，意思就是在多线程或者多进程下，可以在代码中不加改变的直接使用，库和框架会自动解决并发和并行问题。其中 queue.Queue 就是线程安全的，下面就是利用Queue来实现进程间通信的producer-consumer\n\n```python\nfrom queue import Queue\nfrom threading import Thread\n\ndef producer(out_q):\n\twhile True:\n\t\tout_q.put(data)\n        \ndef consumer(in_q):\n\twhile True:\n\t\tdata = in_q.get()\n\t\t\t...\n    \t# 通过q通知 任务完成\n      in_q.task_done()\n        \nq = Queue()\nt1 = Thread(target=consumer, args=(q,))\nt2 = Thread(taget=producer, args=(q,))\nt1.start()\nt2.start()\n\n# wait for all produced items to be consumed\nq.join()\n```\n\n\n\n　　优化点，队列的流量控制，q需要指定大小，必要时候阻塞Queue，以避免连锁效应导致程序运行失常，同时阻塞的队列也会形成死锁，通过非阻塞和设定超时时间，来处理队列满或者超时的情况，避免阻塞\n\n```python\nq = Queue(100)\n\ntry:\n    data = q.get(block=False)\nexcept queue.Empty:\n    pass\n    \ntry:\n    q.put(item, timeout=5.0)\nexcept queue.Full:\n    log.warning('queued item %r discarded!', item)\n```\n\n\n\n### 最终方案\n\n　　利用tornado接受并发的http请求，解析request_body之后，直接入列队伍q.put()，这里利用了tornado 的异步接受并发的特点，直接简化producer过程。接着只需要编写consumer，取出数据，写到文件即可\n\n```python\nfrom eliot import log_call, start_action, to_file\nclass LogHandler(tornado.web.RequestHandler):\n\t# eliot 直接利用装饰器来添加日志功能，只记录input, output\n\t@log_call\n\tdef post(self, *args, **kwargs):\n\t\ttry:\n\t\t\tself.timestamp = time.ctime()\n\t\t\tself.recv_json = json.loads(self.request.body, strict=False)\n\t\t\tq.put((self.timestamp, self.recv_json ))\n\t\t\toutput_dic = {\n                'status': '1',\n                'result': 'success'\n      }\n\t\texcept Exception as e:\n\t\t\tprint('LogHandler Error: ', e)\n\t\t\treturn\n          \n\n# eliot.start_action 通过 with 来记录函数段，有点繁琐，但是对于某段代码的日志记录很方便\ndef consume_msg(queue):\n\twhile True:\n\t\twith start_action(action_type='consume_msg', lenghth = queue.qsize()):\n\t\t\ttry:\n\t\t\t\ttimestamp, msg= q.get()\n\t\t\t\tstart_time = time.time()\n\t\t\t\twith start_action(action_type='save_request_body', timestamp=timestamp,msg=msg):\n\t\t\t\t\tsave_body(msg)\n\t\t\texcept Exception as e:\n\t\t\t\twith start_action(action_type='consume_msg exceptiosn', timestamp=timestamp,e=e, msg=msg):\n\t\t\t\t\tprint('consume_mgs exception : '  , e)\n                    \nconsumer = Process(target=consume_msg, args=(q, ))\nconsumer.daemon = True # 守护进程\nconsumer.start()\n```\n\n\n\n　　至此，多进程的日志记录完成，同时也用上了之前一直想要用eliot日志系统，让日志看起来清爽了许多。\n\n### RabbiMQ\n\nRabbiMQ 的方案也写进来，毕竟原理大同小异，只是初始化和连接的部分需要重新封装一下，而且适用性更广。\n\n```python\n# rabbitmq init\ncredentials = pika.PlainCredentials(RabbitMQ.USERNAME, RabbitMQ.USERPWD)\nconnection = pika.BlockingConnection(\n                pika.ConnectionParameters(RabbitMQ.HOST, RabbitMQ.PORT,\n                                          credentials=credentials)\n    )\n# channel init and declare\nchannel = connection.channel()\nchannel.exchange_declare(exchange=RabbitMQ.EXCHANGE,\n                             exchange_type=RabbitMQ.EXCHANGE_TYPE,\n                             durable=True)\nchannel.queue_declare(queue=RabbitMQ.QUEUE_NAME_ALG, durable=True)\nchannel.queue_bind(exchange=RabbitMQ.EXCHANGE,\n                       queue=RabbitMQ.QUEUE_NAME_ALG,\n                       routing_key=RabbitMQ.ROUTING_KEY)\nchannel.basic_qos(prefetch_count=1)    # 能者多劳\nchannel.basic_consume(on_message_callback=consumer,\n                          queue=RabbitMQ.QUEUE_NAME_ALG, auto_ack=False)\nprint(' [*] Waiting for messages...')\nchannel.start_consuming()\n\ndef consumer(ch, method, properties, body):\n\tbody = json.loads(body, strict=False)\n\tflag = body['flag']\n\t# process body\n  \n\t# Ack manually\n\tch.basic_ack(delivery_tag=method.delivery_tag)\n  \n\n  \n# producer.py\ntry:\n\tcredentials = pika.PlainCredentials(\n                RabbitMQ.USERNAME, RabbitMQ.USERPWD)\n\tconnection = pika.BlockingConnection(\n                pika.ConnectionParameters(RabbitMQ.HOST, RabbitMQ.PORT,\n                                          credentials=credentials)\n            )\n\tchannel = self.connection.channel()\n\tchannel.exchange_declare(exchange=RabbitMQ.EXCHANGE,                                      \t\t\t\t\t\t\t\t\t\t\t\t\t\texchange_type=RabbitMQ.EXCHANGE_TYPE,\n                                          durable=True)\nexcept Exception as e:\n\tpass\n        \nsent_msg = {...}\nbody = json.dumps(sent_msg)\nchannel.basic_publish(exchange=RabbitMQ.EXCHANGE, \t\t\t\t\n                      routing_key=RabbitMQ.ROUTING_KEY,\n                      body=body, \n                      properties=pika.BasicProperties(delivery_mode=2))\n\nchannel.close()\nconnection.close()\n```\n\n\n\n### 计算密集型 和 IO密集型的思考\n\n　　当初写论文的时候，就在机器上跑了很多类型的负载任务，最直观的感受就是，计算密集型的负载比较难找，相对于纯粹的计算任务，很多实际的任务都会因为千奇百怪的原因被消耗在IO上，导致运行效率低下。\n\n> 计算密集型，代码运行效率为主，因为主要时间都消耗在CPU上，Python的脚本语言不适合这类任务，最好用C++/Java实现\n>\n> IO密集型，大部分时间消耗在IO上，C++/Java无法发挥优势，代码量少开发效率最高的Python具备优势\n\n　　所以，Python最好最为原型开发，保持逻辑流程通顺的前提下，可以用其他高效的语言如C++/Java改写最耗时的模块，迭代优化开发。\n\n### 协程：单线程异步\n\n　　虽然Python无法利用多线程的并行优势，但是还是有优化的手段的。如果把条件限制到单线程，那么有一个很自然的优化方式就是单线程异步，也就是只用单线程的情况下，实现程序内部的并发，这个并发是程序可控的，而不是由操作系统来根据一定策略加锁。\n\n　　Python下有协程库asyncio内置了异步IO的支持。编程模型是一个消息循环，从asyncio获取EventLoop引用，把需要执行的协程作为参数传递给EventLoop，实现异步IO,下面直接给出协程版本的producer-consumer\n\n```python\nimport asyncio\nimport random\n\nasync def producer(queue, n):\n\tfor x in range(1, n+1):\n\t\tprint('producing : ', x)    \n\t\t# simulate io job\n\t\tawait asyncio.sleep(random.random())\n\t\titem = str(x)\n\t\tawait queue.put(item)\n        \nasync def consume(queue):\n\twhile True:\n\t\titem = await queue.get()\n    # process item\n    print('consuming : ', item)\n    # simulate io \n    await asyncio.sleep(random.random())\n    # notify queue that the item has been processed\n    queue.task_done()\n        \nasync def run(n):\n    queue = asyncio.Queue()\n    # schedule consumer\n    consumer= asyncio.ensure_future(consume(queue))\n    await producer(queue, n)\n    # wait until consumer processed all items\n    await queue.join()\n    # consumer is still awaiting item, cancel it\n    consumer.cancel()\n    \nloop = asyncio.get_event_loop()\nloop.run_until_complete(run(10))\nloop.close()\n```\n\n\n\n### Conclusion\n\n　　至此，关于同步异步的一些知识点和实践问题梳理完毕，协程的实践经验还是有点欠缺，以后看框架源码的时候，需要多学习和思考。\n\n\n\nReferences:\n\n[Producer/consumer-examlpe_asyncio](https://asyncio.readthedocs.io/en/latest/producer_consumer.html)\n\n[协程-廖雪峰](https://www.liaoxuefeng.com/wiki/1016959663602400/1017968846697824)\n\n[进程-线程-廖雪峰](https://www.liaoxuefeng.com/wiki/1016959663602400/1017631469467456)\n\n[线程间通信-Python_CookBook](https://python3-cookbook.readthedocs.io/zh_CN/latest/c12/p03_communicating_between_threads.html)\n\n[Python下的多进程日志记录方案](https://juejin.cn/post/6844904039210024967#heading-3)\n\n","slug":"同步和异步编程-如何并行写同一个日志文件实践","published":1,"_id":"ckhvqydbn00006z28dot614mu","comments":1,"layout":"post","photos":[],"link":"","content":"<p>　　最近遇到如何并行写日志文件的问题，查看之前的代码，发现是利用RabbitMQ 作为消息中转，把并行的日志流通过HTTP 请求整合成串行的方式，然后通过一个单独的 Rabbit-Consumer 来写入文件，这样就成功避免了多个进程同时写一个文件造成的加锁问题，但是我觉得日志这种I/O为主的任务，动用RabbitMQ 有点大材小用的感觉，Producer-Consumer的总体思想可以不变，但是可以用其他更加轻量级的编程模型来实现，也借此重新优化一下日志输出的，毕竟之前的日志输出感觉一片混乱，在自己熟悉代码的情况下，仍然会有些理解困难，这跟当初自己写代码的时候不重视日志输出这一块有关，甚至很多情急之下写下的print打印信息都最后都没有处理。</p>\n<p>　　在开始之前，必须要回忆一些似曾相识，却又容易混淆的基本概念，因为在不区分这些基本OS名词之前，根本就完全无法下手，不知道应该如何正确的实践，具体到每个代码跟这些概念是如何结合在一起的，基本上完全不知道，学生时代留下的坏习惯作怪，理论脱离实际，这时候也才意识到书到用时方恨少。</p>\n<h4 id=\"阻塞与非阻塞\"><a href=\"#阻塞与非阻塞\" class=\"headerlink\" title=\"阻塞与非阻塞\"></a>阻塞与非阻塞</h4><p>　　这个概念有点复杂，来自OS中线程/进程的生命周期，诸如，就绪，运行，阻塞 ，但是如果仅仅按照这个方向理解，那就比较简单，仅仅就是线程/进程调度程序根据一定的策略，把线程/进程制定相应的状态，来安排和调度CPU资源；然而在OS之中，阻塞的原因很多，并非只是单纯的IO，严格上来讲应该是跟IO类似的原因被阻塞，即请求的资源无法得到，所以这里的阻塞调用，则是更加强调的是，单线程/进程在运行时，那些一定会被阻塞的编程模式，最重要的一个就是同步的编程模式，一旦请求的资源不具备，程序立刻陷入等待，这样就会特别容易被OS识别为闲置的线程/进程，并很快被阻塞。相反，异步则是一种将等待的资源交给外部程序和硬件，要么是子线程/进程，要么就是外部设备，总之试图通过改变通信模式的方式，使主程序始终维持运行态，这样就达到不被阻塞的目的。这就对编程模式有比较大的变动，因为主程序需要应对请求资源返回的各种情况需要预先有响应的程序段来处理，也就是常见的编程过程中回调函数的编写。</p>\n<blockquote>\n<p>阻塞调用是指调用结果返回之前，调用者会进入阻塞状态等待。只有在得到结果之后才会返回。</p>\n<p>非阻塞调用是指在不能立刻得到结果之前，该函数不会阻塞当前线程，而会立刻返回。</p>\n</blockquote>\n<p>阻塞调用：比如 socket 的 recv()，调用这个函数的线程如果没有数据返回，它会一直阻塞着，也就是 recv() 后面的代码都不会执行了，程序就停在 recv() 这里等待，所以一般把 recv() 放在单独的线程里调用。</p>\n<p>非阻塞调用：比如非阻塞socket 的 send()，调用这个函数，它只是把待发送的数据复制到TCP输出缓冲区中，就立刻返回了，线程并不会阻塞，数据有没有发出去 send() 是不知道的，不会等待它发出去才返回的。</p>\n<h4 id=\"并发与并行\"><a href=\"#并发与并行\" class=\"headerlink\" title=\"并发与并行\"></a>并发与并行</h4><p>基本还记得最显著的区别</p>\n<blockquote>\n<p>并发是指一个时间段内，有几个程序都在同一个CPU上运行，但任意一个时刻点上只有一个程序在处理机上运行。</p>\n<p>并行是指一个时间段内，有几个程序都在几个CPU上运行，任意一个时刻点上，有多个程序在同时运行，并且多道程序之间互不干扰。 两者区别如下图</p>\n</blockquote>\n<p><img src=\"1.jpeg\"></p>\n<p><img src=\"2.jpeg\"></p>\n<h4 id=\"同步与异步\"><a href=\"#同步与异步\" class=\"headerlink\" title=\"同步与异步\"></a>同步与异步</h4><p>这个有点基本印象，经过很多次反复理解，依然还是没有很清晰掌握</p>\n<blockquote>\n<p>同步：在发出一个同步调用时，在没有得到结果之前，该调用就不返回</p>\n<p>异步：在发出一个异步调用后，调用者不会立刻得到结果，该调用就返回了</p>\n</blockquote>\n<p>　　同步和阻塞， 异步和非阻塞概念非常类似，但是所涉及的范围还是有所不同，总的来讲应该是2个独立的概念体系。</p>\n<p>　　同步和异步，应该是编程模型中，约定好的一种线程/进程间互相通信的方式，同步则是比较自然的一种程序运作方式，就是自顶向下，资源缺失，主程序等待资源准备，资源就绪继续运行；然而在OS系统调度的指挥下，同步很明显的缺点就是主程序无差别的等待所有资源，很容易程序会因为一些无关紧要的情况而一直处于等待，这其中就有比较大的优化空间；所以根据这种情况一种自然的解决办法是，让程序对各种资源缺失的情况能有对应的方案，最终形成了异步的通信策略，相比于定义上的，调用之后立刻返回这样不知所云的解释，我的理解是，程序是制定了一系列某些资源缺失或者不可用时，程序应该如何运作的逻辑，并且为了保持主程序运行，把程序调用之后的所有职责都交给了子程序，即子程序调用之后，应该由子程序来负责通知主程序，而不是跟同步那样，主程序持续等待子程序准备就绪，而原地踏步；这里的主要区别就是异步时候，主程序调用之后立刻转入下一步不需要这个资源的逻辑，有点类似于，乐观锁和悲观锁的性质，不加判断的断定后续逻辑分支，直接运行。缺点是，感觉异步编程为了解决临界区问题和通信问题，有很多复杂的管控逻辑需要解决，简单来说就是很多坑。好处是，如果这些坑都能解决，程序运行效率会有比较大的提升。</p>\n<p>　　如果同步异步是编程模式的话，那么阻塞和非阻塞相对来说我认为是一个并非严格的概念，如果调用必定会导致主程序闲置，进而被置入阻塞状态，那么就是阻塞，如果调用不会导致主程序闲置，那么就是非阻塞。这其中的关键点就是OS的进程/线程调度，而不是调用的问题，然而阻塞的定义中常常会加入调用是否立刻返回的解释，常常让我跟同步异步的概念分不清楚，诚然调用不返回在当今的OS下一定会引起主程序闲置并阻塞，但是主因必然是主程序闲置等待，如果异步通信，但是仍然没有后续程序逻辑，显然异步程序也会有阻塞的风险。所以，我觉得阻塞这个更应该是一个OS的结果，而不是原因。</p>\n<blockquote>\n<p>阻塞和非阻塞关注的是程序等待调用结果时的状态，是闲置还是持续运行</p>\n</blockquote>\n<p>线程与进程</p>\n<p>　　曾经考试的时候定义背的滚瓜烂熟，然而却完全没有代码经验，感觉彼时都到门口了，缺乏临门一脚，直到现在又千辛万苦来回到这里，重新完成当初没有做到的事情。</p>\n<p>　　这里我在实践中，线程跟并发 搞混了，以至于以为线程之间必然是并发，所以有可能分时共享cuda 的引用，认为利用多线程就可以优化GPU利用率。最后实践中，怎么都达不到预想的目的。仔细想想，尽管同一个进程的资源可以被进程内的所有线程共享，这样如果多个线程是分时共享的，那么cuda引用作为一个共享资源，就可以达到并发使用的目的；但是这个前提是，线程必须是并发的，那么线程真的是并发的吗？如果是曾经的单核单线程CPU，答案是肯定的，现在的基本都是多核多线程CPU了，所以线程级别肯定是并行的，那么cuda引用则必然作为一个临界共享资源，被多个线程抢占使用，多线程并行退化成多线程并发，虽然预想的目的似乎达到了，但是貌似并没有达到优化的目的，至少跟之前编写的单线程基本性能是一样的。</p>\n<p>　　其次是Python的GIL锁, Global Interpreter Lock，即任何Python 线程执行，只有GIL锁的线程才可以运行，也就是说一个进程下的多线程在Python下，永远只有一个线程在运行</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> threading<span class=\"token punctuation\">,</span> multiprocessing\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">loop</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    x <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n    <span class=\"token keyword\">while</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">:</span>\n        x <span class=\"token operator\">=</span> x<span class=\"token operator\">^</span><span class=\"token number\">1</span>\n\n<span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span>multiprocessing<span class=\"token punctuation\">.</span>cpu_count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    t <span class=\"token operator\">=</span> threading<span class=\"token punctuation\">.</span>Thread<span class=\"token punctuation\">(</span>target <span class=\"token operator\">=</span> loop<span class=\"token punctuation\">)</span>\n    t<span class=\"token punctuation\">.</span>start<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre>\n<p>上述程序在多核CPU下占有率只有100%，但是C++/Java改写的死循环，则可以每个核心都跑完，即4核心400%</p>\n<p>更坑的是，GIL锁每个线程只有执行100条字节码的机会，那么有效的利用cuda资源则成为空谈，因为频繁的显存数据进出，比较浪费时间。所以python下一般都是利用多个进程来实现并行，因为多个进程有独立的GIL锁，互不影响。</p>\n<p>　　惭愧的是，自己写了这么久的Python，居然才第一次面对并行化问题，才知道GIL锁有这么多繁琐的坑。理论上证实这样的不可行之后，有点失望，但是也还是保留机会来完成并行写日志这个小任务的，毕竟通过HTTP接口，再怎么并发，框架还是很好的完成的这些繁琐的坑，只需要在producer-consumer 框架下，实现并行接受，串行消费就可以了，这还是比较基础的一个改写，对自己还是有信心的。</p>\n<p>　　线程安全，进程安全，意思就是在多线程或者多进程下，可以在代码中不加改变的直接使用，库和框架会自动解决并发和并行问题。其中 queue.Queue 就是线程安全的，下面就是利用Queue来实现进程间通信的producer-consumer</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> queue <span class=\"token keyword\">import</span> Queue\n<span class=\"token keyword\">from</span> threading <span class=\"token keyword\">import</span> Thread\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">producer</span><span class=\"token punctuation\">(</span>out_q<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">while</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">:</span>\n        out_q<span class=\"token punctuation\">.</span>put<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">consumer</span><span class=\"token punctuation\">(</span>in_q<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">while</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">:</span>\n        data <span class=\"token operator\">=</span> in_q<span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n            <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>\n        <span class=\"token comment\" spellcheck=\"true\"># 通过q通知 任务完成</span>\n      in_q<span class=\"token punctuation\">.</span>task_done<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\nq <span class=\"token operator\">=</span> Queue<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nt1 <span class=\"token operator\">=</span> Thread<span class=\"token punctuation\">(</span>target<span class=\"token operator\">=</span>consumer<span class=\"token punctuation\">,</span> args<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>q<span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nt2 <span class=\"token operator\">=</span> Thread<span class=\"token punctuation\">(</span>taget<span class=\"token operator\">=</span>producer<span class=\"token punctuation\">,</span> args<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>q<span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nt1<span class=\"token punctuation\">.</span>start<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nt2<span class=\"token punctuation\">.</span>start<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># wait for all produced items to be consumed</span>\nq<span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre>\n<p>　　优化点，队列的流量控制，q需要指定大小，必要时候阻塞Queue，以避免连锁效应导致程序运行失常，同时阻塞的队列也会形成死锁，通过非阻塞和设定超时时间，来处理队列满或者超时的情况，避免阻塞</p>\n<pre class=\" language-python\"><code class=\"language-python\">q <span class=\"token operator\">=</span> Queue<span class=\"token punctuation\">(</span><span class=\"token number\">100</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">try</span><span class=\"token punctuation\">:</span>\n    data <span class=\"token operator\">=</span> q<span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span>block<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">except</span> queue<span class=\"token punctuation\">.</span>Empty<span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">pass</span>\n\n<span class=\"token keyword\">try</span><span class=\"token punctuation\">:</span>\n    q<span class=\"token punctuation\">.</span>put<span class=\"token punctuation\">(</span>item<span class=\"token punctuation\">,</span> timeout<span class=\"token operator\">=</span><span class=\"token number\">5.0</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">except</span> queue<span class=\"token punctuation\">.</span>Full<span class=\"token punctuation\">:</span>\n    log<span class=\"token punctuation\">.</span>warning<span class=\"token punctuation\">(</span><span class=\"token string\">'queued item %r discarded!'</span><span class=\"token punctuation\">,</span> item<span class=\"token punctuation\">)</span></code></pre>\n<h3 id=\"最终方案\"><a href=\"#最终方案\" class=\"headerlink\" title=\"最终方案\"></a>最终方案</h3><p>　　利用tornado接受并发的http请求，解析request_body之后，直接入列队伍q.put()，这里利用了tornado 的异步接受并发的特点，直接简化producer过程。接着只需要编写consumer，取出数据，写到文件即可</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> eliot <span class=\"token keyword\">import</span> log_call<span class=\"token punctuation\">,</span> start_action<span class=\"token punctuation\">,</span> to_file\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">LogHandler</span><span class=\"token punctuation\">(</span>tornado<span class=\"token punctuation\">.</span>web<span class=\"token punctuation\">.</span>RequestHandler<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\" spellcheck=\"true\"># eliot 直接利用装饰器来添加日志功能，只记录input, output</span>\n    @log_call\n    <span class=\"token keyword\">def</span> <span class=\"token function\">post</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> <span class=\"token operator\">*</span>args<span class=\"token punctuation\">,</span> <span class=\"token operator\">**</span>kwargs<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">try</span><span class=\"token punctuation\">:</span>\n            self<span class=\"token punctuation\">.</span>timestamp <span class=\"token operator\">=</span> time<span class=\"token punctuation\">.</span>ctime<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n            self<span class=\"token punctuation\">.</span>recv_json <span class=\"token operator\">=</span> json<span class=\"token punctuation\">.</span>loads<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>request<span class=\"token punctuation\">.</span>body<span class=\"token punctuation\">,</span> strict<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span>\n            q<span class=\"token punctuation\">.</span>put<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>timestamp<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>recv_json <span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n            output_dic <span class=\"token operator\">=</span> <span class=\"token operator\">&amp;</span><span class=\"token comment\" spellcheck=\"true\">#123;</span>\n                <span class=\"token string\">'status'</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'1'</span><span class=\"token punctuation\">,</span>\n                <span class=\"token string\">'result'</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'success'</span>\n      <span class=\"token operator\">&amp;</span><span class=\"token comment\" spellcheck=\"true\">#125;</span>\n        <span class=\"token keyword\">except</span> Exception <span class=\"token keyword\">as</span> e<span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'LogHandler Error: '</span><span class=\"token punctuation\">,</span> e<span class=\"token punctuation\">)</span>\n            <span class=\"token keyword\">return</span>\n\n\n<span class=\"token comment\" spellcheck=\"true\"># eliot.start_action 通过 with 来记录函数段，有点繁琐，但是对于某段代码的日志记录很方便</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">consume_msg</span><span class=\"token punctuation\">(</span>queue<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">while</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">with</span> start_action<span class=\"token punctuation\">(</span>action_type<span class=\"token operator\">=</span><span class=\"token string\">'consume_msg'</span><span class=\"token punctuation\">,</span> lenghth <span class=\"token operator\">=</span> queue<span class=\"token punctuation\">.</span>qsize<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">try</span><span class=\"token punctuation\">:</span>\n                timestamp<span class=\"token punctuation\">,</span> msg<span class=\"token operator\">=</span> q<span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n                start_time <span class=\"token operator\">=</span> time<span class=\"token punctuation\">.</span>time<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n                <span class=\"token keyword\">with</span> start_action<span class=\"token punctuation\">(</span>action_type<span class=\"token operator\">=</span><span class=\"token string\">'save_request_body'</span><span class=\"token punctuation\">,</span> timestamp<span class=\"token operator\">=</span>timestamp<span class=\"token punctuation\">,</span>msg<span class=\"token operator\">=</span>msg<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n                    save_body<span class=\"token punctuation\">(</span>msg<span class=\"token punctuation\">)</span>\n            <span class=\"token keyword\">except</span> Exception <span class=\"token keyword\">as</span> e<span class=\"token punctuation\">:</span>\n                <span class=\"token keyword\">with</span> start_action<span class=\"token punctuation\">(</span>action_type<span class=\"token operator\">=</span><span class=\"token string\">'consume_msg exceptiosn'</span><span class=\"token punctuation\">,</span> timestamp<span class=\"token operator\">=</span>timestamp<span class=\"token punctuation\">,</span>e<span class=\"token operator\">=</span>e<span class=\"token punctuation\">,</span> msg<span class=\"token operator\">=</span>msg<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n                    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'consume_mgs exception : '</span>  <span class=\"token punctuation\">,</span> e<span class=\"token punctuation\">)</span>\n\nconsumer <span class=\"token operator\">=</span> Process<span class=\"token punctuation\">(</span>target<span class=\"token operator\">=</span>consume_msg<span class=\"token punctuation\">,</span> args<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>q<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nconsumer<span class=\"token punctuation\">.</span>daemon <span class=\"token operator\">=</span> <span class=\"token boolean\">True</span> <span class=\"token comment\" spellcheck=\"true\"># 守护进程</span>\nconsumer<span class=\"token punctuation\">.</span>start<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre>\n<p>　　至此，多进程的日志记录完成，同时也用上了之前一直想要用eliot日志系统，让日志看起来清爽了许多。</p>\n<h3 id=\"RabbiMQ\"><a href=\"#RabbiMQ\" class=\"headerlink\" title=\"RabbiMQ\"></a>RabbiMQ</h3><p>RabbiMQ 的方案也写进来，毕竟原理大同小异，只是初始化和连接的部分需要重新封装一下，而且适用性更广。</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token comment\" spellcheck=\"true\"># rabbitmq init</span>\ncredentials <span class=\"token operator\">=</span> pika<span class=\"token punctuation\">.</span>PlainCredentials<span class=\"token punctuation\">(</span>RabbitMQ<span class=\"token punctuation\">.</span>USERNAME<span class=\"token punctuation\">,</span> RabbitMQ<span class=\"token punctuation\">.</span>USERPWD<span class=\"token punctuation\">)</span>\nconnection <span class=\"token operator\">=</span> pika<span class=\"token punctuation\">.</span>BlockingConnection<span class=\"token punctuation\">(</span>\n                pika<span class=\"token punctuation\">.</span>ConnectionParameters<span class=\"token punctuation\">(</span>RabbitMQ<span class=\"token punctuation\">.</span>HOST<span class=\"token punctuation\">,</span> RabbitMQ<span class=\"token punctuation\">.</span>PORT<span class=\"token punctuation\">,</span>\n                                          credentials<span class=\"token operator\">=</span>credentials<span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">)</span>\n<span class=\"token comment\" spellcheck=\"true\"># channel init and declare</span>\nchannel <span class=\"token operator\">=</span> connection<span class=\"token punctuation\">.</span>channel<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nchannel<span class=\"token punctuation\">.</span>exchange_declare<span class=\"token punctuation\">(</span>exchange<span class=\"token operator\">=</span>RabbitMQ<span class=\"token punctuation\">.</span>EXCHANGE<span class=\"token punctuation\">,</span>\n                             exchange_type<span class=\"token operator\">=</span>RabbitMQ<span class=\"token punctuation\">.</span>EXCHANGE_TYPE<span class=\"token punctuation\">,</span>\n                             durable<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\nchannel<span class=\"token punctuation\">.</span>queue_declare<span class=\"token punctuation\">(</span>queue<span class=\"token operator\">=</span>RabbitMQ<span class=\"token punctuation\">.</span>QUEUE_NAME_ALG<span class=\"token punctuation\">,</span> durable<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\nchannel<span class=\"token punctuation\">.</span>queue_bind<span class=\"token punctuation\">(</span>exchange<span class=\"token operator\">=</span>RabbitMQ<span class=\"token punctuation\">.</span>EXCHANGE<span class=\"token punctuation\">,</span>\n                       queue<span class=\"token operator\">=</span>RabbitMQ<span class=\"token punctuation\">.</span>QUEUE_NAME_ALG<span class=\"token punctuation\">,</span>\n                       routing_key<span class=\"token operator\">=</span>RabbitMQ<span class=\"token punctuation\">.</span>ROUTING_KEY<span class=\"token punctuation\">)</span>\nchannel<span class=\"token punctuation\">.</span>basic_qos<span class=\"token punctuation\">(</span>prefetch_count<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>    <span class=\"token comment\" spellcheck=\"true\"># 能者多劳</span>\nchannel<span class=\"token punctuation\">.</span>basic_consume<span class=\"token punctuation\">(</span>on_message_callback<span class=\"token operator\">=</span>consumer<span class=\"token punctuation\">,</span>\n                          queue<span class=\"token operator\">=</span>RabbitMQ<span class=\"token punctuation\">.</span>QUEUE_NAME_ALG<span class=\"token punctuation\">,</span> auto_ack<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">' [*] Waiting for messages...'</span><span class=\"token punctuation\">)</span>\nchannel<span class=\"token punctuation\">.</span>start_consuming<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">consumer</span><span class=\"token punctuation\">(</span>ch<span class=\"token punctuation\">,</span> method<span class=\"token punctuation\">,</span> properties<span class=\"token punctuation\">,</span> body<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    body <span class=\"token operator\">=</span> json<span class=\"token punctuation\">.</span>loads<span class=\"token punctuation\">(</span>body<span class=\"token punctuation\">,</span> strict<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span>\n    flag <span class=\"token operator\">=</span> body<span class=\"token punctuation\">[</span><span class=\"token string\">'flag'</span><span class=\"token punctuation\">]</span>\n    <span class=\"token comment\" spellcheck=\"true\"># process body</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># Ack manually</span>\n    ch<span class=\"token punctuation\">.</span>basic_ack<span class=\"token punctuation\">(</span>delivery_tag<span class=\"token operator\">=</span>method<span class=\"token punctuation\">.</span>delivery_tag<span class=\"token punctuation\">)</span>\n\n\n\n<span class=\"token comment\" spellcheck=\"true\"># producer.py</span>\n<span class=\"token keyword\">try</span><span class=\"token punctuation\">:</span>\n    credentials <span class=\"token operator\">=</span> pika<span class=\"token punctuation\">.</span>PlainCredentials<span class=\"token punctuation\">(</span>\n                RabbitMQ<span class=\"token punctuation\">.</span>USERNAME<span class=\"token punctuation\">,</span> RabbitMQ<span class=\"token punctuation\">.</span>USERPWD<span class=\"token punctuation\">)</span>\n    connection <span class=\"token operator\">=</span> pika<span class=\"token punctuation\">.</span>BlockingConnection<span class=\"token punctuation\">(</span>\n                pika<span class=\"token punctuation\">.</span>ConnectionParameters<span class=\"token punctuation\">(</span>RabbitMQ<span class=\"token punctuation\">.</span>HOST<span class=\"token punctuation\">,</span> RabbitMQ<span class=\"token punctuation\">.</span>PORT<span class=\"token punctuation\">,</span>\n                                          credentials<span class=\"token operator\">=</span>credentials<span class=\"token punctuation\">)</span>\n            <span class=\"token punctuation\">)</span>\n    channel <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>connection<span class=\"token punctuation\">.</span>channel<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    channel<span class=\"token punctuation\">.</span>exchange_declare<span class=\"token punctuation\">(</span>exchange<span class=\"token operator\">=</span>RabbitMQ<span class=\"token punctuation\">.</span>EXCHANGE<span class=\"token punctuation\">,</span>                                                                                              exchange_type<span class=\"token operator\">=</span>RabbitMQ<span class=\"token punctuation\">.</span>EXCHANGE_TYPE<span class=\"token punctuation\">,</span>\n                                          durable<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">except</span> Exception <span class=\"token keyword\">as</span> e<span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">pass</span>\n\nsent_msg <span class=\"token operator\">=</span> <span class=\"token operator\">&amp;</span><span class=\"token comment\" spellcheck=\"true\">#123;...&amp;#125;</span>\nbody <span class=\"token operator\">=</span> json<span class=\"token punctuation\">.</span>dumps<span class=\"token punctuation\">(</span>sent_msg<span class=\"token punctuation\">)</span>\nchannel<span class=\"token punctuation\">.</span>basic_publish<span class=\"token punctuation\">(</span>exchange<span class=\"token operator\">=</span>RabbitMQ<span class=\"token punctuation\">.</span>EXCHANGE<span class=\"token punctuation\">,</span>                 \n                      routing_key<span class=\"token operator\">=</span>RabbitMQ<span class=\"token punctuation\">.</span>ROUTING_KEY<span class=\"token punctuation\">,</span>\n                      body<span class=\"token operator\">=</span>body<span class=\"token punctuation\">,</span> \n                      properties<span class=\"token operator\">=</span>pika<span class=\"token punctuation\">.</span>BasicProperties<span class=\"token punctuation\">(</span>delivery_mode<span class=\"token operator\">=</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\nchannel<span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nconnection<span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre>\n<h3 id=\"计算密集型-和-IO密集型的思考\"><a href=\"#计算密集型-和-IO密集型的思考\" class=\"headerlink\" title=\"计算密集型 和 IO密集型的思考\"></a>计算密集型 和 IO密集型的思考</h3><p>　　当初写论文的时候，就在机器上跑了很多类型的负载任务，最直观的感受就是，计算密集型的负载比较难找，相对于纯粹的计算任务，很多实际的任务都会因为千奇百怪的原因被消耗在IO上，导致运行效率低下。</p>\n<blockquote>\n<p>计算密集型，代码运行效率为主，因为主要时间都消耗在CPU上，Python的脚本语言不适合这类任务，最好用C++/Java实现</p>\n<p>IO密集型，大部分时间消耗在IO上，C++/Java无法发挥优势，代码量少开发效率最高的Python具备优势</p>\n</blockquote>\n<p>　　所以，Python最好最为原型开发，保持逻辑流程通顺的前提下，可以用其他高效的语言如C++/Java改写最耗时的模块，迭代优化开发。</p>\n<h3 id=\"协程：单线程异步\"><a href=\"#协程：单线程异步\" class=\"headerlink\" title=\"协程：单线程异步\"></a>协程：单线程异步</h3><p>　　虽然Python无法利用多线程的并行优势，但是还是有优化的手段的。如果把条件限制到单线程，那么有一个很自然的优化方式就是单线程异步，也就是只用单线程的情况下，实现程序内部的并发，这个并发是程序可控的，而不是由操作系统来根据一定策略加锁。</p>\n<p>　　Python下有协程库asyncio内置了异步IO的支持。编程模型是一个消息循环，从asyncio获取EventLoop引用，把需要执行的协程作为参数传递给EventLoop，实现异步IO,下面直接给出协程版本的producer-consumer</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> asyncio\n<span class=\"token keyword\">import</span> random\n\n<span class=\"token keyword\">async</span> <span class=\"token keyword\">def</span> <span class=\"token function\">producer</span><span class=\"token punctuation\">(</span>queue<span class=\"token punctuation\">,</span> n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> n<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'producing : '</span><span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span>    \n        <span class=\"token comment\" spellcheck=\"true\"># simulate io job</span>\n        <span class=\"token keyword\">await</span> asyncio<span class=\"token punctuation\">.</span>sleep<span class=\"token punctuation\">(</span>random<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        item <span class=\"token operator\">=</span> str<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">await</span> queue<span class=\"token punctuation\">.</span>put<span class=\"token punctuation\">(</span>item<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">async</span> <span class=\"token keyword\">def</span> <span class=\"token function\">consume</span><span class=\"token punctuation\">(</span>queue<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">while</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">:</span>\n        item <span class=\"token operator\">=</span> <span class=\"token keyword\">await</span> queue<span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># process item</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'consuming : '</span><span class=\"token punctuation\">,</span> item<span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># simulate io </span>\n    <span class=\"token keyword\">await</span> asyncio<span class=\"token punctuation\">.</span>sleep<span class=\"token punctuation\">(</span>random<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># notify queue that the item has been processed</span>\n    queue<span class=\"token punctuation\">.</span>task_done<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">async</span> <span class=\"token keyword\">def</span> <span class=\"token function\">run</span><span class=\"token punctuation\">(</span>n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    queue <span class=\"token operator\">=</span> asyncio<span class=\"token punctuation\">.</span>Queue<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># schedule consumer</span>\n    consumer<span class=\"token operator\">=</span> asyncio<span class=\"token punctuation\">.</span>ensure_future<span class=\"token punctuation\">(</span>consume<span class=\"token punctuation\">(</span>queue<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">await</span> producer<span class=\"token punctuation\">(</span>queue<span class=\"token punctuation\">,</span> n<span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># wait until consumer processed all items</span>\n    <span class=\"token keyword\">await</span> queue<span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># consumer is still awaiting item, cancel it</span>\n    consumer<span class=\"token punctuation\">.</span>cancel<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\nloop <span class=\"token operator\">=</span> asyncio<span class=\"token punctuation\">.</span>get_event_loop<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nloop<span class=\"token punctuation\">.</span>run_until_complete<span class=\"token punctuation\">(</span>run<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nloop<span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>　　至此，关于同步异步的一些知识点和实践问题梳理完毕，协程的实践经验还是有点欠缺，以后看框架源码的时候，需要多学习和思考。</p>\n<p>References:</p>\n<p><a href=\"https://asyncio.readthedocs.io/en/latest/producer_consumer.html\">Producer/consumer-examlpe_asyncio</a></p>\n<p><a href=\"https://www.liaoxuefeng.com/wiki/1016959663602400/1017968846697824\">协程-廖雪峰</a></p>\n<p><a href=\"https://www.liaoxuefeng.com/wiki/1016959663602400/1017631469467456\">进程-线程-廖雪峰</a></p>\n<p><a href=\"https://python3-cookbook.readthedocs.io/zh_CN/latest/c12/p03_communicating_between_threads.html\">线程间通信-Python_CookBook</a></p>\n<p><a href=\"https://juejin.cn/post/6844904039210024967#heading-3\">Python下的多进程日志记录方案</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>　　最近遇到如何并行写日志文件的问题，查看之前的代码，发现是利用RabbitMQ 作为消息中转，把并行的日志流通过HTTP 请求整合成串行的方式，然后通过一个单独的 Rabbit-Consumer 来写入文件，这样就成功避免了多个进程同时写一个文件造成的加锁问题，但是我觉得日志这种I/O为主的任务，动用RabbitMQ 有点大材小用的感觉，Producer-Consumer的总体思想可以不变，但是可以用其他更加轻量级的编程模型来实现，也借此重新优化一下日志输出的，毕竟之前的日志输出感觉一片混乱，在自己熟悉代码的情况下，仍然会有些理解困难，这跟当初自己写代码的时候不重视日志输出这一块有关，甚至很多情急之下写下的print打印信息都最后都没有处理。</p>\n<p>　　在开始之前，必须要回忆一些似曾相识，却又容易混淆的基本概念，因为在不区分这些基本OS名词之前，根本就完全无法下手，不知道应该如何正确的实践，具体到每个代码跟这些概念是如何结合在一起的，基本上完全不知道，学生时代留下的坏习惯作怪，理论脱离实际，这时候也才意识到书到用时方恨少。</p>\n<h4 id=\"阻塞与非阻塞\"><a href=\"#阻塞与非阻塞\" class=\"headerlink\" title=\"阻塞与非阻塞\"></a>阻塞与非阻塞</h4><p>　　这个概念有点复杂，来自OS中线程/进程的生命周期，诸如，就绪，运行，阻塞 ，但是如果仅仅按照这个方向理解，那就比较简单，仅仅就是线程/进程调度程序根据一定的策略，把线程/进程制定相应的状态，来安排和调度CPU资源；然而在OS之中，阻塞的原因很多，并非只是单纯的IO，严格上来讲应该是跟IO类似的原因被阻塞，即请求的资源无法得到，所以这里的阻塞调用，则是更加强调的是，单线程/进程在运行时，那些一定会被阻塞的编程模式，最重要的一个就是同步的编程模式，一旦请求的资源不具备，程序立刻陷入等待，这样就会特别容易被OS识别为闲置的线程/进程，并很快被阻塞。相反，异步则是一种将等待的资源交给外部程序和硬件，要么是子线程/进程，要么就是外部设备，总之试图通过改变通信模式的方式，使主程序始终维持运行态，这样就达到不被阻塞的目的。这就对编程模式有比较大的变动，因为主程序需要应对请求资源返回的各种情况需要预先有响应的程序段来处理，也就是常见的编程过程中回调函数的编写。</p>\n<blockquote>\n<p>阻塞调用是指调用结果返回之前，调用者会进入阻塞状态等待。只有在得到结果之后才会返回。</p>\n<p>非阻塞调用是指在不能立刻得到结果之前，该函数不会阻塞当前线程，而会立刻返回。</p>\n</blockquote>\n<p>阻塞调用：比如 socket 的 recv()，调用这个函数的线程如果没有数据返回，它会一直阻塞着，也就是 recv() 后面的代码都不会执行了，程序就停在 recv() 这里等待，所以一般把 recv() 放在单独的线程里调用。</p>\n<p>非阻塞调用：比如非阻塞socket 的 send()，调用这个函数，它只是把待发送的数据复制到TCP输出缓冲区中，就立刻返回了，线程并不会阻塞，数据有没有发出去 send() 是不知道的，不会等待它发出去才返回的。</p>\n<h4 id=\"并发与并行\"><a href=\"#并发与并行\" class=\"headerlink\" title=\"并发与并行\"></a>并发与并行</h4><p>基本还记得最显著的区别</p>\n<blockquote>\n<p>并发是指一个时间段内，有几个程序都在同一个CPU上运行，但任意一个时刻点上只有一个程序在处理机上运行。</p>\n<p>并行是指一个时间段内，有几个程序都在几个CPU上运行，任意一个时刻点上，有多个程序在同时运行，并且多道程序之间互不干扰。 两者区别如下图</p>\n</blockquote>\n<p><img src=\"1.jpeg\"></p>\n<p><img src=\"2.jpeg\"></p>\n<h4 id=\"同步与异步\"><a href=\"#同步与异步\" class=\"headerlink\" title=\"同步与异步\"></a>同步与异步</h4><p>这个有点基本印象，经过很多次反复理解，依然还是没有很清晰掌握</p>\n<blockquote>\n<p>同步：在发出一个同步调用时，在没有得到结果之前，该调用就不返回</p>\n<p>异步：在发出一个异步调用后，调用者不会立刻得到结果，该调用就返回了</p>\n</blockquote>\n<p>　　同步和阻塞， 异步和非阻塞概念非常类似，但是所涉及的范围还是有所不同，总的来讲应该是2个独立的概念体系。</p>\n<p>　　同步和异步，应该是编程模型中，约定好的一种线程/进程间互相通信的方式，同步则是比较自然的一种程序运作方式，就是自顶向下，资源缺失，主程序等待资源准备，资源就绪继续运行；然而在OS系统调度的指挥下，同步很明显的缺点就是主程序无差别的等待所有资源，很容易程序会因为一些无关紧要的情况而一直处于等待，这其中就有比较大的优化空间；所以根据这种情况一种自然的解决办法是，让程序对各种资源缺失的情况能有对应的方案，最终形成了异步的通信策略，相比于定义上的，调用之后立刻返回这样不知所云的解释，我的理解是，程序是制定了一系列某些资源缺失或者不可用时，程序应该如何运作的逻辑，并且为了保持主程序运行，把程序调用之后的所有职责都交给了子程序，即子程序调用之后，应该由子程序来负责通知主程序，而不是跟同步那样，主程序持续等待子程序准备就绪，而原地踏步；这里的主要区别就是异步时候，主程序调用之后立刻转入下一步不需要这个资源的逻辑，有点类似于，乐观锁和悲观锁的性质，不加判断的断定后续逻辑分支，直接运行。缺点是，感觉异步编程为了解决临界区问题和通信问题，有很多复杂的管控逻辑需要解决，简单来说就是很多坑。好处是，如果这些坑都能解决，程序运行效率会有比较大的提升。</p>\n<p>　　如果同步异步是编程模式的话，那么阻塞和非阻塞相对来说我认为是一个并非严格的概念，如果调用必定会导致主程序闲置，进而被置入阻塞状态，那么就是阻塞，如果调用不会导致主程序闲置，那么就是非阻塞。这其中的关键点就是OS的进程/线程调度，而不是调用的问题，然而阻塞的定义中常常会加入调用是否立刻返回的解释，常常让我跟同步异步的概念分不清楚，诚然调用不返回在当今的OS下一定会引起主程序闲置并阻塞，但是主因必然是主程序闲置等待，如果异步通信，但是仍然没有后续程序逻辑，显然异步程序也会有阻塞的风险。所以，我觉得阻塞这个更应该是一个OS的结果，而不是原因。</p>\n<blockquote>\n<p>阻塞和非阻塞关注的是程序等待调用结果时的状态，是闲置还是持续运行</p>\n</blockquote>\n<p>线程与进程</p>\n<p>　　曾经考试的时候定义背的滚瓜烂熟，然而却完全没有代码经验，感觉彼时都到门口了，缺乏临门一脚，直到现在又千辛万苦来回到这里，重新完成当初没有做到的事情。</p>\n<p>　　这里我在实践中，线程跟并发 搞混了，以至于以为线程之间必然是并发，所以有可能分时共享cuda 的引用，认为利用多线程就可以优化GPU利用率。最后实践中，怎么都达不到预想的目的。仔细想想，尽管同一个进程的资源可以被进程内的所有线程共享，这样如果多个线程是分时共享的，那么cuda引用作为一个共享资源，就可以达到并发使用的目的；但是这个前提是，线程必须是并发的，那么线程真的是并发的吗？如果是曾经的单核单线程CPU，答案是肯定的，现在的基本都是多核多线程CPU了，所以线程级别肯定是并行的，那么cuda引用则必然作为一个临界共享资源，被多个线程抢占使用，多线程并行退化成多线程并发，虽然预想的目的似乎达到了，但是貌似并没有达到优化的目的，至少跟之前编写的单线程基本性能是一样的。</p>\n<p>　　其次是Python的GIL锁, Global Interpreter Lock，即任何Python 线程执行，只有GIL锁的线程才可以运行，也就是说一个进程下的多线程在Python下，永远只有一个线程在运行</p>\n<pre><code class=\"python\">import threading, multiprocessing\n\ndef loop():\n    x = 0\n    while True:\n        x = x^1\n\nfor i in range(multiprocessing.cpu_count()):\n    t = threading.Thread(target = loop)\n    t.start()</code></pre>\n<p>上述程序在多核CPU下占有率只有100%，但是C++/Java改写的死循环，则可以每个核心都跑完，即4核心400%</p>\n<p>更坑的是，GIL锁每个线程只有执行100条字节码的机会，那么有效的利用cuda资源则成为空谈，因为频繁的显存数据进出，比较浪费时间。所以python下一般都是利用多个进程来实现并行，因为多个进程有独立的GIL锁，互不影响。</p>\n<p>　　惭愧的是，自己写了这么久的Python，居然才第一次面对并行化问题，才知道GIL锁有这么多繁琐的坑。理论上证实这样的不可行之后，有点失望，但是也还是保留机会来完成并行写日志这个小任务的，毕竟通过HTTP接口，再怎么并发，框架还是很好的完成的这些繁琐的坑，只需要在producer-consumer 框架下，实现并行接受，串行消费就可以了，这还是比较基础的一个改写，对自己还是有信心的。</p>\n<p>　　线程安全，进程安全，意思就是在多线程或者多进程下，可以在代码中不加改变的直接使用，库和框架会自动解决并发和并行问题。其中 queue.Queue 就是线程安全的，下面就是利用Queue来实现进程间通信的producer-consumer</p>\n<pre><code class=\"python\">from queue import Queue\nfrom threading import Thread\n\ndef producer(out_q):\n    while True:\n        out_q.put(data)\n\ndef consumer(in_q):\n    while True:\n        data = in_q.get()\n            ...\n        # 通过q通知 任务完成\n      in_q.task_done()\n\nq = Queue()\nt1 = Thread(target=consumer, args=(q,))\nt2 = Thread(taget=producer, args=(q,))\nt1.start()\nt2.start()\n\n# wait for all produced items to be consumed\nq.join()</code></pre>\n<p>　　优化点，队列的流量控制，q需要指定大小，必要时候阻塞Queue，以避免连锁效应导致程序运行失常，同时阻塞的队列也会形成死锁，通过非阻塞和设定超时时间，来处理队列满或者超时的情况，避免阻塞</p>\n<pre><code class=\"python\">q = Queue(100)\n\ntry:\n    data = q.get(block=False)\nexcept queue.Empty:\n    pass\n\ntry:\n    q.put(item, timeout=5.0)\nexcept queue.Full:\n    log.warning(&#39;queued item %r discarded!&#39;, item)</code></pre>\n<h3 id=\"最终方案\"><a href=\"#最终方案\" class=\"headerlink\" title=\"最终方案\"></a>最终方案</h3><p>　　利用tornado接受并发的http请求，解析request_body之后，直接入列队伍q.put()，这里利用了tornado 的异步接受并发的特点，直接简化producer过程。接着只需要编写consumer，取出数据，写到文件即可</p>\n<pre><code class=\"python\">from eliot import log_call, start_action, to_file\nclass LogHandler(tornado.web.RequestHandler):\n    # eliot 直接利用装饰器来添加日志功能，只记录input, output\n    @log_call\n    def post(self, *args, **kwargs):\n        try:\n            self.timestamp = time.ctime()\n            self.recv_json = json.loads(self.request.body, strict=False)\n            q.put((self.timestamp, self.recv_json ))\n            output_dic = &#123;\n                &#39;status&#39;: &#39;1&#39;,\n                &#39;result&#39;: &#39;success&#39;\n      &#125;\n        except Exception as e:\n            print(&#39;LogHandler Error: &#39;, e)\n            return\n\n\n# eliot.start_action 通过 with 来记录函数段，有点繁琐，但是对于某段代码的日志记录很方便\ndef consume_msg(queue):\n    while True:\n        with start_action(action_type=&#39;consume_msg&#39;, lenghth = queue.qsize()):\n            try:\n                timestamp, msg= q.get()\n                start_time = time.time()\n                with start_action(action_type=&#39;save_request_body&#39;, timestamp=timestamp,msg=msg):\n                    save_body(msg)\n            except Exception as e:\n                with start_action(action_type=&#39;consume_msg exceptiosn&#39;, timestamp=timestamp,e=e, msg=msg):\n                    print(&#39;consume_mgs exception : &#39;  , e)\n\nconsumer = Process(target=consume_msg, args=(q, ))\nconsumer.daemon = True # 守护进程\nconsumer.start()</code></pre>\n<p>　　至此，多进程的日志记录完成，同时也用上了之前一直想要用eliot日志系统，让日志看起来清爽了许多。</p>\n<h3 id=\"RabbiMQ\"><a href=\"#RabbiMQ\" class=\"headerlink\" title=\"RabbiMQ\"></a>RabbiMQ</h3><p>RabbiMQ 的方案也写进来，毕竟原理大同小异，只是初始化和连接的部分需要重新封装一下，而且适用性更广。</p>\n<pre><code class=\"python\"># rabbitmq init\ncredentials = pika.PlainCredentials(RabbitMQ.USERNAME, RabbitMQ.USERPWD)\nconnection = pika.BlockingConnection(\n                pika.ConnectionParameters(RabbitMQ.HOST, RabbitMQ.PORT,\n                                          credentials=credentials)\n    )\n# channel init and declare\nchannel = connection.channel()\nchannel.exchange_declare(exchange=RabbitMQ.EXCHANGE,\n                             exchange_type=RabbitMQ.EXCHANGE_TYPE,\n                             durable=True)\nchannel.queue_declare(queue=RabbitMQ.QUEUE_NAME_ALG, durable=True)\nchannel.queue_bind(exchange=RabbitMQ.EXCHANGE,\n                       queue=RabbitMQ.QUEUE_NAME_ALG,\n                       routing_key=RabbitMQ.ROUTING_KEY)\nchannel.basic_qos(prefetch_count=1)    # 能者多劳\nchannel.basic_consume(on_message_callback=consumer,\n                          queue=RabbitMQ.QUEUE_NAME_ALG, auto_ack=False)\nprint(&#39; [*] Waiting for messages...&#39;)\nchannel.start_consuming()\n\ndef consumer(ch, method, properties, body):\n    body = json.loads(body, strict=False)\n    flag = body[&#39;flag&#39;]\n    # process body\n\n    # Ack manually\n    ch.basic_ack(delivery_tag=method.delivery_tag)\n\n\n\n# producer.py\ntry:\n    credentials = pika.PlainCredentials(\n                RabbitMQ.USERNAME, RabbitMQ.USERPWD)\n    connection = pika.BlockingConnection(\n                pika.ConnectionParameters(RabbitMQ.HOST, RabbitMQ.PORT,\n                                          credentials=credentials)\n            )\n    channel = self.connection.channel()\n    channel.exchange_declare(exchange=RabbitMQ.EXCHANGE,                                                                                              exchange_type=RabbitMQ.EXCHANGE_TYPE,\n                                          durable=True)\nexcept Exception as e:\n    pass\n\nsent_msg = &#123;...&#125;\nbody = json.dumps(sent_msg)\nchannel.basic_publish(exchange=RabbitMQ.EXCHANGE,                 \n                      routing_key=RabbitMQ.ROUTING_KEY,\n                      body=body, \n                      properties=pika.BasicProperties(delivery_mode=2))\n\nchannel.close()\nconnection.close()</code></pre>\n<h3 id=\"计算密集型-和-IO密集型的思考\"><a href=\"#计算密集型-和-IO密集型的思考\" class=\"headerlink\" title=\"计算密集型 和 IO密集型的思考\"></a>计算密集型 和 IO密集型的思考</h3><p>　　当初写论文的时候，就在机器上跑了很多类型的负载任务，最直观的感受就是，计算密集型的负载比较难找，相对于纯粹的计算任务，很多实际的任务都会因为千奇百怪的原因被消耗在IO上，导致运行效率低下。</p>\n<blockquote>\n<p>计算密集型，代码运行效率为主，因为主要时间都消耗在CPU上，Python的脚本语言不适合这类任务，最好用C++/Java实现</p>\n<p>IO密集型，大部分时间消耗在IO上，C++/Java无法发挥优势，代码量少开发效率最高的Python具备优势</p>\n</blockquote>\n<p>　　所以，Python最好最为原型开发，保持逻辑流程通顺的前提下，可以用其他高效的语言如C++/Java改写最耗时的模块，迭代优化开发。</p>\n<h3 id=\"协程：单线程异步\"><a href=\"#协程：单线程异步\" class=\"headerlink\" title=\"协程：单线程异步\"></a>协程：单线程异步</h3><p>　　虽然Python无法利用多线程的并行优势，但是还是有优化的手段的。如果把条件限制到单线程，那么有一个很自然的优化方式就是单线程异步，也就是只用单线程的情况下，实现程序内部的并发，这个并发是程序可控的，而不是由操作系统来根据一定策略加锁。</p>\n<p>　　Python下有协程库asyncio内置了异步IO的支持。编程模型是一个消息循环，从asyncio获取EventLoop引用，把需要执行的协程作为参数传递给EventLoop，实现异步IO,下面直接给出协程版本的producer-consumer</p>\n<pre><code class=\"python\">import asyncio\nimport random\n\nasync def producer(queue, n):\n    for x in range(1, n+1):\n        print(&#39;producing : &#39;, x)    \n        # simulate io job\n        await asyncio.sleep(random.random())\n        item = str(x)\n        await queue.put(item)\n\nasync def consume(queue):\n    while True:\n        item = await queue.get()\n    # process item\n    print(&#39;consuming : &#39;, item)\n    # simulate io \n    await asyncio.sleep(random.random())\n    # notify queue that the item has been processed\n    queue.task_done()\n\nasync def run(n):\n    queue = asyncio.Queue()\n    # schedule consumer\n    consumer= asyncio.ensure_future(consume(queue))\n    await producer(queue, n)\n    # wait until consumer processed all items\n    await queue.join()\n    # consumer is still awaiting item, cancel it\n    consumer.cancel()\n\nloop = asyncio.get_event_loop()\nloop.run_until_complete(run(10))\nloop.close()</code></pre>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>　　至此，关于同步异步的一些知识点和实践问题梳理完毕，协程的实践经验还是有点欠缺，以后看框架源码的时候，需要多学习和思考。</p>\n<p>References:</p>\n<p><a href=\"https://asyncio.readthedocs.io/en/latest/producer_consumer.html\">Producer/consumer-examlpe_asyncio</a></p>\n<p><a href=\"https://www.liaoxuefeng.com/wiki/1016959663602400/1017968846697824\">协程-廖雪峰</a></p>\n<p><a href=\"https://www.liaoxuefeng.com/wiki/1016959663602400/1017631469467456\">进程-线程-廖雪峰</a></p>\n<p><a href=\"https://python3-cookbook.readthedocs.io/zh_CN/latest/c12/p03_communicating_between_threads.html\">线程间通信-Python_CookBook</a></p>\n<p><a href=\"https://juejin.cn/post/6844904039210024967#heading-3\">Python下的多进程日志记录方案</a></p>\n"},{"title":"GPU in Pytorch  并行和分布式实践","toc":true,"mathjax":true,"top":false,"cover":true,"date":"2020-11-25T07:07:07.000Z","updated":"2021-01-15T07:10:23.263Z","_content":"\n　　虽然还没有机会用到CUDA集群，但是前段时间对协程和并行化的研究，让我忍不住想要探索一下如何在多个GPU下利用Pytorch加快训练的实践方法，算是为之后并行训练优化一个理论参考吧！\n\n　　Pytorch 大体上有3种实现并行的接口（另外还有一种不利用接口的拼接模型的技巧，之后再单独讨论），分别是：torch.multiprocessing, nn.DataParallel, nn.parallel.DistributedDataParallel，如果是是GPU多卡运行，最佳实践是 nn.parallle.DistributedDataParallel,官方文档 [CUDA SEMANTIC](https://pytorch.org/docs/stable/notes/cuda.html#cuda-nn-ddp-instead) 是这么描述的：\n\n> **Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel**\n>\n> Most use cases involving batched inputs and multiple GPUs should default to using [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel) to utilize more than one GPU.\n>\n> There are significant caveats to using CUDA models with [`multiprocessing`](https://pytorch.org/docs/stable/multiprocessing.html#module-torch.multiprocessing); unless care is taken to meet the data handling requirements exactly, it is likely that your program will have incorrect or undefined behavior.\n>\n> It is recommended to use [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel), instead of [`DataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel) to do multi-GPU training, even if there is only a single node.\n>\n> The difference between [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel) and [`DataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel) is: [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel) uses multiprocessing where a process is created for each GPU, while [`DataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel) uses multithreading. By using multiprocessing, each GPU has its dedicated process, this avoids the performance overhead caused by GIL of Python interpreter.\n>\n> If you use [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel), you could use torch.distributed.launch utility to launch your program, see [Third-party backends](https://pytorch.org/docs/stable/distributed.html#distributed-launch).\n\n\n\n### torch.multiprocessing\n\n　　torch.multiprocessing 是 Python下 multiprocessing 的替代品，接口基本一致，并根据情况进行扩张，建议使用 python: multiprocessing.Queue 在进程中传递Pytorch对象。除此之外，还有很多坑，详见官方笔记 [MULTIPROCESSING BEST PRACTICES](https://pytorch.org/docs/stable/notes/multiprocessing.html), [并行处理最佳实践](https://pytorch.apachecn.org/docs/1.4/64.html)\n\n```python\nimport torch.multiprocessing as mp\nfrom model import MyModel\n\ndef train(model):\n    # Construct data_loader, optimizer, etc.\n    for data, labels in data_loader:\n        optimizer.zero_grad()\n        loss_fn(model(data), labels).backward()\n        optimizer.step()  # This will update the shared parameters\n\nif __name__ == '__main__':\n    num_processes = 4\n    model = MyModel()\n    # NOTE: this is required for the ``fork`` method to work\n    model.share_memory()\n    processes = []\n    for rank in range(num_processes):\n        p = mp.Process(target=train, args=(model,))\n        p.start()\n        processes.append(p)\n    for p in processes:\n        p.join()\n```\n\n\n\n### DataParallel\n\n　　DataParallel原理是，把model 副本copy到所有GPU上，其中每个GPU消耗数据的不同分区，相当于SIMD，把数据条目根据GPU数量重新分配\n\n#### 单机DataParallel并行\n\n```python\nmodel = nn.DataParallel(model)\n```\n\n　　代码验证 outside model 数据维度  和 inside model 维度\n\n```python\nclass Model(nn.Module):\n    # Our model\n\n    def __init__(self, input_size, output_size):\n        super(Model, self).__init__()\n        self.fc = nn.Linear(input_size, output_size)\n\n    def forward(self, input):\n        output = self.fc(input)\n        print(\"\\tIn Model: input size\", input.size(),\n              \"output size\", output.size())\n\n        return output\n        \n\nmodel = Model(input_size, output_size)\nif torch.cuda.device_count() > 1:\n  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n  model = nn.DataParallel(model)\n\nmodel.to(device)\n\nfor data in rand_loader:\n    input = data.to(device)\n    output = model(input)\n    print(\"Outside: input size\", input.size(),\n          \"output_size\", output.size())\n    \n# 2 GPUs\n# on 2 GPUs\nLet's use 2 GPUs!\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\nOutside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n```\n\n优点是，通过消耗输入数据不同分区的方式加快训练过程，缺点是，如果单个模型太大，无法放入单个GPU，就无法运行，这时有个Trick可以把模型分段载入不同GPU，利用拼接的方式完成并行训练，该模型将单个模型拆分到不同的 GPU 上，而不是在每个 GPU 上复制整个模型(具体来说， 假设模型`m`包含 10 层：使用`DataParallel`时，每个 GPU 都具有这 10 层中每个层的副本，而当在两个 GPU 上并行使用模型时，每个 GPU 可以承载 5 层）\n\n#### 单机模型拼接并行\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = torch.nn.Linear(10, 10).to('cuda:0')\n        self.relu = torch.nn.ReLU()\n        self.net2 = torch.nn.Linear(10, 5).to('cuda:1')\n\n    def forward(self, x):\n        x = self.relu(self.net1(x.to('cuda:0')))\n        return self.net2(x.to('cuda:1'))\n```\n\n对于模型太大而无法放入单个 GPU 的情况，上述实现解决了该问题。 但是，分析原理的时候可能已经注意到，如果模型单个GPU可以载入，它将比在单个 GPU 上运行它要慢。 这是因为在任何时间点，两个 GPU 中只有一个在工作，而另一个在那儿什么也没做。 由于中间输出需要在`layer2`和`layer3`之间从`cuda:0`复制到`cuda:1`，因此性能进一步恶化。\n\n除此之外，还有通过异步方式构建流水线的手段加速，这个Trick比较精巧和优雅，思想可以学习一下\n\n#### 单机Pipeline 并行\n\n```python\nclass PipelineParallelResNet50(ModelParallelResNet50):\n    def __init__(self, split_size=20, *args, **kwargs):\n        super(PipelineParallelResNet50, self).__init__(*args, **kwargs)\n        self.split_size = split_size\n\n    def forward(self, x):\n        splits = iter(x.split(self.split_size, dim=0))\n        s_next = next(splits)\n        s_prev = self.seq1(s_next).to('cuda:1')\n        ret = []\n\n        for s_next in splits:\n            # A. s_prev runs on cuda:1\n            s_prev = self.seq2(s_prev)\n            ret.append(self.fc(s_prev.view(s_prev.size(0), -1)))\n\n            # B. s_next runs on cuda:0, which can run concurrently with A\n            s_prev = self.seq1(s_next).to('cuda:1')\n\n        s_prev = self.seq2(s_prev)\n        ret.append(self.fc(s_prev.view(s_prev.size(0), -1)))\n\n        return torch.cat(ret)\n\nsetup = \"model = PipelineParallelResNet50()\"\npp_run_times = timeit.repeat(\n    stmt, setup, number=1, repeat=num_repeat, globals=globals())\npp_mean, pp_std = np.mean(pp_run_times), np.std(pp_run_times)\n\nplot([mp_mean, rn_mean, pp_mean],\n     [mp_std, rn_std, pp_std],\n     ['Model Parallel', 'Single GPU', 'Pipelining Model Parallel'],\n     'mp_vs_rn_vs_pp.png')\n```\n\n在2 GPUs上能有50%的提速，离100%还是有点差距，加速对比如下\n\n###![1](1.png) \n\n### \n\n依次是 2-GPU并行， 1-GPU运行， 2-GPU Pipelining 并行 的运行时间对比\n\n\n\n### nn.parallel.DistributedDataParallel\n\n　　基于torch.distributed 的分布式数据并行，原理和DataParallel类似，但是是跨机器和跨设备级别的数据并行，在每台机器和每台设备上复制，并且每个这样的副本处理一部分输入， 在向后传递过程中，将平均每个节点的梯度，总之，是按批处理维度分块指定设备之间的输入来并行化给定模块的应用程序。\n\n`DistributedDataParallel`可以通过以下两种方式使用：\n\n#### 单进程多GPU\n\n```python\ntorch.distributed.init_process_group(backend=\"nccl\")\n# device_ids will include all GPU devices by default\nmodel = DistributedDataParallel(model) \n\n```\n\n#### 多进程多GPU\n\n　　强烈推荐的使用方式，在单机多GPU的情况下，单进程很容易由于GIL出现利用率不足的问题，这时候多进程就是唯一解决办法。最佳实践是，将DDP(DistributedDataParallel)配合多进程一起使用，每个GPU分配一个进程，会比torch.nn.DataParallel快得多，也是目前Pytorch最快的训练方法。\n\n使用步骤：\n\n1. 在N个GPU的单机上生成N个进程，这个过程可以交给torch.distributed.launch完成\n\n```bash\npython -m torch.distributed.launch --nproc_per_node=n distributed_data_parallel.py\n```\n\n2. 在代码中绑定GPU 编号，同时并行化model\n\n```python\nparser = argparse.ArgumentParser()\n# FOR DISTRIBUTED:  Parse for the local_rank argument, which will be supplied\n# automatically by torch.distributed.launch.\nparser.add_argument(\"--local_rank\", default=0, type=int)\nargs = parser.parse_args()\n\n# 进程内绑定 GPU rank id\ntorch.cuda.set_device(args.local_rank)\n# 构造model\ntorch.distributed.init_process_group(backend='nccl', world_size=n, init_method='env://')\nmodel = torch.nn.parallel.DistributedDataParallel(\n  \t\t\t\t\t\t\t\t\tmodel,\n\t\t\t\t\t\t\t\t\t\tdevice_ids=[args.local_rank],\n\t\t\t\t\t\t\t\t\t\toutput_device=args.local_rank)\n```\n\nNote:\n\n1. nccl 后端是分布式训练使用的推荐的最快后端，适用于单节点和多节点分布式训练\n2. nccl同时支持混合精度分布式训练\n3. no_sync 用于禁用DDP进程之间的梯度同步，直到退出此上下文区域的第一个梯度Forward-Backward中进行梯度同步\n\n```python\nddp = torch.nn.DistributedDataParallel(model, pg)\nwith ddp.no_sync():\nfor input in inputs:\n\tddp(input).backward()  # no synchronization, accumulate grads\nddp(another_input).backward()  # synchronize grads\n```\n\n\n\n### apex.parallel.DistributedDataParallel\n\n　　基本上是torch.nn.parallel.DistributedDataParallel的wrapper，同时调用的时候优化了NCCL的使用和简化了参数\n\n``` bash\n# 调用的时候，注意 n <= 每个节点的GPU数量 同时默认 1个GPU对应1进程\ntorch.distributed.launch --nproc_per_node=n distributed_data_parallel.py\n# 会自动提供的参数目前已知的是:\n# args.local_rank\n# os.environ['WORLD_SIZE']\n\n```\n\n简化的要点：\n\n1. model = DDP(model) 即可，无需再传递 devices_ids output_device\n\n2. init_process_group 中的 init_method='env://'\n\n   ``` python\n   torch.distributed.init_process_group(backend='nccl',init_method='env://')\n   ```\n\n直接给出示例代码\n\n```python\n# distributed_data_parallel.py\nimport torch\nimport argparse\nimport os\nfrom apex import amp\n# FOR DISTRIBUTED: (can also use torch.nn.parallel.DistributedDataParallel instead)\nfrom apex.parallel import DistributedDataParallel\n\nparser = argparse.ArgumentParser()\n# FOR DISTRIBUTED:  Parse for the local_rank argument, which will be supplied\n# automatically by torch.distributed.launch.\nparser.add_argument(\"--local_rank\", default=0, type=int)\nargs = parser.parse_args()\n\n# FOR DISTRIBUTED:  If we are running under torch.distributed.launch,\n# the 'WORLD_SIZE' environment variable will also be set automatically.\nargs.distributed = False\nif 'WORLD_SIZE' in os.environ:\n    args.distributed = int(os.environ['WORLD_SIZE']) > 1\n\nif args.distributed:\n    # FOR DISTRIBUTED:  Set the device according to local_rank.\n    torch.cuda.set_device(args.local_rank)\n\n    # FOR DISTRIBUTED:  Initialize the backend.  torch.distributed.launch will provide\n    # environment variables, and requires that you use init_method=`env://`.\n    torch.distributed.init_process_group(backend='nccl',\n                                         init_method='env://')\n\ntorch.backends.cudnn.benchmark = True\n\nN, D_in, D_out = 64, 1024, 16\n\n# Each process receives its own batch of \"fake input data\" and \"fake target data.\"\n# The \"training loop\" in each process just uses this fake batch over and over.\n# https://github.com/NVIDIA/apex/tree/master/examples/imagenet provides a more realistic\n# example of distributed data sampling for both training and validation.\nx = torch.randn(N, D_in, device='cuda')\ny = torch.randn(N, D_out, device='cuda')\n\nmodel = torch.nn.Linear(D_in, D_out).cuda()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n\nmodel, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n\nif args.distributed:\n    # FOR DISTRIBUTED:  After amp.initialize, wrap the model with\n    # apex.parallel.DistributedDataParallel.\n    model = DistributedDataParallel(model)\n    # torch.nn.parallel.DistributedDataParallel is also fine, with some added args:\n    # model = torch.nn.parallel.DistributedDataParallel(model,\n    #                                                   device_ids=[args.local_rank],\n    #                                                   output_device=args.local_rank)\n\nloss_fn = torch.nn.MSELoss()\n\nfor t in range(500):\n    optimizer.zero_grad()\n    y_pred = model(x)\n    loss = loss_fn(y_pred, y)\n    with amp.scale_loss(loss, optimizer) as scaled_loss:\n        scaled_loss.backward()\n    optimizer.step()\n\nif args.local_rank == 0:\n    print(\"final loss = \", loss)\n```\n\n更复杂的多精度调用见 [mixed precision training with DDP](https://github.com/NVIDIA/apex/tree/master/examples/imagenet)\n\n\n\n### DDP 保存和加载检查点\n\n　　一般是使用torch.save 和 torch.load 来完成，但是在多进程下，优化方法是，仅在一个进程中保存，然后在其他所有进程中加载。\n\n``` python\nimport os\nimport tempfile\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\ndef demo_checkpoint(rank, world_size):\n    setup(rank, world_size)\n\n    # setup devices for this process, rank 1 uses GPUs [0, 1, 2, 3] and\n    # rank 2 uses GPUs [4, 5, 6, 7].\n    n = torch.cuda.device_count() // world_size\n    device_ids = list(range(rank * n, (rank + 1) * n))\n\n    model = ToyModel().to(device_ids[0])\n    # output_device defaults to device_ids[0]\n    ddp_model = DDP(model, device_ids=device_ids)\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    CHECKPOINT_PATH = tempfile.gettempdir() + \"/model.checkpoint\"\n    if rank == 0:\n        # All processes should see same parameters as they all start from same\n        # random parameters and gradients are synchronized in backward passes.\n        # Therefore, saving it in one process is sufficient.\n        torch.save(ddp_model.state_dict(), CHECKPOINT_PATH)\n\n    # Use a barrier() to make sure that process 1 loads the model after process\n    # 0 saves it.\n    dist.barrier()\n    # configure map_location properly\n    rank0_devices = [x - rank * len(device_ids) for x in device_ids]\n    device_pairs = zip(rank0_devices, device_ids)\n    map_location = {'cuda:%d' % x: 'cuda:%d' % y for x, y in device_pairs}\n    ddp_model.load_state_dict(\n        torch.load(CHECKPOINT_PATH, map_location=map_location))\n\n    optimizer.zero_grad()\n    outputs = ddp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(device_ids[0])\n    loss_fn = nn.MSELoss()\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    # Use a barrier() to make sure that all processes have finished reading the\n    # checkpoint\n    dist.barrier()\n\n    if rank == 0:\n        os.remove(CHECKPOINT_PATH)\n\n    cleanup()\n```\n\n\n\n### DDP 与模型拼接并行\n\n　　DDP 还可以与多 GPU 模型一起使用，但是不支持进程内的复制。 您需要为每个模块副本创建一个进程，与每个进程的多个副本相比，通常可以提高性能。 当训练具有大量数据的大型模型时，DDP 包装多 GPU 模型特别有用。 使用此功能时，需要小心地实现多 GPU 模型，以避免使用硬编码的设备，因为会将不同的模型副本放置到不同的设备上。\n\n``` python\nclass ToyMpModel(nn.Module):\n    def __init__(self, dev0, dev1):\n        super(ToyMpModel, self).__init__()\n        self.dev0 = dev0\n        self.dev1 = dev1\n        self.net1 = torch.nn.Linear(10, 10).to(dev0)\n        self.relu = torch.nn.ReLU()\n        self.net2 = torch.nn.Linear(10, 5).to(dev1)\n\n    def forward(self, x):\n        x = x.to(self.dev0)\n        x = self.relu(self.net1(x))\n        x = x.to(self.dev1)\n        return self.net2(x)\n```\n\n将多 GPU 模型传递给 DDP 时，不得设置`device_ids`和`output_device`。 输入和输出数据将通过应用程序或模型`forward()`方法放置在适当的设备中。\n\n``` python\ndef demo_model_parallel(rank, world_size):\n    setup(rank, world_size)\n\n    # setup mp_model and devices for this process\n    dev0 = rank * 2\n    dev1 = rank * 2 + 1\n    mp_model = ToyMpModel(dev0, dev1)\n    ddp_mp_model = DDP(mp_model)\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_mp_model.parameters(), lr=0.001)\n\n    optimizer.zero_grad()\n    # outputs will be on dev1\n    outputs = ddp_mp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(dev1)\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    cleanup()\n\nif __name__ == \"__main__\":\n    run_demo(demo_basic, 2)\n    run_demo(demo_checkpoint, 2)\n\n    if torch.cuda.device_count() >= 8:\n        run_demo(demo_model_parallel, 4)\n```\n\n注意上述setup 是通过 torch.multiprocessing.spawn 来完成多线程启动的，所以初始化方法也需要通过setup单独来指定\n\n```python\nimport os\nimport tempfile\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n\n    # initialize the process group\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n\n    # Explicitly setting seed to make sure that models created in two processes\n    # start from same random weights and biases.\n    torch.manual_seed(42)\n\ndef cleanup():\n    dist.destroy_process_group()\n    \nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(10, 10)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(10, 5)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\ndef demo_basic(rank, world_size):\n    setup(rank, world_size)\n\n    # setup devices for this process, rank 1 uses GPUs [0, 1, 2, 3] and\n    # rank 2 uses GPUs [4, 5, 6, 7].\n    n = torch.cuda.device_count() // world_size\n    device_ids = list(range(rank * n, (rank + 1) * n))\n\n    # create model and move it to device_ids[0]\n    model = ToyModel().to(device_ids[0])\n    # output_device defaults to device_ids[0]\n    ddp_model = DDP(model, device_ids=device_ids)\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    optimizer.zero_grad()\n    outputs = ddp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(device_ids[0])\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    cleanup()\n\ndef run_demo(demo_fn, world_size):\n    mp.spawn(demo_fn,\n             args=(world_size,),\n             nprocs=world_size,\n             join=True)\n```\n\n\n\n### Conclusion\n\n　　至此基本分析完所有Pytorch下的多GPU使用技巧，大体方案还是利用多进程的方式来规避GIL来提升性能，只需要用DDP 来包装model ，同时进行分布式对应的初始化，然后多进程启动即可加速训练，基本没有大的变动。\n\n\n\nReferences:\n\n[torch.nn](https://pytorch.apachecn.org/docs/1.4/75.html)\n\n[DistributedDataParallel API](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html?highlight=distributeddataparallel#torch.nn.parallel.DistributedDataParallel)\n\n[CUDA Semantics](https://pytorch.org/docs/stable/notes/cuda.html#cuda-nn-ddp-instead)\n\n[分布式数据并行入门](https://pytorch.apachecn.org/docs/1.4/34.html)\n\n[用Pytorch编写分布式应用程序](https://pytorch.apachecn.org/docs/1.4/35.html)\n\n[apex.parallel](https://nvidia.github.io/apex/parallel.html)\n\n","source":"_posts/GPU-in-Pytorch-并行和分布式实践.md","raw":"---\ntitle: GPU in Pytorch  并行和分布式实践\ntoc: true\nmathjax: true\ntop: false\ncover: true\ndate: 2020-11-25 15:07:07\nupdated:\ncategories: Pytorch\ntags:\n\t- Pytorch\n\t- CUDA\n\t- GPU\n---\n\n　　虽然还没有机会用到CUDA集群，但是前段时间对协程和并行化的研究，让我忍不住想要探索一下如何在多个GPU下利用Pytorch加快训练的实践方法，算是为之后并行训练优化一个理论参考吧！\n\n　　Pytorch 大体上有3种实现并行的接口（另外还有一种不利用接口的拼接模型的技巧，之后再单独讨论），分别是：torch.multiprocessing, nn.DataParallel, nn.parallel.DistributedDataParallel，如果是是GPU多卡运行，最佳实践是 nn.parallle.DistributedDataParallel,官方文档 [CUDA SEMANTIC](https://pytorch.org/docs/stable/notes/cuda.html#cuda-nn-ddp-instead) 是这么描述的：\n\n> **Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel**\n>\n> Most use cases involving batched inputs and multiple GPUs should default to using [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel) to utilize more than one GPU.\n>\n> There are significant caveats to using CUDA models with [`multiprocessing`](https://pytorch.org/docs/stable/multiprocessing.html#module-torch.multiprocessing); unless care is taken to meet the data handling requirements exactly, it is likely that your program will have incorrect or undefined behavior.\n>\n> It is recommended to use [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel), instead of [`DataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel) to do multi-GPU training, even if there is only a single node.\n>\n> The difference between [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel) and [`DataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel) is: [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel) uses multiprocessing where a process is created for each GPU, while [`DataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel) uses multithreading. By using multiprocessing, each GPU has its dedicated process, this avoids the performance overhead caused by GIL of Python interpreter.\n>\n> If you use [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel), you could use torch.distributed.launch utility to launch your program, see [Third-party backends](https://pytorch.org/docs/stable/distributed.html#distributed-launch).\n\n\n\n### torch.multiprocessing\n\n　　torch.multiprocessing 是 Python下 multiprocessing 的替代品，接口基本一致，并根据情况进行扩张，建议使用 python: multiprocessing.Queue 在进程中传递Pytorch对象。除此之外，还有很多坑，详见官方笔记 [MULTIPROCESSING BEST PRACTICES](https://pytorch.org/docs/stable/notes/multiprocessing.html), [并行处理最佳实践](https://pytorch.apachecn.org/docs/1.4/64.html)\n\n```python\nimport torch.multiprocessing as mp\nfrom model import MyModel\n\ndef train(model):\n    # Construct data_loader, optimizer, etc.\n    for data, labels in data_loader:\n        optimizer.zero_grad()\n        loss_fn(model(data), labels).backward()\n        optimizer.step()  # This will update the shared parameters\n\nif __name__ == '__main__':\n    num_processes = 4\n    model = MyModel()\n    # NOTE: this is required for the ``fork`` method to work\n    model.share_memory()\n    processes = []\n    for rank in range(num_processes):\n        p = mp.Process(target=train, args=(model,))\n        p.start()\n        processes.append(p)\n    for p in processes:\n        p.join()\n```\n\n\n\n### DataParallel\n\n　　DataParallel原理是，把model 副本copy到所有GPU上，其中每个GPU消耗数据的不同分区，相当于SIMD，把数据条目根据GPU数量重新分配\n\n#### 单机DataParallel并行\n\n```python\nmodel = nn.DataParallel(model)\n```\n\n　　代码验证 outside model 数据维度  和 inside model 维度\n\n```python\nclass Model(nn.Module):\n    # Our model\n\n    def __init__(self, input_size, output_size):\n        super(Model, self).__init__()\n        self.fc = nn.Linear(input_size, output_size)\n\n    def forward(self, input):\n        output = self.fc(input)\n        print(\"\\tIn Model: input size\", input.size(),\n              \"output size\", output.size())\n\n        return output\n        \n\nmodel = Model(input_size, output_size)\nif torch.cuda.device_count() > 1:\n  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n  model = nn.DataParallel(model)\n\nmodel.to(device)\n\nfor data in rand_loader:\n    input = data.to(device)\n    output = model(input)\n    print(\"Outside: input size\", input.size(),\n          \"output_size\", output.size())\n    \n# 2 GPUs\n# on 2 GPUs\nLet's use 2 GPUs!\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\nOutside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n```\n\n优点是，通过消耗输入数据不同分区的方式加快训练过程，缺点是，如果单个模型太大，无法放入单个GPU，就无法运行，这时有个Trick可以把模型分段载入不同GPU，利用拼接的方式完成并行训练，该模型将单个模型拆分到不同的 GPU 上，而不是在每个 GPU 上复制整个模型(具体来说， 假设模型`m`包含 10 层：使用`DataParallel`时，每个 GPU 都具有这 10 层中每个层的副本，而当在两个 GPU 上并行使用模型时，每个 GPU 可以承载 5 层）\n\n#### 单机模型拼接并行\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = torch.nn.Linear(10, 10).to('cuda:0')\n        self.relu = torch.nn.ReLU()\n        self.net2 = torch.nn.Linear(10, 5).to('cuda:1')\n\n    def forward(self, x):\n        x = self.relu(self.net1(x.to('cuda:0')))\n        return self.net2(x.to('cuda:1'))\n```\n\n对于模型太大而无法放入单个 GPU 的情况，上述实现解决了该问题。 但是，分析原理的时候可能已经注意到，如果模型单个GPU可以载入，它将比在单个 GPU 上运行它要慢。 这是因为在任何时间点，两个 GPU 中只有一个在工作，而另一个在那儿什么也没做。 由于中间输出需要在`layer2`和`layer3`之间从`cuda:0`复制到`cuda:1`，因此性能进一步恶化。\n\n除此之外，还有通过异步方式构建流水线的手段加速，这个Trick比较精巧和优雅，思想可以学习一下\n\n#### 单机Pipeline 并行\n\n```python\nclass PipelineParallelResNet50(ModelParallelResNet50):\n    def __init__(self, split_size=20, *args, **kwargs):\n        super(PipelineParallelResNet50, self).__init__(*args, **kwargs)\n        self.split_size = split_size\n\n    def forward(self, x):\n        splits = iter(x.split(self.split_size, dim=0))\n        s_next = next(splits)\n        s_prev = self.seq1(s_next).to('cuda:1')\n        ret = []\n\n        for s_next in splits:\n            # A. s_prev runs on cuda:1\n            s_prev = self.seq2(s_prev)\n            ret.append(self.fc(s_prev.view(s_prev.size(0), -1)))\n\n            # B. s_next runs on cuda:0, which can run concurrently with A\n            s_prev = self.seq1(s_next).to('cuda:1')\n\n        s_prev = self.seq2(s_prev)\n        ret.append(self.fc(s_prev.view(s_prev.size(0), -1)))\n\n        return torch.cat(ret)\n\nsetup = \"model = PipelineParallelResNet50()\"\npp_run_times = timeit.repeat(\n    stmt, setup, number=1, repeat=num_repeat, globals=globals())\npp_mean, pp_std = np.mean(pp_run_times), np.std(pp_run_times)\n\nplot([mp_mean, rn_mean, pp_mean],\n     [mp_std, rn_std, pp_std],\n     ['Model Parallel', 'Single GPU', 'Pipelining Model Parallel'],\n     'mp_vs_rn_vs_pp.png')\n```\n\n在2 GPUs上能有50%的提速，离100%还是有点差距，加速对比如下\n\n###![1](1.png) \n\n### \n\n依次是 2-GPU并行， 1-GPU运行， 2-GPU Pipelining 并行 的运行时间对比\n\n\n\n### nn.parallel.DistributedDataParallel\n\n　　基于torch.distributed 的分布式数据并行，原理和DataParallel类似，但是是跨机器和跨设备级别的数据并行，在每台机器和每台设备上复制，并且每个这样的副本处理一部分输入， 在向后传递过程中，将平均每个节点的梯度，总之，是按批处理维度分块指定设备之间的输入来并行化给定模块的应用程序。\n\n`DistributedDataParallel`可以通过以下两种方式使用：\n\n#### 单进程多GPU\n\n```python\ntorch.distributed.init_process_group(backend=\"nccl\")\n# device_ids will include all GPU devices by default\nmodel = DistributedDataParallel(model) \n\n```\n\n#### 多进程多GPU\n\n　　强烈推荐的使用方式，在单机多GPU的情况下，单进程很容易由于GIL出现利用率不足的问题，这时候多进程就是唯一解决办法。最佳实践是，将DDP(DistributedDataParallel)配合多进程一起使用，每个GPU分配一个进程，会比torch.nn.DataParallel快得多，也是目前Pytorch最快的训练方法。\n\n使用步骤：\n\n1. 在N个GPU的单机上生成N个进程，这个过程可以交给torch.distributed.launch完成\n\n```bash\npython -m torch.distributed.launch --nproc_per_node=n distributed_data_parallel.py\n```\n\n2. 在代码中绑定GPU 编号，同时并行化model\n\n```python\nparser = argparse.ArgumentParser()\n# FOR DISTRIBUTED:  Parse for the local_rank argument, which will be supplied\n# automatically by torch.distributed.launch.\nparser.add_argument(\"--local_rank\", default=0, type=int)\nargs = parser.parse_args()\n\n# 进程内绑定 GPU rank id\ntorch.cuda.set_device(args.local_rank)\n# 构造model\ntorch.distributed.init_process_group(backend='nccl', world_size=n, init_method='env://')\nmodel = torch.nn.parallel.DistributedDataParallel(\n  \t\t\t\t\t\t\t\t\tmodel,\n\t\t\t\t\t\t\t\t\t\tdevice_ids=[args.local_rank],\n\t\t\t\t\t\t\t\t\t\toutput_device=args.local_rank)\n```\n\nNote:\n\n1. nccl 后端是分布式训练使用的推荐的最快后端，适用于单节点和多节点分布式训练\n2. nccl同时支持混合精度分布式训练\n3. no_sync 用于禁用DDP进程之间的梯度同步，直到退出此上下文区域的第一个梯度Forward-Backward中进行梯度同步\n\n```python\nddp = torch.nn.DistributedDataParallel(model, pg)\nwith ddp.no_sync():\nfor input in inputs:\n\tddp(input).backward()  # no synchronization, accumulate grads\nddp(another_input).backward()  # synchronize grads\n```\n\n\n\n### apex.parallel.DistributedDataParallel\n\n　　基本上是torch.nn.parallel.DistributedDataParallel的wrapper，同时调用的时候优化了NCCL的使用和简化了参数\n\n``` bash\n# 调用的时候，注意 n <= 每个节点的GPU数量 同时默认 1个GPU对应1进程\ntorch.distributed.launch --nproc_per_node=n distributed_data_parallel.py\n# 会自动提供的参数目前已知的是:\n# args.local_rank\n# os.environ['WORLD_SIZE']\n\n```\n\n简化的要点：\n\n1. model = DDP(model) 即可，无需再传递 devices_ids output_device\n\n2. init_process_group 中的 init_method='env://'\n\n   ``` python\n   torch.distributed.init_process_group(backend='nccl',init_method='env://')\n   ```\n\n直接给出示例代码\n\n```python\n# distributed_data_parallel.py\nimport torch\nimport argparse\nimport os\nfrom apex import amp\n# FOR DISTRIBUTED: (can also use torch.nn.parallel.DistributedDataParallel instead)\nfrom apex.parallel import DistributedDataParallel\n\nparser = argparse.ArgumentParser()\n# FOR DISTRIBUTED:  Parse for the local_rank argument, which will be supplied\n# automatically by torch.distributed.launch.\nparser.add_argument(\"--local_rank\", default=0, type=int)\nargs = parser.parse_args()\n\n# FOR DISTRIBUTED:  If we are running under torch.distributed.launch,\n# the 'WORLD_SIZE' environment variable will also be set automatically.\nargs.distributed = False\nif 'WORLD_SIZE' in os.environ:\n    args.distributed = int(os.environ['WORLD_SIZE']) > 1\n\nif args.distributed:\n    # FOR DISTRIBUTED:  Set the device according to local_rank.\n    torch.cuda.set_device(args.local_rank)\n\n    # FOR DISTRIBUTED:  Initialize the backend.  torch.distributed.launch will provide\n    # environment variables, and requires that you use init_method=`env://`.\n    torch.distributed.init_process_group(backend='nccl',\n                                         init_method='env://')\n\ntorch.backends.cudnn.benchmark = True\n\nN, D_in, D_out = 64, 1024, 16\n\n# Each process receives its own batch of \"fake input data\" and \"fake target data.\"\n# The \"training loop\" in each process just uses this fake batch over and over.\n# https://github.com/NVIDIA/apex/tree/master/examples/imagenet provides a more realistic\n# example of distributed data sampling for both training and validation.\nx = torch.randn(N, D_in, device='cuda')\ny = torch.randn(N, D_out, device='cuda')\n\nmodel = torch.nn.Linear(D_in, D_out).cuda()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n\nmodel, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n\nif args.distributed:\n    # FOR DISTRIBUTED:  After amp.initialize, wrap the model with\n    # apex.parallel.DistributedDataParallel.\n    model = DistributedDataParallel(model)\n    # torch.nn.parallel.DistributedDataParallel is also fine, with some added args:\n    # model = torch.nn.parallel.DistributedDataParallel(model,\n    #                                                   device_ids=[args.local_rank],\n    #                                                   output_device=args.local_rank)\n\nloss_fn = torch.nn.MSELoss()\n\nfor t in range(500):\n    optimizer.zero_grad()\n    y_pred = model(x)\n    loss = loss_fn(y_pred, y)\n    with amp.scale_loss(loss, optimizer) as scaled_loss:\n        scaled_loss.backward()\n    optimizer.step()\n\nif args.local_rank == 0:\n    print(\"final loss = \", loss)\n```\n\n更复杂的多精度调用见 [mixed precision training with DDP](https://github.com/NVIDIA/apex/tree/master/examples/imagenet)\n\n\n\n### DDP 保存和加载检查点\n\n　　一般是使用torch.save 和 torch.load 来完成，但是在多进程下，优化方法是，仅在一个进程中保存，然后在其他所有进程中加载。\n\n``` python\nimport os\nimport tempfile\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\ndef demo_checkpoint(rank, world_size):\n    setup(rank, world_size)\n\n    # setup devices for this process, rank 1 uses GPUs [0, 1, 2, 3] and\n    # rank 2 uses GPUs [4, 5, 6, 7].\n    n = torch.cuda.device_count() // world_size\n    device_ids = list(range(rank * n, (rank + 1) * n))\n\n    model = ToyModel().to(device_ids[0])\n    # output_device defaults to device_ids[0]\n    ddp_model = DDP(model, device_ids=device_ids)\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    CHECKPOINT_PATH = tempfile.gettempdir() + \"/model.checkpoint\"\n    if rank == 0:\n        # All processes should see same parameters as they all start from same\n        # random parameters and gradients are synchronized in backward passes.\n        # Therefore, saving it in one process is sufficient.\n        torch.save(ddp_model.state_dict(), CHECKPOINT_PATH)\n\n    # Use a barrier() to make sure that process 1 loads the model after process\n    # 0 saves it.\n    dist.barrier()\n    # configure map_location properly\n    rank0_devices = [x - rank * len(device_ids) for x in device_ids]\n    device_pairs = zip(rank0_devices, device_ids)\n    map_location = {'cuda:%d' % x: 'cuda:%d' % y for x, y in device_pairs}\n    ddp_model.load_state_dict(\n        torch.load(CHECKPOINT_PATH, map_location=map_location))\n\n    optimizer.zero_grad()\n    outputs = ddp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(device_ids[0])\n    loss_fn = nn.MSELoss()\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    # Use a barrier() to make sure that all processes have finished reading the\n    # checkpoint\n    dist.barrier()\n\n    if rank == 0:\n        os.remove(CHECKPOINT_PATH)\n\n    cleanup()\n```\n\n\n\n### DDP 与模型拼接并行\n\n　　DDP 还可以与多 GPU 模型一起使用，但是不支持进程内的复制。 您需要为每个模块副本创建一个进程，与每个进程的多个副本相比，通常可以提高性能。 当训练具有大量数据的大型模型时，DDP 包装多 GPU 模型特别有用。 使用此功能时，需要小心地实现多 GPU 模型，以避免使用硬编码的设备，因为会将不同的模型副本放置到不同的设备上。\n\n``` python\nclass ToyMpModel(nn.Module):\n    def __init__(self, dev0, dev1):\n        super(ToyMpModel, self).__init__()\n        self.dev0 = dev0\n        self.dev1 = dev1\n        self.net1 = torch.nn.Linear(10, 10).to(dev0)\n        self.relu = torch.nn.ReLU()\n        self.net2 = torch.nn.Linear(10, 5).to(dev1)\n\n    def forward(self, x):\n        x = x.to(self.dev0)\n        x = self.relu(self.net1(x))\n        x = x.to(self.dev1)\n        return self.net2(x)\n```\n\n将多 GPU 模型传递给 DDP 时，不得设置`device_ids`和`output_device`。 输入和输出数据将通过应用程序或模型`forward()`方法放置在适当的设备中。\n\n``` python\ndef demo_model_parallel(rank, world_size):\n    setup(rank, world_size)\n\n    # setup mp_model and devices for this process\n    dev0 = rank * 2\n    dev1 = rank * 2 + 1\n    mp_model = ToyMpModel(dev0, dev1)\n    ddp_mp_model = DDP(mp_model)\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_mp_model.parameters(), lr=0.001)\n\n    optimizer.zero_grad()\n    # outputs will be on dev1\n    outputs = ddp_mp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(dev1)\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    cleanup()\n\nif __name__ == \"__main__\":\n    run_demo(demo_basic, 2)\n    run_demo(demo_checkpoint, 2)\n\n    if torch.cuda.device_count() >= 8:\n        run_demo(demo_model_parallel, 4)\n```\n\n注意上述setup 是通过 torch.multiprocessing.spawn 来完成多线程启动的，所以初始化方法也需要通过setup单独来指定\n\n```python\nimport os\nimport tempfile\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n\n    # initialize the process group\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n\n    # Explicitly setting seed to make sure that models created in two processes\n    # start from same random weights and biases.\n    torch.manual_seed(42)\n\ndef cleanup():\n    dist.destroy_process_group()\n    \nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(10, 10)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(10, 5)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\ndef demo_basic(rank, world_size):\n    setup(rank, world_size)\n\n    # setup devices for this process, rank 1 uses GPUs [0, 1, 2, 3] and\n    # rank 2 uses GPUs [4, 5, 6, 7].\n    n = torch.cuda.device_count() // world_size\n    device_ids = list(range(rank * n, (rank + 1) * n))\n\n    # create model and move it to device_ids[0]\n    model = ToyModel().to(device_ids[0])\n    # output_device defaults to device_ids[0]\n    ddp_model = DDP(model, device_ids=device_ids)\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    optimizer.zero_grad()\n    outputs = ddp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(device_ids[0])\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    cleanup()\n\ndef run_demo(demo_fn, world_size):\n    mp.spawn(demo_fn,\n             args=(world_size,),\n             nprocs=world_size,\n             join=True)\n```\n\n\n\n### Conclusion\n\n　　至此基本分析完所有Pytorch下的多GPU使用技巧，大体方案还是利用多进程的方式来规避GIL来提升性能，只需要用DDP 来包装model ，同时进行分布式对应的初始化，然后多进程启动即可加速训练，基本没有大的变动。\n\n\n\nReferences:\n\n[torch.nn](https://pytorch.apachecn.org/docs/1.4/75.html)\n\n[DistributedDataParallel API](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html?highlight=distributeddataparallel#torch.nn.parallel.DistributedDataParallel)\n\n[CUDA Semantics](https://pytorch.org/docs/stable/notes/cuda.html#cuda-nn-ddp-instead)\n\n[分布式数据并行入门](https://pytorch.apachecn.org/docs/1.4/34.html)\n\n[用Pytorch编写分布式应用程序](https://pytorch.apachecn.org/docs/1.4/35.html)\n\n[apex.parallel](https://nvidia.github.io/apex/parallel.html)\n\n","slug":"GPU-in-Pytorch-并行和分布式实践","published":1,"_id":"ckhyotdja0000l9289pxq20m8","comments":1,"layout":"post","photos":[],"link":"","content":"<p>　　虽然还没有机会用到CUDA集群，但是前段时间对协程和并行化的研究，让我忍不住想要探索一下如何在多个GPU下利用Pytorch加快训练的实践方法，算是为之后并行训练优化一个理论参考吧！</p>\n<p>　　Pytorch 大体上有3种实现并行的接口（另外还有一种不利用接口的拼接模型的技巧，之后再单独讨论），分别是：torch.multiprocessing, nn.DataParallel, nn.parallel.DistributedDataParallel，如果是是GPU多卡运行，最佳实践是 nn.parallle.DistributedDataParallel,官方文档 <a href=\"https://pytorch.org/docs/stable/notes/cuda.html#cuda-nn-ddp-instead\">CUDA SEMANTIC</a> 是这么描述的：</p>\n<blockquote>\n<p><strong>Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel</strong></p>\n<p>Most use cases involving batched inputs and multiple GPUs should default to using <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\"><code>DistributedDataParallel</code></a> to utilize more than one GPU.</p>\n<p>There are significant caveats to using CUDA models with <a href=\"https://pytorch.org/docs/stable/multiprocessing.html#module-torch.multiprocessing\"><code>multiprocessing</code></a>; unless care is taken to meet the data handling requirements exactly, it is likely that your program will have incorrect or undefined behavior.</p>\n<p>It is recommended to use <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\"><code>DistributedDataParallel</code></a>, instead of <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel\"><code>DataParallel</code></a> to do multi-GPU training, even if there is only a single node.</p>\n<p>The difference between <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\"><code>DistributedDataParallel</code></a> and <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel\"><code>DataParallel</code></a> is: <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\"><code>DistributedDataParallel</code></a> uses multiprocessing where a process is created for each GPU, while <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel\"><code>DataParallel</code></a> uses multithreading. By using multiprocessing, each GPU has its dedicated process, this avoids the performance overhead caused by GIL of Python interpreter.</p>\n<p>If you use <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\"><code>DistributedDataParallel</code></a>, you could use torch.distributed.launch utility to launch your program, see <a href=\"https://pytorch.org/docs/stable/distributed.html#distributed-launch\">Third-party backends</a>.</p>\n</blockquote>\n<h3 id=\"torch-multiprocessing\"><a href=\"#torch-multiprocessing\" class=\"headerlink\" title=\"torch.multiprocessing\"></a>torch.multiprocessing</h3><p>　　torch.multiprocessing 是 Python下 multiprocessing 的替代品，接口基本一致，并根据情况进行扩张，建议使用 python: multiprocessing.Queue 在进程中传递Pytorch对象。除此之外，还有很多坑，详见官方笔记 <a href=\"https://pytorch.org/docs/stable/notes/multiprocessing.html\">MULTIPROCESSING BEST PRACTICES</a>, <a href=\"https://pytorch.apachecn.org/docs/1.4/64.html\">并行处理最佳实践</a></p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>multiprocessing <span class=\"token keyword\">as</span> mp\n<span class=\"token keyword\">from</span> model <span class=\"token keyword\">import</span> MyModel\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">train</span><span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\" spellcheck=\"true\"># Construct data_loader, optimizer, etc.</span>\n    <span class=\"token keyword\">for</span> data<span class=\"token punctuation\">,</span> labels <span class=\"token keyword\">in</span> data_loader<span class=\"token punctuation\">:</span>\n        optimizer<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        loss_fn<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> labels<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        optimizer<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\" spellcheck=\"true\"># This will update the shared parameters</span>\n\n<span class=\"token keyword\">if</span> __name__ <span class=\"token operator\">==</span> <span class=\"token string\">'__main__'</span><span class=\"token punctuation\">:</span>\n    num_processes <span class=\"token operator\">=</span> <span class=\"token number\">4</span>\n    model <span class=\"token operator\">=</span> MyModel<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># NOTE: this is required for the ``fork`` method to work</span>\n    model<span class=\"token punctuation\">.</span>share_memory<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    processes <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">for</span> rank <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span>num_processes<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        p <span class=\"token operator\">=</span> mp<span class=\"token punctuation\">.</span>Process<span class=\"token punctuation\">(</span>target<span class=\"token operator\">=</span>train<span class=\"token punctuation\">,</span> args<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        p<span class=\"token punctuation\">.</span>start<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        processes<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>p<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">for</span> p <span class=\"token keyword\">in</span> processes<span class=\"token punctuation\">:</span>\n        p<span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre>\n<h3 id=\"DataParallel\"><a href=\"#DataParallel\" class=\"headerlink\" title=\"DataParallel\"></a>DataParallel</h3><p>　　DataParallel原理是，把model 副本copy到所有GPU上，其中每个GPU消耗数据的不同分区，相当于SIMD，把数据条目根据GPU数量重新分配</p>\n<h4 id=\"单机DataParallel并行\"><a href=\"#单机DataParallel并行\" class=\"headerlink\" title=\"单机DataParallel并行\"></a>单机DataParallel并行</h4><pre class=\" language-python\"><code class=\"language-python\">model <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>DataParallel<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">)</span></code></pre>\n<p>　　代码验证 outside model 数据维度  和 inside model 维度</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">Model</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\" spellcheck=\"true\"># Our model</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> input_size<span class=\"token punctuation\">,</span> output_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        super<span class=\"token punctuation\">(</span>Model<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>fc <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>input_size<span class=\"token punctuation\">,</span> output_size<span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> input<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        output <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>fc<span class=\"token punctuation\">(</span>input<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\tIn Model: input size\"</span><span class=\"token punctuation\">,</span> input<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n              <span class=\"token string\">\"output size\"</span><span class=\"token punctuation\">,</span> output<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n        <span class=\"token keyword\">return</span> output\n\n\nmodel <span class=\"token operator\">=</span> Model<span class=\"token punctuation\">(</span>input_size<span class=\"token punctuation\">,</span> output_size<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">if</span> torch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>device_count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">></span> <span class=\"token number\">1</span><span class=\"token punctuation\">:</span>\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Let's use\"</span><span class=\"token punctuation\">,</span> torch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>device_count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"GPUs!\"</span><span class=\"token punctuation\">)</span>\n  <span class=\"token comment\" spellcheck=\"true\"># dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs</span>\n  model <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>DataParallel<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">)</span>\n\nmodel<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">for</span> data <span class=\"token keyword\">in</span> rand_loader<span class=\"token punctuation\">:</span>\n    input <span class=\"token operator\">=</span> data<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device<span class=\"token punctuation\">)</span>\n    output <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>input<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Outside: input size\"</span><span class=\"token punctuation\">,</span> input<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n          <span class=\"token string\">\"output_size\"</span><span class=\"token punctuation\">,</span> output<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># 2 GPUs</span>\n<span class=\"token comment\" spellcheck=\"true\"># on 2 GPUs</span>\nLet's use <span class=\"token number\">2</span> GPUs!\n    In Model<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    In Model<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nOutside<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">30</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output_size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">30</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    In Model<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    In Model<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nOutside<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">30</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output_size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">30</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    In Model<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    In Model<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nOutside<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">30</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output_size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">30</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    In Model<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    In Model<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nOutside<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output_size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre>\n<p>优点是，通过消耗输入数据不同分区的方式加快训练过程，缺点是，如果单个模型太大，无法放入单个GPU，就无法运行，这时有个Trick可以把模型分段载入不同GPU，利用拼接的方式完成并行训练，该模型将单个模型拆分到不同的 GPU 上，而不是在每个 GPU 上复制整个模型(具体来说， 假设模型<code>m</code>包含 10 层：使用<code>DataParallel</code>时，每个 GPU 都具有这 10 层中每个层的副本，而当在两个 GPU 上并行使用模型时，每个 GPU 可以承载 5 层）</p>\n<h4 id=\"单机模型拼接并行\"><a href=\"#单机模型拼接并行\" class=\"headerlink\" title=\"单机模型拼接并行\"></a>单机模型拼接并行</h4><pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>nn <span class=\"token keyword\">as</span> nn\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>optim <span class=\"token keyword\">as</span> optim\n\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">ToyModel</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        super<span class=\"token punctuation\">(</span>ToyModel<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>net1 <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span><span class=\"token string\">'cuda:0'</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>relu <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>ReLU<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>net2 <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span><span class=\"token string\">'cuda:1'</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        x <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>relu<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>net1<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span><span class=\"token string\">'cuda:0'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> self<span class=\"token punctuation\">.</span>net2<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span><span class=\"token string\">'cuda:1'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre>\n<p>对于模型太大而无法放入单个 GPU 的情况，上述实现解决了该问题。 但是，分析原理的时候可能已经注意到，如果模型单个GPU可以载入，它将比在单个 GPU 上运行它要慢。 这是因为在任何时间点，两个 GPU 中只有一个在工作，而另一个在那儿什么也没做。 由于中间输出需要在<code>layer2</code>和<code>layer3</code>之间从<code>cuda:0</code>复制到<code>cuda:1</code>，因此性能进一步恶化。</p>\n<p>除此之外，还有通过异步方式构建流水线的手段加速，这个Trick比较精巧和优雅，思想可以学习一下</p>\n<h4 id=\"单机Pipeline-并行\"><a href=\"#单机Pipeline-并行\" class=\"headerlink\" title=\"单机Pipeline 并行\"></a>单机Pipeline 并行</h4><pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">PipelineParallelResNet50</span><span class=\"token punctuation\">(</span>ModelParallelResNet50<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> split_size<span class=\"token operator\">=</span><span class=\"token number\">20</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">*</span>args<span class=\"token punctuation\">,</span> <span class=\"token operator\">**</span>kwargs<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        super<span class=\"token punctuation\">(</span>PipelineParallelResNet50<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token operator\">*</span>args<span class=\"token punctuation\">,</span> <span class=\"token operator\">**</span>kwargs<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>split_size <span class=\"token operator\">=</span> split_size\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        splits <span class=\"token operator\">=</span> iter<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>split_size<span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        s_next <span class=\"token operator\">=</span> next<span class=\"token punctuation\">(</span>splits<span class=\"token punctuation\">)</span>\n        s_prev <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>seq1<span class=\"token punctuation\">(</span>s_next<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span><span class=\"token string\">'cuda:1'</span><span class=\"token punctuation\">)</span>\n        ret <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n\n        <span class=\"token keyword\">for</span> s_next <span class=\"token keyword\">in</span> splits<span class=\"token punctuation\">:</span>\n            <span class=\"token comment\" spellcheck=\"true\"># A. s_prev runs on cuda:1</span>\n            s_prev <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>seq2<span class=\"token punctuation\">(</span>s_prev<span class=\"token punctuation\">)</span>\n            ret<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>fc<span class=\"token punctuation\">(</span>s_prev<span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span>s_prev<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n            <span class=\"token comment\" spellcheck=\"true\"># B. s_next runs on cuda:0, which can run concurrently with A</span>\n            s_prev <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>seq1<span class=\"token punctuation\">(</span>s_next<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span><span class=\"token string\">'cuda:1'</span><span class=\"token punctuation\">)</span>\n\n        s_prev <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>seq2<span class=\"token punctuation\">(</span>s_prev<span class=\"token punctuation\">)</span>\n        ret<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>fc<span class=\"token punctuation\">(</span>s_prev<span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span>s_prev<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n        <span class=\"token keyword\">return</span> torch<span class=\"token punctuation\">.</span>cat<span class=\"token punctuation\">(</span>ret<span class=\"token punctuation\">)</span>\n\nsetup <span class=\"token operator\">=</span> <span class=\"token string\">\"model = PipelineParallelResNet50()\"</span>\npp_run_times <span class=\"token operator\">=</span> timeit<span class=\"token punctuation\">.</span>repeat<span class=\"token punctuation\">(</span>\n    stmt<span class=\"token punctuation\">,</span> setup<span class=\"token punctuation\">,</span> number<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> repeat<span class=\"token operator\">=</span>num_repeat<span class=\"token punctuation\">,</span> globals<span class=\"token operator\">=</span>globals<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\npp_mean<span class=\"token punctuation\">,</span> pp_std <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>mean<span class=\"token punctuation\">(</span>pp_run_times<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> np<span class=\"token punctuation\">.</span>std<span class=\"token punctuation\">(</span>pp_run_times<span class=\"token punctuation\">)</span>\n\nplot<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>mp_mean<span class=\"token punctuation\">,</span> rn_mean<span class=\"token punctuation\">,</span> pp_mean<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n     <span class=\"token punctuation\">[</span>mp_std<span class=\"token punctuation\">,</span> rn_std<span class=\"token punctuation\">,</span> pp_std<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n     <span class=\"token punctuation\">[</span><span class=\"token string\">'Model Parallel'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Single GPU'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Pipelining Model Parallel'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n     <span class=\"token string\">'mp_vs_rn_vs_pp.png'</span><span class=\"token punctuation\">)</span></code></pre>\n<p>在2 GPUs上能有50%的提速，离100%还是有点差距，加速对比如下</p>\n<p>###<img src=\"1.png\" alt=\"1\"> </p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>依次是 2-GPU并行， 1-GPU运行， 2-GPU Pipelining 并行 的运行时间对比</p>\n<h3 id=\"nn-parallel-DistributedDataParallel\"><a href=\"#nn-parallel-DistributedDataParallel\" class=\"headerlink\" title=\"nn.parallel.DistributedDataParallel\"></a>nn.parallel.DistributedDataParallel</h3><p>　　基于torch.distributed 的分布式数据并行，原理和DataParallel类似，但是是跨机器和跨设备级别的数据并行，在每台机器和每台设备上复制，并且每个这样的副本处理一部分输入， 在向后传递过程中，将平均每个节点的梯度，总之，是按批处理维度分块指定设备之间的输入来并行化给定模块的应用程序。</p>\n<p><code>DistributedDataParallel</code>可以通过以下两种方式使用：</p>\n<h4 id=\"单进程多GPU\"><a href=\"#单进程多GPU\" class=\"headerlink\" title=\"单进程多GPU\"></a>单进程多GPU</h4><pre class=\" language-python\"><code class=\"language-python\">torch<span class=\"token punctuation\">.</span>distributed<span class=\"token punctuation\">.</span>init_process_group<span class=\"token punctuation\">(</span>backend<span class=\"token operator\">=</span><span class=\"token string\">\"nccl\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\" spellcheck=\"true\"># device_ids will include all GPU devices by default</span>\nmodel <span class=\"token operator\">=</span> DistributedDataParallel<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">)</span> \n</code></pre>\n<h4 id=\"多进程多GPU\"><a href=\"#多进程多GPU\" class=\"headerlink\" title=\"多进程多GPU\"></a>多进程多GPU</h4><p>　　强烈推荐的使用方式，在单机多GPU的情况下，单进程很容易由于GIL出现利用率不足的问题，这时候多进程就是唯一解决办法。最佳实践是，将DDP(DistributedDataParallel)配合多进程一起使用，每个GPU分配一个进程，会比torch.nn.DataParallel快得多，也是目前Pytorch最快的训练方法。</p>\n<p>使用步骤：</p>\n<ol>\n<li>在N个GPU的单机上生成N个进程，这个过程可以交给torch.distributed.launch完成</li>\n</ol>\n<pre class=\" language-bash\"><code class=\"language-bash\">python -m torch.distributed.launch --nproc_per_node<span class=\"token operator\">=</span>n distributed_data_parallel.py</code></pre>\n<ol start=\"2\">\n<li>在代码中绑定GPU 编号，同时并行化model</li>\n</ol>\n<pre class=\" language-python\"><code class=\"language-python\">parser <span class=\"token operator\">=</span> argparse<span class=\"token punctuation\">.</span>ArgumentParser<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\" spellcheck=\"true\"># FOR DISTRIBUTED:  Parse for the local_rank argument, which will be supplied</span>\n<span class=\"token comment\" spellcheck=\"true\"># automatically by torch.distributed.launch.</span>\nparser<span class=\"token punctuation\">.</span>add_argument<span class=\"token punctuation\">(</span><span class=\"token string\">\"--local_rank\"</span><span class=\"token punctuation\">,</span> default<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> type<span class=\"token operator\">=</span>int<span class=\"token punctuation\">)</span>\nargs <span class=\"token operator\">=</span> parser<span class=\"token punctuation\">.</span>parse_args<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># 进程内绑定 GPU rank id</span>\ntorch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>set_device<span class=\"token punctuation\">(</span>args<span class=\"token punctuation\">.</span>local_rank<span class=\"token punctuation\">)</span>\n<span class=\"token comment\" spellcheck=\"true\"># 构造model</span>\ntorch<span class=\"token punctuation\">.</span>distributed<span class=\"token punctuation\">.</span>init_process_group<span class=\"token punctuation\">(</span>backend<span class=\"token operator\">=</span><span class=\"token string\">'nccl'</span><span class=\"token punctuation\">,</span> world_size<span class=\"token operator\">=</span>n<span class=\"token punctuation\">,</span> init_method<span class=\"token operator\">=</span><span class=\"token string\">'env://'</span><span class=\"token punctuation\">)</span>\nmodel <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>parallel<span class=\"token punctuation\">.</span>DistributedDataParallel<span class=\"token punctuation\">(</span>\n                                      model<span class=\"token punctuation\">,</span>\n                                        device_ids<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span>args<span class=\"token punctuation\">.</span>local_rank<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n                                        output_device<span class=\"token operator\">=</span>args<span class=\"token punctuation\">.</span>local_rank<span class=\"token punctuation\">)</span></code></pre>\n<p>Note:</p>\n<ol>\n<li>nccl 后端是分布式训练使用的推荐的最快后端，适用于单节点和多节点分布式训练</li>\n<li>nccl同时支持混合精度分布式训练</li>\n<li>no_sync 用于禁用DDP进程之间的梯度同步，直到退出此上下文区域的第一个梯度Forward-Backward中进行梯度同步</li>\n</ol>\n<pre class=\" language-python\"><code class=\"language-python\">ddp <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>DistributedDataParallel<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span> pg<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">with</span> ddp<span class=\"token punctuation\">.</span>no_sync<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n<span class=\"token keyword\">for</span> input <span class=\"token keyword\">in</span> inputs<span class=\"token punctuation\">:</span>\n    ddp<span class=\"token punctuation\">(</span>input<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\" spellcheck=\"true\"># no synchronization, accumulate grads</span>\nddp<span class=\"token punctuation\">(</span>another_input<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\" spellcheck=\"true\"># synchronize grads</span></code></pre>\n<h3 id=\"apex-parallel-DistributedDataParallel\"><a href=\"#apex-parallel-DistributedDataParallel\" class=\"headerlink\" title=\"apex.parallel.DistributedDataParallel\"></a>apex.parallel.DistributedDataParallel</h3><p>　　基本上是torch.nn.parallel.DistributedDataParallel的wrapper，同时调用的时候优化了NCCL的使用和简化了参数</p>\n<pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token comment\" spellcheck=\"true\"># 调用的时候，注意 n &lt;= 每个节点的GPU数量 同时默认 1个GPU对应1进程</span>\ntorch.distributed.launch --nproc_per_node<span class=\"token operator\">=</span>n distributed_data_parallel.py\n<span class=\"token comment\" spellcheck=\"true\"># 会自动提供的参数目前已知的是:</span>\n<span class=\"token comment\" spellcheck=\"true\"># args.local_rank</span>\n<span class=\"token comment\" spellcheck=\"true\"># os.environ['WORLD_SIZE']</span>\n</code></pre>\n<p>简化的要点：</p>\n<ol>\n<li><p>model = DDP(model) 即可，无需再传递 devices_ids output_device</p>\n</li>\n<li><p>init_process_group 中的 init_method=’env://‘</p>\n<pre class=\" language-python\"><code class=\"language-python\">torch<span class=\"token punctuation\">.</span>distributed<span class=\"token punctuation\">.</span>init_process_group<span class=\"token punctuation\">(</span>backend<span class=\"token operator\">=</span><span class=\"token string\">'nccl'</span><span class=\"token punctuation\">,</span>init_method<span class=\"token operator\">=</span><span class=\"token string\">'env://'</span><span class=\"token punctuation\">)</span></code></pre>\n</li>\n</ol>\n<p>直接给出示例代码</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token comment\" spellcheck=\"true\"># distributed_data_parallel.py</span>\n<span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">import</span> argparse\n<span class=\"token keyword\">import</span> os\n<span class=\"token keyword\">from</span> apex <span class=\"token keyword\">import</span> amp\n<span class=\"token comment\" spellcheck=\"true\"># FOR DISTRIBUTED: (can also use torch.nn.parallel.DistributedDataParallel instead)</span>\n<span class=\"token keyword\">from</span> apex<span class=\"token punctuation\">.</span>parallel <span class=\"token keyword\">import</span> DistributedDataParallel\n\nparser <span class=\"token operator\">=</span> argparse<span class=\"token punctuation\">.</span>ArgumentParser<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\" spellcheck=\"true\"># FOR DISTRIBUTED:  Parse for the local_rank argument, which will be supplied</span>\n<span class=\"token comment\" spellcheck=\"true\"># automatically by torch.distributed.launch.</span>\nparser<span class=\"token punctuation\">.</span>add_argument<span class=\"token punctuation\">(</span><span class=\"token string\">\"--local_rank\"</span><span class=\"token punctuation\">,</span> default<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> type<span class=\"token operator\">=</span>int<span class=\"token punctuation\">)</span>\nargs <span class=\"token operator\">=</span> parser<span class=\"token punctuation\">.</span>parse_args<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># FOR DISTRIBUTED:  If we are running under torch.distributed.launch,</span>\n<span class=\"token comment\" spellcheck=\"true\"># the 'WORLD_SIZE' environment variable will also be set automatically.</span>\nargs<span class=\"token punctuation\">.</span>distributed <span class=\"token operator\">=</span> <span class=\"token boolean\">False</span>\n<span class=\"token keyword\">if</span> <span class=\"token string\">'WORLD_SIZE'</span> <span class=\"token keyword\">in</span> os<span class=\"token punctuation\">.</span>environ<span class=\"token punctuation\">:</span>\n    args<span class=\"token punctuation\">.</span>distributed <span class=\"token operator\">=</span> int<span class=\"token punctuation\">(</span>os<span class=\"token punctuation\">.</span>environ<span class=\"token punctuation\">[</span><span class=\"token string\">'WORLD_SIZE'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">></span> <span class=\"token number\">1</span>\n\n<span class=\"token keyword\">if</span> args<span class=\"token punctuation\">.</span>distributed<span class=\"token punctuation\">:</span>\n    <span class=\"token comment\" spellcheck=\"true\"># FOR DISTRIBUTED:  Set the device according to local_rank.</span>\n    torch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>set_device<span class=\"token punctuation\">(</span>args<span class=\"token punctuation\">.</span>local_rank<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># FOR DISTRIBUTED:  Initialize the backend.  torch.distributed.launch will provide</span>\n    <span class=\"token comment\" spellcheck=\"true\"># environment variables, and requires that you use init_method=`env://`.</span>\n    torch<span class=\"token punctuation\">.</span>distributed<span class=\"token punctuation\">.</span>init_process_group<span class=\"token punctuation\">(</span>backend<span class=\"token operator\">=</span><span class=\"token string\">'nccl'</span><span class=\"token punctuation\">,</span>\n                                         init_method<span class=\"token operator\">=</span><span class=\"token string\">'env://'</span><span class=\"token punctuation\">)</span>\n\ntorch<span class=\"token punctuation\">.</span>backends<span class=\"token punctuation\">.</span>cudnn<span class=\"token punctuation\">.</span>benchmark <span class=\"token operator\">=</span> <span class=\"token boolean\">True</span>\n\nN<span class=\"token punctuation\">,</span> D_in<span class=\"token punctuation\">,</span> D_out <span class=\"token operator\">=</span> <span class=\"token number\">64</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1024</span><span class=\"token punctuation\">,</span> <span class=\"token number\">16</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># Each process receives its own batch of \"fake input data\" and \"fake target data.\"</span>\n<span class=\"token comment\" spellcheck=\"true\"># The \"training loop\" in each process just uses this fake batch over and over.</span>\n<span class=\"token comment\" spellcheck=\"true\"># https://github.com/NVIDIA/apex/tree/master/examples/imagenet provides a more realistic</span>\n<span class=\"token comment\" spellcheck=\"true\"># example of distributed data sampling for both training and validation.</span>\nx <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span>N<span class=\"token punctuation\">,</span> D_in<span class=\"token punctuation\">,</span> device<span class=\"token operator\">=</span><span class=\"token string\">'cuda'</span><span class=\"token punctuation\">)</span>\ny <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span>N<span class=\"token punctuation\">,</span> D_out<span class=\"token punctuation\">,</span> device<span class=\"token operator\">=</span><span class=\"token string\">'cuda'</span><span class=\"token punctuation\">)</span>\n\nmodel <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>D_in<span class=\"token punctuation\">,</span> D_out<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\noptimizer <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>optim<span class=\"token punctuation\">.</span>SGD<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> lr<span class=\"token operator\">=</span><span class=\"token number\">1e</span><span class=\"token operator\">-</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span>\n\nmodel<span class=\"token punctuation\">,</span> optimizer <span class=\"token operator\">=</span> amp<span class=\"token punctuation\">.</span>initialize<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span> optimizer<span class=\"token punctuation\">,</span> opt_level<span class=\"token operator\">=</span><span class=\"token string\">\"O1\"</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">if</span> args<span class=\"token punctuation\">.</span>distributed<span class=\"token punctuation\">:</span>\n    <span class=\"token comment\" spellcheck=\"true\"># FOR DISTRIBUTED:  After amp.initialize, wrap the model with</span>\n    <span class=\"token comment\" spellcheck=\"true\"># apex.parallel.DistributedDataParallel.</span>\n    model <span class=\"token operator\">=</span> DistributedDataParallel<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># torch.nn.parallel.DistributedDataParallel is also fine, with some added args:</span>\n    <span class=\"token comment\" spellcheck=\"true\"># model = torch.nn.parallel.DistributedDataParallel(model,</span>\n    <span class=\"token comment\" spellcheck=\"true\">#                                                   device_ids=[args.local_rank],</span>\n    <span class=\"token comment\" spellcheck=\"true\">#                                                   output_device=args.local_rank)</span>\n\nloss_fn <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>MSELoss<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">for</span> t <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span><span class=\"token number\">500</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    optimizer<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    y_pred <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n    loss <span class=\"token operator\">=</span> loss_fn<span class=\"token punctuation\">(</span>y_pred<span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">with</span> amp<span class=\"token punctuation\">.</span>scale_loss<span class=\"token punctuation\">(</span>loss<span class=\"token punctuation\">,</span> optimizer<span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> scaled_loss<span class=\"token punctuation\">:</span>\n        scaled_loss<span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    optimizer<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">if</span> args<span class=\"token punctuation\">.</span>local_rank <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"final loss = \"</span><span class=\"token punctuation\">,</span> loss<span class=\"token punctuation\">)</span></code></pre>\n<p>更复杂的多精度调用见 <a href=\"https://github.com/NVIDIA/apex/tree/master/examples/imagenet\">mixed precision training with DDP</a></p>\n<h3 id=\"DDP-保存和加载检查点\"><a href=\"#DDP-保存和加载检查点\" class=\"headerlink\" title=\"DDP 保存和加载检查点\"></a>DDP 保存和加载检查点</h3><p>　　一般是使用torch.save 和 torch.load 来完成，但是在多进程下，优化方法是，仅在一个进程中保存，然后在其他所有进程中加载。</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> os\n<span class=\"token keyword\">import</span> tempfile\n<span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>distributed <span class=\"token keyword\">as</span> dist\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>nn <span class=\"token keyword\">as</span> nn\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>optim <span class=\"token keyword\">as</span> optim\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>multiprocessing <span class=\"token keyword\">as</span> mp\n\n<span class=\"token keyword\">from</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>parallel <span class=\"token keyword\">import</span> DistributedDataParallel <span class=\"token keyword\">as</span> DDP\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">demo_checkpoint</span><span class=\"token punctuation\">(</span>rank<span class=\"token punctuation\">,</span> world_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    setup<span class=\"token punctuation\">(</span>rank<span class=\"token punctuation\">,</span> world_size<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># setup devices for this process, rank 1 uses GPUs [0, 1, 2, 3] and</span>\n    <span class=\"token comment\" spellcheck=\"true\"># rank 2 uses GPUs [4, 5, 6, 7].</span>\n    n <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>device_count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">//</span> world_size\n    device_ids <span class=\"token operator\">=</span> list<span class=\"token punctuation\">(</span>range<span class=\"token punctuation\">(</span>rank <span class=\"token operator\">*</span> n<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>rank <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    model <span class=\"token operator\">=</span> ToyModel<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device_ids<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># output_device defaults to device_ids[0]</span>\n    ddp_model <span class=\"token operator\">=</span> DDP<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span> device_ids<span class=\"token operator\">=</span>device_ids<span class=\"token punctuation\">)</span>\n\n    loss_fn <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>MSELoss<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    optimizer <span class=\"token operator\">=</span> optim<span class=\"token punctuation\">.</span>SGD<span class=\"token punctuation\">(</span>ddp_model<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> lr<span class=\"token operator\">=</span><span class=\"token number\">0.001</span><span class=\"token punctuation\">)</span>\n\n    CHECKPOINT_PATH <span class=\"token operator\">=</span> tempfile<span class=\"token punctuation\">.</span>gettempdir<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> <span class=\"token string\">\"/model.checkpoint\"</span>\n    <span class=\"token keyword\">if</span> rank <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n        <span class=\"token comment\" spellcheck=\"true\"># All processes should see same parameters as they all start from same</span>\n        <span class=\"token comment\" spellcheck=\"true\"># random parameters and gradients are synchronized in backward passes.</span>\n        <span class=\"token comment\" spellcheck=\"true\"># Therefore, saving it in one process is sufficient.</span>\n        torch<span class=\"token punctuation\">.</span>save<span class=\"token punctuation\">(</span>ddp_model<span class=\"token punctuation\">.</span>state_dict<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> CHECKPOINT_PATH<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># Use a barrier() to make sure that process 1 loads the model after process</span>\n    <span class=\"token comment\" spellcheck=\"true\"># 0 saves it.</span>\n    dist<span class=\"token punctuation\">.</span>barrier<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># configure map_location properly</span>\n    rank0_devices <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>x <span class=\"token operator\">-</span> rank <span class=\"token operator\">*</span> len<span class=\"token punctuation\">(</span>device_ids<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> device_ids<span class=\"token punctuation\">]</span>\n    device_pairs <span class=\"token operator\">=</span> zip<span class=\"token punctuation\">(</span>rank0_devices<span class=\"token punctuation\">,</span> device_ids<span class=\"token punctuation\">)</span>\n    map_location <span class=\"token operator\">=</span> <span class=\"token operator\">&amp;</span><span class=\"token comment\" spellcheck=\"true\">#123;'cuda:%d' % x: 'cuda:%d' % y for x, y in device_pairs&amp;#125;</span>\n    ddp_model<span class=\"token punctuation\">.</span>load_state_dict<span class=\"token punctuation\">(</span>\n        torch<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span>CHECKPOINT_PATH<span class=\"token punctuation\">,</span> map_location<span class=\"token operator\">=</span>map_location<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    optimizer<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    outputs <span class=\"token operator\">=</span> ddp_model<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span><span class=\"token number\">20</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    labels <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span><span class=\"token number\">20</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device_ids<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    loss_fn <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>MSELoss<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    loss_fn<span class=\"token punctuation\">(</span>outputs<span class=\"token punctuation\">,</span> labels<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    optimizer<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># Use a barrier() to make sure that all processes have finished reading the</span>\n    <span class=\"token comment\" spellcheck=\"true\"># checkpoint</span>\n    dist<span class=\"token punctuation\">.</span>barrier<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">if</span> rank <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n        os<span class=\"token punctuation\">.</span>remove<span class=\"token punctuation\">(</span>CHECKPOINT_PATH<span class=\"token punctuation\">)</span>\n\n    cleanup<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre>\n<h3 id=\"DDP-与模型拼接并行\"><a href=\"#DDP-与模型拼接并行\" class=\"headerlink\" title=\"DDP 与模型拼接并行\"></a>DDP 与模型拼接并行</h3><p>　　DDP 还可以与多 GPU 模型一起使用，但是不支持进程内的复制。 您需要为每个模块副本创建一个进程，与每个进程的多个副本相比，通常可以提高性能。 当训练具有大量数据的大型模型时，DDP 包装多 GPU 模型特别有用。 使用此功能时，需要小心地实现多 GPU 模型，以避免使用硬编码的设备，因为会将不同的模型副本放置到不同的设备上。</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">ToyMpModel</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> dev0<span class=\"token punctuation\">,</span> dev1<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        super<span class=\"token punctuation\">(</span>ToyMpModel<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>dev0 <span class=\"token operator\">=</span> dev0\n        self<span class=\"token punctuation\">.</span>dev1 <span class=\"token operator\">=</span> dev1\n        self<span class=\"token punctuation\">.</span>net1 <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>dev0<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>relu <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>ReLU<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>net2 <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>dev1<span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        x <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>dev0<span class=\"token punctuation\">)</span>\n        x <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>relu<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>net1<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        x <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>dev1<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> self<span class=\"token punctuation\">.</span>net2<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span></code></pre>\n<p>将多 GPU 模型传递给 DDP 时，不得设置<code>device_ids</code>和<code>output_device</code>。 输入和输出数据将通过应用程序或模型<code>forward()</code>方法放置在适当的设备中。</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">demo_model_parallel</span><span class=\"token punctuation\">(</span>rank<span class=\"token punctuation\">,</span> world_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    setup<span class=\"token punctuation\">(</span>rank<span class=\"token punctuation\">,</span> world_size<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># setup mp_model and devices for this process</span>\n    dev0 <span class=\"token operator\">=</span> rank <span class=\"token operator\">*</span> <span class=\"token number\">2</span>\n    dev1 <span class=\"token operator\">=</span> rank <span class=\"token operator\">*</span> <span class=\"token number\">2</span> <span class=\"token operator\">+</span> <span class=\"token number\">1</span>\n    mp_model <span class=\"token operator\">=</span> ToyMpModel<span class=\"token punctuation\">(</span>dev0<span class=\"token punctuation\">,</span> dev1<span class=\"token punctuation\">)</span>\n    ddp_mp_model <span class=\"token operator\">=</span> DDP<span class=\"token punctuation\">(</span>mp_model<span class=\"token punctuation\">)</span>\n\n    loss_fn <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>MSELoss<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    optimizer <span class=\"token operator\">=</span> optim<span class=\"token punctuation\">.</span>SGD<span class=\"token punctuation\">(</span>ddp_mp_model<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> lr<span class=\"token operator\">=</span><span class=\"token number\">0.001</span><span class=\"token punctuation\">)</span>\n\n    optimizer<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># outputs will be on dev1</span>\n    outputs <span class=\"token operator\">=</span> ddp_mp_model<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span><span class=\"token number\">20</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    labels <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span><span class=\"token number\">20</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>dev1<span class=\"token punctuation\">)</span>\n    loss_fn<span class=\"token punctuation\">(</span>outputs<span class=\"token punctuation\">,</span> labels<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    optimizer<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    cleanup<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">if</span> __name__ <span class=\"token operator\">==</span> <span class=\"token string\">\"__main__\"</span><span class=\"token punctuation\">:</span>\n    run_demo<span class=\"token punctuation\">(</span>demo_basic<span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n    run_demo<span class=\"token punctuation\">(</span>demo_checkpoint<span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">if</span> torch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>device_count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">>=</span> <span class=\"token number\">8</span><span class=\"token punctuation\">:</span>\n        run_demo<span class=\"token punctuation\">(</span>demo_model_parallel<span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token punctuation\">)</span></code></pre>\n<p>注意上述setup 是通过 torch.multiprocessing.spawn 来完成多线程启动的，所以初始化方法也需要通过setup单独来指定</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> os\n<span class=\"token keyword\">import</span> tempfile\n<span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>distributed <span class=\"token keyword\">as</span> dist\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>nn <span class=\"token keyword\">as</span> nn\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>optim <span class=\"token keyword\">as</span> optim\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>multiprocessing <span class=\"token keyword\">as</span> mp\n\n<span class=\"token keyword\">from</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>parallel <span class=\"token keyword\">import</span> DistributedDataParallel <span class=\"token keyword\">as</span> DDP\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">setup</span><span class=\"token punctuation\">(</span>rank<span class=\"token punctuation\">,</span> world_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    os<span class=\"token punctuation\">.</span>environ<span class=\"token punctuation\">[</span><span class=\"token string\">'MASTER_ADDR'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token string\">'localhost'</span>\n    os<span class=\"token punctuation\">.</span>environ<span class=\"token punctuation\">[</span><span class=\"token string\">'MASTER_PORT'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token string\">'12355'</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># initialize the process group</span>\n    dist<span class=\"token punctuation\">.</span>init_process_group<span class=\"token punctuation\">(</span><span class=\"token string\">\"gloo\"</span><span class=\"token punctuation\">,</span> rank<span class=\"token operator\">=</span>rank<span class=\"token punctuation\">,</span> world_size<span class=\"token operator\">=</span>world_size<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># Explicitly setting seed to make sure that models created in two processes</span>\n    <span class=\"token comment\" spellcheck=\"true\"># start from same random weights and biases.</span>\n    torch<span class=\"token punctuation\">.</span>manual_seed<span class=\"token punctuation\">(</span><span class=\"token number\">42</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">cleanup</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    dist<span class=\"token punctuation\">.</span>destroy_process_group<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">ToyModel</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        super<span class=\"token punctuation\">(</span>ToyModel<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>net1 <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>relu <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>ReLU<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>net2 <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> self<span class=\"token punctuation\">.</span>net2<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>relu<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>net1<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">demo_basic</span><span class=\"token punctuation\">(</span>rank<span class=\"token punctuation\">,</span> world_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    setup<span class=\"token punctuation\">(</span>rank<span class=\"token punctuation\">,</span> world_size<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># setup devices for this process, rank 1 uses GPUs [0, 1, 2, 3] and</span>\n    <span class=\"token comment\" spellcheck=\"true\"># rank 2 uses GPUs [4, 5, 6, 7].</span>\n    n <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>device_count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">//</span> world_size\n    device_ids <span class=\"token operator\">=</span> list<span class=\"token punctuation\">(</span>range<span class=\"token punctuation\">(</span>rank <span class=\"token operator\">*</span> n<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>rank <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># create model and move it to device_ids[0]</span>\n    model <span class=\"token operator\">=</span> ToyModel<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device_ids<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># output_device defaults to device_ids[0]</span>\n    ddp_model <span class=\"token operator\">=</span> DDP<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span> device_ids<span class=\"token operator\">=</span>device_ids<span class=\"token punctuation\">)</span>\n\n    loss_fn <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>MSELoss<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    optimizer <span class=\"token operator\">=</span> optim<span class=\"token punctuation\">.</span>SGD<span class=\"token punctuation\">(</span>ddp_model<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> lr<span class=\"token operator\">=</span><span class=\"token number\">0.001</span><span class=\"token punctuation\">)</span>\n\n    optimizer<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    outputs <span class=\"token operator\">=</span> ddp_model<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span><span class=\"token number\">20</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    labels <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span><span class=\"token number\">20</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device_ids<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    loss_fn<span class=\"token punctuation\">(</span>outputs<span class=\"token punctuation\">,</span> labels<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    optimizer<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    cleanup<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">run_demo</span><span class=\"token punctuation\">(</span>demo_fn<span class=\"token punctuation\">,</span> world_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    mp<span class=\"token punctuation\">.</span>spawn<span class=\"token punctuation\">(</span>demo_fn<span class=\"token punctuation\">,</span>\n             args<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>world_size<span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n             nprocs<span class=\"token operator\">=</span>world_size<span class=\"token punctuation\">,</span>\n             join<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span></code></pre>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>　　至此基本分析完所有Pytorch下的多GPU使用技巧，大体方案还是利用多进程的方式来规避GIL来提升性能，只需要用DDP 来包装model ，同时进行分布式对应的初始化，然后多进程启动即可加速训练，基本没有大的变动。</p>\n<p>References:</p>\n<p><a href=\"https://pytorch.apachecn.org/docs/1.4/75.html\">torch.nn</a></p>\n<p><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html?highlight=distributeddataparallel#torch.nn.parallel.DistributedDataParallel\">DistributedDataParallel API</a></p>\n<p><a href=\"https://pytorch.org/docs/stable/notes/cuda.html#cuda-nn-ddp-instead\">CUDA Semantics</a></p>\n<p><a href=\"https://pytorch.apachecn.org/docs/1.4/34.html\">分布式数据并行入门</a></p>\n<p><a href=\"https://pytorch.apachecn.org/docs/1.4/35.html\">用Pytorch编写分布式应用程序</a></p>\n<p><a href=\"https://nvidia.github.io/apex/parallel.html\">apex.parallel</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>　　虽然还没有机会用到CUDA集群，但是前段时间对协程和并行化的研究，让我忍不住想要探索一下如何在多个GPU下利用Pytorch加快训练的实践方法，算是为之后并行训练优化一个理论参考吧！</p>\n<p>　　Pytorch 大体上有3种实现并行的接口（另外还有一种不利用接口的拼接模型的技巧，之后再单独讨论），分别是：torch.multiprocessing, nn.DataParallel, nn.parallel.DistributedDataParallel，如果是是GPU多卡运行，最佳实践是 nn.parallle.DistributedDataParallel,官方文档 <a href=\"https://pytorch.org/docs/stable/notes/cuda.html#cuda-nn-ddp-instead\">CUDA SEMANTIC</a> 是这么描述的：</p>\n<blockquote>\n<p><strong>Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel</strong></p>\n<p>Most use cases involving batched inputs and multiple GPUs should default to using <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\"><code>DistributedDataParallel</code></a> to utilize more than one GPU.</p>\n<p>There are significant caveats to using CUDA models with <a href=\"https://pytorch.org/docs/stable/multiprocessing.html#module-torch.multiprocessing\"><code>multiprocessing</code></a>; unless care is taken to meet the data handling requirements exactly, it is likely that your program will have incorrect or undefined behavior.</p>\n<p>It is recommended to use <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\"><code>DistributedDataParallel</code></a>, instead of <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel\"><code>DataParallel</code></a> to do multi-GPU training, even if there is only a single node.</p>\n<p>The difference between <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\"><code>DistributedDataParallel</code></a> and <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel\"><code>DataParallel</code></a> is: <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\"><code>DistributedDataParallel</code></a> uses multiprocessing where a process is created for each GPU, while <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel\"><code>DataParallel</code></a> uses multithreading. By using multiprocessing, each GPU has its dedicated process, this avoids the performance overhead caused by GIL of Python interpreter.</p>\n<p>If you use <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\"><code>DistributedDataParallel</code></a>, you could use torch.distributed.launch utility to launch your program, see <a href=\"https://pytorch.org/docs/stable/distributed.html#distributed-launch\">Third-party backends</a>.</p>\n</blockquote>\n<h3 id=\"torch-multiprocessing\"><a href=\"#torch-multiprocessing\" class=\"headerlink\" title=\"torch.multiprocessing\"></a>torch.multiprocessing</h3><p>　　torch.multiprocessing 是 Python下 multiprocessing 的替代品，接口基本一致，并根据情况进行扩张，建议使用 python: multiprocessing.Queue 在进程中传递Pytorch对象。除此之外，还有很多坑，详见官方笔记 <a href=\"https://pytorch.org/docs/stable/notes/multiprocessing.html\">MULTIPROCESSING BEST PRACTICES</a>, <a href=\"https://pytorch.apachecn.org/docs/1.4/64.html\">并行处理最佳实践</a></p>\n<pre><code class=\"python\">import torch.multiprocessing as mp\nfrom model import MyModel\n\ndef train(model):\n    # Construct data_loader, optimizer, etc.\n    for data, labels in data_loader:\n        optimizer.zero_grad()\n        loss_fn(model(data), labels).backward()\n        optimizer.step()  # This will update the shared parameters\n\nif __name__ == &#39;__main__&#39;:\n    num_processes = 4\n    model = MyModel()\n    # NOTE: this is required for the ``fork`` method to work\n    model.share_memory()\n    processes = []\n    for rank in range(num_processes):\n        p = mp.Process(target=train, args=(model,))\n        p.start()\n        processes.append(p)\n    for p in processes:\n        p.join()</code></pre>\n<h3 id=\"DataParallel\"><a href=\"#DataParallel\" class=\"headerlink\" title=\"DataParallel\"></a>DataParallel</h3><p>　　DataParallel原理是，把model 副本copy到所有GPU上，其中每个GPU消耗数据的不同分区，相当于SIMD，把数据条目根据GPU数量重新分配</p>\n<h4 id=\"单机DataParallel并行\"><a href=\"#单机DataParallel并行\" class=\"headerlink\" title=\"单机DataParallel并行\"></a>单机DataParallel并行</h4><pre><code class=\"python\">model = nn.DataParallel(model)</code></pre>\n<p>　　代码验证 outside model 数据维度  和 inside model 维度</p>\n<pre><code class=\"python\">class Model(nn.Module):\n    # Our model\n\n    def __init__(self, input_size, output_size):\n        super(Model, self).__init__()\n        self.fc = nn.Linear(input_size, output_size)\n\n    def forward(self, input):\n        output = self.fc(input)\n        print(&quot;\\tIn Model: input size&quot;, input.size(),\n              &quot;output size&quot;, output.size())\n\n        return output\n\n\nmodel = Model(input_size, output_size)\nif torch.cuda.device_count() &gt; 1:\n  print(&quot;Let&#39;s use&quot;, torch.cuda.device_count(), &quot;GPUs!&quot;)\n  # dim = 0 [30, xxx] -&gt; [10, ...], [10, ...], [10, ...] on 3 GPUs\n  model = nn.DataParallel(model)\n\nmodel.to(device)\n\nfor data in rand_loader:\n    input = data.to(device)\n    output = model(input)\n    print(&quot;Outside: input size&quot;, input.size(),\n          &quot;output_size&quot;, output.size())\n\n# 2 GPUs\n# on 2 GPUs\nLet&#39;s use 2 GPUs!\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\nOutside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</code></pre>\n<p>优点是，通过消耗输入数据不同分区的方式加快训练过程，缺点是，如果单个模型太大，无法放入单个GPU，就无法运行，这时有个Trick可以把模型分段载入不同GPU，利用拼接的方式完成并行训练，该模型将单个模型拆分到不同的 GPU 上，而不是在每个 GPU 上复制整个模型(具体来说， 假设模型<code>m</code>包含 10 层：使用<code>DataParallel</code>时，每个 GPU 都具有这 10 层中每个层的副本，而当在两个 GPU 上并行使用模型时，每个 GPU 可以承载 5 层）</p>\n<h4 id=\"单机模型拼接并行\"><a href=\"#单机模型拼接并行\" class=\"headerlink\" title=\"单机模型拼接并行\"></a>单机模型拼接并行</h4><pre><code class=\"python\">import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = torch.nn.Linear(10, 10).to(&#39;cuda:0&#39;)\n        self.relu = torch.nn.ReLU()\n        self.net2 = torch.nn.Linear(10, 5).to(&#39;cuda:1&#39;)\n\n    def forward(self, x):\n        x = self.relu(self.net1(x.to(&#39;cuda:0&#39;)))\n        return self.net2(x.to(&#39;cuda:1&#39;))</code></pre>\n<p>对于模型太大而无法放入单个 GPU 的情况，上述实现解决了该问题。 但是，分析原理的时候可能已经注意到，如果模型单个GPU可以载入，它将比在单个 GPU 上运行它要慢。 这是因为在任何时间点，两个 GPU 中只有一个在工作，而另一个在那儿什么也没做。 由于中间输出需要在<code>layer2</code>和<code>layer3</code>之间从<code>cuda:0</code>复制到<code>cuda:1</code>，因此性能进一步恶化。</p>\n<p>除此之外，还有通过异步方式构建流水线的手段加速，这个Trick比较精巧和优雅，思想可以学习一下</p>\n<h4 id=\"单机Pipeline-并行\"><a href=\"#单机Pipeline-并行\" class=\"headerlink\" title=\"单机Pipeline 并行\"></a>单机Pipeline 并行</h4><pre><code class=\"python\">class PipelineParallelResNet50(ModelParallelResNet50):\n    def __init__(self, split_size=20, *args, **kwargs):\n        super(PipelineParallelResNet50, self).__init__(*args, **kwargs)\n        self.split_size = split_size\n\n    def forward(self, x):\n        splits = iter(x.split(self.split_size, dim=0))\n        s_next = next(splits)\n        s_prev = self.seq1(s_next).to(&#39;cuda:1&#39;)\n        ret = []\n\n        for s_next in splits:\n            # A. s_prev runs on cuda:1\n            s_prev = self.seq2(s_prev)\n            ret.append(self.fc(s_prev.view(s_prev.size(0), -1)))\n\n            # B. s_next runs on cuda:0, which can run concurrently with A\n            s_prev = self.seq1(s_next).to(&#39;cuda:1&#39;)\n\n        s_prev = self.seq2(s_prev)\n        ret.append(self.fc(s_prev.view(s_prev.size(0), -1)))\n\n        return torch.cat(ret)\n\nsetup = &quot;model = PipelineParallelResNet50()&quot;\npp_run_times = timeit.repeat(\n    stmt, setup, number=1, repeat=num_repeat, globals=globals())\npp_mean, pp_std = np.mean(pp_run_times), np.std(pp_run_times)\n\nplot([mp_mean, rn_mean, pp_mean],\n     [mp_std, rn_std, pp_std],\n     [&#39;Model Parallel&#39;, &#39;Single GPU&#39;, &#39;Pipelining Model Parallel&#39;],\n     &#39;mp_vs_rn_vs_pp.png&#39;)</code></pre>\n<p>在2 GPUs上能有50%的提速，离100%还是有点差距，加速对比如下</p>\n<p>###<img src=\"1.png\" alt=\"1\"> </p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>依次是 2-GPU并行， 1-GPU运行， 2-GPU Pipelining 并行 的运行时间对比</p>\n<h3 id=\"nn-parallel-DistributedDataParallel\"><a href=\"#nn-parallel-DistributedDataParallel\" class=\"headerlink\" title=\"nn.parallel.DistributedDataParallel\"></a>nn.parallel.DistributedDataParallel</h3><p>　　基于torch.distributed 的分布式数据并行，原理和DataParallel类似，但是是跨机器和跨设备级别的数据并行，在每台机器和每台设备上复制，并且每个这样的副本处理一部分输入， 在向后传递过程中，将平均每个节点的梯度，总之，是按批处理维度分块指定设备之间的输入来并行化给定模块的应用程序。</p>\n<p><code>DistributedDataParallel</code>可以通过以下两种方式使用：</p>\n<h4 id=\"单进程多GPU\"><a href=\"#单进程多GPU\" class=\"headerlink\" title=\"单进程多GPU\"></a>单进程多GPU</h4><pre><code class=\"python\">torch.distributed.init_process_group(backend=&quot;nccl&quot;)\n# device_ids will include all GPU devices by default\nmodel = DistributedDataParallel(model) \n</code></pre>\n<h4 id=\"多进程多GPU\"><a href=\"#多进程多GPU\" class=\"headerlink\" title=\"多进程多GPU\"></a>多进程多GPU</h4><p>　　强烈推荐的使用方式，在单机多GPU的情况下，单进程很容易由于GIL出现利用率不足的问题，这时候多进程就是唯一解决办法。最佳实践是，将DDP(DistributedDataParallel)配合多进程一起使用，每个GPU分配一个进程，会比torch.nn.DataParallel快得多，也是目前Pytorch最快的训练方法。</p>\n<p>使用步骤：</p>\n<ol>\n<li>在N个GPU的单机上生成N个进程，这个过程可以交给torch.distributed.launch完成</li>\n</ol>\n<pre><code class=\"bash\">python -m torch.distributed.launch --nproc_per_node=n distributed_data_parallel.py</code></pre>\n<ol start=\"2\">\n<li>在代码中绑定GPU 编号，同时并行化model</li>\n</ol>\n<pre><code class=\"python\">parser = argparse.ArgumentParser()\n# FOR DISTRIBUTED:  Parse for the local_rank argument, which will be supplied\n# automatically by torch.distributed.launch.\nparser.add_argument(&quot;--local_rank&quot;, default=0, type=int)\nargs = parser.parse_args()\n\n# 进程内绑定 GPU rank id\ntorch.cuda.set_device(args.local_rank)\n# 构造model\ntorch.distributed.init_process_group(backend=&#39;nccl&#39;, world_size=n, init_method=&#39;env://&#39;)\nmodel = torch.nn.parallel.DistributedDataParallel(\n                                      model,\n                                        device_ids=[args.local_rank],\n                                        output_device=args.local_rank)</code></pre>\n<p>Note:</p>\n<ol>\n<li>nccl 后端是分布式训练使用的推荐的最快后端，适用于单节点和多节点分布式训练</li>\n<li>nccl同时支持混合精度分布式训练</li>\n<li>no_sync 用于禁用DDP进程之间的梯度同步，直到退出此上下文区域的第一个梯度Forward-Backward中进行梯度同步</li>\n</ol>\n<pre><code class=\"python\">ddp = torch.nn.DistributedDataParallel(model, pg)\nwith ddp.no_sync():\nfor input in inputs:\n    ddp(input).backward()  # no synchronization, accumulate grads\nddp(another_input).backward()  # synchronize grads</code></pre>\n<h3 id=\"apex-parallel-DistributedDataParallel\"><a href=\"#apex-parallel-DistributedDataParallel\" class=\"headerlink\" title=\"apex.parallel.DistributedDataParallel\"></a>apex.parallel.DistributedDataParallel</h3><p>　　基本上是torch.nn.parallel.DistributedDataParallel的wrapper，同时调用的时候优化了NCCL的使用和简化了参数</p>\n<pre><code class=\"bash\"># 调用的时候，注意 n &lt;= 每个节点的GPU数量 同时默认 1个GPU对应1进程\ntorch.distributed.launch --nproc_per_node=n distributed_data_parallel.py\n# 会自动提供的参数目前已知的是:\n# args.local_rank\n# os.environ[&#39;WORLD_SIZE&#39;]\n</code></pre>\n<p>简化的要点：</p>\n<ol>\n<li><p>model = DDP(model) 即可，无需再传递 devices_ids output_device</p>\n</li>\n<li><p>init_process_group 中的 init_method=’env://‘</p>\n<pre><code class=\"python\">torch.distributed.init_process_group(backend=&#39;nccl&#39;,init_method=&#39;env://&#39;)</code></pre>\n</li>\n</ol>\n<p>直接给出示例代码</p>\n<pre><code class=\"python\"># distributed_data_parallel.py\nimport torch\nimport argparse\nimport os\nfrom apex import amp\n# FOR DISTRIBUTED: (can also use torch.nn.parallel.DistributedDataParallel instead)\nfrom apex.parallel import DistributedDataParallel\n\nparser = argparse.ArgumentParser()\n# FOR DISTRIBUTED:  Parse for the local_rank argument, which will be supplied\n# automatically by torch.distributed.launch.\nparser.add_argument(&quot;--local_rank&quot;, default=0, type=int)\nargs = parser.parse_args()\n\n# FOR DISTRIBUTED:  If we are running under torch.distributed.launch,\n# the &#39;WORLD_SIZE&#39; environment variable will also be set automatically.\nargs.distributed = False\nif &#39;WORLD_SIZE&#39; in os.environ:\n    args.distributed = int(os.environ[&#39;WORLD_SIZE&#39;]) &gt; 1\n\nif args.distributed:\n    # FOR DISTRIBUTED:  Set the device according to local_rank.\n    torch.cuda.set_device(args.local_rank)\n\n    # FOR DISTRIBUTED:  Initialize the backend.  torch.distributed.launch will provide\n    # environment variables, and requires that you use init_method=`env://`.\n    torch.distributed.init_process_group(backend=&#39;nccl&#39;,\n                                         init_method=&#39;env://&#39;)\n\ntorch.backends.cudnn.benchmark = True\n\nN, D_in, D_out = 64, 1024, 16\n\n# Each process receives its own batch of &quot;fake input data&quot; and &quot;fake target data.&quot;\n# The &quot;training loop&quot; in each process just uses this fake batch over and over.\n# https://github.com/NVIDIA/apex/tree/master/examples/imagenet provides a more realistic\n# example of distributed data sampling for both training and validation.\nx = torch.randn(N, D_in, device=&#39;cuda&#39;)\ny = torch.randn(N, D_out, device=&#39;cuda&#39;)\n\nmodel = torch.nn.Linear(D_in, D_out).cuda()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n\nmodel, optimizer = amp.initialize(model, optimizer, opt_level=&quot;O1&quot;)\n\nif args.distributed:\n    # FOR DISTRIBUTED:  After amp.initialize, wrap the model with\n    # apex.parallel.DistributedDataParallel.\n    model = DistributedDataParallel(model)\n    # torch.nn.parallel.DistributedDataParallel is also fine, with some added args:\n    # model = torch.nn.parallel.DistributedDataParallel(model,\n    #                                                   device_ids=[args.local_rank],\n    #                                                   output_device=args.local_rank)\n\nloss_fn = torch.nn.MSELoss()\n\nfor t in range(500):\n    optimizer.zero_grad()\n    y_pred = model(x)\n    loss = loss_fn(y_pred, y)\n    with amp.scale_loss(loss, optimizer) as scaled_loss:\n        scaled_loss.backward()\n    optimizer.step()\n\nif args.local_rank == 0:\n    print(&quot;final loss = &quot;, loss)</code></pre>\n<p>更复杂的多精度调用见 <a href=\"https://github.com/NVIDIA/apex/tree/master/examples/imagenet\">mixed precision training with DDP</a></p>\n<h3 id=\"DDP-保存和加载检查点\"><a href=\"#DDP-保存和加载检查点\" class=\"headerlink\" title=\"DDP 保存和加载检查点\"></a>DDP 保存和加载检查点</h3><p>　　一般是使用torch.save 和 torch.load 来完成，但是在多进程下，优化方法是，仅在一个进程中保存，然后在其他所有进程中加载。</p>\n<pre><code class=\"python\">import os\nimport tempfile\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\ndef demo_checkpoint(rank, world_size):\n    setup(rank, world_size)\n\n    # setup devices for this process, rank 1 uses GPUs [0, 1, 2, 3] and\n    # rank 2 uses GPUs [4, 5, 6, 7].\n    n = torch.cuda.device_count() // world_size\n    device_ids = list(range(rank * n, (rank + 1) * n))\n\n    model = ToyModel().to(device_ids[0])\n    # output_device defaults to device_ids[0]\n    ddp_model = DDP(model, device_ids=device_ids)\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    CHECKPOINT_PATH = tempfile.gettempdir() + &quot;/model.checkpoint&quot;\n    if rank == 0:\n        # All processes should see same parameters as they all start from same\n        # random parameters and gradients are synchronized in backward passes.\n        # Therefore, saving it in one process is sufficient.\n        torch.save(ddp_model.state_dict(), CHECKPOINT_PATH)\n\n    # Use a barrier() to make sure that process 1 loads the model after process\n    # 0 saves it.\n    dist.barrier()\n    # configure map_location properly\n    rank0_devices = [x - rank * len(device_ids) for x in device_ids]\n    device_pairs = zip(rank0_devices, device_ids)\n    map_location = &#123;&#39;cuda:%d&#39; % x: &#39;cuda:%d&#39; % y for x, y in device_pairs&#125;\n    ddp_model.load_state_dict(\n        torch.load(CHECKPOINT_PATH, map_location=map_location))\n\n    optimizer.zero_grad()\n    outputs = ddp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(device_ids[0])\n    loss_fn = nn.MSELoss()\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    # Use a barrier() to make sure that all processes have finished reading the\n    # checkpoint\n    dist.barrier()\n\n    if rank == 0:\n        os.remove(CHECKPOINT_PATH)\n\n    cleanup()</code></pre>\n<h3 id=\"DDP-与模型拼接并行\"><a href=\"#DDP-与模型拼接并行\" class=\"headerlink\" title=\"DDP 与模型拼接并行\"></a>DDP 与模型拼接并行</h3><p>　　DDP 还可以与多 GPU 模型一起使用，但是不支持进程内的复制。 您需要为每个模块副本创建一个进程，与每个进程的多个副本相比，通常可以提高性能。 当训练具有大量数据的大型模型时，DDP 包装多 GPU 模型特别有用。 使用此功能时，需要小心地实现多 GPU 模型，以避免使用硬编码的设备，因为会将不同的模型副本放置到不同的设备上。</p>\n<pre><code class=\"python\">class ToyMpModel(nn.Module):\n    def __init__(self, dev0, dev1):\n        super(ToyMpModel, self).__init__()\n        self.dev0 = dev0\n        self.dev1 = dev1\n        self.net1 = torch.nn.Linear(10, 10).to(dev0)\n        self.relu = torch.nn.ReLU()\n        self.net2 = torch.nn.Linear(10, 5).to(dev1)\n\n    def forward(self, x):\n        x = x.to(self.dev0)\n        x = self.relu(self.net1(x))\n        x = x.to(self.dev1)\n        return self.net2(x)</code></pre>\n<p>将多 GPU 模型传递给 DDP 时，不得设置<code>device_ids</code>和<code>output_device</code>。 输入和输出数据将通过应用程序或模型<code>forward()</code>方法放置在适当的设备中。</p>\n<pre><code class=\"python\">def demo_model_parallel(rank, world_size):\n    setup(rank, world_size)\n\n    # setup mp_model and devices for this process\n    dev0 = rank * 2\n    dev1 = rank * 2 + 1\n    mp_model = ToyMpModel(dev0, dev1)\n    ddp_mp_model = DDP(mp_model)\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_mp_model.parameters(), lr=0.001)\n\n    optimizer.zero_grad()\n    # outputs will be on dev1\n    outputs = ddp_mp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(dev1)\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    cleanup()\n\nif __name__ == &quot;__main__&quot;:\n    run_demo(demo_basic, 2)\n    run_demo(demo_checkpoint, 2)\n\n    if torch.cuda.device_count() &gt;= 8:\n        run_demo(demo_model_parallel, 4)</code></pre>\n<p>注意上述setup 是通过 torch.multiprocessing.spawn 来完成多线程启动的，所以初始化方法也需要通过setup单独来指定</p>\n<pre><code class=\"python\">import os\nimport tempfile\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\ndef setup(rank, world_size):\n    os.environ[&#39;MASTER_ADDR&#39;] = &#39;localhost&#39;\n    os.environ[&#39;MASTER_PORT&#39;] = &#39;12355&#39;\n\n    # initialize the process group\n    dist.init_process_group(&quot;gloo&quot;, rank=rank, world_size=world_size)\n\n    # Explicitly setting seed to make sure that models created in two processes\n    # start from same random weights and biases.\n    torch.manual_seed(42)\n\ndef cleanup():\n    dist.destroy_process_group()\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(10, 10)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(10, 5)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\ndef demo_basic(rank, world_size):\n    setup(rank, world_size)\n\n    # setup devices for this process, rank 1 uses GPUs [0, 1, 2, 3] and\n    # rank 2 uses GPUs [4, 5, 6, 7].\n    n = torch.cuda.device_count() // world_size\n    device_ids = list(range(rank * n, (rank + 1) * n))\n\n    # create model and move it to device_ids[0]\n    model = ToyModel().to(device_ids[0])\n    # output_device defaults to device_ids[0]\n    ddp_model = DDP(model, device_ids=device_ids)\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    optimizer.zero_grad()\n    outputs = ddp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(device_ids[0])\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    cleanup()\n\ndef run_demo(demo_fn, world_size):\n    mp.spawn(demo_fn,\n             args=(world_size,),\n             nprocs=world_size,\n             join=True)</code></pre>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>　　至此基本分析完所有Pytorch下的多GPU使用技巧，大体方案还是利用多进程的方式来规避GIL来提升性能，只需要用DDP 来包装model ，同时进行分布式对应的初始化，然后多进程启动即可加速训练，基本没有大的变动。</p>\n<p>References:</p>\n<p><a href=\"https://pytorch.apachecn.org/docs/1.4/75.html\">torch.nn</a></p>\n<p><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html?highlight=distributeddataparallel#torch.nn.parallel.DistributedDataParallel\">DistributedDataParallel API</a></p>\n<p><a href=\"https://pytorch.org/docs/stable/notes/cuda.html#cuda-nn-ddp-instead\">CUDA Semantics</a></p>\n<p><a href=\"https://pytorch.apachecn.org/docs/1.4/34.html\">分布式数据并行入门</a></p>\n<p><a href=\"https://pytorch.apachecn.org/docs/1.4/35.html\">用Pytorch编写分布式应用程序</a></p>\n<p><a href=\"https://nvidia.github.io/apex/parallel.html\">apex.parallel</a></p>\n"},{"title":"LeetCode 1-100 刷题有感","toc":true,"mathjax":true,"top":false,"cover":true,"date":"2020-12-07T03:06:05.000Z","updated":"2020-12-10T08:01:47.037Z","_content":"\n　　跌跌撞撞终于在前几天完成的LeetCode 100题，刷题过程印证了之前对自我的一个论断，起了个大早，干了个晚集。 早在2012年，自己就开始了解到LeetCode 刷完前100题的神迹，这时LeetCode应该还没创立多久吧！当时国内只有北大的POJ可用，那曲高和寡的界面和难度，着实有点劝退！自己的编程学习之路从来没有一个类似引路人之类的角色，全凭自学，再加上我三分钟热度的神奇特性，所以对ACM之类的算法竞赛全然不知也就毫无意外。彼时，复现数据结构的排序等级实践水平就足以让自己沾沾自喜半天了，毕竟身边有大把无法做到这一点的人，而且还是靠背代码的方式，二叉树倒是无论是理论学习阶段还是代码实现阶段都没有遇到过太大的挑战，图论则是看算法（第四版）就收获了巨大的理论学习上的满足感，听信了据说实践基本不会出现这种级别的题目的话，就止步于此，毕竟图的数据结构确实有点繁琐，代码复杂度也有点棘手，但是在已经完全理解算法理论的基础上，实现起来也至多花一点时间而已。再加上当时眼界受限，以为排序，搜索以及图论之类就已经是算法的全部了。现在的我才真正意识到，理解算法理论和实现算法代码，在学习上是截然不同的2种境界，所幸自己就算没有在电脑上实现代码，也还是保持了记笔记的好习惯，很多自己觉得精妙和优雅的算法，忍不住在笔记本下写下来了，也算是另一种维度的代码实践吧！\n\n　　回到了LeetCode上来，2012年记得是硕士复试需要准备OJ，当时觉得还是颇为新奇，所以比较重视，刷了很多OJ题目，其中就有LeetCode，不过当时光理解题目意思都有点费劲，刷了不到10题的Easy难度的题目就劝退了；复试结束之后，习惯性的放弃了，直到看完算法（第四版），重新理解了Java之后，忍不住想要做点什么，就又开始重新开始刷LeetCode，用Java 勉强刷了几十道medium和easy的题目，虽然大部分都还是做不出来，不清楚常规解题套路，Hard题目一想就是一整天都没什么头绪，最后还是看Discuss才能找到一点感觉，甚至很多时候光理解代码就需要大半天时间。由于LeetCode OJ系统的易用性，免去了调试环境的很多烦恼，能专注于算法编码和Debug，其次社区讨论都非常活跃，遇到不会做的题目或者是思路受阻，基本都能在Discuss下面找到解答，当时虽然抱着模仿性质的想法，试图想要刷完LeetCode 100题，但是止步于好几个Hard题目，记得有一道计算城市天际线面积的题目，整整想了一周，总是感觉算法上，似乎已经找到解决算法，但是编码时候总是磕磕碰碰，自己的想法还没到验证的阶段，就被茫茫然的细节问题给淹没了。最后不得不求助社区，最后理解完正确的代码之后，就这么弃置不顾。完全放弃了在LeetCode中寻找乐趣的想法了。\n\n　　以现在的角度回顾分析之前为什么放弃 LeetCode 刷题，或许会有不错的收获！首先，也是老生常谈的问题，就是自己三分钟热度的问题，也不过多展开了，直接给出结论就是，中立的来看，自己既然有这样不算好也不算坏的特质，别像以前那样，说到三分钟热度，大部分时候都是带着点贬义和自我批判意味。再说，三分钟热度起码给了自己一种不断变换角度来看周遭和自己的机会，抱有对世界最单纯的好奇心。缺点自然也很明显，也就是浅尝辄止。最后的一句话点醒了梦中人，**有三分钟热度，就有三分钟收获**。其次，就是自己刷题时候的无知者无畏的盲目自信，想要试图跳级做题，而不是惯常的循序渐进，徒增难度，却发现自己完全没有做好准备，浪费了很多时间，还严重打击了自信，于是便开始自暴自弃，顺带连之前积累的理论知识都被荒废了。这可能是学生思维的弊端，不够谦卑，更不够自信，这其中的度，没有拿捏得当。诚然，数据结构和算法的理论支撑下，很多算法题都会变的比较容易用常规的套路解答，特别是很多算法笔试和考试中，都已经把最难点给精确的给出来了，照本宣科就可以了。那么，这是否意味着解决了核心难点之后，其他方面是否不值一提呢？真实情况并不是这样的，很多时候并没有意识到这个难点为什么是难点，只是被动的接受知识而已，这理所当然的导致另一个重要的能力的缺失，学会如何灵活的运用这些算法理论，也就是融会贯通。只有当无论什么算法题目，都能很准确的分析出问题本质，根据分析结果找到相应的数据结构工具和手段解决问题，才能完成编码的核心，就是算法伪代码的初步构思；但是是否真的能在OJ中AC，只有代码才能验证。这跟数学证明题一样，在证明过程顺利铺开之前，一切构思都只是可能的解决方案而已，能不能达到预想的目的，还需要证明过程来验证，况且，这个构思可能会被证明过程自我否定掉。这些内容总结起来，其实就是**执行力**。回顾自己大部分的学生时代，遗憾的发现，自己执行力可能是属于非常拉胯的级别。也就勉强仗着的天赋好，和某些时候的灵光乍现以及为数不多的树立并确信要去达成的目标的时候，才能走到现在。近年来，终于能心平气和的分析自己的优缺点的时候，才意识到，循序渐进的构思-行动，小步快走的达成目标的重要性和难能可贵。这一点可能是当初年轻气盛的时候不具备的。再则，其实也是上一个原因派生出来的问题，就是Java本身的标准库和数据结构的不熟悉，谁能想到，自学了Java这么久，居然还是懒得看Java的标准文档，尽管算法（第四版）实现了简化版的，堆栈，队列，二叉树等数据结构，文件I/O也大幅简化，增强了易用性，也让自己缺乏去看繁杂的Java标准实现的动力，所以很多时候都是现学现用，这当然无可厚非，因为熟记这些库是没有必要的，只需要知道是堆栈等这些数据结构的接口就可以了，用的时候再去查就行了，无非就是低效一点，并不至于无法实现的地步，但是结果就是不用就不学，导致直到现在对Java的标准库还是一知半解，前段时间打包个Jar还老费劲了。这样做的当时可能觉得没啥，但是一旦实践起来，就很容易出现问题，在做LeetCode OJ的时候，就集中爆发出来了，就是自己是算法构思，迟迟没办法得到代码验证，常常会卡在Java的实现细节上，当时也没有对构思和实践分的这么开，觉得有个构思好像能解决问题，就硬着头强上，以至于最后败兴而归都没能整明白，究竟是算法构思的问题，还是代码实现的问题；以至于虽然刷了很久的题目，收获不大，反倒是积累了很多Java的问题没有解决。以上的一些做的不够好的地方，基本解释了当初为什么没能坚持刷LeetCode ？ \n\n　　还有一个重要的需要解释的问题就是，为什么刷过的LeetCode 过段时间回头来看，仍然解不出来或者是一知半解？譬如说，现在回去做城市天际线的题目，自己似乎没有自信能100%解出来。主要原因应该是自己并没有规划性的做复习工作，很多算法题做完，并没有留下一个可供复习的笔记和资料为后续复习做准备，也就是在之前Blog做反思的过程中提到的，没有系统行的整理和复习自己的学习笔记和资料，导致每次复习的时候，都不得不重头开始做题，把之前踩过的坑又重新踩了一遍，这在几年后，也就是最近几年开始用Python重做这些题的时候都渐渐体现出来了，这其实完全没有必要，因为发生这样的事情主要原因仅仅只是遗忘了而已，思维过程其实还是在没有复习的情况仍然保有印象。所以，定期的记录刷题方法并整理和复习在这个阶段就变的很重要了，这也是自己强调过很多次的问题了，也是努力在改进，就不多细说了。\n\n　　用Python重刷题的时候，就显得有效率了很多，一来很多题目都比较熟悉了，只需要用Python相对应的数据结构重新实现即可，这个过程其实也加深了很多经常用的数据结构的理解，不常用的类和编程模式也熟悉了不少，也算是一个不错的收获吧，毕竟曾经对实践编码的忽视缺失导致自己眼高手低，失去了很多提升编码能力的机会，编码之后，才真实的认识到编程原本就是一项工程实践，自己怎么会天真到仅仅满足于算法构思，而不是脚踏实地的实现出来；再则，其实也算是一种复习，对之前理解不深刻，一知半解的方法做一个完整的梳理。 这也解释为什么，上周某天下午半天不到的时间，就把最后20题，一次性全部刷完，高效的自己都觉得不可思议。其中甚至包含2道Hard，自己原以为需要借助一些提示才做的出来的题目，也比较顺利的完成了。 刷完100题的时候，感觉真的很兴奋，曾经以为很厉害，遥不可及的神迹，现在自己也站在这里达成了这个目标，虽然耗时比较久，过程断断续续，其实过程基本是大部分时间没怎么刷，突然心血来潮就刷很多题那样，也算是达成一个不大不小的成就吧！\n\n　　接下来该怎么做呢？ 这也是最最重要和需要关注的问题，毕竟尽管自己可以一下午刷几十道题，停不下来，但是我还是会开个小差，摸鱼一整天，更是停不下来的那种。我已经渐渐摸索出应对这种情况的解决办法，就是合理的做规划，给出目标，不多不少，不偏不倚。因为从刷题过程来看，我还是非常享受刷题带来的乐趣的，特别是那些觉得有点困难的题目，AC的时候，可以开心好一阵子了，以至于都忘记要整理自己的代码和笔记了。设想中的刷完100题之后，应该是需要整理一下100题的笔记和要点，作为刷题总结的方式，一篇一篇的成为Blog的主要内容来源，现在看来，似乎这项工作并没有开展起来，因为很多注释都写在代码里面了，现在回过头来一个一个的去复习和整理，觉得有点没必要，似乎只需要关注最主要和核心的题目整理出笔记即可，更主要的原因可能是担心这项工作可能会延误自己刷100-200题的进度，毕竟比起回头复习旧题目，我似乎更喜欢做新题。现在回想一下，其实很多代码注释的内容，都足以作为Blog的文字内容存在了，只需要稍加排版即可成为一份笔记，所以对于刷题的笔记模版，应该无需过于担忧。更值得关注的应该是要求自己刷100之后的新题的时候，务必要求自己必须整理出一份代码说明出来，定期排版成Blog，将定期刷题变成一个习惯，这样就有精力去迎接下一个挑战了，毕竟对自己来说是一件很开心的事情。\n\n　　LeetCode 刷完100题有感就这么多，接下来还是希望自己能贯彻自己的日常目标，现阶段坚持刷题和背单词，再坚持锻炼身体，不要避重就轻的断断续续的逃避，相信有了这几项准备和积累，面对以后的工作和生活的挑战，自己能应对的更加自如，处理的更加高效！","source":"_posts/LeetCode-1-100-刷题有感.md","raw":"---\ntitle: LeetCode 1-100 刷题有感\ntoc: true\nmathjax: true\ntop: false\ncover: true\ndate: 2020-12-07 11:06:05\nupdated:\ncategories: 随笔\ntags:\n\t- Algorithms\n\t- LeetCode\n\t- 随笔\n---\n\n　　跌跌撞撞终于在前几天完成的LeetCode 100题，刷题过程印证了之前对自我的一个论断，起了个大早，干了个晚集。 早在2012年，自己就开始了解到LeetCode 刷完前100题的神迹，这时LeetCode应该还没创立多久吧！当时国内只有北大的POJ可用，那曲高和寡的界面和难度，着实有点劝退！自己的编程学习之路从来没有一个类似引路人之类的角色，全凭自学，再加上我三分钟热度的神奇特性，所以对ACM之类的算法竞赛全然不知也就毫无意外。彼时，复现数据结构的排序等级实践水平就足以让自己沾沾自喜半天了，毕竟身边有大把无法做到这一点的人，而且还是靠背代码的方式，二叉树倒是无论是理论学习阶段还是代码实现阶段都没有遇到过太大的挑战，图论则是看算法（第四版）就收获了巨大的理论学习上的满足感，听信了据说实践基本不会出现这种级别的题目的话，就止步于此，毕竟图的数据结构确实有点繁琐，代码复杂度也有点棘手，但是在已经完全理解算法理论的基础上，实现起来也至多花一点时间而已。再加上当时眼界受限，以为排序，搜索以及图论之类就已经是算法的全部了。现在的我才真正意识到，理解算法理论和实现算法代码，在学习上是截然不同的2种境界，所幸自己就算没有在电脑上实现代码，也还是保持了记笔记的好习惯，很多自己觉得精妙和优雅的算法，忍不住在笔记本下写下来了，也算是另一种维度的代码实践吧！\n\n　　回到了LeetCode上来，2012年记得是硕士复试需要准备OJ，当时觉得还是颇为新奇，所以比较重视，刷了很多OJ题目，其中就有LeetCode，不过当时光理解题目意思都有点费劲，刷了不到10题的Easy难度的题目就劝退了；复试结束之后，习惯性的放弃了，直到看完算法（第四版），重新理解了Java之后，忍不住想要做点什么，就又开始重新开始刷LeetCode，用Java 勉强刷了几十道medium和easy的题目，虽然大部分都还是做不出来，不清楚常规解题套路，Hard题目一想就是一整天都没什么头绪，最后还是看Discuss才能找到一点感觉，甚至很多时候光理解代码就需要大半天时间。由于LeetCode OJ系统的易用性，免去了调试环境的很多烦恼，能专注于算法编码和Debug，其次社区讨论都非常活跃，遇到不会做的题目或者是思路受阻，基本都能在Discuss下面找到解答，当时虽然抱着模仿性质的想法，试图想要刷完LeetCode 100题，但是止步于好几个Hard题目，记得有一道计算城市天际线面积的题目，整整想了一周，总是感觉算法上，似乎已经找到解决算法，但是编码时候总是磕磕碰碰，自己的想法还没到验证的阶段，就被茫茫然的细节问题给淹没了。最后不得不求助社区，最后理解完正确的代码之后，就这么弃置不顾。完全放弃了在LeetCode中寻找乐趣的想法了。\n\n　　以现在的角度回顾分析之前为什么放弃 LeetCode 刷题，或许会有不错的收获！首先，也是老生常谈的问题，就是自己三分钟热度的问题，也不过多展开了，直接给出结论就是，中立的来看，自己既然有这样不算好也不算坏的特质，别像以前那样，说到三分钟热度，大部分时候都是带着点贬义和自我批判意味。再说，三分钟热度起码给了自己一种不断变换角度来看周遭和自己的机会，抱有对世界最单纯的好奇心。缺点自然也很明显，也就是浅尝辄止。最后的一句话点醒了梦中人，**有三分钟热度，就有三分钟收获**。其次，就是自己刷题时候的无知者无畏的盲目自信，想要试图跳级做题，而不是惯常的循序渐进，徒增难度，却发现自己完全没有做好准备，浪费了很多时间，还严重打击了自信，于是便开始自暴自弃，顺带连之前积累的理论知识都被荒废了。这可能是学生思维的弊端，不够谦卑，更不够自信，这其中的度，没有拿捏得当。诚然，数据结构和算法的理论支撑下，很多算法题都会变的比较容易用常规的套路解答，特别是很多算法笔试和考试中，都已经把最难点给精确的给出来了，照本宣科就可以了。那么，这是否意味着解决了核心难点之后，其他方面是否不值一提呢？真实情况并不是这样的，很多时候并没有意识到这个难点为什么是难点，只是被动的接受知识而已，这理所当然的导致另一个重要的能力的缺失，学会如何灵活的运用这些算法理论，也就是融会贯通。只有当无论什么算法题目，都能很准确的分析出问题本质，根据分析结果找到相应的数据结构工具和手段解决问题，才能完成编码的核心，就是算法伪代码的初步构思；但是是否真的能在OJ中AC，只有代码才能验证。这跟数学证明题一样，在证明过程顺利铺开之前，一切构思都只是可能的解决方案而已，能不能达到预想的目的，还需要证明过程来验证，况且，这个构思可能会被证明过程自我否定掉。这些内容总结起来，其实就是**执行力**。回顾自己大部分的学生时代，遗憾的发现，自己执行力可能是属于非常拉胯的级别。也就勉强仗着的天赋好，和某些时候的灵光乍现以及为数不多的树立并确信要去达成的目标的时候，才能走到现在。近年来，终于能心平气和的分析自己的优缺点的时候，才意识到，循序渐进的构思-行动，小步快走的达成目标的重要性和难能可贵。这一点可能是当初年轻气盛的时候不具备的。再则，其实也是上一个原因派生出来的问题，就是Java本身的标准库和数据结构的不熟悉，谁能想到，自学了Java这么久，居然还是懒得看Java的标准文档，尽管算法（第四版）实现了简化版的，堆栈，队列，二叉树等数据结构，文件I/O也大幅简化，增强了易用性，也让自己缺乏去看繁杂的Java标准实现的动力，所以很多时候都是现学现用，这当然无可厚非，因为熟记这些库是没有必要的，只需要知道是堆栈等这些数据结构的接口就可以了，用的时候再去查就行了，无非就是低效一点，并不至于无法实现的地步，但是结果就是不用就不学，导致直到现在对Java的标准库还是一知半解，前段时间打包个Jar还老费劲了。这样做的当时可能觉得没啥，但是一旦实践起来，就很容易出现问题，在做LeetCode OJ的时候，就集中爆发出来了，就是自己是算法构思，迟迟没办法得到代码验证，常常会卡在Java的实现细节上，当时也没有对构思和实践分的这么开，觉得有个构思好像能解决问题，就硬着头强上，以至于最后败兴而归都没能整明白，究竟是算法构思的问题，还是代码实现的问题；以至于虽然刷了很久的题目，收获不大，反倒是积累了很多Java的问题没有解决。以上的一些做的不够好的地方，基本解释了当初为什么没能坚持刷LeetCode ？ \n\n　　还有一个重要的需要解释的问题就是，为什么刷过的LeetCode 过段时间回头来看，仍然解不出来或者是一知半解？譬如说，现在回去做城市天际线的题目，自己似乎没有自信能100%解出来。主要原因应该是自己并没有规划性的做复习工作，很多算法题做完，并没有留下一个可供复习的笔记和资料为后续复习做准备，也就是在之前Blog做反思的过程中提到的，没有系统行的整理和复习自己的学习笔记和资料，导致每次复习的时候，都不得不重头开始做题，把之前踩过的坑又重新踩了一遍，这在几年后，也就是最近几年开始用Python重做这些题的时候都渐渐体现出来了，这其实完全没有必要，因为发生这样的事情主要原因仅仅只是遗忘了而已，思维过程其实还是在没有复习的情况仍然保有印象。所以，定期的记录刷题方法并整理和复习在这个阶段就变的很重要了，这也是自己强调过很多次的问题了，也是努力在改进，就不多细说了。\n\n　　用Python重刷题的时候，就显得有效率了很多，一来很多题目都比较熟悉了，只需要用Python相对应的数据结构重新实现即可，这个过程其实也加深了很多经常用的数据结构的理解，不常用的类和编程模式也熟悉了不少，也算是一个不错的收获吧，毕竟曾经对实践编码的忽视缺失导致自己眼高手低，失去了很多提升编码能力的机会，编码之后，才真实的认识到编程原本就是一项工程实践，自己怎么会天真到仅仅满足于算法构思，而不是脚踏实地的实现出来；再则，其实也算是一种复习，对之前理解不深刻，一知半解的方法做一个完整的梳理。 这也解释为什么，上周某天下午半天不到的时间，就把最后20题，一次性全部刷完，高效的自己都觉得不可思议。其中甚至包含2道Hard，自己原以为需要借助一些提示才做的出来的题目，也比较顺利的完成了。 刷完100题的时候，感觉真的很兴奋，曾经以为很厉害，遥不可及的神迹，现在自己也站在这里达成了这个目标，虽然耗时比较久，过程断断续续，其实过程基本是大部分时间没怎么刷，突然心血来潮就刷很多题那样，也算是达成一个不大不小的成就吧！\n\n　　接下来该怎么做呢？ 这也是最最重要和需要关注的问题，毕竟尽管自己可以一下午刷几十道题，停不下来，但是我还是会开个小差，摸鱼一整天，更是停不下来的那种。我已经渐渐摸索出应对这种情况的解决办法，就是合理的做规划，给出目标，不多不少，不偏不倚。因为从刷题过程来看，我还是非常享受刷题带来的乐趣的，特别是那些觉得有点困难的题目，AC的时候，可以开心好一阵子了，以至于都忘记要整理自己的代码和笔记了。设想中的刷完100题之后，应该是需要整理一下100题的笔记和要点，作为刷题总结的方式，一篇一篇的成为Blog的主要内容来源，现在看来，似乎这项工作并没有开展起来，因为很多注释都写在代码里面了，现在回过头来一个一个的去复习和整理，觉得有点没必要，似乎只需要关注最主要和核心的题目整理出笔记即可，更主要的原因可能是担心这项工作可能会延误自己刷100-200题的进度，毕竟比起回头复习旧题目，我似乎更喜欢做新题。现在回想一下，其实很多代码注释的内容，都足以作为Blog的文字内容存在了，只需要稍加排版即可成为一份笔记，所以对于刷题的笔记模版，应该无需过于担忧。更值得关注的应该是要求自己刷100之后的新题的时候，务必要求自己必须整理出一份代码说明出来，定期排版成Blog，将定期刷题变成一个习惯，这样就有精力去迎接下一个挑战了，毕竟对自己来说是一件很开心的事情。\n\n　　LeetCode 刷完100题有感就这么多，接下来还是希望自己能贯彻自己的日常目标，现阶段坚持刷题和背单词，再坚持锻炼身体，不要避重就轻的断断续续的逃避，相信有了这几项准备和积累，面对以后的工作和生活的挑战，自己能应对的更加自如，处理的更加高效！","slug":"LeetCode-1-100-刷题有感","published":1,"_id":"ckiedp6df0000ue287z2u4kkg","comments":1,"layout":"post","photos":[],"link":"","content":"<p>　　跌跌撞撞终于在前几天完成的LeetCode 100题，刷题过程印证了之前对自我的一个论断，起了个大早，干了个晚集。 早在2012年，自己就开始了解到LeetCode 刷完前100题的神迹，这时LeetCode应该还没创立多久吧！当时国内只有北大的POJ可用，那曲高和寡的界面和难度，着实有点劝退！自己的编程学习之路从来没有一个类似引路人之类的角色，全凭自学，再加上我三分钟热度的神奇特性，所以对ACM之类的算法竞赛全然不知也就毫无意外。彼时，复现数据结构的排序等级实践水平就足以让自己沾沾自喜半天了，毕竟身边有大把无法做到这一点的人，而且还是靠背代码的方式，二叉树倒是无论是理论学习阶段还是代码实现阶段都没有遇到过太大的挑战，图论则是看算法（第四版）就收获了巨大的理论学习上的满足感，听信了据说实践基本不会出现这种级别的题目的话，就止步于此，毕竟图的数据结构确实有点繁琐，代码复杂度也有点棘手，但是在已经完全理解算法理论的基础上，实现起来也至多花一点时间而已。再加上当时眼界受限，以为排序，搜索以及图论之类就已经是算法的全部了。现在的我才真正意识到，理解算法理论和实现算法代码，在学习上是截然不同的2种境界，所幸自己就算没有在电脑上实现代码，也还是保持了记笔记的好习惯，很多自己觉得精妙和优雅的算法，忍不住在笔记本下写下来了，也算是另一种维度的代码实践吧！</p>\n<p>　　回到了LeetCode上来，2012年记得是硕士复试需要准备OJ，当时觉得还是颇为新奇，所以比较重视，刷了很多OJ题目，其中就有LeetCode，不过当时光理解题目意思都有点费劲，刷了不到10题的Easy难度的题目就劝退了；复试结束之后，习惯性的放弃了，直到看完算法（第四版），重新理解了Java之后，忍不住想要做点什么，就又开始重新开始刷LeetCode，用Java 勉强刷了几十道medium和easy的题目，虽然大部分都还是做不出来，不清楚常规解题套路，Hard题目一想就是一整天都没什么头绪，最后还是看Discuss才能找到一点感觉，甚至很多时候光理解代码就需要大半天时间。由于LeetCode OJ系统的易用性，免去了调试环境的很多烦恼，能专注于算法编码和Debug，其次社区讨论都非常活跃，遇到不会做的题目或者是思路受阻，基本都能在Discuss下面找到解答，当时虽然抱着模仿性质的想法，试图想要刷完LeetCode 100题，但是止步于好几个Hard题目，记得有一道计算城市天际线面积的题目，整整想了一周，总是感觉算法上，似乎已经找到解决算法，但是编码时候总是磕磕碰碰，自己的想法还没到验证的阶段，就被茫茫然的细节问题给淹没了。最后不得不求助社区，最后理解完正确的代码之后，就这么弃置不顾。完全放弃了在LeetCode中寻找乐趣的想法了。</p>\n<p>　　以现在的角度回顾分析之前为什么放弃 LeetCode 刷题，或许会有不错的收获！首先，也是老生常谈的问题，就是自己三分钟热度的问题，也不过多展开了，直接给出结论就是，中立的来看，自己既然有这样不算好也不算坏的特质，别像以前那样，说到三分钟热度，大部分时候都是带着点贬义和自我批判意味。再说，三分钟热度起码给了自己一种不断变换角度来看周遭和自己的机会，抱有对世界最单纯的好奇心。缺点自然也很明显，也就是浅尝辄止。最后的一句话点醒了梦中人，<strong>有三分钟热度，就有三分钟收获</strong>。其次，就是自己刷题时候的无知者无畏的盲目自信，想要试图跳级做题，而不是惯常的循序渐进，徒增难度，却发现自己完全没有做好准备，浪费了很多时间，还严重打击了自信，于是便开始自暴自弃，顺带连之前积累的理论知识都被荒废了。这可能是学生思维的弊端，不够谦卑，更不够自信，这其中的度，没有拿捏得当。诚然，数据结构和算法的理论支撑下，很多算法题都会变的比较容易用常规的套路解答，特别是很多算法笔试和考试中，都已经把最难点给精确的给出来了，照本宣科就可以了。那么，这是否意味着解决了核心难点之后，其他方面是否不值一提呢？真实情况并不是这样的，很多时候并没有意识到这个难点为什么是难点，只是被动的接受知识而已，这理所当然的导致另一个重要的能力的缺失，学会如何灵活的运用这些算法理论，也就是融会贯通。只有当无论什么算法题目，都能很准确的分析出问题本质，根据分析结果找到相应的数据结构工具和手段解决问题，才能完成编码的核心，就是算法伪代码的初步构思；但是是否真的能在OJ中AC，只有代码才能验证。这跟数学证明题一样，在证明过程顺利铺开之前，一切构思都只是可能的解决方案而已，能不能达到预想的目的，还需要证明过程来验证，况且，这个构思可能会被证明过程自我否定掉。这些内容总结起来，其实就是<strong>执行力</strong>。回顾自己大部分的学生时代，遗憾的发现，自己执行力可能是属于非常拉胯的级别。也就勉强仗着的天赋好，和某些时候的灵光乍现以及为数不多的树立并确信要去达成的目标的时候，才能走到现在。近年来，终于能心平气和的分析自己的优缺点的时候，才意识到，循序渐进的构思-行动，小步快走的达成目标的重要性和难能可贵。这一点可能是当初年轻气盛的时候不具备的。再则，其实也是上一个原因派生出来的问题，就是Java本身的标准库和数据结构的不熟悉，谁能想到，自学了Java这么久，居然还是懒得看Java的标准文档，尽管算法（第四版）实现了简化版的，堆栈，队列，二叉树等数据结构，文件I/O也大幅简化，增强了易用性，也让自己缺乏去看繁杂的Java标准实现的动力，所以很多时候都是现学现用，这当然无可厚非，因为熟记这些库是没有必要的，只需要知道是堆栈等这些数据结构的接口就可以了，用的时候再去查就行了，无非就是低效一点，并不至于无法实现的地步，但是结果就是不用就不学，导致直到现在对Java的标准库还是一知半解，前段时间打包个Jar还老费劲了。这样做的当时可能觉得没啥，但是一旦实践起来，就很容易出现问题，在做LeetCode OJ的时候，就集中爆发出来了，就是自己是算法构思，迟迟没办法得到代码验证，常常会卡在Java的实现细节上，当时也没有对构思和实践分的这么开，觉得有个构思好像能解决问题，就硬着头强上，以至于最后败兴而归都没能整明白，究竟是算法构思的问题，还是代码实现的问题；以至于虽然刷了很久的题目，收获不大，反倒是积累了很多Java的问题没有解决。以上的一些做的不够好的地方，基本解释了当初为什么没能坚持刷LeetCode ？ </p>\n<p>　　还有一个重要的需要解释的问题就是，为什么刷过的LeetCode 过段时间回头来看，仍然解不出来或者是一知半解？譬如说，现在回去做城市天际线的题目，自己似乎没有自信能100%解出来。主要原因应该是自己并没有规划性的做复习工作，很多算法题做完，并没有留下一个可供复习的笔记和资料为后续复习做准备，也就是在之前Blog做反思的过程中提到的，没有系统行的整理和复习自己的学习笔记和资料，导致每次复习的时候，都不得不重头开始做题，把之前踩过的坑又重新踩了一遍，这在几年后，也就是最近几年开始用Python重做这些题的时候都渐渐体现出来了，这其实完全没有必要，因为发生这样的事情主要原因仅仅只是遗忘了而已，思维过程其实还是在没有复习的情况仍然保有印象。所以，定期的记录刷题方法并整理和复习在这个阶段就变的很重要了，这也是自己强调过很多次的问题了，也是努力在改进，就不多细说了。</p>\n<p>　　用Python重刷题的时候，就显得有效率了很多，一来很多题目都比较熟悉了，只需要用Python相对应的数据结构重新实现即可，这个过程其实也加深了很多经常用的数据结构的理解，不常用的类和编程模式也熟悉了不少，也算是一个不错的收获吧，毕竟曾经对实践编码的忽视缺失导致自己眼高手低，失去了很多提升编码能力的机会，编码之后，才真实的认识到编程原本就是一项工程实践，自己怎么会天真到仅仅满足于算法构思，而不是脚踏实地的实现出来；再则，其实也算是一种复习，对之前理解不深刻，一知半解的方法做一个完整的梳理。 这也解释为什么，上周某天下午半天不到的时间，就把最后20题，一次性全部刷完，高效的自己都觉得不可思议。其中甚至包含2道Hard，自己原以为需要借助一些提示才做的出来的题目，也比较顺利的完成了。 刷完100题的时候，感觉真的很兴奋，曾经以为很厉害，遥不可及的神迹，现在自己也站在这里达成了这个目标，虽然耗时比较久，过程断断续续，其实过程基本是大部分时间没怎么刷，突然心血来潮就刷很多题那样，也算是达成一个不大不小的成就吧！</p>\n<p>　　接下来该怎么做呢？ 这也是最最重要和需要关注的问题，毕竟尽管自己可以一下午刷几十道题，停不下来，但是我还是会开个小差，摸鱼一整天，更是停不下来的那种。我已经渐渐摸索出应对这种情况的解决办法，就是合理的做规划，给出目标，不多不少，不偏不倚。因为从刷题过程来看，我还是非常享受刷题带来的乐趣的，特别是那些觉得有点困难的题目，AC的时候，可以开心好一阵子了，以至于都忘记要整理自己的代码和笔记了。设想中的刷完100题之后，应该是需要整理一下100题的笔记和要点，作为刷题总结的方式，一篇一篇的成为Blog的主要内容来源，现在看来，似乎这项工作并没有开展起来，因为很多注释都写在代码里面了，现在回过头来一个一个的去复习和整理，觉得有点没必要，似乎只需要关注最主要和核心的题目整理出笔记即可，更主要的原因可能是担心这项工作可能会延误自己刷100-200题的进度，毕竟比起回头复习旧题目，我似乎更喜欢做新题。现在回想一下，其实很多代码注释的内容，都足以作为Blog的文字内容存在了，只需要稍加排版即可成为一份笔记，所以对于刷题的笔记模版，应该无需过于担忧。更值得关注的应该是要求自己刷100之后的新题的时候，务必要求自己必须整理出一份代码说明出来，定期排版成Blog，将定期刷题变成一个习惯，这样就有精力去迎接下一个挑战了，毕竟对自己来说是一件很开心的事情。</p>\n<p>　　LeetCode 刷完100题有感就这么多，接下来还是希望自己能贯彻自己的日常目标，现阶段坚持刷题和背单词，再坚持锻炼身体，不要避重就轻的断断续续的逃避，相信有了这几项准备和积累，面对以后的工作和生活的挑战，自己能应对的更加自如，处理的更加高效！</p>\n","site":{"data":{}},"excerpt":"","more":"<p>　　跌跌撞撞终于在前几天完成的LeetCode 100题，刷题过程印证了之前对自我的一个论断，起了个大早，干了个晚集。 早在2012年，自己就开始了解到LeetCode 刷完前100题的神迹，这时LeetCode应该还没创立多久吧！当时国内只有北大的POJ可用，那曲高和寡的界面和难度，着实有点劝退！自己的编程学习之路从来没有一个类似引路人之类的角色，全凭自学，再加上我三分钟热度的神奇特性，所以对ACM之类的算法竞赛全然不知也就毫无意外。彼时，复现数据结构的排序等级实践水平就足以让自己沾沾自喜半天了，毕竟身边有大把无法做到这一点的人，而且还是靠背代码的方式，二叉树倒是无论是理论学习阶段还是代码实现阶段都没有遇到过太大的挑战，图论则是看算法（第四版）就收获了巨大的理论学习上的满足感，听信了据说实践基本不会出现这种级别的题目的话，就止步于此，毕竟图的数据结构确实有点繁琐，代码复杂度也有点棘手，但是在已经完全理解算法理论的基础上，实现起来也至多花一点时间而已。再加上当时眼界受限，以为排序，搜索以及图论之类就已经是算法的全部了。现在的我才真正意识到，理解算法理论和实现算法代码，在学习上是截然不同的2种境界，所幸自己就算没有在电脑上实现代码，也还是保持了记笔记的好习惯，很多自己觉得精妙和优雅的算法，忍不住在笔记本下写下来了，也算是另一种维度的代码实践吧！</p>\n<p>　　回到了LeetCode上来，2012年记得是硕士复试需要准备OJ，当时觉得还是颇为新奇，所以比较重视，刷了很多OJ题目，其中就有LeetCode，不过当时光理解题目意思都有点费劲，刷了不到10题的Easy难度的题目就劝退了；复试结束之后，习惯性的放弃了，直到看完算法（第四版），重新理解了Java之后，忍不住想要做点什么，就又开始重新开始刷LeetCode，用Java 勉强刷了几十道medium和easy的题目，虽然大部分都还是做不出来，不清楚常规解题套路，Hard题目一想就是一整天都没什么头绪，最后还是看Discuss才能找到一点感觉，甚至很多时候光理解代码就需要大半天时间。由于LeetCode OJ系统的易用性，免去了调试环境的很多烦恼，能专注于算法编码和Debug，其次社区讨论都非常活跃，遇到不会做的题目或者是思路受阻，基本都能在Discuss下面找到解答，当时虽然抱着模仿性质的想法，试图想要刷完LeetCode 100题，但是止步于好几个Hard题目，记得有一道计算城市天际线面积的题目，整整想了一周，总是感觉算法上，似乎已经找到解决算法，但是编码时候总是磕磕碰碰，自己的想法还没到验证的阶段，就被茫茫然的细节问题给淹没了。最后不得不求助社区，最后理解完正确的代码之后，就这么弃置不顾。完全放弃了在LeetCode中寻找乐趣的想法了。</p>\n<p>　　以现在的角度回顾分析之前为什么放弃 LeetCode 刷题，或许会有不错的收获！首先，也是老生常谈的问题，就是自己三分钟热度的问题，也不过多展开了，直接给出结论就是，中立的来看，自己既然有这样不算好也不算坏的特质，别像以前那样，说到三分钟热度，大部分时候都是带着点贬义和自我批判意味。再说，三分钟热度起码给了自己一种不断变换角度来看周遭和自己的机会，抱有对世界最单纯的好奇心。缺点自然也很明显，也就是浅尝辄止。最后的一句话点醒了梦中人，<strong>有三分钟热度，就有三分钟收获</strong>。其次，就是自己刷题时候的无知者无畏的盲目自信，想要试图跳级做题，而不是惯常的循序渐进，徒增难度，却发现自己完全没有做好准备，浪费了很多时间，还严重打击了自信，于是便开始自暴自弃，顺带连之前积累的理论知识都被荒废了。这可能是学生思维的弊端，不够谦卑，更不够自信，这其中的度，没有拿捏得当。诚然，数据结构和算法的理论支撑下，很多算法题都会变的比较容易用常规的套路解答，特别是很多算法笔试和考试中，都已经把最难点给精确的给出来了，照本宣科就可以了。那么，这是否意味着解决了核心难点之后，其他方面是否不值一提呢？真实情况并不是这样的，很多时候并没有意识到这个难点为什么是难点，只是被动的接受知识而已，这理所当然的导致另一个重要的能力的缺失，学会如何灵活的运用这些算法理论，也就是融会贯通。只有当无论什么算法题目，都能很准确的分析出问题本质，根据分析结果找到相应的数据结构工具和手段解决问题，才能完成编码的核心，就是算法伪代码的初步构思；但是是否真的能在OJ中AC，只有代码才能验证。这跟数学证明题一样，在证明过程顺利铺开之前，一切构思都只是可能的解决方案而已，能不能达到预想的目的，还需要证明过程来验证，况且，这个构思可能会被证明过程自我否定掉。这些内容总结起来，其实就是<strong>执行力</strong>。回顾自己大部分的学生时代，遗憾的发现，自己执行力可能是属于非常拉胯的级别。也就勉强仗着的天赋好，和某些时候的灵光乍现以及为数不多的树立并确信要去达成的目标的时候，才能走到现在。近年来，终于能心平气和的分析自己的优缺点的时候，才意识到，循序渐进的构思-行动，小步快走的达成目标的重要性和难能可贵。这一点可能是当初年轻气盛的时候不具备的。再则，其实也是上一个原因派生出来的问题，就是Java本身的标准库和数据结构的不熟悉，谁能想到，自学了Java这么久，居然还是懒得看Java的标准文档，尽管算法（第四版）实现了简化版的，堆栈，队列，二叉树等数据结构，文件I/O也大幅简化，增强了易用性，也让自己缺乏去看繁杂的Java标准实现的动力，所以很多时候都是现学现用，这当然无可厚非，因为熟记这些库是没有必要的，只需要知道是堆栈等这些数据结构的接口就可以了，用的时候再去查就行了，无非就是低效一点，并不至于无法实现的地步，但是结果就是不用就不学，导致直到现在对Java的标准库还是一知半解，前段时间打包个Jar还老费劲了。这样做的当时可能觉得没啥，但是一旦实践起来，就很容易出现问题，在做LeetCode OJ的时候，就集中爆发出来了，就是自己是算法构思，迟迟没办法得到代码验证，常常会卡在Java的实现细节上，当时也没有对构思和实践分的这么开，觉得有个构思好像能解决问题，就硬着头强上，以至于最后败兴而归都没能整明白，究竟是算法构思的问题，还是代码实现的问题；以至于虽然刷了很久的题目，收获不大，反倒是积累了很多Java的问题没有解决。以上的一些做的不够好的地方，基本解释了当初为什么没能坚持刷LeetCode ？ </p>\n<p>　　还有一个重要的需要解释的问题就是，为什么刷过的LeetCode 过段时间回头来看，仍然解不出来或者是一知半解？譬如说，现在回去做城市天际线的题目，自己似乎没有自信能100%解出来。主要原因应该是自己并没有规划性的做复习工作，很多算法题做完，并没有留下一个可供复习的笔记和资料为后续复习做准备，也就是在之前Blog做反思的过程中提到的，没有系统行的整理和复习自己的学习笔记和资料，导致每次复习的时候，都不得不重头开始做题，把之前踩过的坑又重新踩了一遍，这在几年后，也就是最近几年开始用Python重做这些题的时候都渐渐体现出来了，这其实完全没有必要，因为发生这样的事情主要原因仅仅只是遗忘了而已，思维过程其实还是在没有复习的情况仍然保有印象。所以，定期的记录刷题方法并整理和复习在这个阶段就变的很重要了，这也是自己强调过很多次的问题了，也是努力在改进，就不多细说了。</p>\n<p>　　用Python重刷题的时候，就显得有效率了很多，一来很多题目都比较熟悉了，只需要用Python相对应的数据结构重新实现即可，这个过程其实也加深了很多经常用的数据结构的理解，不常用的类和编程模式也熟悉了不少，也算是一个不错的收获吧，毕竟曾经对实践编码的忽视缺失导致自己眼高手低，失去了很多提升编码能力的机会，编码之后，才真实的认识到编程原本就是一项工程实践，自己怎么会天真到仅仅满足于算法构思，而不是脚踏实地的实现出来；再则，其实也算是一种复习，对之前理解不深刻，一知半解的方法做一个完整的梳理。 这也解释为什么，上周某天下午半天不到的时间，就把最后20题，一次性全部刷完，高效的自己都觉得不可思议。其中甚至包含2道Hard，自己原以为需要借助一些提示才做的出来的题目，也比较顺利的完成了。 刷完100题的时候，感觉真的很兴奋，曾经以为很厉害，遥不可及的神迹，现在自己也站在这里达成了这个目标，虽然耗时比较久，过程断断续续，其实过程基本是大部分时间没怎么刷，突然心血来潮就刷很多题那样，也算是达成一个不大不小的成就吧！</p>\n<p>　　接下来该怎么做呢？ 这也是最最重要和需要关注的问题，毕竟尽管自己可以一下午刷几十道题，停不下来，但是我还是会开个小差，摸鱼一整天，更是停不下来的那种。我已经渐渐摸索出应对这种情况的解决办法，就是合理的做规划，给出目标，不多不少，不偏不倚。因为从刷题过程来看，我还是非常享受刷题带来的乐趣的，特别是那些觉得有点困难的题目，AC的时候，可以开心好一阵子了，以至于都忘记要整理自己的代码和笔记了。设想中的刷完100题之后，应该是需要整理一下100题的笔记和要点，作为刷题总结的方式，一篇一篇的成为Blog的主要内容来源，现在看来，似乎这项工作并没有开展起来，因为很多注释都写在代码里面了，现在回过头来一个一个的去复习和整理，觉得有点没必要，似乎只需要关注最主要和核心的题目整理出笔记即可，更主要的原因可能是担心这项工作可能会延误自己刷100-200题的进度，毕竟比起回头复习旧题目，我似乎更喜欢做新题。现在回想一下，其实很多代码注释的内容，都足以作为Blog的文字内容存在了，只需要稍加排版即可成为一份笔记，所以对于刷题的笔记模版，应该无需过于担忧。更值得关注的应该是要求自己刷100之后的新题的时候，务必要求自己必须整理出一份代码说明出来，定期排版成Blog，将定期刷题变成一个习惯，这样就有精力去迎接下一个挑战了，毕竟对自己来说是一件很开心的事情。</p>\n<p>　　LeetCode 刷完100题有感就这么多，接下来还是希望自己能贯彻自己的日常目标，现阶段坚持刷题和背单词，再坚持锻炼身体，不要避重就轻的断断续续的逃避，相信有了这几项准备和积累，面对以后的工作和生活的挑战，自己能应对的更加自如，处理的更加高效！</p>\n"},{"title":"LeetCode 115. Distinct Subsequences","toc":true,"mathjax":true,"top":false,"cover":true,"date":"2020-12-09T09:52:16.000Z","updated":"2020-12-10T10:11:23.217Z","_content":"\n> Given two strings `s` and `t`, return *the number of distinct subsequences of `s` which equals `t`*.\n>\n> A string's **subsequence** is a new string formed from the original string by deleting some (can be none) of the characters without disturbing the relative positions of the remaining characters. (i.e., `\"ACE\"` is a subsequence of `\"ABCDE\"` while `\"AEC\"` is not).\n>\n> It's guaranteed the answer fits on a 32-bit signed integer.\n>\n>  \n>\n> **Example 1:**\n>\n> ```\n> Input: s = \"rabbbit\", t = \"rabbit\"\n> Output: 3\n> Explanation:\n> As shown below, there are 3 ways you can generate \"rabbit\" from S.\n> rabbbit\n> rabbbit\n> rabbbit\n> ```\n>\n> **Example 2:**\n>\n> ```\n> Input: s = \"babgbag\", t = \"bag\"\n> Output: 5\n> Explanation:\n> As shown below, there are 5 ways you can generate \"bag\" from S.\n> babgbag\n> babgbag\n> babgbag\n> babgbag\n> babgbag\n> ```\n>\n>  \n>\n> **Constraints:**\n>\n> - `0 <= s.length, t.length <= 1000`\n> - `s` and `t` consist of English letters.\n\n\n\n　　或许是由于之前做过这个题目的原因，所以面对这道Hard题目，心态比较冷静；再加上100题前后的几乎都是二叉树类别的题目，自己面对链表和树类别的题目基本没碰到困难，所以DFS用的手软，到这里有点停不下来的感觉。所以此时第一想法就想到用DFS，虽然预感对于字符串匹配类别的题目，DFS肯定会出现性能问题，但是作为验证算法思想的先导思路，还是打算先实现出来再看，毕竟用DFS写代码，代码真的很短，AC的时候更是有种 吊炸天的 爽快感；结果还是差一点AC；其次，可能是之前做过的原因，虽然第一时间没写出dp递推公式，但是写出了递推矩阵，演算了一番，没想到居然是对的，很容易的转换成了算法代码，结果干净利落的AC了，甚至比之前的AC的时候还要快一倍以上。虽然这不是自己第一次独立解决Hard题目，但是这题没有借鉴之前的代码，而且从构思算法道实践成功基本没遇到什么障碍，还是值得自己兴奋一阵子了。\n\n### 解法一：DFS\n\n　　此题如果要用递归的方法，那么就可以从最简单的字符串匹配入手，就是s和t都是单字符，直接就可以快速返回结果，这就可以精简成3种情况：\n\n1. s == '' 即待匹配的模式t还没有消耗完，就直接没有s可供匹配了，返回0\n2. t == '' 即待匹配的模式t消耗完，返回1\n3. s==t 直接返回1\n4. 递归处理\n\n　　接下来，就开始要做递推式的处理，t的首字符t[0]如果在s中，那么就可以进行递归处理，考虑到s中可能有多个t[0]相同的字符，就要对s和t做不同的切分处理来交给递归处理，最后把所有返回的结果sum一下，就可以了\n\n``` python\nclass Solution:\n    # DFS\n    def numDistinct(self, s: str, t: str) -> int:\n        if len(t) == 0:\n            return 1\n        if len(s) == 0:\n            return 0\n        if s == t:\n            return 1\n\n        r = 0\n        t0 = t[0]\n        pos = -1\n        news = s[pos+1: ]\n        while t0 in news:\n            pos = news.index(t0)\n            news = news[pos+1: ]\n            r += self.numDistinct(news, t[1: ])\n        return r\n```\n\n测试用例通过率：106/116， 离AC只有一步之遥，目测发现是超长的字符串s导致递归性能急剧下降，那么可以确定的是算法思路是没有问题的。接着开始转入DP的算法构思。\n\n### 解法二： 动态规划 Dynamic Programming (DP)\n\n　　由于之前做过这个题，所以非常确定有一个DP算法存在，但是对于这个DP是怎么构建最优子结构的，完全想不起来了。所以，接下来就完全需要自己朝着DP思路构思DP矩阵了。\n\n　　字符串匹配的DP算法有一个非常有名的最长公共子序列(LCS)问题，几乎所有讲解DP的教材都拿这个题目做过示例分析，当时自己对这个递推式还是很不理解，为什么$dp[i][j]$在$s[i]!=t[j]$的时候，可以由$dp[i-1][j]$和$dp[i][j-1]$构成？当初还是理解了好久，才想明白。或许是当初对这个分解的情况思索的很久的原因，所以但凡遇到DP算法的，都会潜意识的往LCS靠；而此题恰好也是字符串匹配的题目，思路方向似乎没什么太大问题。\n\n|      |  r   |  a   |  b   |  b   |  b   |  i   |  t   |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n|  r   |  1   |  0   |  0   |  0   |  0   |  0   |  0   |\n|  a   |  0   |  1   |  0   |  0   |  0   |  0   |  0   |\n|  b   |  0   |  0   |  1   |  1   |  1   |  0   |  0   |\n|  b   |  0   |  0   |  0   |  1   |  2   |  0   |  0   |\n|  i   |  0   |  0   |  0   |  0   |  0   |  3   |  0   |\n|  t   |  0   |  0   |  0   |  0   |  0   |  0   |  3   |\n\n对$dp[i][j]$的定义开始还是不太明确，最初以为是本层i和上层i-1的累加，最后发现不对，最后从意义上的分析，最终确定是来自上一层之前所有可能性的累加，因为$dp[i-1][ :j]$中累加表示上层匹配结束之后，所有可供下层提供入口的数量，而且这个入口只有在$s[i]==t[j]$的时候，才有效，否则直接置为0，最后的返回的结果应该是$sum(dp[-1])$。\n\n最关键的步骤就是正确性验证，在提供的2个测试用例都无误之后，基本就可以开始步入写代码的过程了.\n\n|      |  b   |  a   |  b   |  g   |  b   |  a   |  g   |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n|  b   |  1   |  0   |  1   |  0   |  1   |  0   |  0   |\n|  a   |  0   |  1   |  0   |  0   |  0   |  3   |  0   |\n|  g   |  0   |  0   |  0   |  1   |  0   |  0   |  4   |\n\n由于只需要上层的数据就可以计算本层， 但是并不是完全copy上层的数据，所以没办法用dp常用的边更新数据边覆盖的方式复用同一个List， 所以这里采用了2个List交替使用的方式来逐层更新\n\n``` python\n# dp\n    def numDistinct(self, s, t):\n        dp = [0]* len(s)\n        for i,char in enumerate(s):\n            if char == t[0]:\n                dp[i] = 1\n\n        for char in t[1: ]:\n            count = 0\n            dp_next = [0] * len(s)\n            for i in range(len(s)):\n                if s[i] == char:\n                    dp_next[i] = count\n                count += dp[i]\n            dp = dp_next\n\n        return sum(dp)\n```\n\n36ms AC， 比之前AC的DP还快上一倍，这是最初没想到的。\n\n\n\n### Conclusion\n\n　　这次顺利AC的主要原因还是思路方向的偶然正确性，这跟学生时代做数学题似乎有着异曲同工的地方，就是尝试的手段恰好是对的。如果尝试的手段恰好不对呢？无论是实践还是算法，我领悟到的结论是，多思考问题和事物的本质，才是快速接近正确答案有效的方式。上面的过程也可以发现，在有了正确的递归矩阵的引导下，代码的实现其实很轻松；反倒是，没抓住问题本质的随意和胡乱的尝试，才是导致心态急躁的根源，因为这种胡乱的尝试，似乎给了一种正在接近解决问题的错觉，虽然积累一点关于问题的经验，但是很多时候似乎只是积累无意义的失败经验而已；在适当的失败经验的基础上，更多的思考原因，在发现问题和原因的基础上的尝试，才是有价值和意义的，也才是最有效率的方式。\n\n","source":"_posts/LeetCode-115-Distinct-Subsequences.md","raw":"---\ntitle: LeetCode 115. Distinct Subsequences\ntoc: true\nmathjax: true\ntop: false\ncover: true\ndate: 2020-12-09 17:52:16\nupdated:\ncategories: Algorithms\ntags:\n\t- Algorithms\n\t- LeetCode\n---\n\n> Given two strings `s` and `t`, return *the number of distinct subsequences of `s` which equals `t`*.\n>\n> A string's **subsequence** is a new string formed from the original string by deleting some (can be none) of the characters without disturbing the relative positions of the remaining characters. (i.e., `\"ACE\"` is a subsequence of `\"ABCDE\"` while `\"AEC\"` is not).\n>\n> It's guaranteed the answer fits on a 32-bit signed integer.\n>\n>  \n>\n> **Example 1:**\n>\n> ```\n> Input: s = \"rabbbit\", t = \"rabbit\"\n> Output: 3\n> Explanation:\n> As shown below, there are 3 ways you can generate \"rabbit\" from S.\n> rabbbit\n> rabbbit\n> rabbbit\n> ```\n>\n> **Example 2:**\n>\n> ```\n> Input: s = \"babgbag\", t = \"bag\"\n> Output: 5\n> Explanation:\n> As shown below, there are 5 ways you can generate \"bag\" from S.\n> babgbag\n> babgbag\n> babgbag\n> babgbag\n> babgbag\n> ```\n>\n>  \n>\n> **Constraints:**\n>\n> - `0 <= s.length, t.length <= 1000`\n> - `s` and `t` consist of English letters.\n\n\n\n　　或许是由于之前做过这个题目的原因，所以面对这道Hard题目，心态比较冷静；再加上100题前后的几乎都是二叉树类别的题目，自己面对链表和树类别的题目基本没碰到困难，所以DFS用的手软，到这里有点停不下来的感觉。所以此时第一想法就想到用DFS，虽然预感对于字符串匹配类别的题目，DFS肯定会出现性能问题，但是作为验证算法思想的先导思路，还是打算先实现出来再看，毕竟用DFS写代码，代码真的很短，AC的时候更是有种 吊炸天的 爽快感；结果还是差一点AC；其次，可能是之前做过的原因，虽然第一时间没写出dp递推公式，但是写出了递推矩阵，演算了一番，没想到居然是对的，很容易的转换成了算法代码，结果干净利落的AC了，甚至比之前的AC的时候还要快一倍以上。虽然这不是自己第一次独立解决Hard题目，但是这题没有借鉴之前的代码，而且从构思算法道实践成功基本没遇到什么障碍，还是值得自己兴奋一阵子了。\n\n### 解法一：DFS\n\n　　此题如果要用递归的方法，那么就可以从最简单的字符串匹配入手，就是s和t都是单字符，直接就可以快速返回结果，这就可以精简成3种情况：\n\n1. s == '' 即待匹配的模式t还没有消耗完，就直接没有s可供匹配了，返回0\n2. t == '' 即待匹配的模式t消耗完，返回1\n3. s==t 直接返回1\n4. 递归处理\n\n　　接下来，就开始要做递推式的处理，t的首字符t[0]如果在s中，那么就可以进行递归处理，考虑到s中可能有多个t[0]相同的字符，就要对s和t做不同的切分处理来交给递归处理，最后把所有返回的结果sum一下，就可以了\n\n``` python\nclass Solution:\n    # DFS\n    def numDistinct(self, s: str, t: str) -> int:\n        if len(t) == 0:\n            return 1\n        if len(s) == 0:\n            return 0\n        if s == t:\n            return 1\n\n        r = 0\n        t0 = t[0]\n        pos = -1\n        news = s[pos+1: ]\n        while t0 in news:\n            pos = news.index(t0)\n            news = news[pos+1: ]\n            r += self.numDistinct(news, t[1: ])\n        return r\n```\n\n测试用例通过率：106/116， 离AC只有一步之遥，目测发现是超长的字符串s导致递归性能急剧下降，那么可以确定的是算法思路是没有问题的。接着开始转入DP的算法构思。\n\n### 解法二： 动态规划 Dynamic Programming (DP)\n\n　　由于之前做过这个题，所以非常确定有一个DP算法存在，但是对于这个DP是怎么构建最优子结构的，完全想不起来了。所以，接下来就完全需要自己朝着DP思路构思DP矩阵了。\n\n　　字符串匹配的DP算法有一个非常有名的最长公共子序列(LCS)问题，几乎所有讲解DP的教材都拿这个题目做过示例分析，当时自己对这个递推式还是很不理解，为什么$dp[i][j]$在$s[i]!=t[j]$的时候，可以由$dp[i-1][j]$和$dp[i][j-1]$构成？当初还是理解了好久，才想明白。或许是当初对这个分解的情况思索的很久的原因，所以但凡遇到DP算法的，都会潜意识的往LCS靠；而此题恰好也是字符串匹配的题目，思路方向似乎没什么太大问题。\n\n|      |  r   |  a   |  b   |  b   |  b   |  i   |  t   |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n|  r   |  1   |  0   |  0   |  0   |  0   |  0   |  0   |\n|  a   |  0   |  1   |  0   |  0   |  0   |  0   |  0   |\n|  b   |  0   |  0   |  1   |  1   |  1   |  0   |  0   |\n|  b   |  0   |  0   |  0   |  1   |  2   |  0   |  0   |\n|  i   |  0   |  0   |  0   |  0   |  0   |  3   |  0   |\n|  t   |  0   |  0   |  0   |  0   |  0   |  0   |  3   |\n\n对$dp[i][j]$的定义开始还是不太明确，最初以为是本层i和上层i-1的累加，最后发现不对，最后从意义上的分析，最终确定是来自上一层之前所有可能性的累加，因为$dp[i-1][ :j]$中累加表示上层匹配结束之后，所有可供下层提供入口的数量，而且这个入口只有在$s[i]==t[j]$的时候，才有效，否则直接置为0，最后的返回的结果应该是$sum(dp[-1])$。\n\n最关键的步骤就是正确性验证，在提供的2个测试用例都无误之后，基本就可以开始步入写代码的过程了.\n\n|      |  b   |  a   |  b   |  g   |  b   |  a   |  g   |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n|  b   |  1   |  0   |  1   |  0   |  1   |  0   |  0   |\n|  a   |  0   |  1   |  0   |  0   |  0   |  3   |  0   |\n|  g   |  0   |  0   |  0   |  1   |  0   |  0   |  4   |\n\n由于只需要上层的数据就可以计算本层， 但是并不是完全copy上层的数据，所以没办法用dp常用的边更新数据边覆盖的方式复用同一个List， 所以这里采用了2个List交替使用的方式来逐层更新\n\n``` python\n# dp\n    def numDistinct(self, s, t):\n        dp = [0]* len(s)\n        for i,char in enumerate(s):\n            if char == t[0]:\n                dp[i] = 1\n\n        for char in t[1: ]:\n            count = 0\n            dp_next = [0] * len(s)\n            for i in range(len(s)):\n                if s[i] == char:\n                    dp_next[i] = count\n                count += dp[i]\n            dp = dp_next\n\n        return sum(dp)\n```\n\n36ms AC， 比之前AC的DP还快上一倍，这是最初没想到的。\n\n\n\n### Conclusion\n\n　　这次顺利AC的主要原因还是思路方向的偶然正确性，这跟学生时代做数学题似乎有着异曲同工的地方，就是尝试的手段恰好是对的。如果尝试的手段恰好不对呢？无论是实践还是算法，我领悟到的结论是，多思考问题和事物的本质，才是快速接近正确答案有效的方式。上面的过程也可以发现，在有了正确的递归矩阵的引导下，代码的实现其实很轻松；反倒是，没抓住问题本质的随意和胡乱的尝试，才是导致心态急躁的根源，因为这种胡乱的尝试，似乎给了一种正在接近解决问题的错觉，虽然积累一点关于问题的经验，但是很多时候似乎只是积累无意义的失败经验而已；在适当的失败经验的基础上，更多的思考原因，在发现问题和原因的基础上的尝试，才是有价值和意义的，也才是最有效率的方式。\n\n","slug":"LeetCode-115-Distinct-Subsequences","published":1,"_id":"ckiijvpg20000ce28c4jz8x92","comments":1,"layout":"post","photos":[],"link":"","content":"<blockquote>\n<p>Given two strings <code>s</code> and <code>t</code>, return <em>the number of distinct subsequences of <code>s</code> which equals <code>t</code></em>.</p>\n<p>A string’s <strong>subsequence</strong> is a new string formed from the original string by deleting some (can be none) of the characters without disturbing the relative positions of the remaining characters. (i.e., <code>&quot;ACE&quot;</code> is a subsequence of <code>&quot;ABCDE&quot;</code> while <code>&quot;AEC&quot;</code> is not).</p>\n<p>It’s guaranteed the answer fits on a 32-bit signed integer.</p>\n<p><strong>Example 1:</strong></p>\n<pre><code>Input: s = &quot;rabbbit&quot;, t = &quot;rabbit&quot;\nOutput: 3\nExplanation:\nAs shown below, there are 3 ways you can generate &quot;rabbit&quot; from S.\nrabbbit\nrabbbit\nrabbbit</code></pre>\n<p><strong>Example 2:</strong></p>\n<pre><code>Input: s = &quot;babgbag&quot;, t = &quot;bag&quot;\nOutput: 5\nExplanation:\nAs shown below, there are 5 ways you can generate &quot;bag&quot; from S.\nbabgbag\nbabgbag\nbabgbag\nbabgbag\nbabgbag</code></pre>\n<p><strong>Constraints:</strong></p>\n<ul>\n<li><code>0 &lt;= s.length, t.length &lt;= 1000</code></li>\n<li><code>s</code> and <code>t</code> consist of English letters.</li>\n</ul>\n</blockquote>\n<p>　　或许是由于之前做过这个题目的原因，所以面对这道Hard题目，心态比较冷静；再加上100题前后的几乎都是二叉树类别的题目，自己面对链表和树类别的题目基本没碰到困难，所以DFS用的手软，到这里有点停不下来的感觉。所以此时第一想法就想到用DFS，虽然预感对于字符串匹配类别的题目，DFS肯定会出现性能问题，但是作为验证算法思想的先导思路，还是打算先实现出来再看，毕竟用DFS写代码，代码真的很短，AC的时候更是有种 吊炸天的 爽快感；结果还是差一点AC；其次，可能是之前做过的原因，虽然第一时间没写出dp递推公式，但是写出了递推矩阵，演算了一番，没想到居然是对的，很容易的转换成了算法代码，结果干净利落的AC了，甚至比之前的AC的时候还要快一倍以上。虽然这不是自己第一次独立解决Hard题目，但是这题没有借鉴之前的代码，而且从构思算法道实践成功基本没遇到什么障碍，还是值得自己兴奋一阵子了。</p>\n<h3 id=\"解法一：DFS\"><a href=\"#解法一：DFS\" class=\"headerlink\" title=\"解法一：DFS\"></a>解法一：DFS</h3><p>　　此题如果要用递归的方法，那么就可以从最简单的字符串匹配入手，就是s和t都是单字符，直接就可以快速返回结果，这就可以精简成3种情况：</p>\n<ol>\n<li>s == ‘’ 即待匹配的模式t还没有消耗完，就直接没有s可供匹配了，返回0</li>\n<li>t == ‘’ 即待匹配的模式t消耗完，返回1</li>\n<li>s==t 直接返回1</li>\n<li>递归处理</li>\n</ol>\n<p>　　接下来，就开始要做递推式的处理，t的首字符t[0]如果在s中，那么就可以进行递归处理，考虑到s中可能有多个t[0]相同的字符，就要对s和t做不同的切分处理来交给递归处理，最后把所有返回的结果sum一下，就可以了</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">Solution</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\" spellcheck=\"true\"># DFS</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">numDistinct</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> s<span class=\"token punctuation\">:</span> str<span class=\"token punctuation\">,</span> t<span class=\"token punctuation\">:</span> str<span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span><span class=\"token operator\">></span> int<span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">if</span> len<span class=\"token punctuation\">(</span>t<span class=\"token punctuation\">)</span> <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">return</span> <span class=\"token number\">1</span>\n        <span class=\"token keyword\">if</span> len<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span> <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">return</span> <span class=\"token number\">0</span>\n        <span class=\"token keyword\">if</span> s <span class=\"token operator\">==</span> t<span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">return</span> <span class=\"token number\">1</span>\n\n        r <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n        t0 <span class=\"token operator\">=</span> t<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\n        pos <span class=\"token operator\">=</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span>\n        news <span class=\"token operator\">=</span> s<span class=\"token punctuation\">[</span>pos<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">]</span>\n        <span class=\"token keyword\">while</span> t0 <span class=\"token keyword\">in</span> news<span class=\"token punctuation\">:</span>\n            pos <span class=\"token operator\">=</span> news<span class=\"token punctuation\">.</span>index<span class=\"token punctuation\">(</span>t0<span class=\"token punctuation\">)</span>\n            news <span class=\"token operator\">=</span> news<span class=\"token punctuation\">[</span>pos<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">]</span>\n            r <span class=\"token operator\">+=</span> self<span class=\"token punctuation\">.</span>numDistinct<span class=\"token punctuation\">(</span>news<span class=\"token punctuation\">,</span> t<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> r</code></pre>\n<p>测试用例通过率：106/116， 离AC只有一步之遥，目测发现是超长的字符串s导致递归性能急剧下降，那么可以确定的是算法思路是没有问题的。接着开始转入DP的算法构思。</p>\n<h3 id=\"解法二：-动态规划-Dynamic-Programming-DP\"><a href=\"#解法二：-动态规划-Dynamic-Programming-DP\" class=\"headerlink\" title=\"解法二： 动态规划 Dynamic Programming (DP)\"></a>解法二： 动态规划 Dynamic Programming (DP)</h3><p>　　由于之前做过这个题，所以非常确定有一个DP算法存在，但是对于这个DP是怎么构建最优子结构的，完全想不起来了。所以，接下来就完全需要自己朝着DP思路构思DP矩阵了。</p>\n<p>　　字符串匹配的DP算法有一个非常有名的最长公共子序列(LCS)问题，几乎所有讲解DP的教材都拿这个题目做过示例分析，当时自己对这个递推式还是很不理解，为什么$dp[i][j]$在$s[i]!=t[j]$的时候，可以由$dp[i-1][j]$和$dp[i][j-1]$构成？当初还是理解了好久，才想明白。或许是当初对这个分解的情况思索的很久的原因，所以但凡遇到DP算法的，都会潜意识的往LCS靠；而此题恰好也是字符串匹配的题目，思路方向似乎没什么太大问题。</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\"></th>\n<th align=\"center\">r</th>\n<th align=\"center\">a</th>\n<th align=\"center\">b</th>\n<th align=\"center\">b</th>\n<th align=\"center\">b</th>\n<th align=\"center\">i</th>\n<th align=\"center\">t</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">r</td>\n<td align=\"center\">1</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n</tr>\n<tr>\n<td align=\"center\">a</td>\n<td align=\"center\">0</td>\n<td align=\"center\">1</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n</tr>\n<tr>\n<td align=\"center\">b</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">1</td>\n<td align=\"center\">1</td>\n<td align=\"center\">1</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n</tr>\n<tr>\n<td align=\"center\">b</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">1</td>\n<td align=\"center\">2</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n</tr>\n<tr>\n<td align=\"center\">i</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">3</td>\n<td align=\"center\">0</td>\n</tr>\n<tr>\n<td align=\"center\">t</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">3</td>\n</tr>\n</tbody></table>\n<p>对$dp[i][j]$的定义开始还是不太明确，最初以为是本层i和上层i-1的累加，最后发现不对，最后从意义上的分析，最终确定是来自上一层之前所有可能性的累加，因为$dp[i-1][ :j]$中累加表示上层匹配结束之后，所有可供下层提供入口的数量，而且这个入口只有在$s[i]==t[j]$的时候，才有效，否则直接置为0，最后的返回的结果应该是$sum(dp[-1])$。</p>\n<p>最关键的步骤就是正确性验证，在提供的2个测试用例都无误之后，基本就可以开始步入写代码的过程了.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\"></th>\n<th align=\"center\">b</th>\n<th align=\"center\">a</th>\n<th align=\"center\">b</th>\n<th align=\"center\">g</th>\n<th align=\"center\">b</th>\n<th align=\"center\">a</th>\n<th align=\"center\">g</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">b</td>\n<td align=\"center\">1</td>\n<td align=\"center\">0</td>\n<td align=\"center\">1</td>\n<td align=\"center\">0</td>\n<td align=\"center\">1</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n</tr>\n<tr>\n<td align=\"center\">a</td>\n<td align=\"center\">0</td>\n<td align=\"center\">1</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">3</td>\n<td align=\"center\">0</td>\n</tr>\n<tr>\n<td align=\"center\">g</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">1</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">4</td>\n</tr>\n</tbody></table>\n<p>由于只需要上层的数据就可以计算本层， 但是并不是完全copy上层的数据，所以没办法用dp常用的边更新数据边覆盖的方式复用同一个List， 所以这里采用了2个List交替使用的方式来逐层更新</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token comment\" spellcheck=\"true\"># dp</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">numDistinct</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> s<span class=\"token punctuation\">,</span> t<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        dp <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token operator\">*</span> len<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">for</span> i<span class=\"token punctuation\">,</span>char <span class=\"token keyword\">in</span> enumerate<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">if</span> char <span class=\"token operator\">==</span> t<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n                dp<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token number\">1</span>\n\n        <span class=\"token keyword\">for</span> char <span class=\"token keyword\">in</span> t<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n            count <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n            dp_next <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">*</span> len<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span>\n            <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span>len<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n                <span class=\"token keyword\">if</span> s<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span> <span class=\"token operator\">==</span> char<span class=\"token punctuation\">:</span>\n                    dp_next<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> count\n                count <span class=\"token operator\">+=</span> dp<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span>\n            dp <span class=\"token operator\">=</span> dp_next\n\n        <span class=\"token keyword\">return</span> sum<span class=\"token punctuation\">(</span>dp<span class=\"token punctuation\">)</span></code></pre>\n<p>36ms AC， 比之前AC的DP还快上一倍，这是最初没想到的。</p>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>　　这次顺利AC的主要原因还是思路方向的偶然正确性，这跟学生时代做数学题似乎有着异曲同工的地方，就是尝试的手段恰好是对的。如果尝试的手段恰好不对呢？无论是实践还是算法，我领悟到的结论是，多思考问题和事物的本质，才是快速接近正确答案有效的方式。上面的过程也可以发现，在有了正确的递归矩阵的引导下，代码的实现其实很轻松；反倒是，没抓住问题本质的随意和胡乱的尝试，才是导致心态急躁的根源，因为这种胡乱的尝试，似乎给了一种正在接近解决问题的错觉，虽然积累一点关于问题的经验，但是很多时候似乎只是积累无意义的失败经验而已；在适当的失败经验的基础上，更多的思考原因，在发现问题和原因的基础上的尝试，才是有价值和意义的，也才是最有效率的方式。</p>\n","site":{"data":{}},"excerpt":"","more":"<blockquote>\n<p>Given two strings <code>s</code> and <code>t</code>, return <em>the number of distinct subsequences of <code>s</code> which equals <code>t</code></em>.</p>\n<p>A string’s <strong>subsequence</strong> is a new string formed from the original string by deleting some (can be none) of the characters without disturbing the relative positions of the remaining characters. (i.e., <code>&quot;ACE&quot;</code> is a subsequence of <code>&quot;ABCDE&quot;</code> while <code>&quot;AEC&quot;</code> is not).</p>\n<p>It’s guaranteed the answer fits on a 32-bit signed integer.</p>\n<p><strong>Example 1:</strong></p>\n<pre><code>Input: s = &quot;rabbbit&quot;, t = &quot;rabbit&quot;\nOutput: 3\nExplanation:\nAs shown below, there are 3 ways you can generate &quot;rabbit&quot; from S.\nrabbbit\nrabbbit\nrabbbit</code></pre>\n<p><strong>Example 2:</strong></p>\n<pre><code>Input: s = &quot;babgbag&quot;, t = &quot;bag&quot;\nOutput: 5\nExplanation:\nAs shown below, there are 5 ways you can generate &quot;bag&quot; from S.\nbabgbag\nbabgbag\nbabgbag\nbabgbag\nbabgbag</code></pre>\n<p><strong>Constraints:</strong></p>\n<ul>\n<li><code>0 &lt;= s.length, t.length &lt;= 1000</code></li>\n<li><code>s</code> and <code>t</code> consist of English letters.</li>\n</ul>\n</blockquote>\n<p>　　或许是由于之前做过这个题目的原因，所以面对这道Hard题目，心态比较冷静；再加上100题前后的几乎都是二叉树类别的题目，自己面对链表和树类别的题目基本没碰到困难，所以DFS用的手软，到这里有点停不下来的感觉。所以此时第一想法就想到用DFS，虽然预感对于字符串匹配类别的题目，DFS肯定会出现性能问题，但是作为验证算法思想的先导思路，还是打算先实现出来再看，毕竟用DFS写代码，代码真的很短，AC的时候更是有种 吊炸天的 爽快感；结果还是差一点AC；其次，可能是之前做过的原因，虽然第一时间没写出dp递推公式，但是写出了递推矩阵，演算了一番，没想到居然是对的，很容易的转换成了算法代码，结果干净利落的AC了，甚至比之前的AC的时候还要快一倍以上。虽然这不是自己第一次独立解决Hard题目，但是这题没有借鉴之前的代码，而且从构思算法道实践成功基本没遇到什么障碍，还是值得自己兴奋一阵子了。</p>\n<h3 id=\"解法一：DFS\"><a href=\"#解法一：DFS\" class=\"headerlink\" title=\"解法一：DFS\"></a>解法一：DFS</h3><p>　　此题如果要用递归的方法，那么就可以从最简单的字符串匹配入手，就是s和t都是单字符，直接就可以快速返回结果，这就可以精简成3种情况：</p>\n<ol>\n<li>s == ‘’ 即待匹配的模式t还没有消耗完，就直接没有s可供匹配了，返回0</li>\n<li>t == ‘’ 即待匹配的模式t消耗完，返回1</li>\n<li>s==t 直接返回1</li>\n<li>递归处理</li>\n</ol>\n<p>　　接下来，就开始要做递推式的处理，t的首字符t[0]如果在s中，那么就可以进行递归处理，考虑到s中可能有多个t[0]相同的字符，就要对s和t做不同的切分处理来交给递归处理，最后把所有返回的结果sum一下，就可以了</p>\n<pre><code class=\"python\">class Solution:\n    # DFS\n    def numDistinct(self, s: str, t: str) -&gt; int:\n        if len(t) == 0:\n            return 1\n        if len(s) == 0:\n            return 0\n        if s == t:\n            return 1\n\n        r = 0\n        t0 = t[0]\n        pos = -1\n        news = s[pos+1: ]\n        while t0 in news:\n            pos = news.index(t0)\n            news = news[pos+1: ]\n            r += self.numDistinct(news, t[1: ])\n        return r</code></pre>\n<p>测试用例通过率：106/116， 离AC只有一步之遥，目测发现是超长的字符串s导致递归性能急剧下降，那么可以确定的是算法思路是没有问题的。接着开始转入DP的算法构思。</p>\n<h3 id=\"解法二：-动态规划-Dynamic-Programming-DP\"><a href=\"#解法二：-动态规划-Dynamic-Programming-DP\" class=\"headerlink\" title=\"解法二： 动态规划 Dynamic Programming (DP)\"></a>解法二： 动态规划 Dynamic Programming (DP)</h3><p>　　由于之前做过这个题，所以非常确定有一个DP算法存在，但是对于这个DP是怎么构建最优子结构的，完全想不起来了。所以，接下来就完全需要自己朝着DP思路构思DP矩阵了。</p>\n<p>　　字符串匹配的DP算法有一个非常有名的最长公共子序列(LCS)问题，几乎所有讲解DP的教材都拿这个题目做过示例分析，当时自己对这个递推式还是很不理解，为什么$dp[i][j]$在$s[i]!=t[j]$的时候，可以由$dp[i-1][j]$和$dp[i][j-1]$构成？当初还是理解了好久，才想明白。或许是当初对这个分解的情况思索的很久的原因，所以但凡遇到DP算法的，都会潜意识的往LCS靠；而此题恰好也是字符串匹配的题目，思路方向似乎没什么太大问题。</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\"></th>\n<th align=\"center\">r</th>\n<th align=\"center\">a</th>\n<th align=\"center\">b</th>\n<th align=\"center\">b</th>\n<th align=\"center\">b</th>\n<th align=\"center\">i</th>\n<th align=\"center\">t</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">r</td>\n<td align=\"center\">1</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n</tr>\n<tr>\n<td align=\"center\">a</td>\n<td align=\"center\">0</td>\n<td align=\"center\">1</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n</tr>\n<tr>\n<td align=\"center\">b</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">1</td>\n<td align=\"center\">1</td>\n<td align=\"center\">1</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n</tr>\n<tr>\n<td align=\"center\">b</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">1</td>\n<td align=\"center\">2</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n</tr>\n<tr>\n<td align=\"center\">i</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">3</td>\n<td align=\"center\">0</td>\n</tr>\n<tr>\n<td align=\"center\">t</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">3</td>\n</tr>\n</tbody></table>\n<p>对$dp[i][j]$的定义开始还是不太明确，最初以为是本层i和上层i-1的累加，最后发现不对，最后从意义上的分析，最终确定是来自上一层之前所有可能性的累加，因为$dp[i-1][ :j]$中累加表示上层匹配结束之后，所有可供下层提供入口的数量，而且这个入口只有在$s[i]==t[j]$的时候，才有效，否则直接置为0，最后的返回的结果应该是$sum(dp[-1])$。</p>\n<p>最关键的步骤就是正确性验证，在提供的2个测试用例都无误之后，基本就可以开始步入写代码的过程了.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\"></th>\n<th align=\"center\">b</th>\n<th align=\"center\">a</th>\n<th align=\"center\">b</th>\n<th align=\"center\">g</th>\n<th align=\"center\">b</th>\n<th align=\"center\">a</th>\n<th align=\"center\">g</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">b</td>\n<td align=\"center\">1</td>\n<td align=\"center\">0</td>\n<td align=\"center\">1</td>\n<td align=\"center\">0</td>\n<td align=\"center\">1</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n</tr>\n<tr>\n<td align=\"center\">a</td>\n<td align=\"center\">0</td>\n<td align=\"center\">1</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">3</td>\n<td align=\"center\">0</td>\n</tr>\n<tr>\n<td align=\"center\">g</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">1</td>\n<td align=\"center\">0</td>\n<td align=\"center\">0</td>\n<td align=\"center\">4</td>\n</tr>\n</tbody></table>\n<p>由于只需要上层的数据就可以计算本层， 但是并不是完全copy上层的数据，所以没办法用dp常用的边更新数据边覆盖的方式复用同一个List， 所以这里采用了2个List交替使用的方式来逐层更新</p>\n<pre><code class=\"python\"># dp\n    def numDistinct(self, s, t):\n        dp = [0]* len(s)\n        for i,char in enumerate(s):\n            if char == t[0]:\n                dp[i] = 1\n\n        for char in t[1: ]:\n            count = 0\n            dp_next = [0] * len(s)\n            for i in range(len(s)):\n                if s[i] == char:\n                    dp_next[i] = count\n                count += dp[i]\n            dp = dp_next\n\n        return sum(dp)</code></pre>\n<p>36ms AC， 比之前AC的DP还快上一倍，这是最初没想到的。</p>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>　　这次顺利AC的主要原因还是思路方向的偶然正确性，这跟学生时代做数学题似乎有着异曲同工的地方，就是尝试的手段恰好是对的。如果尝试的手段恰好不对呢？无论是实践还是算法，我领悟到的结论是，多思考问题和事物的本质，才是快速接近正确答案有效的方式。上面的过程也可以发现，在有了正确的递归矩阵的引导下，代码的实现其实很轻松；反倒是，没抓住问题本质的随意和胡乱的尝试，才是导致心态急躁的根源，因为这种胡乱的尝试，似乎给了一种正在接近解决问题的错觉，虽然积累一点关于问题的经验，但是很多时候似乎只是积累无意义的失败经验而已；在适当的失败经验的基础上，更多的思考原因，在发现问题和原因的基础上的尝试，才是有价值和意义的，也才是最有效率的方式。</p>\n"},{"title":"LeetCode 126. Word Ladder II","toc":true,"mathjax":true,"top":false,"cover":true,"date":"2020-12-15T03:33:20.000Z","updated":"2020-12-17T03:07:06.076Z","_content":"\n> Given two words (*beginWord* and *endWord*), and a dictionary's word list, find all shortest transformation sequence(s) from *beginWord* to *endWord*, such that:\n>\n> 1. Only one letter can be changed at a time\n> 2. Each transformed word must exist in the word list. Note that *beginWord* is *not* a transformed word.\n>\n> **Note:**\n>\n> - Return an empty list if there is no such transformation sequence.\n> - All words have the same length.\n> - All words contain only lowercase alphabetic characters.\n> - You may assume no duplicates in the word list.\n> - You may assume *beginWord* and *endWord* are non-empty and are not the same.\n>\n> **Example 1:**\n>\n> ```\n> Input:\n> beginWord = \"hit\",\n> endWord = \"cog\",\n> wordList = [\"hot\",\"dot\",\"dog\",\"lot\",\"log\",\"cog\"]\n> \n> Output:\n> [\n>   [\"hit\",\"hot\",\"dot\",\"dog\",\"cog\"],\n>   [\"hit\",\"hot\",\"lot\",\"log\",\"cog\"]\n> ]\n> ```\n>\n> **Example 2:**\n>\n> ```\n> Input:\n> beginWord = \"hit\"\n> endWord = \"cog\"\n> wordList = [\"hot\",\"dot\",\"dog\",\"lot\",\"log\"]\n> \n> Output: []\n> \n> Explanation: The endWord \"cog\" is not in wordList, therefore no possible transformation\n> ```\n\n　　这是一道似曾相识的Hard题，却意外的卡了好几天，好在最后还是凭借自己完成的算法方案，差一点AC，TLE的程度，最后试图优化无果，搜索Discuss，发现Python高赞方案跟自己的如出一辙，只不过在最费时的状态图构建上，做了很好的优化，借助这个优化思路，也很快获得了AC，学到了很好的优化经验。\n\n　　说到似曾相识，是因为自己曾经做过的Frog Jump 也是一道Hard题，现在回想起来，那可能是自己独立实现AC的第一道Hard，这才有了后来渐渐想要不借助提示挑战Hard的勇气。虽说Frog Jump 是一道Hard，但是解题过程意外的轻松，很容易的就找到了算法方案，基本就是常规的思路，然后实现代码过程也没遇到很多问题，最后一次AC，倍感意外。彼时就有点反思道，是不是自己高估了Hard，所以被这个束缚，才一直觉得自己搞不定才没有去尝试。现在想来，可能只是因为经验不足，用于解题的算法工具不够充足才会有Hard无法突破的心理障碍，比如，Frog Jump , Jump Game 的多种变种以及这题Word Ladder 基本都跟算法（第四版）中的 确定有穷状态自动机DFA 和 不确定有穷状态自动机 NFA的解法一致，无论是最初AC，还是后来的Debug优化，基本都是围绕Pattern 构建的状态图流转即可。此题甚至都无需构建状态图，只需要寻找合适的数据结构来标记状态即可，多少跟图论算法比较相似。\n\n### 算法思路分析\n\n　　本质上，这其实是一道图论题目，在wordList提供的图之中，搜索从beginWord 到 endWord 的最短路径，最短路径可以直接用BFS求解，因为题设只需要输出最短路径的可能值即可，所以找到终点之后，程序即可中止。\n\n　　算法上，还是延续之前的各种Jump的变种思路，难点应该还是在于实现上，毕竟这些变种题会有各种约束条件的变化导致代码上会有些许不同，这个恐怕要在写代码的时候才能意识得到。\n\n### 算法实现\n\n　　跟DFA/NFA类似，先把wordList转化成图表达，搜索所有的有连接关系的单词对，并保存起来。\n\n```python\n# 判断 字符串 s1 s2 是否有边连接\ndef isPair(self, s1, s2):\n\tc = 0\n\tfor i,j in zip(s1, s2):\n\t\tif c > 1:\n\t\t\treturn False\n\t\tif i != j:\n\t\t\tc += 1\n\treturn c == 1\n      \n# word 在wordList 搜索所有可能的边\ndef constructidx(self, word, wordList, des2src):\n\tfor w in wordList:\n\tif w != word and self.isPair(w, word):\n\t\tdes2src[w].add(word)\n\t\tdes2src[word].add(w)\n\treturn\n```\n\n　　自己虽然知道在搜索连接边的isPair写的有点简单粗暴，有很大的优化空间，因为随着wordList的length增大，这个调用次数也是呈指数上升的，所以isPair的一点小优化能极大的降低TLE概率，事实也确实如此。无奈自己面对这样的基础优化，还是有点无能无力，似乎陷入了这个第一想法就得到结果的算法思路限制，无法彻底的想到新思路，最后在Discuss找到了这一段的简洁实现，利用wordList_set的在搜索阶段的改进很明显，除此之外以word为基础重新构建的方法会快一点，毕竟一个单词每一个位置都用26个字母重新替换一下，也需要$O(N)$的时间复杂度，综合起来$O(MN)$；而上述的代码可能需要$O(M^2)$， 其中M代表wordList 长度，N代表word本身的长度，这个角度来看，M较大的可能性还是比较高的。\n\n```python\ndef constructidx_1(self, word, wordList_set, des2src):\n\tfor i in range(len(word)):\n\t\tfor char in string.ascii_lowercase:\n\t\t\ttmp = word[ :i] + char + word[i+1: ]\n\t\t\tif tmp in wordList_set:\n\t\t\t\tdes2src[word].add(tmp)\n\t\t\t\t# des2src[tmp].add(word)\n\treturn\n```\n\n　　也是这一段的优化最终将TLE的代码转化为AC的代码\n\n　　接下来就是BFS搜索的过程了，自己在写这段代码的时候，不知道是不是脑子抽风了，又犯了还没想好整体构思，就提枪上阵的毛病，本来是一段很基础的BFS搜索的过程，结果由于访问状态标记的代码没想明白，就随意的放置访问状态的代码，结果调试了1天，才发现问题所在，期间出现了各种匪夷所思的访问状态变化的问题，搞得自己经常莫名其妙，各种怀疑是不是其他地方出了问题。实在是有点不应该。\n\n　　可能是以前的BFS都需要遍历所有的节点之后，自动返回，所以直接采用queue的方式，queue为空自动返回即可，不需要标记BFS目前在第几层，然而此题是需要求最短路径，显然是需要采取层次遍历的方式依此进行，所以用q, q_next依此交替的方式，q是目前的遍历层次，q_next是接下来需要遍历的层次；其次，状态标记visited 的问题，此题由于要输出所有可能的最短路径，所以不仅进入q_next的新节点需要标记回溯路径，有可能q_next中的某个节点a，在此层被2个不同的节点b和c连接，此时需要把这2条路径需要标记回溯路径，之前这种可能性被自己忽略了，所以每当有多条的路径的时候，自己的代码只输出了一条，最后发现是遍历b和c的时候，b-a 使a 进入q_next, 并且visited[a] = True,标记b-a的回溯路径， 而当 c-a 需要标记回溯路径的时候，却由于visited[a]==True 被跳过了，所以才有那段不伦不类的if elif 的分支判断\n\n```python\nq = deque()\nq.append(endWord)\nvisited = {endWord:True}\nbp = defaultdict(set)\n#res = defaultdict(list)\n#res[endWord] = [ [endWord] ]\nwhile len(q) > 0 :\n\tq_next = deque()\n\tfor e in q:\n\t\t# self.constructidx(e, wordList_set, des2src)\n\t\tself.constructidx_1(e, wordList_set, des2src)\n\t\tfor src in des2src[e]:\n\t\t\tif not visited.get(src, False)  :\n\t\t\t\tvisited[src] = True\n\t\t\t\tq_next.append(src)\n\n#\t\t\t\tr = [ [src] ]if len(res[e]) == 0 else [[src] + re for re in res[e]]\n#\t\t\t\tres[src].extend(r)\n\t\t\t\tbp[src].add(e)\n\t\t\telif src in q_next:\n#\t\t\t\tr = [ [src] ]if len(res[e]) == 0 else [[src] + re for re in res[e]]\n#\t\t\t\tres[src].extend(r)\n\t\t\t\tbp[src].add(e)\n\tif beginWord in q_next:\n\t\tbreak\n\twordList_set = wordList_set - set(bp.keys())\n\tq = q_next\n  \n# return res[beginWord]\nres = []\nself.backPath(beginWord, endWord, [], res, bp)\nreturn res\n\n# 回溯还原最短路径\ndef backPath(self, start,endWord, r ,res,bp):\n        if start == endWord:\n            res.append(r+[endWord])\n            return\n\n        for st in bp[start]:\n            self.backPath(st, endWord, r+[start], res, bp)\n\n        return\n```\n\n上述注释的代码，是除了回溯之外，另一种BFS中，直接保存结果的方法，当初以为是TLE的主因，最后证明其实不是，所以也是一种输出结果的方法，原理是用字典res，保存所有以word开头的可能的路径列表。\n\n### BFS优化\n\n　　Discuss Python高赞解法大体BFS跟我的一致，但是我前面也分析过，我这个奇怪的if/elif分支其实还是有很大的优化空间，再则自己的状态划分不佳其实也是导致这么奇怪分支的一个原因，所以高赞的解法直接把这二者完美的统一了，窃以为也是一个简洁优雅的解法，值得学习。这里res 跟我上述的res相反，是保存以word结尾的路径列表的字典\n\n```python\nq = defaultdict(list)\nq[beginWord] = [[beginWord]]\nwordList_set = set(wordList)\nres = []\nwhile len(q) > 0:\n  q_next = defaultdict(list)\n  for word in q:\n    if word == endWord:\n      res.extend(k for k in q[word])\n    for i in len(word):\n      for char in string.ascii_lowercase:\n        tmp = word[ :i] + char + word[i+1: ]\n        # 未访问状态的列表中搜索拼接的可能的值，逐渐减少搜索空间，\n        #也是一种很好的优化方法，即逻辑上实现了 visited 状态验证，\n        #同时一层一更新减少了搜索空间，用set实现极大的优化了搜索时间\n        if tmp in wordList_set:\n\t\t\t\t\tq_next[tmp] = [r + [tmp] for r in q[word]]\n          \n\t# wordList_set 实际上是保存了所有未访问状态的列表，\n  #BFS的时候每一层更新一次，避免奇怪的if/elif 分支判断，\n  #是更准确的状态划分方法\n\twordList_set = wordList_set - set(q_next.keys())\n  q = q_next\nreturn res\n```\n\n### Conclusion\n\n　　算法优化阶段的时间复杂度分析做的不是很到位，其实似乎还是懒居多，细想一下，M和N之间的对比，似乎也能找到一点蛛丝马迹；其次，BFS状态划分没有因地制宜，还是想以套路直接鲁莽上阵，其实在发现应该按照层次BFS的时候，此题应该已经解决了，解析之后的后续优化工作还是需要多积累一些套路经验，特别是Python本来就比较慢的基础上，时间复杂度分析的重要性可能会越来越大。\n\n","source":"_posts/LeetCode-126-Word-Ladder-II.md","raw":"---\ntitle: LeetCode 126. Word Ladder II\ntoc: true\nmathjax: true\ntop: false\ncover: true\ndate: 2020-12-15 11:33:20\nupdated:\ncategories: Algorithms\ntags:\n\t- Algorithms\n\t- LeetCode\n---\n\n> Given two words (*beginWord* and *endWord*), and a dictionary's word list, find all shortest transformation sequence(s) from *beginWord* to *endWord*, such that:\n>\n> 1. Only one letter can be changed at a time\n> 2. Each transformed word must exist in the word list. Note that *beginWord* is *not* a transformed word.\n>\n> **Note:**\n>\n> - Return an empty list if there is no such transformation sequence.\n> - All words have the same length.\n> - All words contain only lowercase alphabetic characters.\n> - You may assume no duplicates in the word list.\n> - You may assume *beginWord* and *endWord* are non-empty and are not the same.\n>\n> **Example 1:**\n>\n> ```\n> Input:\n> beginWord = \"hit\",\n> endWord = \"cog\",\n> wordList = [\"hot\",\"dot\",\"dog\",\"lot\",\"log\",\"cog\"]\n> \n> Output:\n> [\n>   [\"hit\",\"hot\",\"dot\",\"dog\",\"cog\"],\n>   [\"hit\",\"hot\",\"lot\",\"log\",\"cog\"]\n> ]\n> ```\n>\n> **Example 2:**\n>\n> ```\n> Input:\n> beginWord = \"hit\"\n> endWord = \"cog\"\n> wordList = [\"hot\",\"dot\",\"dog\",\"lot\",\"log\"]\n> \n> Output: []\n> \n> Explanation: The endWord \"cog\" is not in wordList, therefore no possible transformation\n> ```\n\n　　这是一道似曾相识的Hard题，却意外的卡了好几天，好在最后还是凭借自己完成的算法方案，差一点AC，TLE的程度，最后试图优化无果，搜索Discuss，发现Python高赞方案跟自己的如出一辙，只不过在最费时的状态图构建上，做了很好的优化，借助这个优化思路，也很快获得了AC，学到了很好的优化经验。\n\n　　说到似曾相识，是因为自己曾经做过的Frog Jump 也是一道Hard题，现在回想起来，那可能是自己独立实现AC的第一道Hard，这才有了后来渐渐想要不借助提示挑战Hard的勇气。虽说Frog Jump 是一道Hard，但是解题过程意外的轻松，很容易的就找到了算法方案，基本就是常规的思路，然后实现代码过程也没遇到很多问题，最后一次AC，倍感意外。彼时就有点反思道，是不是自己高估了Hard，所以被这个束缚，才一直觉得自己搞不定才没有去尝试。现在想来，可能只是因为经验不足，用于解题的算法工具不够充足才会有Hard无法突破的心理障碍，比如，Frog Jump , Jump Game 的多种变种以及这题Word Ladder 基本都跟算法（第四版）中的 确定有穷状态自动机DFA 和 不确定有穷状态自动机 NFA的解法一致，无论是最初AC，还是后来的Debug优化，基本都是围绕Pattern 构建的状态图流转即可。此题甚至都无需构建状态图，只需要寻找合适的数据结构来标记状态即可，多少跟图论算法比较相似。\n\n### 算法思路分析\n\n　　本质上，这其实是一道图论题目，在wordList提供的图之中，搜索从beginWord 到 endWord 的最短路径，最短路径可以直接用BFS求解，因为题设只需要输出最短路径的可能值即可，所以找到终点之后，程序即可中止。\n\n　　算法上，还是延续之前的各种Jump的变种思路，难点应该还是在于实现上，毕竟这些变种题会有各种约束条件的变化导致代码上会有些许不同，这个恐怕要在写代码的时候才能意识得到。\n\n### 算法实现\n\n　　跟DFA/NFA类似，先把wordList转化成图表达，搜索所有的有连接关系的单词对，并保存起来。\n\n```python\n# 判断 字符串 s1 s2 是否有边连接\ndef isPair(self, s1, s2):\n\tc = 0\n\tfor i,j in zip(s1, s2):\n\t\tif c > 1:\n\t\t\treturn False\n\t\tif i != j:\n\t\t\tc += 1\n\treturn c == 1\n      \n# word 在wordList 搜索所有可能的边\ndef constructidx(self, word, wordList, des2src):\n\tfor w in wordList:\n\tif w != word and self.isPair(w, word):\n\t\tdes2src[w].add(word)\n\t\tdes2src[word].add(w)\n\treturn\n```\n\n　　自己虽然知道在搜索连接边的isPair写的有点简单粗暴，有很大的优化空间，因为随着wordList的length增大，这个调用次数也是呈指数上升的，所以isPair的一点小优化能极大的降低TLE概率，事实也确实如此。无奈自己面对这样的基础优化，还是有点无能无力，似乎陷入了这个第一想法就得到结果的算法思路限制，无法彻底的想到新思路，最后在Discuss找到了这一段的简洁实现，利用wordList_set的在搜索阶段的改进很明显，除此之外以word为基础重新构建的方法会快一点，毕竟一个单词每一个位置都用26个字母重新替换一下，也需要$O(N)$的时间复杂度，综合起来$O(MN)$；而上述的代码可能需要$O(M^2)$， 其中M代表wordList 长度，N代表word本身的长度，这个角度来看，M较大的可能性还是比较高的。\n\n```python\ndef constructidx_1(self, word, wordList_set, des2src):\n\tfor i in range(len(word)):\n\t\tfor char in string.ascii_lowercase:\n\t\t\ttmp = word[ :i] + char + word[i+1: ]\n\t\t\tif tmp in wordList_set:\n\t\t\t\tdes2src[word].add(tmp)\n\t\t\t\t# des2src[tmp].add(word)\n\treturn\n```\n\n　　也是这一段的优化最终将TLE的代码转化为AC的代码\n\n　　接下来就是BFS搜索的过程了，自己在写这段代码的时候，不知道是不是脑子抽风了，又犯了还没想好整体构思，就提枪上阵的毛病，本来是一段很基础的BFS搜索的过程，结果由于访问状态标记的代码没想明白，就随意的放置访问状态的代码，结果调试了1天，才发现问题所在，期间出现了各种匪夷所思的访问状态变化的问题，搞得自己经常莫名其妙，各种怀疑是不是其他地方出了问题。实在是有点不应该。\n\n　　可能是以前的BFS都需要遍历所有的节点之后，自动返回，所以直接采用queue的方式，queue为空自动返回即可，不需要标记BFS目前在第几层，然而此题是需要求最短路径，显然是需要采取层次遍历的方式依此进行，所以用q, q_next依此交替的方式，q是目前的遍历层次，q_next是接下来需要遍历的层次；其次，状态标记visited 的问题，此题由于要输出所有可能的最短路径，所以不仅进入q_next的新节点需要标记回溯路径，有可能q_next中的某个节点a，在此层被2个不同的节点b和c连接，此时需要把这2条路径需要标记回溯路径，之前这种可能性被自己忽略了，所以每当有多条的路径的时候，自己的代码只输出了一条，最后发现是遍历b和c的时候，b-a 使a 进入q_next, 并且visited[a] = True,标记b-a的回溯路径， 而当 c-a 需要标记回溯路径的时候，却由于visited[a]==True 被跳过了，所以才有那段不伦不类的if elif 的分支判断\n\n```python\nq = deque()\nq.append(endWord)\nvisited = {endWord:True}\nbp = defaultdict(set)\n#res = defaultdict(list)\n#res[endWord] = [ [endWord] ]\nwhile len(q) > 0 :\n\tq_next = deque()\n\tfor e in q:\n\t\t# self.constructidx(e, wordList_set, des2src)\n\t\tself.constructidx_1(e, wordList_set, des2src)\n\t\tfor src in des2src[e]:\n\t\t\tif not visited.get(src, False)  :\n\t\t\t\tvisited[src] = True\n\t\t\t\tq_next.append(src)\n\n#\t\t\t\tr = [ [src] ]if len(res[e]) == 0 else [[src] + re for re in res[e]]\n#\t\t\t\tres[src].extend(r)\n\t\t\t\tbp[src].add(e)\n\t\t\telif src in q_next:\n#\t\t\t\tr = [ [src] ]if len(res[e]) == 0 else [[src] + re for re in res[e]]\n#\t\t\t\tres[src].extend(r)\n\t\t\t\tbp[src].add(e)\n\tif beginWord in q_next:\n\t\tbreak\n\twordList_set = wordList_set - set(bp.keys())\n\tq = q_next\n  \n# return res[beginWord]\nres = []\nself.backPath(beginWord, endWord, [], res, bp)\nreturn res\n\n# 回溯还原最短路径\ndef backPath(self, start,endWord, r ,res,bp):\n        if start == endWord:\n            res.append(r+[endWord])\n            return\n\n        for st in bp[start]:\n            self.backPath(st, endWord, r+[start], res, bp)\n\n        return\n```\n\n上述注释的代码，是除了回溯之外，另一种BFS中，直接保存结果的方法，当初以为是TLE的主因，最后证明其实不是，所以也是一种输出结果的方法，原理是用字典res，保存所有以word开头的可能的路径列表。\n\n### BFS优化\n\n　　Discuss Python高赞解法大体BFS跟我的一致，但是我前面也分析过，我这个奇怪的if/elif分支其实还是有很大的优化空间，再则自己的状态划分不佳其实也是导致这么奇怪分支的一个原因，所以高赞的解法直接把这二者完美的统一了，窃以为也是一个简洁优雅的解法，值得学习。这里res 跟我上述的res相反，是保存以word结尾的路径列表的字典\n\n```python\nq = defaultdict(list)\nq[beginWord] = [[beginWord]]\nwordList_set = set(wordList)\nres = []\nwhile len(q) > 0:\n  q_next = defaultdict(list)\n  for word in q:\n    if word == endWord:\n      res.extend(k for k in q[word])\n    for i in len(word):\n      for char in string.ascii_lowercase:\n        tmp = word[ :i] + char + word[i+1: ]\n        # 未访问状态的列表中搜索拼接的可能的值，逐渐减少搜索空间，\n        #也是一种很好的优化方法，即逻辑上实现了 visited 状态验证，\n        #同时一层一更新减少了搜索空间，用set实现极大的优化了搜索时间\n        if tmp in wordList_set:\n\t\t\t\t\tq_next[tmp] = [r + [tmp] for r in q[word]]\n          \n\t# wordList_set 实际上是保存了所有未访问状态的列表，\n  #BFS的时候每一层更新一次，避免奇怪的if/elif 分支判断，\n  #是更准确的状态划分方法\n\twordList_set = wordList_set - set(q_next.keys())\n  q = q_next\nreturn res\n```\n\n### Conclusion\n\n　　算法优化阶段的时间复杂度分析做的不是很到位，其实似乎还是懒居多，细想一下，M和N之间的对比，似乎也能找到一点蛛丝马迹；其次，BFS状态划分没有因地制宜，还是想以套路直接鲁莽上阵，其实在发现应该按照层次BFS的时候，此题应该已经解决了，解析之后的后续优化工作还是需要多积累一些套路经验，特别是Python本来就比较慢的基础上，时间复杂度分析的重要性可能会越来越大。\n\n","slug":"LeetCode-126-Word-Ladder-II","published":1,"_id":"ckipop2xt0000ej28e6qn6mng","comments":1,"layout":"post","photos":[],"link":"","content":"<blockquote>\n<p>Given two words (<em>beginWord</em> and <em>endWord</em>), and a dictionary’s word list, find all shortest transformation sequence(s) from <em>beginWord</em> to <em>endWord</em>, such that:</p>\n<ol>\n<li>Only one letter can be changed at a time</li>\n<li>Each transformed word must exist in the word list. Note that <em>beginWord</em> is <em>not</em> a transformed word.</li>\n</ol>\n<p><strong>Note:</strong></p>\n<ul>\n<li>Return an empty list if there is no such transformation sequence.</li>\n<li>All words have the same length.</li>\n<li>All words contain only lowercase alphabetic characters.</li>\n<li>You may assume no duplicates in the word list.</li>\n<li>You may assume <em>beginWord</em> and <em>endWord</em> are non-empty and are not the same.</li>\n</ul>\n<p><strong>Example 1:</strong></p>\n<pre><code>Input:\nbeginWord = &quot;hit&quot;,\nendWord = &quot;cog&quot;,\nwordList = [&quot;hot&quot;,&quot;dot&quot;,&quot;dog&quot;,&quot;lot&quot;,&quot;log&quot;,&quot;cog&quot;]\n\nOutput:\n[\n  [&quot;hit&quot;,&quot;hot&quot;,&quot;dot&quot;,&quot;dog&quot;,&quot;cog&quot;],\n  [&quot;hit&quot;,&quot;hot&quot;,&quot;lot&quot;,&quot;log&quot;,&quot;cog&quot;]\n]</code></pre>\n<p><strong>Example 2:</strong></p>\n<pre><code>Input:\nbeginWord = &quot;hit&quot;\nendWord = &quot;cog&quot;\nwordList = [&quot;hot&quot;,&quot;dot&quot;,&quot;dog&quot;,&quot;lot&quot;,&quot;log&quot;]\n\nOutput: []\n\nExplanation: The endWord &quot;cog&quot; is not in wordList, therefore no possible transformation</code></pre>\n</blockquote>\n<p>　　这是一道似曾相识的Hard题，却意外的卡了好几天，好在最后还是凭借自己完成的算法方案，差一点AC，TLE的程度，最后试图优化无果，搜索Discuss，发现Python高赞方案跟自己的如出一辙，只不过在最费时的状态图构建上，做了很好的优化，借助这个优化思路，也很快获得了AC，学到了很好的优化经验。</p>\n<p>　　说到似曾相识，是因为自己曾经做过的Frog Jump 也是一道Hard题，现在回想起来，那可能是自己独立实现AC的第一道Hard，这才有了后来渐渐想要不借助提示挑战Hard的勇气。虽说Frog Jump 是一道Hard，但是解题过程意外的轻松，很容易的就找到了算法方案，基本就是常规的思路，然后实现代码过程也没遇到很多问题，最后一次AC，倍感意外。彼时就有点反思道，是不是自己高估了Hard，所以被这个束缚，才一直觉得自己搞不定才没有去尝试。现在想来，可能只是因为经验不足，用于解题的算法工具不够充足才会有Hard无法突破的心理障碍，比如，Frog Jump , Jump Game 的多种变种以及这题Word Ladder 基本都跟算法（第四版）中的 确定有穷状态自动机DFA 和 不确定有穷状态自动机 NFA的解法一致，无论是最初AC，还是后来的Debug优化，基本都是围绕Pattern 构建的状态图流转即可。此题甚至都无需构建状态图，只需要寻找合适的数据结构来标记状态即可，多少跟图论算法比较相似。</p>\n<h3 id=\"算法思路分析\"><a href=\"#算法思路分析\" class=\"headerlink\" title=\"算法思路分析\"></a>算法思路分析</h3><p>　　本质上，这其实是一道图论题目，在wordList提供的图之中，搜索从beginWord 到 endWord 的最短路径，最短路径可以直接用BFS求解，因为题设只需要输出最短路径的可能值即可，所以找到终点之后，程序即可中止。</p>\n<p>　　算法上，还是延续之前的各种Jump的变种思路，难点应该还是在于实现上，毕竟这些变种题会有各种约束条件的变化导致代码上会有些许不同，这个恐怕要在写代码的时候才能意识得到。</p>\n<h3 id=\"算法实现\"><a href=\"#算法实现\" class=\"headerlink\" title=\"算法实现\"></a>算法实现</h3><p>　　跟DFA/NFA类似，先把wordList转化成图表达，搜索所有的有连接关系的单词对，并保存起来。</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token comment\" spellcheck=\"true\"># 判断 字符串 s1 s2 是否有边连接</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">isPair</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> s1<span class=\"token punctuation\">,</span> s2<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    c <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n    <span class=\"token keyword\">for</span> i<span class=\"token punctuation\">,</span>j <span class=\"token keyword\">in</span> zip<span class=\"token punctuation\">(</span>s1<span class=\"token punctuation\">,</span> s2<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">if</span> c <span class=\"token operator\">></span> <span class=\"token number\">1</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">return</span> <span class=\"token boolean\">False</span>\n        <span class=\"token keyword\">if</span> i <span class=\"token operator\">!=</span> j<span class=\"token punctuation\">:</span>\n            c <span class=\"token operator\">+=</span> <span class=\"token number\">1</span>\n    <span class=\"token keyword\">return</span> c <span class=\"token operator\">==</span> <span class=\"token number\">1</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># word 在wordList 搜索所有可能的边</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">constructidx</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> word<span class=\"token punctuation\">,</span> wordList<span class=\"token punctuation\">,</span> des2src<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">for</span> w <span class=\"token keyword\">in</span> wordList<span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">if</span> w <span class=\"token operator\">!=</span> word <span class=\"token operator\">and</span> self<span class=\"token punctuation\">.</span>isPair<span class=\"token punctuation\">(</span>w<span class=\"token punctuation\">,</span> word<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        des2src<span class=\"token punctuation\">[</span>w<span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>word<span class=\"token punctuation\">)</span>\n        des2src<span class=\"token punctuation\">[</span>word<span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>w<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span></code></pre>\n<p>　　自己虽然知道在搜索连接边的isPair写的有点简单粗暴，有很大的优化空间，因为随着wordList的length增大，这个调用次数也是呈指数上升的，所以isPair的一点小优化能极大的降低TLE概率，事实也确实如此。无奈自己面对这样的基础优化，还是有点无能无力，似乎陷入了这个第一想法就得到结果的算法思路限制，无法彻底的想到新思路，最后在Discuss找到了这一段的简洁实现，利用wordList_set的在搜索阶段的改进很明显，除此之外以word为基础重新构建的方法会快一点，毕竟一个单词每一个位置都用26个字母重新替换一下，也需要$O(N)$的时间复杂度，综合起来$O(MN)$；而上述的代码可能需要$O(M^2)$， 其中M代表wordList 长度，N代表word本身的长度，这个角度来看，M较大的可能性还是比较高的。</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">constructidx_1</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> word<span class=\"token punctuation\">,</span> wordList_set<span class=\"token punctuation\">,</span> des2src<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span>len<span class=\"token punctuation\">(</span>word<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">for</span> char <span class=\"token keyword\">in</span> string<span class=\"token punctuation\">.</span>ascii_lowercase<span class=\"token punctuation\">:</span>\n            tmp <span class=\"token operator\">=</span> word<span class=\"token punctuation\">[</span> <span class=\"token punctuation\">:</span>i<span class=\"token punctuation\">]</span> <span class=\"token operator\">+</span> char <span class=\"token operator\">+</span> word<span class=\"token punctuation\">[</span>i<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">]</span>\n            <span class=\"token keyword\">if</span> tmp <span class=\"token keyword\">in</span> wordList_set<span class=\"token punctuation\">:</span>\n                des2src<span class=\"token punctuation\">[</span>word<span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>tmp<span class=\"token punctuation\">)</span>\n                <span class=\"token comment\" spellcheck=\"true\"># des2src[tmp].add(word)</span>\n    <span class=\"token keyword\">return</span></code></pre>\n<p>　　也是这一段的优化最终将TLE的代码转化为AC的代码</p>\n<p>　　接下来就是BFS搜索的过程了，自己在写这段代码的时候，不知道是不是脑子抽风了，又犯了还没想好整体构思，就提枪上阵的毛病，本来是一段很基础的BFS搜索的过程，结果由于访问状态标记的代码没想明白，就随意的放置访问状态的代码，结果调试了1天，才发现问题所在，期间出现了各种匪夷所思的访问状态变化的问题，搞得自己经常莫名其妙，各种怀疑是不是其他地方出了问题。实在是有点不应该。</p>\n<p>　　可能是以前的BFS都需要遍历所有的节点之后，自动返回，所以直接采用queue的方式，queue为空自动返回即可，不需要标记BFS目前在第几层，然而此题是需要求最短路径，显然是需要采取层次遍历的方式依此进行，所以用q, q_next依此交替的方式，q是目前的遍历层次，q_next是接下来需要遍历的层次；其次，状态标记visited 的问题，此题由于要输出所有可能的最短路径，所以不仅进入q_next的新节点需要标记回溯路径，有可能q_next中的某个节点a，在此层被2个不同的节点b和c连接，此时需要把这2条路径需要标记回溯路径，之前这种可能性被自己忽略了，所以每当有多条的路径的时候，自己的代码只输出了一条，最后发现是遍历b和c的时候，b-a 使a 进入q_next, 并且visited[a] = True,标记b-a的回溯路径， 而当 c-a 需要标记回溯路径的时候，却由于visited[a]==True 被跳过了，所以才有那段不伦不类的if elif 的分支判断</p>\n<pre class=\" language-python\"><code class=\"language-python\">q <span class=\"token operator\">=</span> deque<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nq<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>endWord<span class=\"token punctuation\">)</span>\nvisited <span class=\"token operator\">=</span> <span class=\"token operator\">&amp;</span><span class=\"token comment\" spellcheck=\"true\">#123;endWord:True&amp;#125;</span>\nbp <span class=\"token operator\">=</span> defaultdict<span class=\"token punctuation\">(</span>set<span class=\"token punctuation\">)</span>\n<span class=\"token comment\" spellcheck=\"true\">#res = defaultdict(list)</span>\n<span class=\"token comment\" spellcheck=\"true\">#res[endWord] = [ [endWord] ]</span>\n<span class=\"token keyword\">while</span> len<span class=\"token punctuation\">(</span>q<span class=\"token punctuation\">)</span> <span class=\"token operator\">></span> <span class=\"token number\">0</span> <span class=\"token punctuation\">:</span>\n    q_next <span class=\"token operator\">=</span> deque<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">for</span> e <span class=\"token keyword\">in</span> q<span class=\"token punctuation\">:</span>\n        <span class=\"token comment\" spellcheck=\"true\"># self.constructidx(e, wordList_set, des2src)</span>\n        self<span class=\"token punctuation\">.</span>constructidx_1<span class=\"token punctuation\">(</span>e<span class=\"token punctuation\">,</span> wordList_set<span class=\"token punctuation\">,</span> des2src<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">for</span> src <span class=\"token keyword\">in</span> des2src<span class=\"token punctuation\">[</span>e<span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">if</span> <span class=\"token operator\">not</span> visited<span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span>src<span class=\"token punctuation\">,</span> <span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span>  <span class=\"token punctuation\">:</span>\n                visited<span class=\"token punctuation\">[</span>src<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token boolean\">True</span>\n                q_next<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>src<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\">#                r = [ [src] ]if len(res[e]) == 0 else [[src] + re for re in res[e]]</span>\n<span class=\"token comment\" spellcheck=\"true\">#                res[src].extend(r)</span>\n                bp<span class=\"token punctuation\">[</span>src<span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>e<span class=\"token punctuation\">)</span>\n            <span class=\"token keyword\">elif</span> src <span class=\"token keyword\">in</span> q_next<span class=\"token punctuation\">:</span>\n<span class=\"token comment\" spellcheck=\"true\">#                r = [ [src] ]if len(res[e]) == 0 else [[src] + re for re in res[e]]</span>\n<span class=\"token comment\" spellcheck=\"true\">#                res[src].extend(r)</span>\n                bp<span class=\"token punctuation\">[</span>src<span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>e<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">if</span> beginWord <span class=\"token keyword\">in</span> q_next<span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">break</span>\n    wordList_set <span class=\"token operator\">=</span> wordList_set <span class=\"token operator\">-</span> set<span class=\"token punctuation\">(</span>bp<span class=\"token punctuation\">.</span>keys<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    q <span class=\"token operator\">=</span> q_next\n\n<span class=\"token comment\" spellcheck=\"true\"># return res[beginWord]</span>\nres <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\nself<span class=\"token punctuation\">.</span>backPath<span class=\"token punctuation\">(</span>beginWord<span class=\"token punctuation\">,</span> endWord<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> res<span class=\"token punctuation\">,</span> bp<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">return</span> res\n\n<span class=\"token comment\" spellcheck=\"true\"># 回溯还原最短路径</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">backPath</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> start<span class=\"token punctuation\">,</span>endWord<span class=\"token punctuation\">,</span> r <span class=\"token punctuation\">,</span>res<span class=\"token punctuation\">,</span>bp<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">if</span> start <span class=\"token operator\">==</span> endWord<span class=\"token punctuation\">:</span>\n            res<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>r<span class=\"token operator\">+</span><span class=\"token punctuation\">[</span>endWord<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n            <span class=\"token keyword\">return</span>\n\n        <span class=\"token keyword\">for</span> st <span class=\"token keyword\">in</span> bp<span class=\"token punctuation\">[</span>start<span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n            self<span class=\"token punctuation\">.</span>backPath<span class=\"token punctuation\">(</span>st<span class=\"token punctuation\">,</span> endWord<span class=\"token punctuation\">,</span> r<span class=\"token operator\">+</span><span class=\"token punctuation\">[</span>start<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> res<span class=\"token punctuation\">,</span> bp<span class=\"token punctuation\">)</span>\n\n        <span class=\"token keyword\">return</span></code></pre>\n<p>上述注释的代码，是除了回溯之外，另一种BFS中，直接保存结果的方法，当初以为是TLE的主因，最后证明其实不是，所以也是一种输出结果的方法，原理是用字典res，保存所有以word开头的可能的路径列表。</p>\n<h3 id=\"BFS优化\"><a href=\"#BFS优化\" class=\"headerlink\" title=\"BFS优化\"></a>BFS优化</h3><p>　　Discuss Python高赞解法大体BFS跟我的一致，但是我前面也分析过，我这个奇怪的if/elif分支其实还是有很大的优化空间，再则自己的状态划分不佳其实也是导致这么奇怪分支的一个原因，所以高赞的解法直接把这二者完美的统一了，窃以为也是一个简洁优雅的解法，值得学习。这里res 跟我上述的res相反，是保存以word结尾的路径列表的字典</p>\n<pre class=\" language-python\"><code class=\"language-python\">q <span class=\"token operator\">=</span> defaultdict<span class=\"token punctuation\">(</span>list<span class=\"token punctuation\">)</span>\nq<span class=\"token punctuation\">[</span>beginWord<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span>beginWord<span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span>\nwordList_set <span class=\"token operator\">=</span> set<span class=\"token punctuation\">(</span>wordList<span class=\"token punctuation\">)</span>\nres <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n<span class=\"token keyword\">while</span> len<span class=\"token punctuation\">(</span>q<span class=\"token punctuation\">)</span> <span class=\"token operator\">></span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n  q_next <span class=\"token operator\">=</span> defaultdict<span class=\"token punctuation\">(</span>list<span class=\"token punctuation\">)</span>\n  <span class=\"token keyword\">for</span> word <span class=\"token keyword\">in</span> q<span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">if</span> word <span class=\"token operator\">==</span> endWord<span class=\"token punctuation\">:</span>\n      res<span class=\"token punctuation\">.</span>extend<span class=\"token punctuation\">(</span>k <span class=\"token keyword\">for</span> k <span class=\"token keyword\">in</span> q<span class=\"token punctuation\">[</span>word<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> len<span class=\"token punctuation\">(</span>word<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n      <span class=\"token keyword\">for</span> char <span class=\"token keyword\">in</span> string<span class=\"token punctuation\">.</span>ascii_lowercase<span class=\"token punctuation\">:</span>\n        tmp <span class=\"token operator\">=</span> word<span class=\"token punctuation\">[</span> <span class=\"token punctuation\">:</span>i<span class=\"token punctuation\">]</span> <span class=\"token operator\">+</span> char <span class=\"token operator\">+</span> word<span class=\"token punctuation\">[</span>i<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">]</span>\n        <span class=\"token comment\" spellcheck=\"true\"># 未访问状态的列表中搜索拼接的可能的值，逐渐减少搜索空间，</span>\n        <span class=\"token comment\" spellcheck=\"true\">#也是一种很好的优化方法，即逻辑上实现了 visited 状态验证，</span>\n        <span class=\"token comment\" spellcheck=\"true\">#同时一层一更新减少了搜索空间，用set实现极大的优化了搜索时间</span>\n        <span class=\"token keyword\">if</span> tmp <span class=\"token keyword\">in</span> wordList_set<span class=\"token punctuation\">:</span>\n                    q_next<span class=\"token punctuation\">[</span>tmp<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>r <span class=\"token operator\">+</span> <span class=\"token punctuation\">[</span>tmp<span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> r <span class=\"token keyword\">in</span> q<span class=\"token punctuation\">[</span>word<span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># wordList_set 实际上是保存了所有未访问状态的列表，</span>\n  <span class=\"token comment\" spellcheck=\"true\">#BFS的时候每一层更新一次，避免奇怪的if/elif 分支判断，</span>\n  <span class=\"token comment\" spellcheck=\"true\">#是更准确的状态划分方法</span>\n    wordList_set <span class=\"token operator\">=</span> wordList_set <span class=\"token operator\">-</span> set<span class=\"token punctuation\">(</span>q_next<span class=\"token punctuation\">.</span>keys<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n  q <span class=\"token operator\">=</span> q_next\n<span class=\"token keyword\">return</span> res</code></pre>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>　　算法优化阶段的时间复杂度分析做的不是很到位，其实似乎还是懒居多，细想一下，M和N之间的对比，似乎也能找到一点蛛丝马迹；其次，BFS状态划分没有因地制宜，还是想以套路直接鲁莽上阵，其实在发现应该按照层次BFS的时候，此题应该已经解决了，解析之后的后续优化工作还是需要多积累一些套路经验，特别是Python本来就比较慢的基础上，时间复杂度分析的重要性可能会越来越大。</p>\n","site":{"data":{}},"excerpt":"","more":"<blockquote>\n<p>Given two words (<em>beginWord</em> and <em>endWord</em>), and a dictionary’s word list, find all shortest transformation sequence(s) from <em>beginWord</em> to <em>endWord</em>, such that:</p>\n<ol>\n<li>Only one letter can be changed at a time</li>\n<li>Each transformed word must exist in the word list. Note that <em>beginWord</em> is <em>not</em> a transformed word.</li>\n</ol>\n<p><strong>Note:</strong></p>\n<ul>\n<li>Return an empty list if there is no such transformation sequence.</li>\n<li>All words have the same length.</li>\n<li>All words contain only lowercase alphabetic characters.</li>\n<li>You may assume no duplicates in the word list.</li>\n<li>You may assume <em>beginWord</em> and <em>endWord</em> are non-empty and are not the same.</li>\n</ul>\n<p><strong>Example 1:</strong></p>\n<pre><code>Input:\nbeginWord = &quot;hit&quot;,\nendWord = &quot;cog&quot;,\nwordList = [&quot;hot&quot;,&quot;dot&quot;,&quot;dog&quot;,&quot;lot&quot;,&quot;log&quot;,&quot;cog&quot;]\n\nOutput:\n[\n  [&quot;hit&quot;,&quot;hot&quot;,&quot;dot&quot;,&quot;dog&quot;,&quot;cog&quot;],\n  [&quot;hit&quot;,&quot;hot&quot;,&quot;lot&quot;,&quot;log&quot;,&quot;cog&quot;]\n]</code></pre>\n<p><strong>Example 2:</strong></p>\n<pre><code>Input:\nbeginWord = &quot;hit&quot;\nendWord = &quot;cog&quot;\nwordList = [&quot;hot&quot;,&quot;dot&quot;,&quot;dog&quot;,&quot;lot&quot;,&quot;log&quot;]\n\nOutput: []\n\nExplanation: The endWord &quot;cog&quot; is not in wordList, therefore no possible transformation</code></pre>\n</blockquote>\n<p>　　这是一道似曾相识的Hard题，却意外的卡了好几天，好在最后还是凭借自己完成的算法方案，差一点AC，TLE的程度，最后试图优化无果，搜索Discuss，发现Python高赞方案跟自己的如出一辙，只不过在最费时的状态图构建上，做了很好的优化，借助这个优化思路，也很快获得了AC，学到了很好的优化经验。</p>\n<p>　　说到似曾相识，是因为自己曾经做过的Frog Jump 也是一道Hard题，现在回想起来，那可能是自己独立实现AC的第一道Hard，这才有了后来渐渐想要不借助提示挑战Hard的勇气。虽说Frog Jump 是一道Hard，但是解题过程意外的轻松，很容易的就找到了算法方案，基本就是常规的思路，然后实现代码过程也没遇到很多问题，最后一次AC，倍感意外。彼时就有点反思道，是不是自己高估了Hard，所以被这个束缚，才一直觉得自己搞不定才没有去尝试。现在想来，可能只是因为经验不足，用于解题的算法工具不够充足才会有Hard无法突破的心理障碍，比如，Frog Jump , Jump Game 的多种变种以及这题Word Ladder 基本都跟算法（第四版）中的 确定有穷状态自动机DFA 和 不确定有穷状态自动机 NFA的解法一致，无论是最初AC，还是后来的Debug优化，基本都是围绕Pattern 构建的状态图流转即可。此题甚至都无需构建状态图，只需要寻找合适的数据结构来标记状态即可，多少跟图论算法比较相似。</p>\n<h3 id=\"算法思路分析\"><a href=\"#算法思路分析\" class=\"headerlink\" title=\"算法思路分析\"></a>算法思路分析</h3><p>　　本质上，这其实是一道图论题目，在wordList提供的图之中，搜索从beginWord 到 endWord 的最短路径，最短路径可以直接用BFS求解，因为题设只需要输出最短路径的可能值即可，所以找到终点之后，程序即可中止。</p>\n<p>　　算法上，还是延续之前的各种Jump的变种思路，难点应该还是在于实现上，毕竟这些变种题会有各种约束条件的变化导致代码上会有些许不同，这个恐怕要在写代码的时候才能意识得到。</p>\n<h3 id=\"算法实现\"><a href=\"#算法实现\" class=\"headerlink\" title=\"算法实现\"></a>算法实现</h3><p>　　跟DFA/NFA类似，先把wordList转化成图表达，搜索所有的有连接关系的单词对，并保存起来。</p>\n<pre><code class=\"python\"># 判断 字符串 s1 s2 是否有边连接\ndef isPair(self, s1, s2):\n    c = 0\n    for i,j in zip(s1, s2):\n        if c &gt; 1:\n            return False\n        if i != j:\n            c += 1\n    return c == 1\n\n# word 在wordList 搜索所有可能的边\ndef constructidx(self, word, wordList, des2src):\n    for w in wordList:\n    if w != word and self.isPair(w, word):\n        des2src[w].add(word)\n        des2src[word].add(w)\n    return</code></pre>\n<p>　　自己虽然知道在搜索连接边的isPair写的有点简单粗暴，有很大的优化空间，因为随着wordList的length增大，这个调用次数也是呈指数上升的，所以isPair的一点小优化能极大的降低TLE概率，事实也确实如此。无奈自己面对这样的基础优化，还是有点无能无力，似乎陷入了这个第一想法就得到结果的算法思路限制，无法彻底的想到新思路，最后在Discuss找到了这一段的简洁实现，利用wordList_set的在搜索阶段的改进很明显，除此之外以word为基础重新构建的方法会快一点，毕竟一个单词每一个位置都用26个字母重新替换一下，也需要$O(N)$的时间复杂度，综合起来$O(MN)$；而上述的代码可能需要$O(M^2)$， 其中M代表wordList 长度，N代表word本身的长度，这个角度来看，M较大的可能性还是比较高的。</p>\n<pre><code class=\"python\">def constructidx_1(self, word, wordList_set, des2src):\n    for i in range(len(word)):\n        for char in string.ascii_lowercase:\n            tmp = word[ :i] + char + word[i+1: ]\n            if tmp in wordList_set:\n                des2src[word].add(tmp)\n                # des2src[tmp].add(word)\n    return</code></pre>\n<p>　　也是这一段的优化最终将TLE的代码转化为AC的代码</p>\n<p>　　接下来就是BFS搜索的过程了，自己在写这段代码的时候，不知道是不是脑子抽风了，又犯了还没想好整体构思，就提枪上阵的毛病，本来是一段很基础的BFS搜索的过程，结果由于访问状态标记的代码没想明白，就随意的放置访问状态的代码，结果调试了1天，才发现问题所在，期间出现了各种匪夷所思的访问状态变化的问题，搞得自己经常莫名其妙，各种怀疑是不是其他地方出了问题。实在是有点不应该。</p>\n<p>　　可能是以前的BFS都需要遍历所有的节点之后，自动返回，所以直接采用queue的方式，queue为空自动返回即可，不需要标记BFS目前在第几层，然而此题是需要求最短路径，显然是需要采取层次遍历的方式依此进行，所以用q, q_next依此交替的方式，q是目前的遍历层次，q_next是接下来需要遍历的层次；其次，状态标记visited 的问题，此题由于要输出所有可能的最短路径，所以不仅进入q_next的新节点需要标记回溯路径，有可能q_next中的某个节点a，在此层被2个不同的节点b和c连接，此时需要把这2条路径需要标记回溯路径，之前这种可能性被自己忽略了，所以每当有多条的路径的时候，自己的代码只输出了一条，最后发现是遍历b和c的时候，b-a 使a 进入q_next, 并且visited[a] = True,标记b-a的回溯路径， 而当 c-a 需要标记回溯路径的时候，却由于visited[a]==True 被跳过了，所以才有那段不伦不类的if elif 的分支判断</p>\n<pre><code class=\"python\">q = deque()\nq.append(endWord)\nvisited = &#123;endWord:True&#125;\nbp = defaultdict(set)\n#res = defaultdict(list)\n#res[endWord] = [ [endWord] ]\nwhile len(q) &gt; 0 :\n    q_next = deque()\n    for e in q:\n        # self.constructidx(e, wordList_set, des2src)\n        self.constructidx_1(e, wordList_set, des2src)\n        for src in des2src[e]:\n            if not visited.get(src, False)  :\n                visited[src] = True\n                q_next.append(src)\n\n#                r = [ [src] ]if len(res[e]) == 0 else [[src] + re for re in res[e]]\n#                res[src].extend(r)\n                bp[src].add(e)\n            elif src in q_next:\n#                r = [ [src] ]if len(res[e]) == 0 else [[src] + re for re in res[e]]\n#                res[src].extend(r)\n                bp[src].add(e)\n    if beginWord in q_next:\n        break\n    wordList_set = wordList_set - set(bp.keys())\n    q = q_next\n\n# return res[beginWord]\nres = []\nself.backPath(beginWord, endWord, [], res, bp)\nreturn res\n\n# 回溯还原最短路径\ndef backPath(self, start,endWord, r ,res,bp):\n        if start == endWord:\n            res.append(r+[endWord])\n            return\n\n        for st in bp[start]:\n            self.backPath(st, endWord, r+[start], res, bp)\n\n        return</code></pre>\n<p>上述注释的代码，是除了回溯之外，另一种BFS中，直接保存结果的方法，当初以为是TLE的主因，最后证明其实不是，所以也是一种输出结果的方法，原理是用字典res，保存所有以word开头的可能的路径列表。</p>\n<h3 id=\"BFS优化\"><a href=\"#BFS优化\" class=\"headerlink\" title=\"BFS优化\"></a>BFS优化</h3><p>　　Discuss Python高赞解法大体BFS跟我的一致，但是我前面也分析过，我这个奇怪的if/elif分支其实还是有很大的优化空间，再则自己的状态划分不佳其实也是导致这么奇怪分支的一个原因，所以高赞的解法直接把这二者完美的统一了，窃以为也是一个简洁优雅的解法，值得学习。这里res 跟我上述的res相反，是保存以word结尾的路径列表的字典</p>\n<pre><code class=\"python\">q = defaultdict(list)\nq[beginWord] = [[beginWord]]\nwordList_set = set(wordList)\nres = []\nwhile len(q) &gt; 0:\n  q_next = defaultdict(list)\n  for word in q:\n    if word == endWord:\n      res.extend(k for k in q[word])\n    for i in len(word):\n      for char in string.ascii_lowercase:\n        tmp = word[ :i] + char + word[i+1: ]\n        # 未访问状态的列表中搜索拼接的可能的值，逐渐减少搜索空间，\n        #也是一种很好的优化方法，即逻辑上实现了 visited 状态验证，\n        #同时一层一更新减少了搜索空间，用set实现极大的优化了搜索时间\n        if tmp in wordList_set:\n                    q_next[tmp] = [r + [tmp] for r in q[word]]\n\n    # wordList_set 实际上是保存了所有未访问状态的列表，\n  #BFS的时候每一层更新一次，避免奇怪的if/elif 分支判断，\n  #是更准确的状态划分方法\n    wordList_set = wordList_set - set(q_next.keys())\n  q = q_next\nreturn res</code></pre>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>　　算法优化阶段的时间复杂度分析做的不是很到位，其实似乎还是懒居多，细想一下，M和N之间的对比，似乎也能找到一点蛛丝马迹；其次，BFS状态划分没有因地制宜，还是想以套路直接鲁莽上阵，其实在发现应该按照层次BFS的时候，此题应该已经解决了，解析之后的后续优化工作还是需要多积累一些套路经验，特别是Python本来就比较慢的基础上，时间复杂度分析的重要性可能会越来越大。</p>\n"},{"title":"132. Palindrome Partitioning II","toc":true,"mathjax":true,"top":false,"cover":true,"date":"2020-12-17T03:06:04.000Z","updated":"2020-12-17T09:33:00.529Z","_content":"\n> Given a string `s`, partition `s` such that every substring of the partition is a palindrome.\n>\n> Return *the minimum cuts needed* for a palindrome partitioning of `s`.\n>\n>  \n>\n> **Example 1:**\n>\n> ```\n> Input: s = \"aab\"\n> Output: 1\n> Explanation: The palindrome partitioning [\"aa\",\"b\"] could be produced using 1 cut.\n> ```\n>\n> **Example 2:**\n>\n> ```\n> Input: s = \"a\"\n> Output: 0\n> ```\n>\n> **Example 3:**\n>\n> ```\n> Input: s = \"ab\"\n> Output: 1\n> ```\n>\n>  \n>\n> **Constraints:**\n>\n> - `1 <= s.length <= 2000`\n> - `s` consists of lower-case English letters only.\n\n　　初识回文字符串的时候，觉得这是个很棘手的问题，毕竟涉及到正序逆序的检索问题，光是字符串匹配和正则表达式都能整出DFA和NFA这种复杂的状态机，何况是回文字符这种感觉状态机似乎都没什么办法的问题。没想到的是，回文字符的识别手段如此简单粗暴，就是寻找中心点，然后从中点向外的边扩展边匹配，原理上是没问题的，但是一般在讲解玩暴力解法之后，都会给出更加优雅精妙的优化版算法，但是在回文字符串的问题上，似乎到这里就戛然而止了，可能跟自己接触回文字符串的方式有关，因为自己并不是在教材上看到这个问题的，而是直接在OJ上面临这个问题的，最初理所当然的用暴力解法先过一遍，没想到居然AC了，后面就没有对这个问题进行过更多的思考了，直到遇到这个加强版的回文字符问题，检测substring是否是回文字符，并且暴力解法完全无法AC，这才逼迫自己想办法优化暴力解法。\n\n### 问题解析\n\n　　substring 回文字符检测，在132题之前，其实有一个131 Palindrome Partitioning I 的 medium版本，要求输出所有可能的s 的 substring 切分，并且以列表的形式返回所有可能的切分，原理上此问题是131问题的子集，因为既然都已经输出了所有可能的切分，那么搜索最短切分只需要列出所有切分的长度，取最小即可；然而在OJ的设置上，这样是没办法在132上AC的，因为我在131用的仍然是暴力的解法小改版，只不过在切分的时候发现可能会面临重复递归调用的问题，也就是动态规划DP常常解释的，重复子结构的问题，我采取的备忘录的方式来缓解自顶向下调用时的重复调用问题，在完全没有构思出DP递推式 的前提下，就这样压线AC了，搞得我自己都不知道，这个AC的代码究竟用的是递归还是DP。\n\n```python\n# 131 Palindrome Partitioning I\nclass Solution:\n    def partition(self, s: str) -> List[List[str]]:\n        res = []\n        memo = {}\n        self.splitPalindrome(s, [], res, memo)\n        return res\n        \n    def isPalindrome(self, s, memo):\n        if len(s) == 0:\n            return False\n        if len(s) == 1:\n            return True\n\n        if s in memo:\n            return memo[s]\n\n        news = '_'.join(s)\n        mid = len(news) // 2\n        for i in range(1,mid+1):\n            pre = mid - i\n            post = mid + i \n            if news[pre] != news[post]:\n                memo[s] = False\n                return False\n        memo[s] = True\n        return True\n\n    def splitPalindrome(self, s, rs, res,memo):\n        if len(s) == 0:\n            res.append(rs)\n            return\n        if len(s) == 1:\n            res.append(rs+[s])\n            return\n\n        for i in range(1,len(s)+1):\n            if self.isPalindrome(s[ :i], memo):\n                self.splitPalindrome(s[i: ], rs + [s[ :i]], res, memo)\n        return\n\n```\n\n　　主要思路还是围绕DFS为主，通过不断的切分s，达到给rs添加成员的目的，直到最后s划分完毕，添加到res中，围绕一个s不断划分是有指数级别的可能性，针对不同的切分区域可能会有相同的substring出现，用memo来缓存，最终AC.\n\n　　相同的思路在此题是行不通的，这也是这题是Hard的缘由，故意设置的陷阱，所以就需要找到可以优化的点。\n\n　　首先，搜索最小切分次数和遍历所有切分可能，其实还是有比较大的优化空间，因为前者的达到最小切分条件即可立即返回，后者则是需要遍历所有可能才返回。难点就是如何定义最小切分条件，其实最小切分条件这个定义似乎跟最优子结构的定义很相似，比如，算法导论 讲到 动态规划时，解释最优子结构的核心的时候用到的切钢筋的示例，给定一个长度为n的钢筋，其最优的切割方法实际是确定的，这个确定的切割方法就可以作为一个结果供后续上层调用，即是，最优解一定由子问题的最优解组成。问题是，除了长度n的约束外，最主要的约束条件是string的起点和终点，即$s[i][j]$ ，因为不同于切钢筋，不同部位的的钢筋，只需要长度一样，就看作同样的；这里不同的substring，是否是回文字符的可能性不同，那么这就涉及到是否可以的切分的一个约束条件，也就是作为最优子结构，还有一个前提的是否是回文字符的条件，这就需要知道所有可能的$s[i][j]$是否是回文字符串的情况，貌似这个问题本身也是一个至少需要$O(N^2)$的复杂度的问题，因为遍历所有可能的substring 至少需要$O(N^2)$；其实当初分析到这里，心里觉得多少有点没底，所以也就没朝着这个方向努力，还在想各种其他Trick，最后都没有成功，还是回到了先如何实现判断所有substring是否是回文的方向上来。\n\n#### 最长回文子串问题 Longest Palindrome Substring\n\n　　曾经看过一篇[最长回文子串的博文](https://segmentfault.com/a/1190000003914228)，除了1.暴力解法和2.在间隙中添加标记字符来作为对称中点来解决奇数和偶数字符串对称轴不同的问题的$O(N^2)$解法，还有一种在解法2的基础上寻找非常细微的优化点的Manacher算法，不过这个算法过于精细，以至于我只记得大概的算法优化点，完全不记得细节了，所以没办法用上，这个之后再讨论；但是篇博文最后的动态规划解法比较好理解，直接就是遍历$s[i][j]$的上三角部分，然后根据dp递推式$dp[i][j] = dp[i+1][j-1] ,if \\quad s[i]==s[j] $ ，由于$dp[i][j]==dp[j][i]$，所以只需要求出上三角即可\n\n```python\n# dp 求substring 上三角是否是 回文字符\n    def longestPalindrome(self, s):\n        length = len(s)\n        dp = [[False]*length for _ in range(length)]\n        for i in range(length):\n            j = i\n            while j >= 0:\n                if s[j] == s[i] and (i-j<2 or dp[j+1][i-1]):\n                    dp[j][i] = True\n                j -= 1\n        return dp\n```\n\n时间复杂度$O(N^2)$，似乎和改进后的暴力解法差不多，不过够用了。\n\n后来受到这个启发，发现改进的暴力解法$O(N^2)$也可以AC，这里也直接给出\n\n```python\n# 普通 O(N**2) 解法\n    def longestPalindrome(self, s):\n        length = len(s)\n        d = [[False]*length for _ in range(length)]\n\n        news = '_'.join(s)\n        for i in range(len(news)):\n            d[i//2][i//2] = True\n            end = min(i, len(news)-i)\n            flag = True\n            for j in range(1,end+1):\n                pre = i-j\n                post = i+j\n              \n                if news[pre] != '_' and  flag:\n                    if  news[pre] == news[post]:\n                        d[pre//2][post//2] = True\n                    else:\n                        flag = False\n        return d\n```\n\n\n\n#### DFS搜索最短切分\n\n　　接下来就是常规的，DFS来切分并累加切分次数，跟131题套路类似，都需要判断切分前是否直接本身就是回文，必须满足这个条件之后才能继续切分\n\n```python\n# s2-1 自顶向下 备忘录dp AC 456ms\n    def calLength(self, start, s, dp, memo):\n        if start == len(s):\n            return 0\n        if start == len(s)-1:\n            memo[start][len(s)-1] = 1\n            return 1\n        if dp[start][len(s)-1]:\n            memo[start][len(s)-1] = 1\n            return 1\n        if memo[start][len(s)-1] != 0:\n            return memo[start][len(s)-1]\n\n        res = float('inf')\n        for i in range(start, len(s)):\n            if dp[start][i] :\n                res = min(res, self.calLength(i+1, s, dp , memo))\n        memo[start][len(s)-1] = res + 1\n        return memo[start][len(s)-1]\n```\n\n　最后成功AC\n\n　除了DFS，还在想是否可以改造成DP算法，顺便把之前没有写成的递推公式也顺便推导一下，这个阶段的DP和前面求最长回文子串不是一个问题，所以递推式需要重新归纳总结：\n\n$dp[i][j] = min(dp[i][k] + dp[k+1][j] + 1) ,if \\quad s[i:j+1]$不是回文字符串\n\n由于需要用到 dp 中i-j 之间的所有值，可以采用类似希尔排序的类似的逐渐递增gap的方式，每个新的gap计算之前，旧的更小的gap的所有片段dp值都已经计算出来\n\n```python\ndef calLength(self, s, dp ):\n        length = len(s)\n        dp_2 = [[0]*length for _ in range(length)]\n        if dp[0][length-1]:\n            dp_2[0][-1] = 0\n            return dp_2\n        # s1 gap dp\n        for gap in range(1, length):\n            for i in range(length-gap):\n                j = i+gap\n                if not dp[i][j]:\n                    r = float('inf')\n                    for k in range(i, j):\n                        r = min(r, dp_2[i][k] + dp_2[k+1][j] + 1)\n                    \n                    dp_2[i][j] = r\n\t\t\t\treturn dp_2\n```\n\n时间复杂度$O(N^3)$，结果自然也是TLE，不过目前至少没看到算法层面的错误，或许能有办法优化成$O(N^2)$的算法，目前还没看出怎么改进。\n\n\n\n### Manacher算法 \n\n　　从原来上来讲，其实这应该是很微妙的一个优化，就是一步一步计算每一个字符包括间隙为中心点的回文字符的最大长度RL[i]，由于i之前的RL都是已经计算出来的，寻找某个包括i的pos为中心的最长RL[pos]，此时以pos为对称轴，寻找i的对称点j\n\n![1](1.png)\n\n![2](2.png)\n\n由于RL[j]已知，那么这个RL[j]就可以做RL[i]的起点，这样RL[i]就不需要从0开始计算，但是由于RL[j]的长度可能会超过pos，这样就需要分2种情况来寻找R[i]的起点,一种是较短的情况，如上图，另一种是超过pos，如下图，就需要降低RL[i]的起点为从蓝色部分开始\n\n![3](3.png)\n\n当然还有最后一种情况，就是i已经超过了maxRight ，则需要从0开始计算![4](4.png)\n\n```python\ndef manacher(s):\n  s = '_'.join(s)\n  rl = [0]*len(s)\n  maxRight = 0\n  pos = 0\n  maxLen = 0\n  \n  for i in range(len(s)):\n    if i < maxRight:\n      rl[i] = min(rl[2*pos-i], maxRight-i)\n    else:\n      rl[i] = 1\n    # 尝试扩展边界rl[i]\n    while i-rl[i]>=0 and i+rl[i]<len(s) and s[i-rl[i]]==s[i+rl[i]]:\n      rl[i] += 1\n      \n    if rl[i]+i-1>maxRight:\n      maxRight = rl[i] + i -1\n      pos = i\n      \n    maxLen = max(maxLen, rl[i])\n  return maxLen-1\n```\n\n虽说声称时间复杂度是$O(N)$，但是我看代码怎么都像是$O(N^2)$，起码最坏的情况下我想应该是的，假如每次求得的对称RL[j]都接近0，那么RL[i]就得从头遍历，但是另一方面，RL[j]本身就很快的返回了，如果RL[j]很长，那么对应的RL[i]就收益，可以高起点开始计算；从这个角度来看，起码能节省一半的时间，正常情况下应该介于$O(N)$和$O(N^2)$之间，真实情况如何，我也不是很确定。\n\n### Conclusion\n\n　　终于有机会把回文字符这块好好的补习了一下，初识manacher算法认真来讲，是有点失望的，相比于NFA还有最短路径之类的神奇算法，这个有点小打小闹的优化改进，不过还是不得不佩服，就这么一个$O(N^2)$的算法，硬是被人找到机会给优化掉了，真的不能小看程序员想要改进算法的决心啊！\n\n\n\nReference:\n\n[最长回文子串-Manacher算法](https://segmentfault.com/a/1190000003914228)\n\n","source":"_posts/132-Palindrome-Partitioning-II.md","raw":"---\ntitle: 132. Palindrome Partitioning II\ntoc: true\nmathjax: true\ntop: false\ncover: true\ndate: 2020-12-17 11:06:04\nupdated:\ncategories: Algorithms\ntags:\n\t- Algorithms\n\t- LeetCode\n---\n\n> Given a string `s`, partition `s` such that every substring of the partition is a palindrome.\n>\n> Return *the minimum cuts needed* for a palindrome partitioning of `s`.\n>\n>  \n>\n> **Example 1:**\n>\n> ```\n> Input: s = \"aab\"\n> Output: 1\n> Explanation: The palindrome partitioning [\"aa\",\"b\"] could be produced using 1 cut.\n> ```\n>\n> **Example 2:**\n>\n> ```\n> Input: s = \"a\"\n> Output: 0\n> ```\n>\n> **Example 3:**\n>\n> ```\n> Input: s = \"ab\"\n> Output: 1\n> ```\n>\n>  \n>\n> **Constraints:**\n>\n> - `1 <= s.length <= 2000`\n> - `s` consists of lower-case English letters only.\n\n　　初识回文字符串的时候，觉得这是个很棘手的问题，毕竟涉及到正序逆序的检索问题，光是字符串匹配和正则表达式都能整出DFA和NFA这种复杂的状态机，何况是回文字符这种感觉状态机似乎都没什么办法的问题。没想到的是，回文字符的识别手段如此简单粗暴，就是寻找中心点，然后从中点向外的边扩展边匹配，原理上是没问题的，但是一般在讲解玩暴力解法之后，都会给出更加优雅精妙的优化版算法，但是在回文字符串的问题上，似乎到这里就戛然而止了，可能跟自己接触回文字符串的方式有关，因为自己并不是在教材上看到这个问题的，而是直接在OJ上面临这个问题的，最初理所当然的用暴力解法先过一遍，没想到居然AC了，后面就没有对这个问题进行过更多的思考了，直到遇到这个加强版的回文字符问题，检测substring是否是回文字符，并且暴力解法完全无法AC，这才逼迫自己想办法优化暴力解法。\n\n### 问题解析\n\n　　substring 回文字符检测，在132题之前，其实有一个131 Palindrome Partitioning I 的 medium版本，要求输出所有可能的s 的 substring 切分，并且以列表的形式返回所有可能的切分，原理上此问题是131问题的子集，因为既然都已经输出了所有可能的切分，那么搜索最短切分只需要列出所有切分的长度，取最小即可；然而在OJ的设置上，这样是没办法在132上AC的，因为我在131用的仍然是暴力的解法小改版，只不过在切分的时候发现可能会面临重复递归调用的问题，也就是动态规划DP常常解释的，重复子结构的问题，我采取的备忘录的方式来缓解自顶向下调用时的重复调用问题，在完全没有构思出DP递推式 的前提下，就这样压线AC了，搞得我自己都不知道，这个AC的代码究竟用的是递归还是DP。\n\n```python\n# 131 Palindrome Partitioning I\nclass Solution:\n    def partition(self, s: str) -> List[List[str]]:\n        res = []\n        memo = {}\n        self.splitPalindrome(s, [], res, memo)\n        return res\n        \n    def isPalindrome(self, s, memo):\n        if len(s) == 0:\n            return False\n        if len(s) == 1:\n            return True\n\n        if s in memo:\n            return memo[s]\n\n        news = '_'.join(s)\n        mid = len(news) // 2\n        for i in range(1,mid+1):\n            pre = mid - i\n            post = mid + i \n            if news[pre] != news[post]:\n                memo[s] = False\n                return False\n        memo[s] = True\n        return True\n\n    def splitPalindrome(self, s, rs, res,memo):\n        if len(s) == 0:\n            res.append(rs)\n            return\n        if len(s) == 1:\n            res.append(rs+[s])\n            return\n\n        for i in range(1,len(s)+1):\n            if self.isPalindrome(s[ :i], memo):\n                self.splitPalindrome(s[i: ], rs + [s[ :i]], res, memo)\n        return\n\n```\n\n　　主要思路还是围绕DFS为主，通过不断的切分s，达到给rs添加成员的目的，直到最后s划分完毕，添加到res中，围绕一个s不断划分是有指数级别的可能性，针对不同的切分区域可能会有相同的substring出现，用memo来缓存，最终AC.\n\n　　相同的思路在此题是行不通的，这也是这题是Hard的缘由，故意设置的陷阱，所以就需要找到可以优化的点。\n\n　　首先，搜索最小切分次数和遍历所有切分可能，其实还是有比较大的优化空间，因为前者的达到最小切分条件即可立即返回，后者则是需要遍历所有可能才返回。难点就是如何定义最小切分条件，其实最小切分条件这个定义似乎跟最优子结构的定义很相似，比如，算法导论 讲到 动态规划时，解释最优子结构的核心的时候用到的切钢筋的示例，给定一个长度为n的钢筋，其最优的切割方法实际是确定的，这个确定的切割方法就可以作为一个结果供后续上层调用，即是，最优解一定由子问题的最优解组成。问题是，除了长度n的约束外，最主要的约束条件是string的起点和终点，即$s[i][j]$ ，因为不同于切钢筋，不同部位的的钢筋，只需要长度一样，就看作同样的；这里不同的substring，是否是回文字符的可能性不同，那么这就涉及到是否可以的切分的一个约束条件，也就是作为最优子结构，还有一个前提的是否是回文字符的条件，这就需要知道所有可能的$s[i][j]$是否是回文字符串的情况，貌似这个问题本身也是一个至少需要$O(N^2)$的复杂度的问题，因为遍历所有可能的substring 至少需要$O(N^2)$；其实当初分析到这里，心里觉得多少有点没底，所以也就没朝着这个方向努力，还在想各种其他Trick，最后都没有成功，还是回到了先如何实现判断所有substring是否是回文的方向上来。\n\n#### 最长回文子串问题 Longest Palindrome Substring\n\n　　曾经看过一篇[最长回文子串的博文](https://segmentfault.com/a/1190000003914228)，除了1.暴力解法和2.在间隙中添加标记字符来作为对称中点来解决奇数和偶数字符串对称轴不同的问题的$O(N^2)$解法，还有一种在解法2的基础上寻找非常细微的优化点的Manacher算法，不过这个算法过于精细，以至于我只记得大概的算法优化点，完全不记得细节了，所以没办法用上，这个之后再讨论；但是篇博文最后的动态规划解法比较好理解，直接就是遍历$s[i][j]$的上三角部分，然后根据dp递推式$dp[i][j] = dp[i+1][j-1] ,if \\quad s[i]==s[j] $ ，由于$dp[i][j]==dp[j][i]$，所以只需要求出上三角即可\n\n```python\n# dp 求substring 上三角是否是 回文字符\n    def longestPalindrome(self, s):\n        length = len(s)\n        dp = [[False]*length for _ in range(length)]\n        for i in range(length):\n            j = i\n            while j >= 0:\n                if s[j] == s[i] and (i-j<2 or dp[j+1][i-1]):\n                    dp[j][i] = True\n                j -= 1\n        return dp\n```\n\n时间复杂度$O(N^2)$，似乎和改进后的暴力解法差不多，不过够用了。\n\n后来受到这个启发，发现改进的暴力解法$O(N^2)$也可以AC，这里也直接给出\n\n```python\n# 普通 O(N**2) 解法\n    def longestPalindrome(self, s):\n        length = len(s)\n        d = [[False]*length for _ in range(length)]\n\n        news = '_'.join(s)\n        for i in range(len(news)):\n            d[i//2][i//2] = True\n            end = min(i, len(news)-i)\n            flag = True\n            for j in range(1,end+1):\n                pre = i-j\n                post = i+j\n              \n                if news[pre] != '_' and  flag:\n                    if  news[pre] == news[post]:\n                        d[pre//2][post//2] = True\n                    else:\n                        flag = False\n        return d\n```\n\n\n\n#### DFS搜索最短切分\n\n　　接下来就是常规的，DFS来切分并累加切分次数，跟131题套路类似，都需要判断切分前是否直接本身就是回文，必须满足这个条件之后才能继续切分\n\n```python\n# s2-1 自顶向下 备忘录dp AC 456ms\n    def calLength(self, start, s, dp, memo):\n        if start == len(s):\n            return 0\n        if start == len(s)-1:\n            memo[start][len(s)-1] = 1\n            return 1\n        if dp[start][len(s)-1]:\n            memo[start][len(s)-1] = 1\n            return 1\n        if memo[start][len(s)-1] != 0:\n            return memo[start][len(s)-1]\n\n        res = float('inf')\n        for i in range(start, len(s)):\n            if dp[start][i] :\n                res = min(res, self.calLength(i+1, s, dp , memo))\n        memo[start][len(s)-1] = res + 1\n        return memo[start][len(s)-1]\n```\n\n　最后成功AC\n\n　除了DFS，还在想是否可以改造成DP算法，顺便把之前没有写成的递推公式也顺便推导一下，这个阶段的DP和前面求最长回文子串不是一个问题，所以递推式需要重新归纳总结：\n\n$dp[i][j] = min(dp[i][k] + dp[k+1][j] + 1) ,if \\quad s[i:j+1]$不是回文字符串\n\n由于需要用到 dp 中i-j 之间的所有值，可以采用类似希尔排序的类似的逐渐递增gap的方式，每个新的gap计算之前，旧的更小的gap的所有片段dp值都已经计算出来\n\n```python\ndef calLength(self, s, dp ):\n        length = len(s)\n        dp_2 = [[0]*length for _ in range(length)]\n        if dp[0][length-1]:\n            dp_2[0][-1] = 0\n            return dp_2\n        # s1 gap dp\n        for gap in range(1, length):\n            for i in range(length-gap):\n                j = i+gap\n                if not dp[i][j]:\n                    r = float('inf')\n                    for k in range(i, j):\n                        r = min(r, dp_2[i][k] + dp_2[k+1][j] + 1)\n                    \n                    dp_2[i][j] = r\n\t\t\t\treturn dp_2\n```\n\n时间复杂度$O(N^3)$，结果自然也是TLE，不过目前至少没看到算法层面的错误，或许能有办法优化成$O(N^2)$的算法，目前还没看出怎么改进。\n\n\n\n### Manacher算法 \n\n　　从原来上来讲，其实这应该是很微妙的一个优化，就是一步一步计算每一个字符包括间隙为中心点的回文字符的最大长度RL[i]，由于i之前的RL都是已经计算出来的，寻找某个包括i的pos为中心的最长RL[pos]，此时以pos为对称轴，寻找i的对称点j\n\n![1](1.png)\n\n![2](2.png)\n\n由于RL[j]已知，那么这个RL[j]就可以做RL[i]的起点，这样RL[i]就不需要从0开始计算，但是由于RL[j]的长度可能会超过pos，这样就需要分2种情况来寻找R[i]的起点,一种是较短的情况，如上图，另一种是超过pos，如下图，就需要降低RL[i]的起点为从蓝色部分开始\n\n![3](3.png)\n\n当然还有最后一种情况，就是i已经超过了maxRight ，则需要从0开始计算![4](4.png)\n\n```python\ndef manacher(s):\n  s = '_'.join(s)\n  rl = [0]*len(s)\n  maxRight = 0\n  pos = 0\n  maxLen = 0\n  \n  for i in range(len(s)):\n    if i < maxRight:\n      rl[i] = min(rl[2*pos-i], maxRight-i)\n    else:\n      rl[i] = 1\n    # 尝试扩展边界rl[i]\n    while i-rl[i]>=0 and i+rl[i]<len(s) and s[i-rl[i]]==s[i+rl[i]]:\n      rl[i] += 1\n      \n    if rl[i]+i-1>maxRight:\n      maxRight = rl[i] + i -1\n      pos = i\n      \n    maxLen = max(maxLen, rl[i])\n  return maxLen-1\n```\n\n虽说声称时间复杂度是$O(N)$，但是我看代码怎么都像是$O(N^2)$，起码最坏的情况下我想应该是的，假如每次求得的对称RL[j]都接近0，那么RL[i]就得从头遍历，但是另一方面，RL[j]本身就很快的返回了，如果RL[j]很长，那么对应的RL[i]就收益，可以高起点开始计算；从这个角度来看，起码能节省一半的时间，正常情况下应该介于$O(N)$和$O(N^2)$之间，真实情况如何，我也不是很确定。\n\n### Conclusion\n\n　　终于有机会把回文字符这块好好的补习了一下，初识manacher算法认真来讲，是有点失望的，相比于NFA还有最短路径之类的神奇算法，这个有点小打小闹的优化改进，不过还是不得不佩服，就这么一个$O(N^2)$的算法，硬是被人找到机会给优化掉了，真的不能小看程序员想要改进算法的决心啊！\n\n\n\nReference:\n\n[最长回文子串-Manacher算法](https://segmentfault.com/a/1190000003914228)\n\n","slug":"132-Palindrome-Partitioning-II","published":1,"_id":"ckisn8crf0000vy28agpvc03h","comments":1,"layout":"post","photos":[],"link":"","content":"<blockquote>\n<p>Given a string <code>s</code>, partition <code>s</code> such that every substring of the partition is a palindrome.</p>\n<p>Return <em>the minimum cuts needed</em> for a palindrome partitioning of <code>s</code>.</p>\n<p><strong>Example 1:</strong></p>\n<pre><code>Input: s = &quot;aab&quot;\nOutput: 1\nExplanation: The palindrome partitioning [&quot;aa&quot;,&quot;b&quot;] could be produced using 1 cut.</code></pre>\n<p><strong>Example 2:</strong></p>\n<pre><code>Input: s = &quot;a&quot;\nOutput: 0</code></pre>\n<p><strong>Example 3:</strong></p>\n<pre><code>Input: s = &quot;ab&quot;\nOutput: 1</code></pre>\n<p><strong>Constraints:</strong></p>\n<ul>\n<li><code>1 &lt;= s.length &lt;= 2000</code></li>\n<li><code>s</code> consists of lower-case English letters only.</li>\n</ul>\n</blockquote>\n<p>　　初识回文字符串的时候，觉得这是个很棘手的问题，毕竟涉及到正序逆序的检索问题，光是字符串匹配和正则表达式都能整出DFA和NFA这种复杂的状态机，何况是回文字符这种感觉状态机似乎都没什么办法的问题。没想到的是，回文字符的识别手段如此简单粗暴，就是寻找中心点，然后从中点向外的边扩展边匹配，原理上是没问题的，但是一般在讲解玩暴力解法之后，都会给出更加优雅精妙的优化版算法，但是在回文字符串的问题上，似乎到这里就戛然而止了，可能跟自己接触回文字符串的方式有关，因为自己并不是在教材上看到这个问题的，而是直接在OJ上面临这个问题的，最初理所当然的用暴力解法先过一遍，没想到居然AC了，后面就没有对这个问题进行过更多的思考了，直到遇到这个加强版的回文字符问题，检测substring是否是回文字符，并且暴力解法完全无法AC，这才逼迫自己想办法优化暴力解法。</p>\n<h3 id=\"问题解析\"><a href=\"#问题解析\" class=\"headerlink\" title=\"问题解析\"></a>问题解析</h3><p>　　substring 回文字符检测，在132题之前，其实有一个131 Palindrome Partitioning I 的 medium版本，要求输出所有可能的s 的 substring 切分，并且以列表的形式返回所有可能的切分，原理上此问题是131问题的子集，因为既然都已经输出了所有可能的切分，那么搜索最短切分只需要列出所有切分的长度，取最小即可；然而在OJ的设置上，这样是没办法在132上AC的，因为我在131用的仍然是暴力的解法小改版，只不过在切分的时候发现可能会面临重复递归调用的问题，也就是动态规划DP常常解释的，重复子结构的问题，我采取的备忘录的方式来缓解自顶向下调用时的重复调用问题，在完全没有构思出DP递推式 的前提下，就这样压线AC了，搞得我自己都不知道，这个AC的代码究竟用的是递归还是DP。</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token comment\" spellcheck=\"true\"># 131 Palindrome Partitioning I</span>\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">Solution</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">partition</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> s<span class=\"token punctuation\">:</span> str<span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span><span class=\"token operator\">></span> List<span class=\"token punctuation\">[</span>List<span class=\"token punctuation\">[</span>str<span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n        res <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n        memo <span class=\"token operator\">=</span> <span class=\"token operator\">&amp;</span><span class=\"token comment\" spellcheck=\"true\">#123;&amp;#125;</span>\n        self<span class=\"token punctuation\">.</span>splitPalindrome<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> res<span class=\"token punctuation\">,</span> memo<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> res\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">isPalindrome</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> s<span class=\"token punctuation\">,</span> memo<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">if</span> len<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span> <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">return</span> <span class=\"token boolean\">False</span>\n        <span class=\"token keyword\">if</span> len<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span> <span class=\"token operator\">==</span> <span class=\"token number\">1</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">return</span> <span class=\"token boolean\">True</span>\n\n        <span class=\"token keyword\">if</span> s <span class=\"token keyword\">in</span> memo<span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">return</span> memo<span class=\"token punctuation\">[</span>s<span class=\"token punctuation\">]</span>\n\n        news <span class=\"token operator\">=</span> <span class=\"token string\">'_'</span><span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span>\n        mid <span class=\"token operator\">=</span> len<span class=\"token punctuation\">(</span>news<span class=\"token punctuation\">)</span> <span class=\"token operator\">//</span> <span class=\"token number\">2</span>\n        <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span>mid<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            pre <span class=\"token operator\">=</span> mid <span class=\"token operator\">-</span> i\n            post <span class=\"token operator\">=</span> mid <span class=\"token operator\">+</span> i \n            <span class=\"token keyword\">if</span> news<span class=\"token punctuation\">[</span>pre<span class=\"token punctuation\">]</span> <span class=\"token operator\">!=</span> news<span class=\"token punctuation\">[</span>post<span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n                memo<span class=\"token punctuation\">[</span>s<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token boolean\">False</span>\n                <span class=\"token keyword\">return</span> <span class=\"token boolean\">False</span>\n        memo<span class=\"token punctuation\">[</span>s<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token boolean\">True</span>\n        <span class=\"token keyword\">return</span> <span class=\"token boolean\">True</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">splitPalindrome</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> s<span class=\"token punctuation\">,</span> rs<span class=\"token punctuation\">,</span> res<span class=\"token punctuation\">,</span>memo<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">if</span> len<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span> <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n            res<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>rs<span class=\"token punctuation\">)</span>\n            <span class=\"token keyword\">return</span>\n        <span class=\"token keyword\">if</span> len<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span> <span class=\"token operator\">==</span> <span class=\"token number\">1</span><span class=\"token punctuation\">:</span>\n            res<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>rs<span class=\"token operator\">+</span><span class=\"token punctuation\">[</span>s<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n            <span class=\"token keyword\">return</span>\n\n        <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span>len<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span><span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">if</span> self<span class=\"token punctuation\">.</span>isPalindrome<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">[</span> <span class=\"token punctuation\">:</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> memo<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n                self<span class=\"token punctuation\">.</span>splitPalindrome<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">:</span> <span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> rs <span class=\"token operator\">+</span> <span class=\"token punctuation\">[</span>s<span class=\"token punctuation\">[</span> <span class=\"token punctuation\">:</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> res<span class=\"token punctuation\">,</span> memo<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span>\n</code></pre>\n<p>　　主要思路还是围绕DFS为主，通过不断的切分s，达到给rs添加成员的目的，直到最后s划分完毕，添加到res中，围绕一个s不断划分是有指数级别的可能性，针对不同的切分区域可能会有相同的substring出现，用memo来缓存，最终AC.</p>\n<p>　　相同的思路在此题是行不通的，这也是这题是Hard的缘由，故意设置的陷阱，所以就需要找到可以优化的点。</p>\n<p>　　首先，搜索最小切分次数和遍历所有切分可能，其实还是有比较大的优化空间，因为前者的达到最小切分条件即可立即返回，后者则是需要遍历所有可能才返回。难点就是如何定义最小切分条件，其实最小切分条件这个定义似乎跟最优子结构的定义很相似，比如，算法导论 讲到 动态规划时，解释最优子结构的核心的时候用到的切钢筋的示例，给定一个长度为n的钢筋，其最优的切割方法实际是确定的，这个确定的切割方法就可以作为一个结果供后续上层调用，即是，最优解一定由子问题的最优解组成。问题是，除了长度n的约束外，最主要的约束条件是string的起点和终点，即$s[i][j]$ ，因为不同于切钢筋，不同部位的的钢筋，只需要长度一样，就看作同样的；这里不同的substring，是否是回文字符的可能性不同，那么这就涉及到是否可以的切分的一个约束条件，也就是作为最优子结构，还有一个前提的是否是回文字符的条件，这就需要知道所有可能的$s[i][j]$是否是回文字符串的情况，貌似这个问题本身也是一个至少需要$O(N^2)$的复杂度的问题，因为遍历所有可能的substring 至少需要$O(N^2)$；其实当初分析到这里，心里觉得多少有点没底，所以也就没朝着这个方向努力，还在想各种其他Trick，最后都没有成功，还是回到了先如何实现判断所有substring是否是回文的方向上来。</p>\n<h4 id=\"最长回文子串问题-Longest-Palindrome-Substring\"><a href=\"#最长回文子串问题-Longest-Palindrome-Substring\" class=\"headerlink\" title=\"最长回文子串问题 Longest Palindrome Substring\"></a>最长回文子串问题 Longest Palindrome Substring</h4><p>　　曾经看过一篇<a href=\"https://segmentfault.com/a/1190000003914228\">最长回文子串的博文</a>，除了1.暴力解法和2.在间隙中添加标记字符来作为对称中点来解决奇数和偶数字符串对称轴不同的问题的$O(N^2)$解法，还有一种在解法2的基础上寻找非常细微的优化点的Manacher算法，不过这个算法过于精细，以至于我只记得大概的算法优化点，完全不记得细节了，所以没办法用上，这个之后再讨论；但是篇博文最后的动态规划解法比较好理解，直接就是遍历$s[i][j]$的上三角部分，然后根据dp递推式$dp[i][j] = dp[i+1][j-1] ,if \\quad s[i]==s[j] $ ，由于$dp[i][j]==dp[j][i]$，所以只需要求出上三角即可</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token comment\" spellcheck=\"true\"># dp 求substring 上三角是否是 回文字符</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">longestPalindrome</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> s<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        length <span class=\"token operator\">=</span> len<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span>\n        dp <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">]</span><span class=\"token operator\">*</span>length <span class=\"token keyword\">for</span> _ <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span>length<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\n        <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span>length<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            j <span class=\"token operator\">=</span> i\n            <span class=\"token keyword\">while</span> j <span class=\"token operator\">>=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n                <span class=\"token keyword\">if</span> s<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">]</span> <span class=\"token operator\">==</span> s<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span> <span class=\"token operator\">and</span> <span class=\"token punctuation\">(</span>i<span class=\"token operator\">-</span>j<span class=\"token operator\">&lt;</span><span class=\"token number\">2</span> <span class=\"token operator\">or</span> dp<span class=\"token punctuation\">[</span>j<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span>i<span class=\"token number\">-1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n                    dp<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token boolean\">True</span>\n                j <span class=\"token operator\">-=</span> <span class=\"token number\">1</span>\n        <span class=\"token keyword\">return</span> dp</code></pre>\n<p>时间复杂度$O(N^2)$，似乎和改进后的暴力解法差不多，不过够用了。</p>\n<p>后来受到这个启发，发现改进的暴力解法$O(N^2)$也可以AC，这里也直接给出</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token comment\" spellcheck=\"true\"># 普通 O(N**2) 解法</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">longestPalindrome</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> s<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        length <span class=\"token operator\">=</span> len<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span>\n        d <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">]</span><span class=\"token operator\">*</span>length <span class=\"token keyword\">for</span> _ <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span>length<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\n\n        news <span class=\"token operator\">=</span> <span class=\"token string\">'_'</span><span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span>len<span class=\"token punctuation\">(</span>news<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            d<span class=\"token punctuation\">[</span>i<span class=\"token operator\">//</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span>i<span class=\"token operator\">//</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token boolean\">True</span>\n            end <span class=\"token operator\">=</span> min<span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">,</span> len<span class=\"token punctuation\">(</span>news<span class=\"token punctuation\">)</span><span class=\"token operator\">-</span>i<span class=\"token punctuation\">)</span>\n            flag <span class=\"token operator\">=</span> <span class=\"token boolean\">True</span>\n            <span class=\"token keyword\">for</span> j <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span>end<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n                pre <span class=\"token operator\">=</span> i<span class=\"token operator\">-</span>j\n                post <span class=\"token operator\">=</span> i<span class=\"token operator\">+</span>j\n\n                <span class=\"token keyword\">if</span> news<span class=\"token punctuation\">[</span>pre<span class=\"token punctuation\">]</span> <span class=\"token operator\">!=</span> <span class=\"token string\">'_'</span> <span class=\"token operator\">and</span>  flag<span class=\"token punctuation\">:</span>\n                    <span class=\"token keyword\">if</span>  news<span class=\"token punctuation\">[</span>pre<span class=\"token punctuation\">]</span> <span class=\"token operator\">==</span> news<span class=\"token punctuation\">[</span>post<span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n                        d<span class=\"token punctuation\">[</span>pre<span class=\"token operator\">//</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span>post<span class=\"token operator\">//</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token boolean\">True</span>\n                    <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n                        flag <span class=\"token operator\">=</span> <span class=\"token boolean\">False</span>\n        <span class=\"token keyword\">return</span> d</code></pre>\n<h4 id=\"DFS搜索最短切分\"><a href=\"#DFS搜索最短切分\" class=\"headerlink\" title=\"DFS搜索最短切分\"></a>DFS搜索最短切分</h4><p>　　接下来就是常规的，DFS来切分并累加切分次数，跟131题套路类似，都需要判断切分前是否直接本身就是回文，必须满足这个条件之后才能继续切分</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token comment\" spellcheck=\"true\"># s2-1 自顶向下 备忘录dp AC 456ms</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">calLength</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> start<span class=\"token punctuation\">,</span> s<span class=\"token punctuation\">,</span> dp<span class=\"token punctuation\">,</span> memo<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">if</span> start <span class=\"token operator\">==</span> len<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">return</span> <span class=\"token number\">0</span>\n        <span class=\"token keyword\">if</span> start <span class=\"token operator\">==</span> len<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">:</span>\n            memo<span class=\"token punctuation\">[</span>start<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span>len<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token number\">1</span>\n            <span class=\"token keyword\">return</span> <span class=\"token number\">1</span>\n        <span class=\"token keyword\">if</span> dp<span class=\"token punctuation\">[</span>start<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span>len<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n            memo<span class=\"token punctuation\">[</span>start<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span>len<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token number\">1</span>\n            <span class=\"token keyword\">return</span> <span class=\"token number\">1</span>\n        <span class=\"token keyword\">if</span> memo<span class=\"token punctuation\">[</span>start<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span>len<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">!=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">return</span> memo<span class=\"token punctuation\">[</span>start<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span>len<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span>\n\n        res <span class=\"token operator\">=</span> float<span class=\"token punctuation\">(</span><span class=\"token string\">'inf'</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span>start<span class=\"token punctuation\">,</span> len<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">if</span> dp<span class=\"token punctuation\">[</span>start<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span> <span class=\"token punctuation\">:</span>\n                res <span class=\"token operator\">=</span> min<span class=\"token punctuation\">(</span>res<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>calLength<span class=\"token punctuation\">(</span>i<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> s<span class=\"token punctuation\">,</span> dp <span class=\"token punctuation\">,</span> memo<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        memo<span class=\"token punctuation\">[</span>start<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span>len<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> res <span class=\"token operator\">+</span> <span class=\"token number\">1</span>\n        <span class=\"token keyword\">return</span> memo<span class=\"token punctuation\">[</span>start<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span>len<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span></code></pre>\n<p>　最后成功AC</p>\n<p>　除了DFS，还在想是否可以改造成DP算法，顺便把之前没有写成的递推公式也顺便推导一下，这个阶段的DP和前面求最长回文子串不是一个问题，所以递推式需要重新归纳总结：</p>\n<p>$dp[i][j] = min(dp[i][k] + dp[k+1][j] + 1) ,if \\quad s[i:j+1]$不是回文字符串</p>\n<p>由于需要用到 dp 中i-j 之间的所有值，可以采用类似希尔排序的类似的逐渐递增gap的方式，每个新的gap计算之前，旧的更小的gap的所有片段dp值都已经计算出来</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">calLength</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> s<span class=\"token punctuation\">,</span> dp <span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        length <span class=\"token operator\">=</span> len<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span>\n        dp_2 <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token operator\">*</span>length <span class=\"token keyword\">for</span> _ <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span>length<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\n        <span class=\"token keyword\">if</span> dp<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span>length<span class=\"token number\">-1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n            dp_2<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n            <span class=\"token keyword\">return</span> dp_2\n        <span class=\"token comment\" spellcheck=\"true\"># s1 gap dp</span>\n        <span class=\"token keyword\">for</span> gap <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> length<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span>length<span class=\"token operator\">-</span>gap<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n                j <span class=\"token operator\">=</span> i<span class=\"token operator\">+</span>gap\n                <span class=\"token keyword\">if</span> <span class=\"token operator\">not</span> dp<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n                    r <span class=\"token operator\">=</span> float<span class=\"token punctuation\">(</span><span class=\"token string\">'inf'</span><span class=\"token punctuation\">)</span>\n                    <span class=\"token keyword\">for</span> k <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">,</span> j<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n                        r <span class=\"token operator\">=</span> min<span class=\"token punctuation\">(</span>r<span class=\"token punctuation\">,</span> dp_2<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span>k<span class=\"token punctuation\">]</span> <span class=\"token operator\">+</span> dp_2<span class=\"token punctuation\">[</span>k<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">]</span> <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n\n                    dp_2<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> r\n                <span class=\"token keyword\">return</span> dp_2</code></pre>\n<p>时间复杂度$O(N^3)$，结果自然也是TLE，不过目前至少没看到算法层面的错误，或许能有办法优化成$O(N^2)$的算法，目前还没看出怎么改进。</p>\n<h3 id=\"Manacher算法\"><a href=\"#Manacher算法\" class=\"headerlink\" title=\"Manacher算法\"></a>Manacher算法</h3><p>　　从原来上来讲，其实这应该是很微妙的一个优化，就是一步一步计算每一个字符包括间隙为中心点的回文字符的最大长度RL[i]，由于i之前的RL都是已经计算出来的，寻找某个包括i的pos为中心的最长RL[pos]，此时以pos为对称轴，寻找i的对称点j</p>\n<p><img src=\"1.png\" alt=\"1\"></p>\n<p><img src=\"2.png\" alt=\"2\"></p>\n<p>由于RL[j]已知，那么这个RL[j]就可以做RL[i]的起点，这样RL[i]就不需要从0开始计算，但是由于RL[j]的长度可能会超过pos，这样就需要分2种情况来寻找R[i]的起点,一种是较短的情况，如上图，另一种是超过pos，如下图，就需要降低RL[i]的起点为从蓝色部分开始</p>\n<p><img src=\"3.png\" alt=\"3\"></p>\n<p>当然还有最后一种情况，就是i已经超过了maxRight ，则需要从0开始计算<img src=\"4.png\" alt=\"4\"></p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">manacher</span><span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  s <span class=\"token operator\">=</span> <span class=\"token string\">'_'</span><span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span>\n  rl <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token operator\">*</span>len<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span>\n  maxRight <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n  pos <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n  maxLen <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n\n  <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span>len<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">if</span> i <span class=\"token operator\">&lt;</span> maxRight<span class=\"token punctuation\">:</span>\n      rl<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> min<span class=\"token punctuation\">(</span>rl<span class=\"token punctuation\">[</span><span class=\"token number\">2</span><span class=\"token operator\">*</span>pos<span class=\"token operator\">-</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> maxRight<span class=\"token operator\">-</span>i<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n      rl<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token number\">1</span>\n    <span class=\"token comment\" spellcheck=\"true\"># 尝试扩展边界rl[i]</span>\n    <span class=\"token keyword\">while</span> i<span class=\"token operator\">-</span>rl<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token operator\">>=</span><span class=\"token number\">0</span> <span class=\"token operator\">and</span> i<span class=\"token operator\">+</span>rl<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token operator\">&lt;</span>len<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span> <span class=\"token operator\">and</span> s<span class=\"token punctuation\">[</span>i<span class=\"token operator\">-</span>rl<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token operator\">==</span>s<span class=\"token punctuation\">[</span>i<span class=\"token operator\">+</span>rl<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n      rl<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span> <span class=\"token operator\">+=</span> <span class=\"token number\">1</span>\n\n    <span class=\"token keyword\">if</span> rl<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token operator\">+</span>i<span class=\"token number\">-1</span><span class=\"token operator\">></span>maxRight<span class=\"token punctuation\">:</span>\n      maxRight <span class=\"token operator\">=</span> rl<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span> <span class=\"token operator\">+</span> i <span class=\"token operator\">-</span><span class=\"token number\">1</span>\n      pos <span class=\"token operator\">=</span> i\n\n    maxLen <span class=\"token operator\">=</span> max<span class=\"token punctuation\">(</span>maxLen<span class=\"token punctuation\">,</span> rl<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n  <span class=\"token keyword\">return</span> maxLen<span class=\"token number\">-1</span></code></pre>\n<p>虽说声称时间复杂度是$O(N)$，但是我看代码怎么都像是$O(N^2)$，起码最坏的情况下我想应该是的，假如每次求得的对称RL[j]都接近0，那么RL[i]就得从头遍历，但是另一方面，RL[j]本身就很快的返回了，如果RL[j]很长，那么对应的RL[i]就收益，可以高起点开始计算；从这个角度来看，起码能节省一半的时间，正常情况下应该介于$O(N)$和$O(N^2)$之间，真实情况如何，我也不是很确定。</p>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>　　终于有机会把回文字符这块好好的补习了一下，初识manacher算法认真来讲，是有点失望的，相比于NFA还有最短路径之类的神奇算法，这个有点小打小闹的优化改进，不过还是不得不佩服，就这么一个$O(N^2)$的算法，硬是被人找到机会给优化掉了，真的不能小看程序员想要改进算法的决心啊！</p>\n<p>Reference:</p>\n<p><a href=\"https://segmentfault.com/a/1190000003914228\">最长回文子串-Manacher算法</a></p>\n","site":{"data":{}},"excerpt":"","more":"<blockquote>\n<p>Given a string <code>s</code>, partition <code>s</code> such that every substring of the partition is a palindrome.</p>\n<p>Return <em>the minimum cuts needed</em> for a palindrome partitioning of <code>s</code>.</p>\n<p><strong>Example 1:</strong></p>\n<pre><code>Input: s = &quot;aab&quot;\nOutput: 1\nExplanation: The palindrome partitioning [&quot;aa&quot;,&quot;b&quot;] could be produced using 1 cut.</code></pre>\n<p><strong>Example 2:</strong></p>\n<pre><code>Input: s = &quot;a&quot;\nOutput: 0</code></pre>\n<p><strong>Example 3:</strong></p>\n<pre><code>Input: s = &quot;ab&quot;\nOutput: 1</code></pre>\n<p><strong>Constraints:</strong></p>\n<ul>\n<li><code>1 &lt;= s.length &lt;= 2000</code></li>\n<li><code>s</code> consists of lower-case English letters only.</li>\n</ul>\n</blockquote>\n<p>　　初识回文字符串的时候，觉得这是个很棘手的问题，毕竟涉及到正序逆序的检索问题，光是字符串匹配和正则表达式都能整出DFA和NFA这种复杂的状态机，何况是回文字符这种感觉状态机似乎都没什么办法的问题。没想到的是，回文字符的识别手段如此简单粗暴，就是寻找中心点，然后从中点向外的边扩展边匹配，原理上是没问题的，但是一般在讲解玩暴力解法之后，都会给出更加优雅精妙的优化版算法，但是在回文字符串的问题上，似乎到这里就戛然而止了，可能跟自己接触回文字符串的方式有关，因为自己并不是在教材上看到这个问题的，而是直接在OJ上面临这个问题的，最初理所当然的用暴力解法先过一遍，没想到居然AC了，后面就没有对这个问题进行过更多的思考了，直到遇到这个加强版的回文字符问题，检测substring是否是回文字符，并且暴力解法完全无法AC，这才逼迫自己想办法优化暴力解法。</p>\n<h3 id=\"问题解析\"><a href=\"#问题解析\" class=\"headerlink\" title=\"问题解析\"></a>问题解析</h3><p>　　substring 回文字符检测，在132题之前，其实有一个131 Palindrome Partitioning I 的 medium版本，要求输出所有可能的s 的 substring 切分，并且以列表的形式返回所有可能的切分，原理上此问题是131问题的子集，因为既然都已经输出了所有可能的切分，那么搜索最短切分只需要列出所有切分的长度，取最小即可；然而在OJ的设置上，这样是没办法在132上AC的，因为我在131用的仍然是暴力的解法小改版，只不过在切分的时候发现可能会面临重复递归调用的问题，也就是动态规划DP常常解释的，重复子结构的问题，我采取的备忘录的方式来缓解自顶向下调用时的重复调用问题，在完全没有构思出DP递推式 的前提下，就这样压线AC了，搞得我自己都不知道，这个AC的代码究竟用的是递归还是DP。</p>\n<pre><code class=\"python\"># 131 Palindrome Partitioning I\nclass Solution:\n    def partition(self, s: str) -&gt; List[List[str]]:\n        res = []\n        memo = &#123;&#125;\n        self.splitPalindrome(s, [], res, memo)\n        return res\n\n    def isPalindrome(self, s, memo):\n        if len(s) == 0:\n            return False\n        if len(s) == 1:\n            return True\n\n        if s in memo:\n            return memo[s]\n\n        news = &#39;_&#39;.join(s)\n        mid = len(news) // 2\n        for i in range(1,mid+1):\n            pre = mid - i\n            post = mid + i \n            if news[pre] != news[post]:\n                memo[s] = False\n                return False\n        memo[s] = True\n        return True\n\n    def splitPalindrome(self, s, rs, res,memo):\n        if len(s) == 0:\n            res.append(rs)\n            return\n        if len(s) == 1:\n            res.append(rs+[s])\n            return\n\n        for i in range(1,len(s)+1):\n            if self.isPalindrome(s[ :i], memo):\n                self.splitPalindrome(s[i: ], rs + [s[ :i]], res, memo)\n        return\n</code></pre>\n<p>　　主要思路还是围绕DFS为主，通过不断的切分s，达到给rs添加成员的目的，直到最后s划分完毕，添加到res中，围绕一个s不断划分是有指数级别的可能性，针对不同的切分区域可能会有相同的substring出现，用memo来缓存，最终AC.</p>\n<p>　　相同的思路在此题是行不通的，这也是这题是Hard的缘由，故意设置的陷阱，所以就需要找到可以优化的点。</p>\n<p>　　首先，搜索最小切分次数和遍历所有切分可能，其实还是有比较大的优化空间，因为前者的达到最小切分条件即可立即返回，后者则是需要遍历所有可能才返回。难点就是如何定义最小切分条件，其实最小切分条件这个定义似乎跟最优子结构的定义很相似，比如，算法导论 讲到 动态规划时，解释最优子结构的核心的时候用到的切钢筋的示例，给定一个长度为n的钢筋，其最优的切割方法实际是确定的，这个确定的切割方法就可以作为一个结果供后续上层调用，即是，最优解一定由子问题的最优解组成。问题是，除了长度n的约束外，最主要的约束条件是string的起点和终点，即$s[i][j]$ ，因为不同于切钢筋，不同部位的的钢筋，只需要长度一样，就看作同样的；这里不同的substring，是否是回文字符的可能性不同，那么这就涉及到是否可以的切分的一个约束条件，也就是作为最优子结构，还有一个前提的是否是回文字符的条件，这就需要知道所有可能的$s[i][j]$是否是回文字符串的情况，貌似这个问题本身也是一个至少需要$O(N^2)$的复杂度的问题，因为遍历所有可能的substring 至少需要$O(N^2)$；其实当初分析到这里，心里觉得多少有点没底，所以也就没朝着这个方向努力，还在想各种其他Trick，最后都没有成功，还是回到了先如何实现判断所有substring是否是回文的方向上来。</p>\n<h4 id=\"最长回文子串问题-Longest-Palindrome-Substring\"><a href=\"#最长回文子串问题-Longest-Palindrome-Substring\" class=\"headerlink\" title=\"最长回文子串问题 Longest Palindrome Substring\"></a>最长回文子串问题 Longest Palindrome Substring</h4><p>　　曾经看过一篇<a href=\"https://segmentfault.com/a/1190000003914228\">最长回文子串的博文</a>，除了1.暴力解法和2.在间隙中添加标记字符来作为对称中点来解决奇数和偶数字符串对称轴不同的问题的$O(N^2)$解法，还有一种在解法2的基础上寻找非常细微的优化点的Manacher算法，不过这个算法过于精细，以至于我只记得大概的算法优化点，完全不记得细节了，所以没办法用上，这个之后再讨论；但是篇博文最后的动态规划解法比较好理解，直接就是遍历$s[i][j]$的上三角部分，然后根据dp递推式$dp[i][j] = dp[i+1][j-1] ,if \\quad s[i]==s[j] $ ，由于$dp[i][j]==dp[j][i]$，所以只需要求出上三角即可</p>\n<pre><code class=\"python\"># dp 求substring 上三角是否是 回文字符\n    def longestPalindrome(self, s):\n        length = len(s)\n        dp = [[False]*length for _ in range(length)]\n        for i in range(length):\n            j = i\n            while j &gt;= 0:\n                if s[j] == s[i] and (i-j&lt;2 or dp[j+1][i-1]):\n                    dp[j][i] = True\n                j -= 1\n        return dp</code></pre>\n<p>时间复杂度$O(N^2)$，似乎和改进后的暴力解法差不多，不过够用了。</p>\n<p>后来受到这个启发，发现改进的暴力解法$O(N^2)$也可以AC，这里也直接给出</p>\n<pre><code class=\"python\"># 普通 O(N**2) 解法\n    def longestPalindrome(self, s):\n        length = len(s)\n        d = [[False]*length for _ in range(length)]\n\n        news = &#39;_&#39;.join(s)\n        for i in range(len(news)):\n            d[i//2][i//2] = True\n            end = min(i, len(news)-i)\n            flag = True\n            for j in range(1,end+1):\n                pre = i-j\n                post = i+j\n\n                if news[pre] != &#39;_&#39; and  flag:\n                    if  news[pre] == news[post]:\n                        d[pre//2][post//2] = True\n                    else:\n                        flag = False\n        return d</code></pre>\n<h4 id=\"DFS搜索最短切分\"><a href=\"#DFS搜索最短切分\" class=\"headerlink\" title=\"DFS搜索最短切分\"></a>DFS搜索最短切分</h4><p>　　接下来就是常规的，DFS来切分并累加切分次数，跟131题套路类似，都需要判断切分前是否直接本身就是回文，必须满足这个条件之后才能继续切分</p>\n<pre><code class=\"python\"># s2-1 自顶向下 备忘录dp AC 456ms\n    def calLength(self, start, s, dp, memo):\n        if start == len(s):\n            return 0\n        if start == len(s)-1:\n            memo[start][len(s)-1] = 1\n            return 1\n        if dp[start][len(s)-1]:\n            memo[start][len(s)-1] = 1\n            return 1\n        if memo[start][len(s)-1] != 0:\n            return memo[start][len(s)-1]\n\n        res = float(&#39;inf&#39;)\n        for i in range(start, len(s)):\n            if dp[start][i] :\n                res = min(res, self.calLength(i+1, s, dp , memo))\n        memo[start][len(s)-1] = res + 1\n        return memo[start][len(s)-1]</code></pre>\n<p>　最后成功AC</p>\n<p>　除了DFS，还在想是否可以改造成DP算法，顺便把之前没有写成的递推公式也顺便推导一下，这个阶段的DP和前面求最长回文子串不是一个问题，所以递推式需要重新归纳总结：</p>\n<p>$dp[i][j] = min(dp[i][k] + dp[k+1][j] + 1) ,if \\quad s[i:j+1]$不是回文字符串</p>\n<p>由于需要用到 dp 中i-j 之间的所有值，可以采用类似希尔排序的类似的逐渐递增gap的方式，每个新的gap计算之前，旧的更小的gap的所有片段dp值都已经计算出来</p>\n<pre><code class=\"python\">def calLength(self, s, dp ):\n        length = len(s)\n        dp_2 = [[0]*length for _ in range(length)]\n        if dp[0][length-1]:\n            dp_2[0][-1] = 0\n            return dp_2\n        # s1 gap dp\n        for gap in range(1, length):\n            for i in range(length-gap):\n                j = i+gap\n                if not dp[i][j]:\n                    r = float(&#39;inf&#39;)\n                    for k in range(i, j):\n                        r = min(r, dp_2[i][k] + dp_2[k+1][j] + 1)\n\n                    dp_2[i][j] = r\n                return dp_2</code></pre>\n<p>时间复杂度$O(N^3)$，结果自然也是TLE，不过目前至少没看到算法层面的错误，或许能有办法优化成$O(N^2)$的算法，目前还没看出怎么改进。</p>\n<h3 id=\"Manacher算法\"><a href=\"#Manacher算法\" class=\"headerlink\" title=\"Manacher算法\"></a>Manacher算法</h3><p>　　从原来上来讲，其实这应该是很微妙的一个优化，就是一步一步计算每一个字符包括间隙为中心点的回文字符的最大长度RL[i]，由于i之前的RL都是已经计算出来的，寻找某个包括i的pos为中心的最长RL[pos]，此时以pos为对称轴，寻找i的对称点j</p>\n<p><img src=\"1.png\" alt=\"1\"></p>\n<p><img src=\"2.png\" alt=\"2\"></p>\n<p>由于RL[j]已知，那么这个RL[j]就可以做RL[i]的起点，这样RL[i]就不需要从0开始计算，但是由于RL[j]的长度可能会超过pos，这样就需要分2种情况来寻找R[i]的起点,一种是较短的情况，如上图，另一种是超过pos，如下图，就需要降低RL[i]的起点为从蓝色部分开始</p>\n<p><img src=\"3.png\" alt=\"3\"></p>\n<p>当然还有最后一种情况，就是i已经超过了maxRight ，则需要从0开始计算<img src=\"4.png\" alt=\"4\"></p>\n<pre><code class=\"python\">def manacher(s):\n  s = &#39;_&#39;.join(s)\n  rl = [0]*len(s)\n  maxRight = 0\n  pos = 0\n  maxLen = 0\n\n  for i in range(len(s)):\n    if i &lt; maxRight:\n      rl[i] = min(rl[2*pos-i], maxRight-i)\n    else:\n      rl[i] = 1\n    # 尝试扩展边界rl[i]\n    while i-rl[i]&gt;=0 and i+rl[i]&lt;len(s) and s[i-rl[i]]==s[i+rl[i]]:\n      rl[i] += 1\n\n    if rl[i]+i-1&gt;maxRight:\n      maxRight = rl[i] + i -1\n      pos = i\n\n    maxLen = max(maxLen, rl[i])\n  return maxLen-1</code></pre>\n<p>虽说声称时间复杂度是$O(N)$，但是我看代码怎么都像是$O(N^2)$，起码最坏的情况下我想应该是的，假如每次求得的对称RL[j]都接近0，那么RL[i]就得从头遍历，但是另一方面，RL[j]本身就很快的返回了，如果RL[j]很长，那么对应的RL[i]就收益，可以高起点开始计算；从这个角度来看，起码能节省一半的时间，正常情况下应该介于$O(N)$和$O(N^2)$之间，真实情况如何，我也不是很确定。</p>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>　　终于有机会把回文字符这块好好的补习了一下，初识manacher算法认真来讲，是有点失望的，相比于NFA还有最短路径之类的神奇算法，这个有点小打小闹的优化改进，不过还是不得不佩服，就这么一个$O(N^2)$的算法，硬是被人找到机会给优化掉了，真的不能小看程序员想要改进算法的决心啊！</p>\n<p>Reference:</p>\n<p><a href=\"https://segmentfault.com/a/1190000003914228\">最长回文子串-Manacher算法</a></p>\n"},{"title":"Python 下的多进程和多线程编程","toc":true,"mathjax":true,"top":false,"cover":true,"date":"2020-12-30T06:22:34.000Z","updated":"2021-01-04T03:28:00.190Z","_content":"\n　　之前在并发写日志文件的时候简单的梳理了一下同步和异步的机制，不过对于Python的并发实践涉及还是比较浅显，现在这篇文章希望能彻底从代码角度来重新理解并发编程。\n\n## 什么是进程(Process) 和 线程(Thread)\n\n> 进程是操作系统分配资源的最小单元\n>\n> 线程是操作系统调度的最小单元\n>\n> 一个应用至少包括一个进程，而一个进程包含至少一个线程\n\n　　通常来说，多进程和多线程都是实现多任务的方式，而多任务的设计无论是进程还是线程，都遵循Master-Worker 模式，Master负责分配任务，Worker负责执行任务。在Python编程模型下，这二者的实践方法也非常类似。\n\n　　从效果上来看，进程间的独立性更高一点，这体现在一个进程崩溃，不会影响其他进程和主进程的执行，比如，Apache的多进程模式。另一方面，进程由于是OS直接创建和调度的，所以相同的代码可能在不同OS下效果会有差异，这体现在OS对同时并发的进程数目是有一个数目限制的，一般情况下，这是一个经验数目的限制，并不是你的系统无法承载这么多的进程数，虽然在大部分情况下，这个限制都是日常使用中无法达到的上界，但是我认为这就跟IPv4 和 32 位系统类似，只是在当时的条件下，定下的一个无法轻易达到的上限标准，在高速的发展迭代下，成为的日后亟待解决的遗留问题。其次，进程的创建开销也因OS不同也有所差异，比如Unix/Linux 的fork开销就远小于WIndows的进程创建。\n\n　　多线程模式则是多进程的精细化调度，我的理解是，这源于超线程技术的发展，使得一个CPU内核通过常用的寄存器和Cache等CPU内部常用部件的堆叠，达到模拟出2个CPU内核的效果，这当然是一个很优秀的技术，因为确实从软件层面可以把一个CPU线程当作内核直接调用，同时性能上也获得了几乎线性的提升，甚至在Unix/Linux查看CPU参数的指令lscpu下，CPU(s)都是直接显示的内核的线程数目，而非内核数目，常常让初识Linux的我感到非常迷惑。我相信，基于超线程的性能提升，编译器和各种软件的迭代是不可能忽视这么重要且好用的技术的。不过对于一个没有经历过这个技术更迭的人来说，什么多线程，多进程，本质上不是一个东西吗？搞得神神叨叨的。除此之外，大部分进程和线程的介绍，都基于定义的扩展，认真来讲，光看定义，就完全看不出这二者的差异，何况扩展。所以关于线程，虽然我知道的很多，比如说，线程是进程的执行单元，一个线程崩溃，进程则直接挂掉，所有线程共享进程的内存等等，但是却有完全不明白为什么会有这些设置。现在看来，当初没有搞明白这些，着实是有点遗憾，除了教育方式和教材的原因，自己的求知欲也被这些陡峭的学习曲线给淹没了，因为在一个每一个术语都不大理解的地方，哪有好奇心和求知欲的容身之处，早就被从小被训练好的敬畏心给覆盖了，不懂，可是我又不敢问！扯远了，回到线程上，从线程的起源来理解，这一些的设置就豁然开朗了，线程作为执行单元，和CPU提供的线程接口强相关的，而CPU本身就是整个计算机系统的最主要的执行部件，理所当然的线程更接近执行向，而当线程执行失败的时候，整个进程的执行理所当然的受阻挂掉，线程的作为执行者，应当获得进程所有的资源，所以进程下的线程都具备进程的全部资源权限。\n\n　　在OS层面，线程的实现是有区别的，Windows的多线程效率是比多进程要高的，所以微软的IIS服务器基于多线程，显而易见的稳定性问题不如Apache。不够随着逐步的发展，现在又都是多进程+多线程的混合模式，搞得确实头大。不过参照，RISC 和 CISC的发展，Intel 后来的x86 和x64虽然都仍然维持的了 CISC的接口，但是在微指令层面，仍然是RISC的技术，这也使得其仍然可以从RISC的发展中获益，想到，当初的计算机组成的教材，也是突兀的抛出了微指令的概念，最后学完微指令也摸不着头，直到看到计算机体系结构中关于RISC和CISC的部分，才从重新理解的微指令架构。这次层面上，进程和线程的在Apache和IIS的表现形式或许多少能得到一点启示。不过当今Nginx的事件驱动的异步IO设计大行其道，Python的协程多少还算是找对了方向。\n\n　　一个很好的关于进程和线程的比喻解释，形象的道出了二者的关系\n\n> - 计算机的核心是CPU，它承担了所有的计算任务。它就像一座工厂，时刻在运行。\n> - 假定工厂的电力有限，一次只能供给一个车间使用。也就是说，一个车间开工的时候，其他车间都必须停工。背后的含义就是，单个CPU一次只能运行一个任务。编者注: 多核的CPU就像有了多个发电厂，使多工厂(多进程)实现可能。\n> - 进程就好比工厂的车间，它代表CPU所能处理的单个任务。任一时刻，CPU总是运行一个进程，其他进程处于非运行状态。\n> - 一个车间里，可以有很多工人。他们协同完成一个任务。\n> - 线程就好比车间里的工人。一个进程可以包括多个线程。\n> - 车间的空间是工人们共享的，比如许多房间是每个工人都可以进出的。这象征一个进程的内存空间是共享的，每个线程都可以使用这些共享内存。\n> - 可是，每间房间的大小不同，有些房间最多只能容纳一个人，比如厕所。里面有人的时候，其他人就不能进去了。这代表一个线程使用某些共享内存时，其他线程必须等它结束，才能使用这一块内存。\n> - 一个防止他人进入的简单方法，就是门口加一把锁。先到的人锁上门，后到的人看到上锁，就在门口排队，等锁打开再进去。这就叫\"互斥锁\"（Mutual exclusion，缩写 Mutex），防止多个线程同时读写某一块内存区域。\n> - 还有些房间，可以同时容纳n个人，比如厨房。也就是说，如果人数大于n，多出来的人只能在外面等着。这好比某些内存区域，只能供给固定数目的线程使用。\n> - 这时的解决方法，就是在门口挂n把钥匙。进去的人就取一把钥匙，出来时再把钥匙挂回原处。后到的人发现钥匙架空了，就知道必须在门口排队等着了。这种做法叫做\"信号量\"（Semaphore），用来保证多个线程不会互相冲突。\n> - 不难看出，mutex是semaphore的一种特殊情况（n=1时）。也就是说，完全可以用后者替代前者。但是，因为mutex较为简单，且效率高，所以在必须保证资源独占的情况下，还是采用这种设计。\n\n来自阮一峰的博客[进程和线程的简单解释](http://www.ruanyifeng.com/blog/2013/04/processes_and_threads.html)\n\n\n\n## Python的多进程编程 multiprocessing\n\n　　首先给出单进程顺序执行的测试代码，给出一个计算$8^{20}$的任务，同时辅以sleep 2s的任务。\n\n```python\nimport time\nimport os\n\ndef long_time_task():\n    print('当前进程: {}'.format(os.getpid()))\n    time.sleep(2)\n    print(\"结果: {}\".format(8 ** 20))\n\nif __name__ == \"__main__\":\n    print('当前母进程: {}'.format(os.getpid()))\n    start = time.time()\n    for i in range(2):\n        long_time_task()\n\n    end = time.time()\n    print(\"用时{}秒\".format((end-start)))\n    \n当前母进程: 33121\n当前进程: 33121\n结果: 1152921504606846976\n当前进程: 33121\n结果: 1152921504606846976\n用时4.004350185394287秒\n```\n\n　　基本是sleep 的4s，计算任务基本不耗时间\n\n　　接下来是多进程改写\n\n```python\nfrom multiprocessing import Process\nimport os\nimport time\n\n\ndef long_time_task(i):\n    print('子进程: {} - 任务{}'.format(os.getpid(), i))\n    time.sleep(2)\n    print(\"结果: {}\".format(8 ** 20))\n\nif __name__=='__main__':\n    print('当前母进程: {}'.format(os.getpid()))\n    start = time.time()\n    p1 = Process(target=long_time_task, args=(1,))\n    p2 = Process(target=long_time_task, args=(2,))\n    print('等待所有子进程完成。')\n    p1.start()\n    p2.start()\n    p1.join()\n    p2.join()\n    end = time.time()\n    print(\"总共用时{}秒\".format((end - start)))\n    \n当前母进程: 33121\n等待所有子进程完成。\n子进程: 34270 - 任务1\n子进程: 34271 - 任务2\n结果: 1152921504606846976\n结果: 1152921504606846976\n总共用时2.009559392929077秒\n```\n\n　　并行的效率得到体现，执行时间减半。\n\n　　p.join()的理解是，主进程会等待子进程执行完毕，才开始继续从p.join()之后开始执行，否则主进程会直接输出总共用时，然后子进程接着执行完再输出。\n\n```\n当前母进程: 33121\n等待所有子进程完成。\n子进程: 34669 - 任务1\n总共用时0.002809762954711914秒\n子进程: 34670 - 任务2\n结果: 1152921504606846976\n结果: 1152921504606846976\n```\n\n　　最后就是对进程管理调度，由于OS的不同以及提高CPU利用率的需求，更是因为程序员懒得一个一个手动启动Process进程，产生的一个统一的进程管理的接口的需求，这就诞生的进程池Pool的接口。\n\n　　Pool类可以提供指定数量的进程供用户调用，当有新的请求提交到Pool中时，如果进程池还没有满，就会创建一个新的进程来执行请求。如果池满，请求就会告知先等待，直到池中有进程结束，才会创建新的进程来执行这些请求。\n\n下面介绍一下multiprocessing 模块下的Pool类的几个方法：\n\n1. apply_async\n\n函数原型：apply_async(func[, args=()[, kwds={}[, callback=None]]])\n\n其作用是向进程池提交需要执行的函数及参数， 各个进程采用非阻塞（异步）的调用方式，即每个子进程只管运行自己的，不管其它进程是否已经完成。这是默认方式。\n\n2. map()\n\n函数原型：map(func, iterable[, chunksize=None])\n\nPool类中的map方法，与内置的map函数用法行为基本一致，它会使进程阻塞直到结果返回。 注意：虽然第二个参数是一个迭代器，但在实际使用中，必须在整个队列都就绪后，程序才会运行子进程。\n\n3. map_async()\n\n函数原型：map_async(func, iterable[, chunksize[, callback]])\n与map用法一致，但是它是非阻塞的。其有关事项见apply_async。\n\n4. close()\n\n关闭进程池（pool），使其不在接受新的任务。\n\n5. terminate()\n\n结束工作进程，不在处理未处理的任务。\n\n6. join()\n\n主进程阻塞等待子进程的退出， join方法要在close或terminate之后使用。\n\n\n\n　　这里设计一个有意思设计用例，来验证并观察进程的行为模式是否按照理论上的理解运行，即是创建大小为4的Pool，却同时启动5个计算任务，最后观察运行时间，理论上应该是开始的4个进程是并行，后面的任务等前面的进程空出来之后，再开始安排进程来计算。\n\n```python\nfrom multiprocessing import Pool, cpu_count\nimport os\nimport time\n\n\ndef long_time_task(i):\n    print('子进程: {} - 任务{}'.format(os.getpid(), i))\n    time.sleep(2)\n    print(\"结果: {}\".format(8 ** 20))\n\n\nif __name__=='__main__':\n    print(\"CPU内核数:{}\".format(cpu_count()))\n    print('当前母进程: {}'.format(os.getpid()))\n    start = time.time()\n    p = Pool(4)\n    for i in range(5):\n        p.apply_async(long_time_task, args=(i,))\n    print('等待所有子进程完成。')\n    p.close()\n    p.join()\n    end = time.time()\n    print(\"总共用时{}秒\".format((end - start)))\n    \nCPU内核数:64\n当前母进程: 33121\n等待所有子进程完成。\n子进程: 37454 - 任务0\n子进程: 37455 - 任务1\n子进程: 37456 - 任务2\n子进程: 37457 - 任务3\n结果: 1152921504606846976\n结果: 1152921504606846976\n子进程: 37454 - 任务4\n结果: 1152921504606846976\n结果: 1152921504606846976\n结果: 1152921504606846976\n总共用时4.115360736846924秒\n```\n\n结果跟预想的基本一致，是并行的4个任务的2s+后续一个任务的2s，总共4s+\n\n由于Python的GIL（全局解释器锁）的存在，多线程的代码实际上一个时刻只有一个线程在执行，所以如果要充分利用多核CPU资源，一般都是通过多进程来实现的。\n\n　　Add，多进程的数据共享和通信，multiprocessing.Queue 使用实例\n\n```python\nfrom multiprocessing import Process, Queue\nimport os, time, random\n\n# 写数据进程执行的代码:\ndef write(q):\n    print('Process to write: {}'.format(os.getpid()))\n    for value in ['A', 'B', 'C']:\n        print('Put %s to queue...' % value)\n        q.put(value)\n        time.sleep(random.random())\n\n# 读数据进程执行的代码:\ndef read(q):\n    print('Process to read:{}'.format(os.getpid()))\n    while True:\n        value = q.get(True)\n        print('Get %s from queue.' % value)\n\nif __name__=='__main__':\n    # 父进程创建Queue，并传给各个子进程：\n    q = Queue()\n    pw = Process(target=write, args=(q,))\n    pr = Process(target=read, args=(q,))\n    # 启动子进程pw，写入:\n    pw.start()\n    # 启动子进程pr，读取:\n    pr.start()\n    # 等待pw结束:\n    pw.join()\n    # pr进程里是死循环，无法等待其结束，只能强行终止:\n    pr.terminate()\n    \nProcess to write: 39190\nPut A to queue...\nProcess to read:39191\nGet A from queue.\nPut B to queue...\nGet B from queue.\nPut C to queue...\nGet C from queue.\n```\n\n##\n\n## Python 的多线程编程 Threading \n\n　　接口其实跟多进程一样\n\n```python\nimport threading\nimport time\n\n\ndef long_time_task(i):\n    print('当前子线程: {} - 任务{}'.format(threading.current_thread().name, i))\n    time.sleep(2)\n    print(\"结果: {}\".format(8 ** 20))\n\n\nif __name__=='__main__':\n    start = time.time()\n    print('这是主线程：{}'.format(threading.current_thread().name))\n    t1 = threading.Thread(target=long_time_task, args=(1,))\n    t2 = threading.Thread(target=long_time_task, args=(2,))\n    t1.start()\n    t2.start()\n    \n    t1.join()\n    t2.join()\n    end = time.time()\n    print(\"总共用时{}秒\".format((end - start)))\n    \n这是主线程：MainThread\n当前子线程: Thread-1316 - 任务1\n当前子线程: Thread-1317 - 任务2\n结果: 1152921504606846976\n结果: 1152921504606846976\n总共用时2.0029757022857666秒\n```\n\n连结果都跟多进程一样，不是说GIL导致始终是单线程的性能吗？观察任务的组成会发现，计算任务基本不耗时间，主要是sleep ,但是sleep在运行中是会被当作类似IO的操作，被识别为闲置的线程，这时候GIL直接就开始切换线程了，所以sleep的时间实际上是并行执行的，因为不依赖CPU计算，依此推测计算部分还是应该是单线程的，因为依赖CPU，那只需要增加计算任务的时间即可，那就加大指数为 $8^{1000000}$\n\n```python\ndef long_time_task(i):\n    print('当前子线程: {} - 任务{}'.format(threading.current_thread().name, i))\n    time.sleep(2)\n    t = time.time()\n    print(\"结果: {}\".format(8 ** 1000000))\n    print('计算用时： ', time.time()-t)\n# 单独执行时间 long_time_task(1)\n计算时间 : 11.462863683700562\n总共用时13.43696641921997秒\n# 多线程时间 \n计算时间 : 22.85654377937317\n计算时间 : 23.088494300842285\n总共用时25.10121512413025秒\n\n```\n\n结果上，总计算时间是翻倍的，但是每个线程的输出时间却是翻倍后的22s，而不是单线程的11s，考虑到并发执行，每个线程执行到任何一个时间，都有可能被中断，切换到其他进程，这里的单线程输出时间就可以理解了，那就是线程1执行玩计算任务，还没输出时间，就被切换到线程2的计算来了，最后统一输出线程1和线程2的时间，那么结果二者都是22s左右。总之，可以确定的计算密集型任务，Python的GIL充分保证的单线程的性能。\n\n　　多线程的数据共享和通信，寻找线程安全的数据结构即可，queue.Queue就是的\n\n```python\nfrom queue import Queue\nimport random, threading, time\n\n\n# 生产者类\nclass Producer(threading.Thread):\n    def __init__(self, name, queue):\n        threading.Thread.__init__(self, name=name)\n        self.queue = queue\n\n    def run(self):\n        for i in range(1, 5):\n            print(\"{} is producing {} to the queue!\".format(self.getName(), i))\n            self.queue.put(i)\n            time.sleep(random.randrange(10) / 5)\n        print(\"%s finished!\" % self.getName())\n\n\n# 消费者类\nclass Consumer(threading.Thread):\n    def __init__(self, name, queue):\n        threading.Thread.__init__(self, name=name)\n        self.queue = queue\n\n    def run(self):\n        for i in range(1, 5):\n            val = self.queue.get()\n            print(\"{} is consuming {} in the queue.\".format(self.getName(), val))\n            time.sleep(random.randrange(10))\n        print(\"%s finished!\" % self.getName())\n\n\ndef main():\n    queue = Queue()\n    producer = Producer('Producer', queue)\n    consumer = Consumer('Consumer', queue)\n\n    producer.start()\n    consumer.start()\n    \n    producer.join()\n    consumer.join()\n    print('All threads finished!')\n\n\nif __name__ == '__main__':\n    main()\n```\n\n\n\n## 计算密集型 vs IO密集型\n\n　　对于计算密集型任务，python下的多线程改写是不会有性能提升的，只有多进程才能利用多核CPU，这是明显区别于其他语言的特点，因为在其他高效语言中，多进程和多线程都是作为并发的手段，同时改善程序性能的，而且本身Python作为脚本语言，在计算密集型任务面前运行效率就不高，此时更高阶的手段是用C语言重构 或者 引入高效的其他库来代替。\n\n　　对于IO密集型，Python本身的多线程就能较好的应对，因为线程调度保证了IO任务的并行执行，此时就算是CPU有多个线程资源，任务本身就无法充分利用，所以Python的单线程限制并不构成瓶颈，就算用其他语言改写成真多线程，提升也不明显，所以类似web应用的IO密集型任务，Python足矣。\n\n## 异步IO ，事件驱动模型， 协程，单进程单线程模型\n\n　　考虑到CPU和IO之间巨大的速度差异，一个任务在执行的过程中大部分时间都在等待IO操作，单进程单线程模型会导致别的任务无法并行执行，因此，才需要多进程模型或者多线程模型来支持多任务并发执行。\n\n　　现代操作系统对IO操作已经做了巨大的改进，最大的特点就是支持异步IO。如果充分利用操作系统提供的异步IO支持，就可以用单进程单线程模型来执行多任务，这种全新的模型称为事件驱动模型，Nginx就是支持异步IO的Web服务器，它在单核CPU上采用单进程模型就可以高效地支持多任务。在多核CPU上，可以运行多个进程（数量与CPU核心数相同），充分利用多核CPU。由于系统总的进程数量十分有限，因此操作系统调度非常高效。用异步IO编程模型来实现多任务是一个主要的趋势。\n\n　　对应到Python语言，单线程的异步编程模型称为协程，有了协程的支持，就可以基于事件驱动编写高效的多任务程序。\n\n\n\n## Conclude\n\n　　用代码完整的梳理了一下并发下的进程和线程，同时总结了一下使用场景，很多知识都连接起来了，接下来一个重要的部分就是Python的核心部分，异步IO编程模型-协程\n\n　　关于协程的实践部分，写篇文章会主要介绍。\n\n\n\nReferences:\n\n[一文看懂Python多进程与多线程编程](https://zhuanlan.zhihu.com/p/46368084)\n\n[进程 vs 线程 廖雪峰](https://www.liaoxuefeng.com/wiki/1016959663602400/1017631469467456)\n\n[线程间通信 Python3 CookBook](https://python3-cookbook.readthedocs.io/zh_CN/latest/c12/p03_communicating_between_threads.html)\n\n","source":"_posts/Python-下的多进程和多线程编程.md","raw":"---\ntitle: Python 下的多进程和多线程编程\ntoc: true\nmathjax: true\ntop: false\ncover: true\ndate: 2020-12-30 14:22:34\nupdated:\ncategories: Algorithms\ntags:\n\t- Algorithms\n\t- OS\n---\n\n　　之前在并发写日志文件的时候简单的梳理了一下同步和异步的机制，不过对于Python的并发实践涉及还是比较浅显，现在这篇文章希望能彻底从代码角度来重新理解并发编程。\n\n## 什么是进程(Process) 和 线程(Thread)\n\n> 进程是操作系统分配资源的最小单元\n>\n> 线程是操作系统调度的最小单元\n>\n> 一个应用至少包括一个进程，而一个进程包含至少一个线程\n\n　　通常来说，多进程和多线程都是实现多任务的方式，而多任务的设计无论是进程还是线程，都遵循Master-Worker 模式，Master负责分配任务，Worker负责执行任务。在Python编程模型下，这二者的实践方法也非常类似。\n\n　　从效果上来看，进程间的独立性更高一点，这体现在一个进程崩溃，不会影响其他进程和主进程的执行，比如，Apache的多进程模式。另一方面，进程由于是OS直接创建和调度的，所以相同的代码可能在不同OS下效果会有差异，这体现在OS对同时并发的进程数目是有一个数目限制的，一般情况下，这是一个经验数目的限制，并不是你的系统无法承载这么多的进程数，虽然在大部分情况下，这个限制都是日常使用中无法达到的上界，但是我认为这就跟IPv4 和 32 位系统类似，只是在当时的条件下，定下的一个无法轻易达到的上限标准，在高速的发展迭代下，成为的日后亟待解决的遗留问题。其次，进程的创建开销也因OS不同也有所差异，比如Unix/Linux 的fork开销就远小于WIndows的进程创建。\n\n　　多线程模式则是多进程的精细化调度，我的理解是，这源于超线程技术的发展，使得一个CPU内核通过常用的寄存器和Cache等CPU内部常用部件的堆叠，达到模拟出2个CPU内核的效果，这当然是一个很优秀的技术，因为确实从软件层面可以把一个CPU线程当作内核直接调用，同时性能上也获得了几乎线性的提升，甚至在Unix/Linux查看CPU参数的指令lscpu下，CPU(s)都是直接显示的内核的线程数目，而非内核数目，常常让初识Linux的我感到非常迷惑。我相信，基于超线程的性能提升，编译器和各种软件的迭代是不可能忽视这么重要且好用的技术的。不过对于一个没有经历过这个技术更迭的人来说，什么多线程，多进程，本质上不是一个东西吗？搞得神神叨叨的。除此之外，大部分进程和线程的介绍，都基于定义的扩展，认真来讲，光看定义，就完全看不出这二者的差异，何况扩展。所以关于线程，虽然我知道的很多，比如说，线程是进程的执行单元，一个线程崩溃，进程则直接挂掉，所有线程共享进程的内存等等，但是却有完全不明白为什么会有这些设置。现在看来，当初没有搞明白这些，着实是有点遗憾，除了教育方式和教材的原因，自己的求知欲也被这些陡峭的学习曲线给淹没了，因为在一个每一个术语都不大理解的地方，哪有好奇心和求知欲的容身之处，早就被从小被训练好的敬畏心给覆盖了，不懂，可是我又不敢问！扯远了，回到线程上，从线程的起源来理解，这一些的设置就豁然开朗了，线程作为执行单元，和CPU提供的线程接口强相关的，而CPU本身就是整个计算机系统的最主要的执行部件，理所当然的线程更接近执行向，而当线程执行失败的时候，整个进程的执行理所当然的受阻挂掉，线程的作为执行者，应当获得进程所有的资源，所以进程下的线程都具备进程的全部资源权限。\n\n　　在OS层面，线程的实现是有区别的，Windows的多线程效率是比多进程要高的，所以微软的IIS服务器基于多线程，显而易见的稳定性问题不如Apache。不够随着逐步的发展，现在又都是多进程+多线程的混合模式，搞得确实头大。不过参照，RISC 和 CISC的发展，Intel 后来的x86 和x64虽然都仍然维持的了 CISC的接口，但是在微指令层面，仍然是RISC的技术，这也使得其仍然可以从RISC的发展中获益，想到，当初的计算机组成的教材，也是突兀的抛出了微指令的概念，最后学完微指令也摸不着头，直到看到计算机体系结构中关于RISC和CISC的部分，才从重新理解的微指令架构。这次层面上，进程和线程的在Apache和IIS的表现形式或许多少能得到一点启示。不过当今Nginx的事件驱动的异步IO设计大行其道，Python的协程多少还算是找对了方向。\n\n　　一个很好的关于进程和线程的比喻解释，形象的道出了二者的关系\n\n> - 计算机的核心是CPU，它承担了所有的计算任务。它就像一座工厂，时刻在运行。\n> - 假定工厂的电力有限，一次只能供给一个车间使用。也就是说，一个车间开工的时候，其他车间都必须停工。背后的含义就是，单个CPU一次只能运行一个任务。编者注: 多核的CPU就像有了多个发电厂，使多工厂(多进程)实现可能。\n> - 进程就好比工厂的车间，它代表CPU所能处理的单个任务。任一时刻，CPU总是运行一个进程，其他进程处于非运行状态。\n> - 一个车间里，可以有很多工人。他们协同完成一个任务。\n> - 线程就好比车间里的工人。一个进程可以包括多个线程。\n> - 车间的空间是工人们共享的，比如许多房间是每个工人都可以进出的。这象征一个进程的内存空间是共享的，每个线程都可以使用这些共享内存。\n> - 可是，每间房间的大小不同，有些房间最多只能容纳一个人，比如厕所。里面有人的时候，其他人就不能进去了。这代表一个线程使用某些共享内存时，其他线程必须等它结束，才能使用这一块内存。\n> - 一个防止他人进入的简单方法，就是门口加一把锁。先到的人锁上门，后到的人看到上锁，就在门口排队，等锁打开再进去。这就叫\"互斥锁\"（Mutual exclusion，缩写 Mutex），防止多个线程同时读写某一块内存区域。\n> - 还有些房间，可以同时容纳n个人，比如厨房。也就是说，如果人数大于n，多出来的人只能在外面等着。这好比某些内存区域，只能供给固定数目的线程使用。\n> - 这时的解决方法，就是在门口挂n把钥匙。进去的人就取一把钥匙，出来时再把钥匙挂回原处。后到的人发现钥匙架空了，就知道必须在门口排队等着了。这种做法叫做\"信号量\"（Semaphore），用来保证多个线程不会互相冲突。\n> - 不难看出，mutex是semaphore的一种特殊情况（n=1时）。也就是说，完全可以用后者替代前者。但是，因为mutex较为简单，且效率高，所以在必须保证资源独占的情况下，还是采用这种设计。\n\n来自阮一峰的博客[进程和线程的简单解释](http://www.ruanyifeng.com/blog/2013/04/processes_and_threads.html)\n\n\n\n## Python的多进程编程 multiprocessing\n\n　　首先给出单进程顺序执行的测试代码，给出一个计算$8^{20}$的任务，同时辅以sleep 2s的任务。\n\n```python\nimport time\nimport os\n\ndef long_time_task():\n    print('当前进程: {}'.format(os.getpid()))\n    time.sleep(2)\n    print(\"结果: {}\".format(8 ** 20))\n\nif __name__ == \"__main__\":\n    print('当前母进程: {}'.format(os.getpid()))\n    start = time.time()\n    for i in range(2):\n        long_time_task()\n\n    end = time.time()\n    print(\"用时{}秒\".format((end-start)))\n    \n当前母进程: 33121\n当前进程: 33121\n结果: 1152921504606846976\n当前进程: 33121\n结果: 1152921504606846976\n用时4.004350185394287秒\n```\n\n　　基本是sleep 的4s，计算任务基本不耗时间\n\n　　接下来是多进程改写\n\n```python\nfrom multiprocessing import Process\nimport os\nimport time\n\n\ndef long_time_task(i):\n    print('子进程: {} - 任务{}'.format(os.getpid(), i))\n    time.sleep(2)\n    print(\"结果: {}\".format(8 ** 20))\n\nif __name__=='__main__':\n    print('当前母进程: {}'.format(os.getpid()))\n    start = time.time()\n    p1 = Process(target=long_time_task, args=(1,))\n    p2 = Process(target=long_time_task, args=(2,))\n    print('等待所有子进程完成。')\n    p1.start()\n    p2.start()\n    p1.join()\n    p2.join()\n    end = time.time()\n    print(\"总共用时{}秒\".format((end - start)))\n    \n当前母进程: 33121\n等待所有子进程完成。\n子进程: 34270 - 任务1\n子进程: 34271 - 任务2\n结果: 1152921504606846976\n结果: 1152921504606846976\n总共用时2.009559392929077秒\n```\n\n　　并行的效率得到体现，执行时间减半。\n\n　　p.join()的理解是，主进程会等待子进程执行完毕，才开始继续从p.join()之后开始执行，否则主进程会直接输出总共用时，然后子进程接着执行完再输出。\n\n```\n当前母进程: 33121\n等待所有子进程完成。\n子进程: 34669 - 任务1\n总共用时0.002809762954711914秒\n子进程: 34670 - 任务2\n结果: 1152921504606846976\n结果: 1152921504606846976\n```\n\n　　最后就是对进程管理调度，由于OS的不同以及提高CPU利用率的需求，更是因为程序员懒得一个一个手动启动Process进程，产生的一个统一的进程管理的接口的需求，这就诞生的进程池Pool的接口。\n\n　　Pool类可以提供指定数量的进程供用户调用，当有新的请求提交到Pool中时，如果进程池还没有满，就会创建一个新的进程来执行请求。如果池满，请求就会告知先等待，直到池中有进程结束，才会创建新的进程来执行这些请求。\n\n下面介绍一下multiprocessing 模块下的Pool类的几个方法：\n\n1. apply_async\n\n函数原型：apply_async(func[, args=()[, kwds={}[, callback=None]]])\n\n其作用是向进程池提交需要执行的函数及参数， 各个进程采用非阻塞（异步）的调用方式，即每个子进程只管运行自己的，不管其它进程是否已经完成。这是默认方式。\n\n2. map()\n\n函数原型：map(func, iterable[, chunksize=None])\n\nPool类中的map方法，与内置的map函数用法行为基本一致，它会使进程阻塞直到结果返回。 注意：虽然第二个参数是一个迭代器，但在实际使用中，必须在整个队列都就绪后，程序才会运行子进程。\n\n3. map_async()\n\n函数原型：map_async(func, iterable[, chunksize[, callback]])\n与map用法一致，但是它是非阻塞的。其有关事项见apply_async。\n\n4. close()\n\n关闭进程池（pool），使其不在接受新的任务。\n\n5. terminate()\n\n结束工作进程，不在处理未处理的任务。\n\n6. join()\n\n主进程阻塞等待子进程的退出， join方法要在close或terminate之后使用。\n\n\n\n　　这里设计一个有意思设计用例，来验证并观察进程的行为模式是否按照理论上的理解运行，即是创建大小为4的Pool，却同时启动5个计算任务，最后观察运行时间，理论上应该是开始的4个进程是并行，后面的任务等前面的进程空出来之后，再开始安排进程来计算。\n\n```python\nfrom multiprocessing import Pool, cpu_count\nimport os\nimport time\n\n\ndef long_time_task(i):\n    print('子进程: {} - 任务{}'.format(os.getpid(), i))\n    time.sleep(2)\n    print(\"结果: {}\".format(8 ** 20))\n\n\nif __name__=='__main__':\n    print(\"CPU内核数:{}\".format(cpu_count()))\n    print('当前母进程: {}'.format(os.getpid()))\n    start = time.time()\n    p = Pool(4)\n    for i in range(5):\n        p.apply_async(long_time_task, args=(i,))\n    print('等待所有子进程完成。')\n    p.close()\n    p.join()\n    end = time.time()\n    print(\"总共用时{}秒\".format((end - start)))\n    \nCPU内核数:64\n当前母进程: 33121\n等待所有子进程完成。\n子进程: 37454 - 任务0\n子进程: 37455 - 任务1\n子进程: 37456 - 任务2\n子进程: 37457 - 任务3\n结果: 1152921504606846976\n结果: 1152921504606846976\n子进程: 37454 - 任务4\n结果: 1152921504606846976\n结果: 1152921504606846976\n结果: 1152921504606846976\n总共用时4.115360736846924秒\n```\n\n结果跟预想的基本一致，是并行的4个任务的2s+后续一个任务的2s，总共4s+\n\n由于Python的GIL（全局解释器锁）的存在，多线程的代码实际上一个时刻只有一个线程在执行，所以如果要充分利用多核CPU资源，一般都是通过多进程来实现的。\n\n　　Add，多进程的数据共享和通信，multiprocessing.Queue 使用实例\n\n```python\nfrom multiprocessing import Process, Queue\nimport os, time, random\n\n# 写数据进程执行的代码:\ndef write(q):\n    print('Process to write: {}'.format(os.getpid()))\n    for value in ['A', 'B', 'C']:\n        print('Put %s to queue...' % value)\n        q.put(value)\n        time.sleep(random.random())\n\n# 读数据进程执行的代码:\ndef read(q):\n    print('Process to read:{}'.format(os.getpid()))\n    while True:\n        value = q.get(True)\n        print('Get %s from queue.' % value)\n\nif __name__=='__main__':\n    # 父进程创建Queue，并传给各个子进程：\n    q = Queue()\n    pw = Process(target=write, args=(q,))\n    pr = Process(target=read, args=(q,))\n    # 启动子进程pw，写入:\n    pw.start()\n    # 启动子进程pr，读取:\n    pr.start()\n    # 等待pw结束:\n    pw.join()\n    # pr进程里是死循环，无法等待其结束，只能强行终止:\n    pr.terminate()\n    \nProcess to write: 39190\nPut A to queue...\nProcess to read:39191\nGet A from queue.\nPut B to queue...\nGet B from queue.\nPut C to queue...\nGet C from queue.\n```\n\n##\n\n## Python 的多线程编程 Threading \n\n　　接口其实跟多进程一样\n\n```python\nimport threading\nimport time\n\n\ndef long_time_task(i):\n    print('当前子线程: {} - 任务{}'.format(threading.current_thread().name, i))\n    time.sleep(2)\n    print(\"结果: {}\".format(8 ** 20))\n\n\nif __name__=='__main__':\n    start = time.time()\n    print('这是主线程：{}'.format(threading.current_thread().name))\n    t1 = threading.Thread(target=long_time_task, args=(1,))\n    t2 = threading.Thread(target=long_time_task, args=(2,))\n    t1.start()\n    t2.start()\n    \n    t1.join()\n    t2.join()\n    end = time.time()\n    print(\"总共用时{}秒\".format((end - start)))\n    \n这是主线程：MainThread\n当前子线程: Thread-1316 - 任务1\n当前子线程: Thread-1317 - 任务2\n结果: 1152921504606846976\n结果: 1152921504606846976\n总共用时2.0029757022857666秒\n```\n\n连结果都跟多进程一样，不是说GIL导致始终是单线程的性能吗？观察任务的组成会发现，计算任务基本不耗时间，主要是sleep ,但是sleep在运行中是会被当作类似IO的操作，被识别为闲置的线程，这时候GIL直接就开始切换线程了，所以sleep的时间实际上是并行执行的，因为不依赖CPU计算，依此推测计算部分还是应该是单线程的，因为依赖CPU，那只需要增加计算任务的时间即可，那就加大指数为 $8^{1000000}$\n\n```python\ndef long_time_task(i):\n    print('当前子线程: {} - 任务{}'.format(threading.current_thread().name, i))\n    time.sleep(2)\n    t = time.time()\n    print(\"结果: {}\".format(8 ** 1000000))\n    print('计算用时： ', time.time()-t)\n# 单独执行时间 long_time_task(1)\n计算时间 : 11.462863683700562\n总共用时13.43696641921997秒\n# 多线程时间 \n计算时间 : 22.85654377937317\n计算时间 : 23.088494300842285\n总共用时25.10121512413025秒\n\n```\n\n结果上，总计算时间是翻倍的，但是每个线程的输出时间却是翻倍后的22s，而不是单线程的11s，考虑到并发执行，每个线程执行到任何一个时间，都有可能被中断，切换到其他进程，这里的单线程输出时间就可以理解了，那就是线程1执行玩计算任务，还没输出时间，就被切换到线程2的计算来了，最后统一输出线程1和线程2的时间，那么结果二者都是22s左右。总之，可以确定的计算密集型任务，Python的GIL充分保证的单线程的性能。\n\n　　多线程的数据共享和通信，寻找线程安全的数据结构即可，queue.Queue就是的\n\n```python\nfrom queue import Queue\nimport random, threading, time\n\n\n# 生产者类\nclass Producer(threading.Thread):\n    def __init__(self, name, queue):\n        threading.Thread.__init__(self, name=name)\n        self.queue = queue\n\n    def run(self):\n        for i in range(1, 5):\n            print(\"{} is producing {} to the queue!\".format(self.getName(), i))\n            self.queue.put(i)\n            time.sleep(random.randrange(10) / 5)\n        print(\"%s finished!\" % self.getName())\n\n\n# 消费者类\nclass Consumer(threading.Thread):\n    def __init__(self, name, queue):\n        threading.Thread.__init__(self, name=name)\n        self.queue = queue\n\n    def run(self):\n        for i in range(1, 5):\n            val = self.queue.get()\n            print(\"{} is consuming {} in the queue.\".format(self.getName(), val))\n            time.sleep(random.randrange(10))\n        print(\"%s finished!\" % self.getName())\n\n\ndef main():\n    queue = Queue()\n    producer = Producer('Producer', queue)\n    consumer = Consumer('Consumer', queue)\n\n    producer.start()\n    consumer.start()\n    \n    producer.join()\n    consumer.join()\n    print('All threads finished!')\n\n\nif __name__ == '__main__':\n    main()\n```\n\n\n\n## 计算密集型 vs IO密集型\n\n　　对于计算密集型任务，python下的多线程改写是不会有性能提升的，只有多进程才能利用多核CPU，这是明显区别于其他语言的特点，因为在其他高效语言中，多进程和多线程都是作为并发的手段，同时改善程序性能的，而且本身Python作为脚本语言，在计算密集型任务面前运行效率就不高，此时更高阶的手段是用C语言重构 或者 引入高效的其他库来代替。\n\n　　对于IO密集型，Python本身的多线程就能较好的应对，因为线程调度保证了IO任务的并行执行，此时就算是CPU有多个线程资源，任务本身就无法充分利用，所以Python的单线程限制并不构成瓶颈，就算用其他语言改写成真多线程，提升也不明显，所以类似web应用的IO密集型任务，Python足矣。\n\n## 异步IO ，事件驱动模型， 协程，单进程单线程模型\n\n　　考虑到CPU和IO之间巨大的速度差异，一个任务在执行的过程中大部分时间都在等待IO操作，单进程单线程模型会导致别的任务无法并行执行，因此，才需要多进程模型或者多线程模型来支持多任务并发执行。\n\n　　现代操作系统对IO操作已经做了巨大的改进，最大的特点就是支持异步IO。如果充分利用操作系统提供的异步IO支持，就可以用单进程单线程模型来执行多任务，这种全新的模型称为事件驱动模型，Nginx就是支持异步IO的Web服务器，它在单核CPU上采用单进程模型就可以高效地支持多任务。在多核CPU上，可以运行多个进程（数量与CPU核心数相同），充分利用多核CPU。由于系统总的进程数量十分有限，因此操作系统调度非常高效。用异步IO编程模型来实现多任务是一个主要的趋势。\n\n　　对应到Python语言，单线程的异步编程模型称为协程，有了协程的支持，就可以基于事件驱动编写高效的多任务程序。\n\n\n\n## Conclude\n\n　　用代码完整的梳理了一下并发下的进程和线程，同时总结了一下使用场景，很多知识都连接起来了，接下来一个重要的部分就是Python的核心部分，异步IO编程模型-协程\n\n　　关于协程的实践部分，写篇文章会主要介绍。\n\n\n\nReferences:\n\n[一文看懂Python多进程与多线程编程](https://zhuanlan.zhihu.com/p/46368084)\n\n[进程 vs 线程 廖雪峰](https://www.liaoxuefeng.com/wiki/1016959663602400/1017631469467456)\n\n[线程间通信 Python3 CookBook](https://python3-cookbook.readthedocs.io/zh_CN/latest/c12/p03_communicating_between_threads.html)\n\n","slug":"Python-下的多进程和多线程编程","published":1,"_id":"ckjb842170000pl28gqvdaaxk","comments":1,"layout":"post","photos":[],"link":"","content":"<p>　　之前在并发写日志文件的时候简单的梳理了一下同步和异步的机制，不过对于Python的并发实践涉及还是比较浅显，现在这篇文章希望能彻底从代码角度来重新理解并发编程。</p>\n<h2 id=\"什么是进程-Process-和-线程-Thread\"><a href=\"#什么是进程-Process-和-线程-Thread\" class=\"headerlink\" title=\"什么是进程(Process) 和 线程(Thread)\"></a>什么是进程(Process) 和 线程(Thread)</h2><blockquote>\n<p>进程是操作系统分配资源的最小单元</p>\n<p>线程是操作系统调度的最小单元</p>\n<p>一个应用至少包括一个进程，而一个进程包含至少一个线程</p>\n</blockquote>\n<p>　　通常来说，多进程和多线程都是实现多任务的方式，而多任务的设计无论是进程还是线程，都遵循Master-Worker 模式，Master负责分配任务，Worker负责执行任务。在Python编程模型下，这二者的实践方法也非常类似。</p>\n<p>　　从效果上来看，进程间的独立性更高一点，这体现在一个进程崩溃，不会影响其他进程和主进程的执行，比如，Apache的多进程模式。另一方面，进程由于是OS直接创建和调度的，所以相同的代码可能在不同OS下效果会有差异，这体现在OS对同时并发的进程数目是有一个数目限制的，一般情况下，这是一个经验数目的限制，并不是你的系统无法承载这么多的进程数，虽然在大部分情况下，这个限制都是日常使用中无法达到的上界，但是我认为这就跟IPv4 和 32 位系统类似，只是在当时的条件下，定下的一个无法轻易达到的上限标准，在高速的发展迭代下，成为的日后亟待解决的遗留问题。其次，进程的创建开销也因OS不同也有所差异，比如Unix/Linux 的fork开销就远小于WIndows的进程创建。</p>\n<p>　　多线程模式则是多进程的精细化调度，我的理解是，这源于超线程技术的发展，使得一个CPU内核通过常用的寄存器和Cache等CPU内部常用部件的堆叠，达到模拟出2个CPU内核的效果，这当然是一个很优秀的技术，因为确实从软件层面可以把一个CPU线程当作内核直接调用，同时性能上也获得了几乎线性的提升，甚至在Unix/Linux查看CPU参数的指令lscpu下，CPU(s)都是直接显示的内核的线程数目，而非内核数目，常常让初识Linux的我感到非常迷惑。我相信，基于超线程的性能提升，编译器和各种软件的迭代是不可能忽视这么重要且好用的技术的。不过对于一个没有经历过这个技术更迭的人来说，什么多线程，多进程，本质上不是一个东西吗？搞得神神叨叨的。除此之外，大部分进程和线程的介绍，都基于定义的扩展，认真来讲，光看定义，就完全看不出这二者的差异，何况扩展。所以关于线程，虽然我知道的很多，比如说，线程是进程的执行单元，一个线程崩溃，进程则直接挂掉，所有线程共享进程的内存等等，但是却有完全不明白为什么会有这些设置。现在看来，当初没有搞明白这些，着实是有点遗憾，除了教育方式和教材的原因，自己的求知欲也被这些陡峭的学习曲线给淹没了，因为在一个每一个术语都不大理解的地方，哪有好奇心和求知欲的容身之处，早就被从小被训练好的敬畏心给覆盖了，不懂，可是我又不敢问！扯远了，回到线程上，从线程的起源来理解，这一些的设置就豁然开朗了，线程作为执行单元，和CPU提供的线程接口强相关的，而CPU本身就是整个计算机系统的最主要的执行部件，理所当然的线程更接近执行向，而当线程执行失败的时候，整个进程的执行理所当然的受阻挂掉，线程的作为执行者，应当获得进程所有的资源，所以进程下的线程都具备进程的全部资源权限。</p>\n<p>　　在OS层面，线程的实现是有区别的，Windows的多线程效率是比多进程要高的，所以微软的IIS服务器基于多线程，显而易见的稳定性问题不如Apache。不够随着逐步的发展，现在又都是多进程+多线程的混合模式，搞得确实头大。不过参照，RISC 和 CISC的发展，Intel 后来的x86 和x64虽然都仍然维持的了 CISC的接口，但是在微指令层面，仍然是RISC的技术，这也使得其仍然可以从RISC的发展中获益，想到，当初的计算机组成的教材，也是突兀的抛出了微指令的概念，最后学完微指令也摸不着头，直到看到计算机体系结构中关于RISC和CISC的部分，才从重新理解的微指令架构。这次层面上，进程和线程的在Apache和IIS的表现形式或许多少能得到一点启示。不过当今Nginx的事件驱动的异步IO设计大行其道，Python的协程多少还算是找对了方向。</p>\n<p>　　一个很好的关于进程和线程的比喻解释，形象的道出了二者的关系</p>\n<blockquote>\n<ul>\n<li>计算机的核心是CPU，它承担了所有的计算任务。它就像一座工厂，时刻在运行。</li>\n<li>假定工厂的电力有限，一次只能供给一个车间使用。也就是说，一个车间开工的时候，其他车间都必须停工。背后的含义就是，单个CPU一次只能运行一个任务。编者注: 多核的CPU就像有了多个发电厂，使多工厂(多进程)实现可能。</li>\n<li>进程就好比工厂的车间，它代表CPU所能处理的单个任务。任一时刻，CPU总是运行一个进程，其他进程处于非运行状态。</li>\n<li>一个车间里，可以有很多工人。他们协同完成一个任务。</li>\n<li>线程就好比车间里的工人。一个进程可以包括多个线程。</li>\n<li>车间的空间是工人们共享的，比如许多房间是每个工人都可以进出的。这象征一个进程的内存空间是共享的，每个线程都可以使用这些共享内存。</li>\n<li>可是，每间房间的大小不同，有些房间最多只能容纳一个人，比如厕所。里面有人的时候，其他人就不能进去了。这代表一个线程使用某些共享内存时，其他线程必须等它结束，才能使用这一块内存。</li>\n<li>一个防止他人进入的简单方法，就是门口加一把锁。先到的人锁上门，后到的人看到上锁，就在门口排队，等锁打开再进去。这就叫”互斥锁”（Mutual exclusion，缩写 Mutex），防止多个线程同时读写某一块内存区域。</li>\n<li>还有些房间，可以同时容纳n个人，比如厨房。也就是说，如果人数大于n，多出来的人只能在外面等着。这好比某些内存区域，只能供给固定数目的线程使用。</li>\n<li>这时的解决方法，就是在门口挂n把钥匙。进去的人就取一把钥匙，出来时再把钥匙挂回原处。后到的人发现钥匙架空了，就知道必须在门口排队等着了。这种做法叫做”信号量”（Semaphore），用来保证多个线程不会互相冲突。</li>\n<li>不难看出，mutex是semaphore的一种特殊情况（n=1时）。也就是说，完全可以用后者替代前者。但是，因为mutex较为简单，且效率高，所以在必须保证资源独占的情况下，还是采用这种设计。</li>\n</ul>\n</blockquote>\n<p>来自阮一峰的博客<a href=\"http://www.ruanyifeng.com/blog/2013/04/processes_and_threads.html\">进程和线程的简单解释</a></p>\n<h2 id=\"Python的多进程编程-multiprocessing\"><a href=\"#Python的多进程编程-multiprocessing\" class=\"headerlink\" title=\"Python的多进程编程 multiprocessing\"></a>Python的多进程编程 multiprocessing</h2><p>　　首先给出单进程顺序执行的测试代码，给出一个计算$8^{20}$的任务，同时辅以sleep 2s的任务。</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> time\n<span class=\"token keyword\">import</span> os\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">long_time_task</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'当前进程: &amp;#123;&amp;#125;'</span><span class=\"token punctuation\">.</span>format<span class=\"token punctuation\">(</span>os<span class=\"token punctuation\">.</span>getpid<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    time<span class=\"token punctuation\">.</span>sleep<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"结果: &amp;#123;&amp;#125;\"</span><span class=\"token punctuation\">.</span>format<span class=\"token punctuation\">(</span><span class=\"token number\">8</span> <span class=\"token operator\">**</span> <span class=\"token number\">20</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">if</span> __name__ <span class=\"token operator\">==</span> <span class=\"token string\">\"__main__\"</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'当前母进程: &amp;#123;&amp;#125;'</span><span class=\"token punctuation\">.</span>format<span class=\"token punctuation\">(</span>os<span class=\"token punctuation\">.</span>getpid<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    start <span class=\"token operator\">=</span> time<span class=\"token punctuation\">.</span>time<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        long_time_task<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    end <span class=\"token operator\">=</span> time<span class=\"token punctuation\">.</span>time<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"用时&amp;#123;&amp;#125;秒\"</span><span class=\"token punctuation\">.</span>format<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>end<span class=\"token operator\">-</span>start<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n当前母进程<span class=\"token punctuation\">:</span> <span class=\"token number\">33121</span>\n当前进程<span class=\"token punctuation\">:</span> <span class=\"token number\">33121</span>\n结果<span class=\"token punctuation\">:</span> <span class=\"token number\">1152921504606846976</span>\n当前进程<span class=\"token punctuation\">:</span> <span class=\"token number\">33121</span>\n结果<span class=\"token punctuation\">:</span> <span class=\"token number\">1152921504606846976</span>\n用时<span class=\"token number\">4.004350185394287</span>秒</code></pre>\n<p>　　基本是sleep 的4s，计算任务基本不耗时间</p>\n<p>　　接下来是多进程改写</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> multiprocessing <span class=\"token keyword\">import</span> Process\n<span class=\"token keyword\">import</span> os\n<span class=\"token keyword\">import</span> time\n\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">long_time_task</span><span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'子进程: &amp;#123;&amp;#125; - 任务&amp;#123;&amp;#125;'</span><span class=\"token punctuation\">.</span>format<span class=\"token punctuation\">(</span>os<span class=\"token punctuation\">.</span>getpid<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> i<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    time<span class=\"token punctuation\">.</span>sleep<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"结果: &amp;#123;&amp;#125;\"</span><span class=\"token punctuation\">.</span>format<span class=\"token punctuation\">(</span><span class=\"token number\">8</span> <span class=\"token operator\">**</span> <span class=\"token number\">20</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">if</span> __name__<span class=\"token operator\">==</span><span class=\"token string\">'__main__'</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'当前母进程: &amp;#123;&amp;#125;'</span><span class=\"token punctuation\">.</span>format<span class=\"token punctuation\">(</span>os<span class=\"token punctuation\">.</span>getpid<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    start <span class=\"token operator\">=</span> time<span class=\"token punctuation\">.</span>time<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    p1 <span class=\"token operator\">=</span> Process<span class=\"token punctuation\">(</span>target<span class=\"token operator\">=</span>long_time_task<span class=\"token punctuation\">,</span> args<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    p2 <span class=\"token operator\">=</span> Process<span class=\"token punctuation\">(</span>target<span class=\"token operator\">=</span>long_time_task<span class=\"token punctuation\">,</span> args<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'等待所有子进程完成。'</span><span class=\"token punctuation\">)</span>\n    p1<span class=\"token punctuation\">.</span>start<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    p2<span class=\"token punctuation\">.</span>start<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    p1<span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    p2<span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    end <span class=\"token operator\">=</span> time<span class=\"token punctuation\">.</span>time<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"总共用时&amp;#123;&amp;#125;秒\"</span><span class=\"token punctuation\">.</span>format<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>end <span class=\"token operator\">-</span> start<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n当前母进程<span class=\"token punctuation\">:</span> <span class=\"token number\">33121</span>\n等待所有子进程完成。\n子进程<span class=\"token punctuation\">:</span> <span class=\"token number\">34270</span> <span class=\"token operator\">-</span> 任务<span class=\"token number\">1</span>\n子进程<span class=\"token punctuation\">:</span> <span class=\"token number\">34271</span> <span class=\"token operator\">-</span> 任务<span class=\"token number\">2</span>\n结果<span class=\"token punctuation\">:</span> <span class=\"token number\">1152921504606846976</span>\n结果<span class=\"token punctuation\">:</span> <span class=\"token number\">1152921504606846976</span>\n总共用时<span class=\"token number\">2.009559392929077</span>秒</code></pre>\n<p>　　并行的效率得到体现，执行时间减半。</p>\n<p>　　p.join()的理解是，主进程会等待子进程执行完毕，才开始继续从p.join()之后开始执行，否则主进程会直接输出总共用时，然后子进程接着执行完再输出。</p>\n<pre><code>当前母进程: 33121\n等待所有子进程完成。\n子进程: 34669 - 任务1\n总共用时0.002809762954711914秒\n子进程: 34670 - 任务2\n结果: 1152921504606846976\n结果: 1152921504606846976</code></pre>\n<p>　　最后就是对进程管理调度，由于OS的不同以及提高CPU利用率的需求，更是因为程序员懒得一个一个手动启动Process进程，产生的一个统一的进程管理的接口的需求，这就诞生的进程池Pool的接口。</p>\n<p>　　Pool类可以提供指定数量的进程供用户调用，当有新的请求提交到Pool中时，如果进程池还没有满，就会创建一个新的进程来执行请求。如果池满，请求就会告知先等待，直到池中有进程结束，才会创建新的进程来执行这些请求。</p>\n<p>下面介绍一下multiprocessing 模块下的Pool类的几个方法：</p>\n<ol>\n<li>apply_async</li>\n</ol>\n<p>函数原型：apply_async(func[, args=()[, kwds={}[, callback=None]]])</p>\n<p>其作用是向进程池提交需要执行的函数及参数， 各个进程采用非阻塞（异步）的调用方式，即每个子进程只管运行自己的，不管其它进程是否已经完成。这是默认方式。</p>\n<ol start=\"2\">\n<li>map()</li>\n</ol>\n<p>函数原型：map(func, iterable[, chunksize=None])</p>\n<p>Pool类中的map方法，与内置的map函数用法行为基本一致，它会使进程阻塞直到结果返回。 注意：虽然第二个参数是一个迭代器，但在实际使用中，必须在整个队列都就绪后，程序才会运行子进程。</p>\n<ol start=\"3\">\n<li>map_async()</li>\n</ol>\n<p>函数原型：map_async(func, iterable[, chunksize[, callback]])<br>与map用法一致，但是它是非阻塞的。其有关事项见apply_async。</p>\n<ol start=\"4\">\n<li>close()</li>\n</ol>\n<p>关闭进程池（pool），使其不在接受新的任务。</p>\n<ol start=\"5\">\n<li>terminate()</li>\n</ol>\n<p>结束工作进程，不在处理未处理的任务。</p>\n<ol start=\"6\">\n<li>join()</li>\n</ol>\n<p>主进程阻塞等待子进程的退出， join方法要在close或terminate之后使用。</p>\n<p>　　这里设计一个有意思设计用例，来验证并观察进程的行为模式是否按照理论上的理解运行，即是创建大小为4的Pool，却同时启动5个计算任务，最后观察运行时间，理论上应该是开始的4个进程是并行，后面的任务等前面的进程空出来之后，再开始安排进程来计算。</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> multiprocessing <span class=\"token keyword\">import</span> Pool<span class=\"token punctuation\">,</span> cpu_count\n<span class=\"token keyword\">import</span> os\n<span class=\"token keyword\">import</span> time\n\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">long_time_task</span><span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'子进程: &amp;#123;&amp;#125; - 任务&amp;#123;&amp;#125;'</span><span class=\"token punctuation\">.</span>format<span class=\"token punctuation\">(</span>os<span class=\"token punctuation\">.</span>getpid<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> i<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    time<span class=\"token punctuation\">.</span>sleep<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"结果: &amp;#123;&amp;#125;\"</span><span class=\"token punctuation\">.</span>format<span class=\"token punctuation\">(</span><span class=\"token number\">8</span> <span class=\"token operator\">**</span> <span class=\"token number\">20</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n\n<span class=\"token keyword\">if</span> __name__<span class=\"token operator\">==</span><span class=\"token string\">'__main__'</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"CPU内核数:&amp;#123;&amp;#125;\"</span><span class=\"token punctuation\">.</span>format<span class=\"token punctuation\">(</span>cpu_count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'当前母进程: &amp;#123;&amp;#125;'</span><span class=\"token punctuation\">.</span>format<span class=\"token punctuation\">(</span>os<span class=\"token punctuation\">.</span>getpid<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    start <span class=\"token operator\">=</span> time<span class=\"token punctuation\">.</span>time<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    p <span class=\"token operator\">=</span> Pool<span class=\"token punctuation\">(</span><span class=\"token number\">4</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span><span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        p<span class=\"token punctuation\">.</span>apply_async<span class=\"token punctuation\">(</span>long_time_task<span class=\"token punctuation\">,</span> args<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'等待所有子进程完成。'</span><span class=\"token punctuation\">)</span>\n    p<span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    p<span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    end <span class=\"token operator\">=</span> time<span class=\"token punctuation\">.</span>time<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"总共用时&amp;#123;&amp;#125;秒\"</span><span class=\"token punctuation\">.</span>format<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>end <span class=\"token operator\">-</span> start<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\nCPU内核数<span class=\"token punctuation\">:</span><span class=\"token number\">64</span>\n当前母进程<span class=\"token punctuation\">:</span> <span class=\"token number\">33121</span>\n等待所有子进程完成。\n子进程<span class=\"token punctuation\">:</span> <span class=\"token number\">37454</span> <span class=\"token operator\">-</span> 任务<span class=\"token number\">0</span>\n子进程<span class=\"token punctuation\">:</span> <span class=\"token number\">37455</span> <span class=\"token operator\">-</span> 任务<span class=\"token number\">1</span>\n子进程<span class=\"token punctuation\">:</span> <span class=\"token number\">37456</span> <span class=\"token operator\">-</span> 任务<span class=\"token number\">2</span>\n子进程<span class=\"token punctuation\">:</span> <span class=\"token number\">37457</span> <span class=\"token operator\">-</span> 任务<span class=\"token number\">3</span>\n结果<span class=\"token punctuation\">:</span> <span class=\"token number\">1152921504606846976</span>\n结果<span class=\"token punctuation\">:</span> <span class=\"token number\">1152921504606846976</span>\n子进程<span class=\"token punctuation\">:</span> <span class=\"token number\">37454</span> <span class=\"token operator\">-</span> 任务<span class=\"token number\">4</span>\n结果<span class=\"token punctuation\">:</span> <span class=\"token number\">1152921504606846976</span>\n结果<span class=\"token punctuation\">:</span> <span class=\"token number\">1152921504606846976</span>\n结果<span class=\"token punctuation\">:</span> <span class=\"token number\">1152921504606846976</span>\n总共用时<span class=\"token number\">4.115360736846924</span>秒</code></pre>\n<p>结果跟预想的基本一致，是并行的4个任务的2s+后续一个任务的2s，总共4s+</p>\n<p>由于Python的GIL（全局解释器锁）的存在，多线程的代码实际上一个时刻只有一个线程在执行，所以如果要充分利用多核CPU资源，一般都是通过多进程来实现的。</p>\n<p>　　Add，多进程的数据共享和通信，multiprocessing.Queue 使用实例</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> multiprocessing <span class=\"token keyword\">import</span> Process<span class=\"token punctuation\">,</span> Queue\n<span class=\"token keyword\">import</span> os<span class=\"token punctuation\">,</span> time<span class=\"token punctuation\">,</span> random\n\n<span class=\"token comment\" spellcheck=\"true\"># 写数据进程执行的代码:</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">write</span><span class=\"token punctuation\">(</span>q<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'Process to write: &amp;#123;&amp;#125;'</span><span class=\"token punctuation\">.</span>format<span class=\"token punctuation\">(</span>os<span class=\"token punctuation\">.</span>getpid<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">for</span> value <span class=\"token keyword\">in</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'A'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'B'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'C'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'Put %s to queue...'</span> <span class=\"token operator\">%</span> value<span class=\"token punctuation\">)</span>\n        q<span class=\"token punctuation\">.</span>put<span class=\"token punctuation\">(</span>value<span class=\"token punctuation\">)</span>\n        time<span class=\"token punctuation\">.</span>sleep<span class=\"token punctuation\">(</span>random<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># 读数据进程执行的代码:</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">read</span><span class=\"token punctuation\">(</span>q<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'Process to read:&amp;#123;&amp;#125;'</span><span class=\"token punctuation\">.</span>format<span class=\"token punctuation\">(</span>os<span class=\"token punctuation\">.</span>getpid<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">while</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">:</span>\n        value <span class=\"token operator\">=</span> q<span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'Get %s from queue.'</span> <span class=\"token operator\">%</span> value<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">if</span> __name__<span class=\"token operator\">==</span><span class=\"token string\">'__main__'</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\" spellcheck=\"true\"># 父进程创建Queue，并传给各个子进程：</span>\n    q <span class=\"token operator\">=</span> Queue<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    pw <span class=\"token operator\">=</span> Process<span class=\"token punctuation\">(</span>target<span class=\"token operator\">=</span>write<span class=\"token punctuation\">,</span> args<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>q<span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    pr <span class=\"token operator\">=</span> Process<span class=\"token punctuation\">(</span>target<span class=\"token operator\">=</span>read<span class=\"token punctuation\">,</span> args<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>q<span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># 启动子进程pw，写入:</span>\n    pw<span class=\"token punctuation\">.</span>start<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># 启动子进程pr，读取:</span>\n    pr<span class=\"token punctuation\">.</span>start<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># 等待pw结束:</span>\n    pw<span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># pr进程里是死循环，无法等待其结束，只能强行终止:</span>\n    pr<span class=\"token punctuation\">.</span>terminate<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\nProcess to write<span class=\"token punctuation\">:</span> <span class=\"token number\">39190</span>\nPut A to queue<span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>\nProcess to read<span class=\"token punctuation\">:</span><span class=\"token number\">39191</span>\nGet A <span class=\"token keyword\">from</span> queue<span class=\"token punctuation\">.</span>\nPut B to queue<span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>\nGet B <span class=\"token keyword\">from</span> queue<span class=\"token punctuation\">.</span>\nPut C to queue<span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>\nGet C <span class=\"token keyword\">from</span> queue<span class=\"token punctuation\">.</span></code></pre>\n<p>##</p>\n<h2 id=\"Python-的多线程编程-Threading\"><a href=\"#Python-的多线程编程-Threading\" class=\"headerlink\" title=\"Python 的多线程编程 Threading\"></a>Python 的多线程编程 Threading</h2><p>　　接口其实跟多进程一样</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> threading\n<span class=\"token keyword\">import</span> time\n\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">long_time_task</span><span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'当前子线程: &amp;#123;&amp;#125; - 任务&amp;#123;&amp;#125;'</span><span class=\"token punctuation\">.</span>format<span class=\"token punctuation\">(</span>threading<span class=\"token punctuation\">.</span>current_thread<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>name<span class=\"token punctuation\">,</span> i<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    time<span class=\"token punctuation\">.</span>sleep<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"结果: &amp;#123;&amp;#125;\"</span><span class=\"token punctuation\">.</span>format<span class=\"token punctuation\">(</span><span class=\"token number\">8</span> <span class=\"token operator\">**</span> <span class=\"token number\">20</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n\n<span class=\"token keyword\">if</span> __name__<span class=\"token operator\">==</span><span class=\"token string\">'__main__'</span><span class=\"token punctuation\">:</span>\n    start <span class=\"token operator\">=</span> time<span class=\"token punctuation\">.</span>time<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'这是主线程：&amp;#123;&amp;#125;'</span><span class=\"token punctuation\">.</span>format<span class=\"token punctuation\">(</span>threading<span class=\"token punctuation\">.</span>current_thread<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>name<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    t1 <span class=\"token operator\">=</span> threading<span class=\"token punctuation\">.</span>Thread<span class=\"token punctuation\">(</span>target<span class=\"token operator\">=</span>long_time_task<span class=\"token punctuation\">,</span> args<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    t2 <span class=\"token operator\">=</span> threading<span class=\"token punctuation\">.</span>Thread<span class=\"token punctuation\">(</span>target<span class=\"token operator\">=</span>long_time_task<span class=\"token punctuation\">,</span> args<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    t1<span class=\"token punctuation\">.</span>start<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    t2<span class=\"token punctuation\">.</span>start<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    t1<span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    t2<span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    end <span class=\"token operator\">=</span> time<span class=\"token punctuation\">.</span>time<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"总共用时&amp;#123;&amp;#125;秒\"</span><span class=\"token punctuation\">.</span>format<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>end <span class=\"token operator\">-</span> start<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n这是主线程：MainThread\n当前子线程<span class=\"token punctuation\">:</span> Thread<span class=\"token number\">-1316</span> <span class=\"token operator\">-</span> 任务<span class=\"token number\">1</span>\n当前子线程<span class=\"token punctuation\">:</span> Thread<span class=\"token number\">-1317</span> <span class=\"token operator\">-</span> 任务<span class=\"token number\">2</span>\n结果<span class=\"token punctuation\">:</span> <span class=\"token number\">1152921504606846976</span>\n结果<span class=\"token punctuation\">:</span> <span class=\"token number\">1152921504606846976</span>\n总共用时<span class=\"token number\">2.0029757022857666</span>秒</code></pre>\n<p>连结果都跟多进程一样，不是说GIL导致始终是单线程的性能吗？观察任务的组成会发现，计算任务基本不耗时间，主要是sleep ,但是sleep在运行中是会被当作类似IO的操作，被识别为闲置的线程，这时候GIL直接就开始切换线程了，所以sleep的时间实际上是并行执行的，因为不依赖CPU计算，依此推测计算部分还是应该是单线程的，因为依赖CPU，那只需要增加计算任务的时间即可，那就加大指数为 $8^{1000000}$</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">long_time_task</span><span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'当前子线程: &amp;#123;&amp;#125; - 任务&amp;#123;&amp;#125;'</span><span class=\"token punctuation\">.</span>format<span class=\"token punctuation\">(</span>threading<span class=\"token punctuation\">.</span>current_thread<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>name<span class=\"token punctuation\">,</span> i<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    time<span class=\"token punctuation\">.</span>sleep<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n    t <span class=\"token operator\">=</span> time<span class=\"token punctuation\">.</span>time<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"结果: &amp;#123;&amp;#125;\"</span><span class=\"token punctuation\">.</span>format<span class=\"token punctuation\">(</span><span class=\"token number\">8</span> <span class=\"token operator\">**</span> <span class=\"token number\">1000000</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'计算用时： '</span><span class=\"token punctuation\">,</span> time<span class=\"token punctuation\">.</span>time<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token operator\">-</span>t<span class=\"token punctuation\">)</span>\n<span class=\"token comment\" spellcheck=\"true\"># 单独执行时间 long_time_task(1)</span>\n计算时间 <span class=\"token punctuation\">:</span> <span class=\"token number\">11.462863683700562</span>\n总共用时<span class=\"token number\">13.43696641921997</span>秒\n<span class=\"token comment\" spellcheck=\"true\"># 多线程时间 </span>\n计算时间 <span class=\"token punctuation\">:</span> <span class=\"token number\">22.85654377937317</span>\n计算时间 <span class=\"token punctuation\">:</span> <span class=\"token number\">23.088494300842285</span>\n总共用时<span class=\"token number\">25.10121512413025</span>秒\n</code></pre>\n<p>结果上，总计算时间是翻倍的，但是每个线程的输出时间却是翻倍后的22s，而不是单线程的11s，考虑到并发执行，每个线程执行到任何一个时间，都有可能被中断，切换到其他进程，这里的单线程输出时间就可以理解了，那就是线程1执行玩计算任务，还没输出时间，就被切换到线程2的计算来了，最后统一输出线程1和线程2的时间，那么结果二者都是22s左右。总之，可以确定的计算密集型任务，Python的GIL充分保证的单线程的性能。</p>\n<p>　　多线程的数据共享和通信，寻找线程安全的数据结构即可，queue.Queue就是的</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> queue <span class=\"token keyword\">import</span> Queue\n<span class=\"token keyword\">import</span> random<span class=\"token punctuation\">,</span> threading<span class=\"token punctuation\">,</span> time\n\n\n<span class=\"token comment\" spellcheck=\"true\"># 生产者类</span>\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">Producer</span><span class=\"token punctuation\">(</span>threading<span class=\"token punctuation\">.</span>Thread<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> name<span class=\"token punctuation\">,</span> queue<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        threading<span class=\"token punctuation\">.</span>Thread<span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> name<span class=\"token operator\">=</span>name<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>queue <span class=\"token operator\">=</span> queue\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">run</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"&amp;#123;&amp;#125; is producing &amp;#123;&amp;#125; to the queue!\"</span><span class=\"token punctuation\">.</span>format<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>getName<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> i<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n            self<span class=\"token punctuation\">.</span>queue<span class=\"token punctuation\">.</span>put<span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">)</span>\n            time<span class=\"token punctuation\">.</span>sleep<span class=\"token punctuation\">(</span>random<span class=\"token punctuation\">.</span>randrange<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> <span class=\"token number\">5</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"%s finished!\"</span> <span class=\"token operator\">%</span> self<span class=\"token punctuation\">.</span>getName<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n\n<span class=\"token comment\" spellcheck=\"true\"># 消费者类</span>\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">Consumer</span><span class=\"token punctuation\">(</span>threading<span class=\"token punctuation\">.</span>Thread<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> name<span class=\"token punctuation\">,</span> queue<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        threading<span class=\"token punctuation\">.</span>Thread<span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> name<span class=\"token operator\">=</span>name<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>queue <span class=\"token operator\">=</span> queue\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">run</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            val <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>queue<span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n            <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"&amp;#123;&amp;#125; is consuming &amp;#123;&amp;#125; in the queue.\"</span><span class=\"token punctuation\">.</span>format<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>getName<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> val<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n            time<span class=\"token punctuation\">.</span>sleep<span class=\"token punctuation\">(</span>random<span class=\"token punctuation\">.</span>randrange<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"%s finished!\"</span> <span class=\"token operator\">%</span> self<span class=\"token punctuation\">.</span>getName<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">main</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    queue <span class=\"token operator\">=</span> Queue<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    producer <span class=\"token operator\">=</span> Producer<span class=\"token punctuation\">(</span><span class=\"token string\">'Producer'</span><span class=\"token punctuation\">,</span> queue<span class=\"token punctuation\">)</span>\n    consumer <span class=\"token operator\">=</span> Consumer<span class=\"token punctuation\">(</span><span class=\"token string\">'Consumer'</span><span class=\"token punctuation\">,</span> queue<span class=\"token punctuation\">)</span>\n\n    producer<span class=\"token punctuation\">.</span>start<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    consumer<span class=\"token punctuation\">.</span>start<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    producer<span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    consumer<span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'All threads finished!'</span><span class=\"token punctuation\">)</span>\n\n\n<span class=\"token keyword\">if</span> __name__ <span class=\"token operator\">==</span> <span class=\"token string\">'__main__'</span><span class=\"token punctuation\">:</span>\n    main<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre>\n<h2 id=\"计算密集型-vs-IO密集型\"><a href=\"#计算密集型-vs-IO密集型\" class=\"headerlink\" title=\"计算密集型 vs IO密集型\"></a>计算密集型 vs IO密集型</h2><p>　　对于计算密集型任务，python下的多线程改写是不会有性能提升的，只有多进程才能利用多核CPU，这是明显区别于其他语言的特点，因为在其他高效语言中，多进程和多线程都是作为并发的手段，同时改善程序性能的，而且本身Python作为脚本语言，在计算密集型任务面前运行效率就不高，此时更高阶的手段是用C语言重构 或者 引入高效的其他库来代替。</p>\n<p>　　对于IO密集型，Python本身的多线程就能较好的应对，因为线程调度保证了IO任务的并行执行，此时就算是CPU有多个线程资源，任务本身就无法充分利用，所以Python的单线程限制并不构成瓶颈，就算用其他语言改写成真多线程，提升也不明显，所以类似web应用的IO密集型任务，Python足矣。</p>\n<h2 id=\"异步IO-，事件驱动模型，-协程，单进程单线程模型\"><a href=\"#异步IO-，事件驱动模型，-协程，单进程单线程模型\" class=\"headerlink\" title=\"异步IO ，事件驱动模型， 协程，单进程单线程模型\"></a>异步IO ，事件驱动模型， 协程，单进程单线程模型</h2><p>　　考虑到CPU和IO之间巨大的速度差异，一个任务在执行的过程中大部分时间都在等待IO操作，单进程单线程模型会导致别的任务无法并行执行，因此，才需要多进程模型或者多线程模型来支持多任务并发执行。</p>\n<p>　　现代操作系统对IO操作已经做了巨大的改进，最大的特点就是支持异步IO。如果充分利用操作系统提供的异步IO支持，就可以用单进程单线程模型来执行多任务，这种全新的模型称为事件驱动模型，Nginx就是支持异步IO的Web服务器，它在单核CPU上采用单进程模型就可以高效地支持多任务。在多核CPU上，可以运行多个进程（数量与CPU核心数相同），充分利用多核CPU。由于系统总的进程数量十分有限，因此操作系统调度非常高效。用异步IO编程模型来实现多任务是一个主要的趋势。</p>\n<p>　　对应到Python语言，单线程的异步编程模型称为协程，有了协程的支持，就可以基于事件驱动编写高效的多任务程序。</p>\n<h2 id=\"Conclude\"><a href=\"#Conclude\" class=\"headerlink\" title=\"Conclude\"></a>Conclude</h2><p>　　用代码完整的梳理了一下并发下的进程和线程，同时总结了一下使用场景，很多知识都连接起来了，接下来一个重要的部分就是Python的核心部分，异步IO编程模型-协程</p>\n<p>　　关于协程的实践部分，写篇文章会主要介绍。</p>\n<p>References:</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/46368084\">一文看懂Python多进程与多线程编程</a></p>\n<p><a href=\"https://www.liaoxuefeng.com/wiki/1016959663602400/1017631469467456\">进程 vs 线程 廖雪峰</a></p>\n<p><a href=\"https://python3-cookbook.readthedocs.io/zh_CN/latest/c12/p03_communicating_between_threads.html\">线程间通信 Python3 CookBook</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>　　之前在并发写日志文件的时候简单的梳理了一下同步和异步的机制，不过对于Python的并发实践涉及还是比较浅显，现在这篇文章希望能彻底从代码角度来重新理解并发编程。</p>\n<h2 id=\"什么是进程-Process-和-线程-Thread\"><a href=\"#什么是进程-Process-和-线程-Thread\" class=\"headerlink\" title=\"什么是进程(Process) 和 线程(Thread)\"></a>什么是进程(Process) 和 线程(Thread)</h2><blockquote>\n<p>进程是操作系统分配资源的最小单元</p>\n<p>线程是操作系统调度的最小单元</p>\n<p>一个应用至少包括一个进程，而一个进程包含至少一个线程</p>\n</blockquote>\n<p>　　通常来说，多进程和多线程都是实现多任务的方式，而多任务的设计无论是进程还是线程，都遵循Master-Worker 模式，Master负责分配任务，Worker负责执行任务。在Python编程模型下，这二者的实践方法也非常类似。</p>\n<p>　　从效果上来看，进程间的独立性更高一点，这体现在一个进程崩溃，不会影响其他进程和主进程的执行，比如，Apache的多进程模式。另一方面，进程由于是OS直接创建和调度的，所以相同的代码可能在不同OS下效果会有差异，这体现在OS对同时并发的进程数目是有一个数目限制的，一般情况下，这是一个经验数目的限制，并不是你的系统无法承载这么多的进程数，虽然在大部分情况下，这个限制都是日常使用中无法达到的上界，但是我认为这就跟IPv4 和 32 位系统类似，只是在当时的条件下，定下的一个无法轻易达到的上限标准，在高速的发展迭代下，成为的日后亟待解决的遗留问题。其次，进程的创建开销也因OS不同也有所差异，比如Unix/Linux 的fork开销就远小于WIndows的进程创建。</p>\n<p>　　多线程模式则是多进程的精细化调度，我的理解是，这源于超线程技术的发展，使得一个CPU内核通过常用的寄存器和Cache等CPU内部常用部件的堆叠，达到模拟出2个CPU内核的效果，这当然是一个很优秀的技术，因为确实从软件层面可以把一个CPU线程当作内核直接调用，同时性能上也获得了几乎线性的提升，甚至在Unix/Linux查看CPU参数的指令lscpu下，CPU(s)都是直接显示的内核的线程数目，而非内核数目，常常让初识Linux的我感到非常迷惑。我相信，基于超线程的性能提升，编译器和各种软件的迭代是不可能忽视这么重要且好用的技术的。不过对于一个没有经历过这个技术更迭的人来说，什么多线程，多进程，本质上不是一个东西吗？搞得神神叨叨的。除此之外，大部分进程和线程的介绍，都基于定义的扩展，认真来讲，光看定义，就完全看不出这二者的差异，何况扩展。所以关于线程，虽然我知道的很多，比如说，线程是进程的执行单元，一个线程崩溃，进程则直接挂掉，所有线程共享进程的内存等等，但是却有完全不明白为什么会有这些设置。现在看来，当初没有搞明白这些，着实是有点遗憾，除了教育方式和教材的原因，自己的求知欲也被这些陡峭的学习曲线给淹没了，因为在一个每一个术语都不大理解的地方，哪有好奇心和求知欲的容身之处，早就被从小被训练好的敬畏心给覆盖了，不懂，可是我又不敢问！扯远了，回到线程上，从线程的起源来理解，这一些的设置就豁然开朗了，线程作为执行单元，和CPU提供的线程接口强相关的，而CPU本身就是整个计算机系统的最主要的执行部件，理所当然的线程更接近执行向，而当线程执行失败的时候，整个进程的执行理所当然的受阻挂掉，线程的作为执行者，应当获得进程所有的资源，所以进程下的线程都具备进程的全部资源权限。</p>\n<p>　　在OS层面，线程的实现是有区别的，Windows的多线程效率是比多进程要高的，所以微软的IIS服务器基于多线程，显而易见的稳定性问题不如Apache。不够随着逐步的发展，现在又都是多进程+多线程的混合模式，搞得确实头大。不过参照，RISC 和 CISC的发展，Intel 后来的x86 和x64虽然都仍然维持的了 CISC的接口，但是在微指令层面，仍然是RISC的技术，这也使得其仍然可以从RISC的发展中获益，想到，当初的计算机组成的教材，也是突兀的抛出了微指令的概念，最后学完微指令也摸不着头，直到看到计算机体系结构中关于RISC和CISC的部分，才从重新理解的微指令架构。这次层面上，进程和线程的在Apache和IIS的表现形式或许多少能得到一点启示。不过当今Nginx的事件驱动的异步IO设计大行其道，Python的协程多少还算是找对了方向。</p>\n<p>　　一个很好的关于进程和线程的比喻解释，形象的道出了二者的关系</p>\n<blockquote>\n<ul>\n<li>计算机的核心是CPU，它承担了所有的计算任务。它就像一座工厂，时刻在运行。</li>\n<li>假定工厂的电力有限，一次只能供给一个车间使用。也就是说，一个车间开工的时候，其他车间都必须停工。背后的含义就是，单个CPU一次只能运行一个任务。编者注: 多核的CPU就像有了多个发电厂，使多工厂(多进程)实现可能。</li>\n<li>进程就好比工厂的车间，它代表CPU所能处理的单个任务。任一时刻，CPU总是运行一个进程，其他进程处于非运行状态。</li>\n<li>一个车间里，可以有很多工人。他们协同完成一个任务。</li>\n<li>线程就好比车间里的工人。一个进程可以包括多个线程。</li>\n<li>车间的空间是工人们共享的，比如许多房间是每个工人都可以进出的。这象征一个进程的内存空间是共享的，每个线程都可以使用这些共享内存。</li>\n<li>可是，每间房间的大小不同，有些房间最多只能容纳一个人，比如厕所。里面有人的时候，其他人就不能进去了。这代表一个线程使用某些共享内存时，其他线程必须等它结束，才能使用这一块内存。</li>\n<li>一个防止他人进入的简单方法，就是门口加一把锁。先到的人锁上门，后到的人看到上锁，就在门口排队，等锁打开再进去。这就叫”互斥锁”（Mutual exclusion，缩写 Mutex），防止多个线程同时读写某一块内存区域。</li>\n<li>还有些房间，可以同时容纳n个人，比如厨房。也就是说，如果人数大于n，多出来的人只能在外面等着。这好比某些内存区域，只能供给固定数目的线程使用。</li>\n<li>这时的解决方法，就是在门口挂n把钥匙。进去的人就取一把钥匙，出来时再把钥匙挂回原处。后到的人发现钥匙架空了，就知道必须在门口排队等着了。这种做法叫做”信号量”（Semaphore），用来保证多个线程不会互相冲突。</li>\n<li>不难看出，mutex是semaphore的一种特殊情况（n=1时）。也就是说，完全可以用后者替代前者。但是，因为mutex较为简单，且效率高，所以在必须保证资源独占的情况下，还是采用这种设计。</li>\n</ul>\n</blockquote>\n<p>来自阮一峰的博客<a href=\"http://www.ruanyifeng.com/blog/2013/04/processes_and_threads.html\">进程和线程的简单解释</a></p>\n<h2 id=\"Python的多进程编程-multiprocessing\"><a href=\"#Python的多进程编程-multiprocessing\" class=\"headerlink\" title=\"Python的多进程编程 multiprocessing\"></a>Python的多进程编程 multiprocessing</h2><p>　　首先给出单进程顺序执行的测试代码，给出一个计算$8^{20}$的任务，同时辅以sleep 2s的任务。</p>\n<pre><code class=\"python\">import time\nimport os\n\ndef long_time_task():\n    print(&#39;当前进程: &#123;&#125;&#39;.format(os.getpid()))\n    time.sleep(2)\n    print(&quot;结果: &#123;&#125;&quot;.format(8 ** 20))\n\nif __name__ == &quot;__main__&quot;:\n    print(&#39;当前母进程: &#123;&#125;&#39;.format(os.getpid()))\n    start = time.time()\n    for i in range(2):\n        long_time_task()\n\n    end = time.time()\n    print(&quot;用时&#123;&#125;秒&quot;.format((end-start)))\n\n当前母进程: 33121\n当前进程: 33121\n结果: 1152921504606846976\n当前进程: 33121\n结果: 1152921504606846976\n用时4.004350185394287秒</code></pre>\n<p>　　基本是sleep 的4s，计算任务基本不耗时间</p>\n<p>　　接下来是多进程改写</p>\n<pre><code class=\"python\">from multiprocessing import Process\nimport os\nimport time\n\n\ndef long_time_task(i):\n    print(&#39;子进程: &#123;&#125; - 任务&#123;&#125;&#39;.format(os.getpid(), i))\n    time.sleep(2)\n    print(&quot;结果: &#123;&#125;&quot;.format(8 ** 20))\n\nif __name__==&#39;__main__&#39;:\n    print(&#39;当前母进程: &#123;&#125;&#39;.format(os.getpid()))\n    start = time.time()\n    p1 = Process(target=long_time_task, args=(1,))\n    p2 = Process(target=long_time_task, args=(2,))\n    print(&#39;等待所有子进程完成。&#39;)\n    p1.start()\n    p2.start()\n    p1.join()\n    p2.join()\n    end = time.time()\n    print(&quot;总共用时&#123;&#125;秒&quot;.format((end - start)))\n\n当前母进程: 33121\n等待所有子进程完成。\n子进程: 34270 - 任务1\n子进程: 34271 - 任务2\n结果: 1152921504606846976\n结果: 1152921504606846976\n总共用时2.009559392929077秒</code></pre>\n<p>　　并行的效率得到体现，执行时间减半。</p>\n<p>　　p.join()的理解是，主进程会等待子进程执行完毕，才开始继续从p.join()之后开始执行，否则主进程会直接输出总共用时，然后子进程接着执行完再输出。</p>\n<pre><code>当前母进程: 33121\n等待所有子进程完成。\n子进程: 34669 - 任务1\n总共用时0.002809762954711914秒\n子进程: 34670 - 任务2\n结果: 1152921504606846976\n结果: 1152921504606846976</code></pre>\n<p>　　最后就是对进程管理调度，由于OS的不同以及提高CPU利用率的需求，更是因为程序员懒得一个一个手动启动Process进程，产生的一个统一的进程管理的接口的需求，这就诞生的进程池Pool的接口。</p>\n<p>　　Pool类可以提供指定数量的进程供用户调用，当有新的请求提交到Pool中时，如果进程池还没有满，就会创建一个新的进程来执行请求。如果池满，请求就会告知先等待，直到池中有进程结束，才会创建新的进程来执行这些请求。</p>\n<p>下面介绍一下multiprocessing 模块下的Pool类的几个方法：</p>\n<ol>\n<li>apply_async</li>\n</ol>\n<p>函数原型：apply_async(func[, args=()[, kwds={}[, callback=None]]])</p>\n<p>其作用是向进程池提交需要执行的函数及参数， 各个进程采用非阻塞（异步）的调用方式，即每个子进程只管运行自己的，不管其它进程是否已经完成。这是默认方式。</p>\n<ol start=\"2\">\n<li>map()</li>\n</ol>\n<p>函数原型：map(func, iterable[, chunksize=None])</p>\n<p>Pool类中的map方法，与内置的map函数用法行为基本一致，它会使进程阻塞直到结果返回。 注意：虽然第二个参数是一个迭代器，但在实际使用中，必须在整个队列都就绪后，程序才会运行子进程。</p>\n<ol start=\"3\">\n<li>map_async()</li>\n</ol>\n<p>函数原型：map_async(func, iterable[, chunksize[, callback]])<br>与map用法一致，但是它是非阻塞的。其有关事项见apply_async。</p>\n<ol start=\"4\">\n<li>close()</li>\n</ol>\n<p>关闭进程池（pool），使其不在接受新的任务。</p>\n<ol start=\"5\">\n<li>terminate()</li>\n</ol>\n<p>结束工作进程，不在处理未处理的任务。</p>\n<ol start=\"6\">\n<li>join()</li>\n</ol>\n<p>主进程阻塞等待子进程的退出， join方法要在close或terminate之后使用。</p>\n<p>　　这里设计一个有意思设计用例，来验证并观察进程的行为模式是否按照理论上的理解运行，即是创建大小为4的Pool，却同时启动5个计算任务，最后观察运行时间，理论上应该是开始的4个进程是并行，后面的任务等前面的进程空出来之后，再开始安排进程来计算。</p>\n<pre><code class=\"python\">from multiprocessing import Pool, cpu_count\nimport os\nimport time\n\n\ndef long_time_task(i):\n    print(&#39;子进程: &#123;&#125; - 任务&#123;&#125;&#39;.format(os.getpid(), i))\n    time.sleep(2)\n    print(&quot;结果: &#123;&#125;&quot;.format(8 ** 20))\n\n\nif __name__==&#39;__main__&#39;:\n    print(&quot;CPU内核数:&#123;&#125;&quot;.format(cpu_count()))\n    print(&#39;当前母进程: &#123;&#125;&#39;.format(os.getpid()))\n    start = time.time()\n    p = Pool(4)\n    for i in range(5):\n        p.apply_async(long_time_task, args=(i,))\n    print(&#39;等待所有子进程完成。&#39;)\n    p.close()\n    p.join()\n    end = time.time()\n    print(&quot;总共用时&#123;&#125;秒&quot;.format((end - start)))\n\nCPU内核数:64\n当前母进程: 33121\n等待所有子进程完成。\n子进程: 37454 - 任务0\n子进程: 37455 - 任务1\n子进程: 37456 - 任务2\n子进程: 37457 - 任务3\n结果: 1152921504606846976\n结果: 1152921504606846976\n子进程: 37454 - 任务4\n结果: 1152921504606846976\n结果: 1152921504606846976\n结果: 1152921504606846976\n总共用时4.115360736846924秒</code></pre>\n<p>结果跟预想的基本一致，是并行的4个任务的2s+后续一个任务的2s，总共4s+</p>\n<p>由于Python的GIL（全局解释器锁）的存在，多线程的代码实际上一个时刻只有一个线程在执行，所以如果要充分利用多核CPU资源，一般都是通过多进程来实现的。</p>\n<p>　　Add，多进程的数据共享和通信，multiprocessing.Queue 使用实例</p>\n<pre><code class=\"python\">from multiprocessing import Process, Queue\nimport os, time, random\n\n# 写数据进程执行的代码:\ndef write(q):\n    print(&#39;Process to write: &#123;&#125;&#39;.format(os.getpid()))\n    for value in [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;]:\n        print(&#39;Put %s to queue...&#39; % value)\n        q.put(value)\n        time.sleep(random.random())\n\n# 读数据进程执行的代码:\ndef read(q):\n    print(&#39;Process to read:&#123;&#125;&#39;.format(os.getpid()))\n    while True:\n        value = q.get(True)\n        print(&#39;Get %s from queue.&#39; % value)\n\nif __name__==&#39;__main__&#39;:\n    # 父进程创建Queue，并传给各个子进程：\n    q = Queue()\n    pw = Process(target=write, args=(q,))\n    pr = Process(target=read, args=(q,))\n    # 启动子进程pw，写入:\n    pw.start()\n    # 启动子进程pr，读取:\n    pr.start()\n    # 等待pw结束:\n    pw.join()\n    # pr进程里是死循环，无法等待其结束，只能强行终止:\n    pr.terminate()\n\nProcess to write: 39190\nPut A to queue...\nProcess to read:39191\nGet A from queue.\nPut B to queue...\nGet B from queue.\nPut C to queue...\nGet C from queue.</code></pre>\n<p>##</p>\n<h2 id=\"Python-的多线程编程-Threading\"><a href=\"#Python-的多线程编程-Threading\" class=\"headerlink\" title=\"Python 的多线程编程 Threading\"></a>Python 的多线程编程 Threading</h2><p>　　接口其实跟多进程一样</p>\n<pre><code class=\"python\">import threading\nimport time\n\n\ndef long_time_task(i):\n    print(&#39;当前子线程: &#123;&#125; - 任务&#123;&#125;&#39;.format(threading.current_thread().name, i))\n    time.sleep(2)\n    print(&quot;结果: &#123;&#125;&quot;.format(8 ** 20))\n\n\nif __name__==&#39;__main__&#39;:\n    start = time.time()\n    print(&#39;这是主线程：&#123;&#125;&#39;.format(threading.current_thread().name))\n    t1 = threading.Thread(target=long_time_task, args=(1,))\n    t2 = threading.Thread(target=long_time_task, args=(2,))\n    t1.start()\n    t2.start()\n\n    t1.join()\n    t2.join()\n    end = time.time()\n    print(&quot;总共用时&#123;&#125;秒&quot;.format((end - start)))\n\n这是主线程：MainThread\n当前子线程: Thread-1316 - 任务1\n当前子线程: Thread-1317 - 任务2\n结果: 1152921504606846976\n结果: 1152921504606846976\n总共用时2.0029757022857666秒</code></pre>\n<p>连结果都跟多进程一样，不是说GIL导致始终是单线程的性能吗？观察任务的组成会发现，计算任务基本不耗时间，主要是sleep ,但是sleep在运行中是会被当作类似IO的操作，被识别为闲置的线程，这时候GIL直接就开始切换线程了，所以sleep的时间实际上是并行执行的，因为不依赖CPU计算，依此推测计算部分还是应该是单线程的，因为依赖CPU，那只需要增加计算任务的时间即可，那就加大指数为 $8^{1000000}$</p>\n<pre><code class=\"python\">def long_time_task(i):\n    print(&#39;当前子线程: &#123;&#125; - 任务&#123;&#125;&#39;.format(threading.current_thread().name, i))\n    time.sleep(2)\n    t = time.time()\n    print(&quot;结果: &#123;&#125;&quot;.format(8 ** 1000000))\n    print(&#39;计算用时： &#39;, time.time()-t)\n# 单独执行时间 long_time_task(1)\n计算时间 : 11.462863683700562\n总共用时13.43696641921997秒\n# 多线程时间 \n计算时间 : 22.85654377937317\n计算时间 : 23.088494300842285\n总共用时25.10121512413025秒\n</code></pre>\n<p>结果上，总计算时间是翻倍的，但是每个线程的输出时间却是翻倍后的22s，而不是单线程的11s，考虑到并发执行，每个线程执行到任何一个时间，都有可能被中断，切换到其他进程，这里的单线程输出时间就可以理解了，那就是线程1执行玩计算任务，还没输出时间，就被切换到线程2的计算来了，最后统一输出线程1和线程2的时间，那么结果二者都是22s左右。总之，可以确定的计算密集型任务，Python的GIL充分保证的单线程的性能。</p>\n<p>　　多线程的数据共享和通信，寻找线程安全的数据结构即可，queue.Queue就是的</p>\n<pre><code class=\"python\">from queue import Queue\nimport random, threading, time\n\n\n# 生产者类\nclass Producer(threading.Thread):\n    def __init__(self, name, queue):\n        threading.Thread.__init__(self, name=name)\n        self.queue = queue\n\n    def run(self):\n        for i in range(1, 5):\n            print(&quot;&#123;&#125; is producing &#123;&#125; to the queue!&quot;.format(self.getName(), i))\n            self.queue.put(i)\n            time.sleep(random.randrange(10) / 5)\n        print(&quot;%s finished!&quot; % self.getName())\n\n\n# 消费者类\nclass Consumer(threading.Thread):\n    def __init__(self, name, queue):\n        threading.Thread.__init__(self, name=name)\n        self.queue = queue\n\n    def run(self):\n        for i in range(1, 5):\n            val = self.queue.get()\n            print(&quot;&#123;&#125; is consuming &#123;&#125; in the queue.&quot;.format(self.getName(), val))\n            time.sleep(random.randrange(10))\n        print(&quot;%s finished!&quot; % self.getName())\n\n\ndef main():\n    queue = Queue()\n    producer = Producer(&#39;Producer&#39;, queue)\n    consumer = Consumer(&#39;Consumer&#39;, queue)\n\n    producer.start()\n    consumer.start()\n\n    producer.join()\n    consumer.join()\n    print(&#39;All threads finished!&#39;)\n\n\nif __name__ == &#39;__main__&#39;:\n    main()</code></pre>\n<h2 id=\"计算密集型-vs-IO密集型\"><a href=\"#计算密集型-vs-IO密集型\" class=\"headerlink\" title=\"计算密集型 vs IO密集型\"></a>计算密集型 vs IO密集型</h2><p>　　对于计算密集型任务，python下的多线程改写是不会有性能提升的，只有多进程才能利用多核CPU，这是明显区别于其他语言的特点，因为在其他高效语言中，多进程和多线程都是作为并发的手段，同时改善程序性能的，而且本身Python作为脚本语言，在计算密集型任务面前运行效率就不高，此时更高阶的手段是用C语言重构 或者 引入高效的其他库来代替。</p>\n<p>　　对于IO密集型，Python本身的多线程就能较好的应对，因为线程调度保证了IO任务的并行执行，此时就算是CPU有多个线程资源，任务本身就无法充分利用，所以Python的单线程限制并不构成瓶颈，就算用其他语言改写成真多线程，提升也不明显，所以类似web应用的IO密集型任务，Python足矣。</p>\n<h2 id=\"异步IO-，事件驱动模型，-协程，单进程单线程模型\"><a href=\"#异步IO-，事件驱动模型，-协程，单进程单线程模型\" class=\"headerlink\" title=\"异步IO ，事件驱动模型， 协程，单进程单线程模型\"></a>异步IO ，事件驱动模型， 协程，单进程单线程模型</h2><p>　　考虑到CPU和IO之间巨大的速度差异，一个任务在执行的过程中大部分时间都在等待IO操作，单进程单线程模型会导致别的任务无法并行执行，因此，才需要多进程模型或者多线程模型来支持多任务并发执行。</p>\n<p>　　现代操作系统对IO操作已经做了巨大的改进，最大的特点就是支持异步IO。如果充分利用操作系统提供的异步IO支持，就可以用单进程单线程模型来执行多任务，这种全新的模型称为事件驱动模型，Nginx就是支持异步IO的Web服务器，它在单核CPU上采用单进程模型就可以高效地支持多任务。在多核CPU上，可以运行多个进程（数量与CPU核心数相同），充分利用多核CPU。由于系统总的进程数量十分有限，因此操作系统调度非常高效。用异步IO编程模型来实现多任务是一个主要的趋势。</p>\n<p>　　对应到Python语言，单线程的异步编程模型称为协程，有了协程的支持，就可以基于事件驱动编写高效的多任务程序。</p>\n<h2 id=\"Conclude\"><a href=\"#Conclude\" class=\"headerlink\" title=\"Conclude\"></a>Conclude</h2><p>　　用代码完整的梳理了一下并发下的进程和线程，同时总结了一下使用场景，很多知识都连接起来了，接下来一个重要的部分就是Python的核心部分，异步IO编程模型-协程</p>\n<p>　　关于协程的实践部分，写篇文章会主要介绍。</p>\n<p>References:</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/46368084\">一文看懂Python多进程与多线程编程</a></p>\n<p><a href=\"https://www.liaoxuefeng.com/wiki/1016959663602400/1017631469467456\">进程 vs 线程 廖雪峰</a></p>\n<p><a href=\"https://python3-cookbook.readthedocs.io/zh_CN/latest/c12/p03_communicating_between_threads.html\">线程间通信 Python3 CookBook</a></p>\n"},{"title":"2020 - 失望与重生","toc":true,"mathjax":true,"top":false,"cover":true,"date":"2021-01-04T08:06:49.000Z","updated":"2021-01-04T08:08:13.230Z","_content":"\n","source":"_drafts/2020-失望与重生.md","raw":"---\ntitle: 2020 - 失望与重生\ntoc: true\nmathjax: true\ntop: false\ncover: true\ndate: 2021-01-04 16:06:49\nupdated:\ncategories: 随笔\ntags:\n\t- 随笔\n\t- 总结\n---\n\n","slug":"2020-失望与重生","published":0,"comments":1,"layout":"post","photos":[],"link":"","_id":"ckowlff4h0000ye2852873zj8","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"业余时间的创造力和执行力","toc":true,"mathjax":true,"top":false,"cover":true,"date":"2021-04-23T06:56:17.000Z","updated":"2021-05-10T02:53:15.929Z","_content":"\n　　尽管上一篇Blog，有详尽分析过自己在执行力和自律方面存在的问题，在实践中慢慢摸索，算是找到门道，渐渐在工作时间找到了目标和认真的态度，工作和学习逐渐步入正轨，但是一直有一大块的业余时间并没有充分利用起来，挣扎过数次，但是每次都是无功而返，禁不住让我深思到底是哪方面的原因，让自己迟迟不愿意利用业余时间来做自己想要做的事情。\n\n　　首先讨论一下自己在执行力上的一些努力和尝试，认识到自己的目标是提升科研能力的时候，工作时间就变得非常充实起来，面对陌生的技术和框架，开始应对的游刃有余起来，特别是用一颗平常心去面对每周的得与失，能很清楚的看到自己做得好的地方和做的差的地方，在周总结中正确的点明之后，就算没有刻意去纠正，意识到做得不好的这种心态也就慢慢的让自己在精进；同时强调了总结和复习的重要性，并且非常认真的实践起来，主要原因还是认识到了，复习的性价比太高了，在刚刚实践或者是学习之后，知识和经验的新鲜期尚存的时候，复习起来只需要数十分钟的总结，就能让这部分知识长久的保存起来，似乎就像压缩包一样，把大量的文本内容压缩成很小的一个包，然后分门别类的存在大脑仓库里，而且只占用很小的空间，怎么看都是非常高效的学习手段。这个道理在学生时代其实早就有了成功的实践经验了，只不过切换到工作的时候，渐渐淡忘了，这次也是在边实践边观察中，重新发现了这项工作的重要性，慢慢的开始重视起来了。保持这个习惯，相比在以后的实践中，也会逐渐发现新的东西。\n\n　　上述就是一些收获，此外，还收获了一些教训，就是自己的努力想把当下的事情做的最好的完美主义倾向，让事情进度变得比预想中要更加缓慢，一段时间之后，无论是目标本身进入瓶劲期，还是自己最初的热情消退之后，自己又开始有点对自己某些事情原地踏步的状态产生了不满甚至嫌弃的想法，近乎本能的产生了自卑感，不加思考的打退堂鼓并要开始逃避放弃了。直到把这种思维路径完整的描述出来之后，我才发现，这是一个思维陷阱。自己主观上并没有任何打算放弃的想法，只是因为一点小小的挫折就开始逃避，我觉得是非常不应该的。况且，我想我并不是那么不堪一击的脆弱的人。但是为何总在潜意识里会掉入这样的思维陷阱呢？每次事情没有进展的时候，我总是本能听到了一句对自己斥责：“连这点事情都做不好，还不如当初放弃算了，你可真是个十足的废物！”，让我想到了不开心的童年，从现在的我出发观察小时候的我，发现那时的我，是一个自尊心极强的人，下棋输了会忍不住落泪，做任何事都有要争第一的乐观和自信，这样的行为肯定会经常遭遇失败和挫折，如果只是这样，我想自己会通过不断的尝试和学习来改善，因为我非常明白不服输的人是多么的可怕；但是我却经常在挫折之后，还不断遭遇嘲弄，特别是长辈看到自尊心强的小朋友的时候，经常会被逗笑，而在我看来，这是一种十足的嘲笑，这让随后到来的安慰显得那么虚假，甚至很多时候，嘲笑之后并没有安慰和鼓励，这是长辈心情好的时候发生的事情；一旦心情不好的时候，迎接我的只有无尽的说教，训斥和辱骂以及体罚，愈强的自尊，只会遭致愈强烈的责罚。身体的伤痛会消失，言语的折磨却仍然存在，尽管我早就刻意去忘记那些听到的责骂，但是我总是潜意识里觉得自己是个废物，会为自己没做到的事情失落自责，并且会常常对自己说这句话：“连这点事情都做不好，还不如当初放弃算了，你可真是个十足的废物！”，再加上，至今我都有非常强烈的想要让长辈满意的本能，所以才会特别在意他们的看法和评价，所以开始觉得自己或许真的是个废物，或者真的当初放弃的话会不会更好？然而结果只是成为他们口中更加没用的那个人。 所以，一旦意识到自己在某个方面没什么进展的时候，就会没来由的失落，不断对自己重复这句话，试图激励自己，结果只得到反效果，然后开始逃避直到放弃。到后来，这个套路用的如此之熟练，以至于碰见不想面对的事情，直接逃避就好了，甚至连这个事情大概是什么估计也只有个模糊的印象。~~（每当回忆童年和十年前的事情的时刻，心总是会忍不住抽痛，物理意义上的心脏部位的疼痛，甚至我开始有点儿怀恋这种感觉，因为只有这种或许称之为心碎的感觉，让我非常确定，这些不愉快的经历不是我在回忆里过分夸大这种痛苦，而是无论是在事情发生的时候还是回忆的时候，无差别出现的心疼，告诉我这些过往都是真实可触的，童年的回忆就到此为止吧，因为我已经有点无法承受这样的抽痛了）~~\n\n　　这种因事情进展不顺利，而去选择回避的做法令我吃了很多苦头，明明内心是渴望的挑战，却在需要放手一搏的时候，只是因为可能不是那么重要的挫折便放弃了，韧性不足，对自己缺乏耐心是主因，其次就是没有想要去发掘导致不顺利的主要原因，如果理性去分析的话，会发现很多时候，都是一些外部因素或者是自己没有重视等花点精力去纠正就能轻松解决的事情。比如鼓的练习，每次练到快要有成果的时候，卡在一些稍微有点难度的地方，不想花时间去克服，却又希望着能完整的把曲子演奏出来，毛毛躁躁的尝试强行演奏，并不理想的结果则阻止了自己更进一步；背单词也是，长期受到拖延症的困扰，只要gap了一段时间，就更加不想重新拿起单词书了，只想着破罐子破摔，无法坦然面对自己懒散拖延的事实；写Blog也是，建站初期，兴趣盎然，什么事都想写下来，感觉很有收获，一旦有这种自满自足的感觉，就不由自主的开始gap，直到消耗完这种自满或者遭遇更大的挫折，就开始痛定思痛的发奋起来，抑或是不愿意去提起这种不理想的残缺的拖延状态，无视紧接着就是毫无缘由的放弃。这还不包括那些可能本质上并不适合自己的一些浅尝辄止的尝试。林林总总，诸如此类的问题，其实大部分都是源于，**目的单纯的追求，却用功利的手段去实现**，最后执行力被功利的手段本身给否定了，完全忘记了初衷可能跟功利完全无关，只是为了满足自己的内心而已。比如，前段时间看到一些不错的MAD视频，觉得质量很不错，觉得有机会自己也想把喜欢的动漫做一个类似的剪辑，顺便熟悉一下如何做视频，但是由于优先级比较低，再加上空余时间有点舍不得花在这种心理上觉得是正事的目标上，就在一直拖到现在都没什么太大进展，再回顾这个事情的时候，完全忘记了自己的初衷，所以就算安排了学习剪辑视频之后，也迟迟不愿意开始动手操作，久拖之下，就开始打退堂鼓了，甚至质疑自己为什么要做这个，直到近日看一个新海诚的非常优质的MAD，大为震动，又重燃做视频的冲动。尽管事情本质上是自己没有在最佳时机做这件事，导致另一个时间来执行的时候，出现后续乏力的情况；但是在事情没有进展的时候，总是往放弃的方向靠，则不是解决问题的好办法，起码要对自己对事情多一份耐心，解决拖延并不是只有放弃这一条路可走，发现问题，分析问题，解决问题才是正解；对自己宽容并不是让自己纵容得过且过的行为，而是从失败的灰烬中寻找新的出路。**找到问题的根本的原因，是解决任何问题的核心**，如果在做视频的时候，目标薄弱，意志力涣散，与其在不知道和什么斗争的情况下挣扎，还不如仔细思考一下，为什么会出现这种状况，再做下一步行动，很显然，意识到是没有找到做视频的理由的情况下，看几部厉害的MAD，牵引出心里的创意和想法，而不是盲目的模仿，就跟写代码一样，最重要的部分并不是敲代码，而是对代码的功能的规划和定义。另一方面，其实看到很棒的MAD的时候，会产生一种羡慕嫉妒，感觉自己似乎无论如何也无法想出这样的创意的念头，这也限制了自己的目标，更进一步思考的话，我本来一开始就没打算成为这样的MAD作者，只是看到很厉害的作品的时候，惊叹的同时，进化出的不正常的想要变成跟他一样的的想法，想来跟小时候“学学别人家的孩子”之类的教导不无关系，其实更应该珍视的是自己的想法和创意。 总之，找到初心之后，怎么规划接下来做视频的事情，会更加游刃有余。\n\n　　无论是做视频，写Blog还是学鼓，都是一件很单纯的事情，就是付出了时间和练习，可以很确定能达成的事情，通过回顾自己的写日记的历程就可以发现，我并没有接受任何文字方面的训练，学生时期的作文能力一塌糊涂，直到发现曾经接受的作文训练都是些矫揉造作的无病呻吟，通过王小波的文字，发现了文字的魅力并非词藻的堆彻，而是思想的表达和知识的总结，打开了文字的新世界，用逻辑思维来重新整理每日所思所想以及所为，总是能发现很多新东西，不断的认识自己，同时总结经验和知识，利用费曼学习法，不断增强对知识的理解。能达到这个目的，我就知足了。只是偶尔会出现写Blog卡壳的情况，大部分都是还没想好要写什么的情况下仓促下笔导致的，\n\n\n\n","source":"_drafts/业余时间的创造力和执行力.md","raw":"---\ntitle: 业余时间的创造力和执行力\ntoc: true\nmathjax: true\ntop: false\ncover: true\ndate: 2021-04-23 14:56:17\nupdated:\ncategories: 随笔\ntags: \n\t- 随笔\n---\n\n　　尽管上一篇Blog，有详尽分析过自己在执行力和自律方面存在的问题，在实践中慢慢摸索，算是找到门道，渐渐在工作时间找到了目标和认真的态度，工作和学习逐渐步入正轨，但是一直有一大块的业余时间并没有充分利用起来，挣扎过数次，但是每次都是无功而返，禁不住让我深思到底是哪方面的原因，让自己迟迟不愿意利用业余时间来做自己想要做的事情。\n\n　　首先讨论一下自己在执行力上的一些努力和尝试，认识到自己的目标是提升科研能力的时候，工作时间就变得非常充实起来，面对陌生的技术和框架，开始应对的游刃有余起来，特别是用一颗平常心去面对每周的得与失，能很清楚的看到自己做得好的地方和做的差的地方，在周总结中正确的点明之后，就算没有刻意去纠正，意识到做得不好的这种心态也就慢慢的让自己在精进；同时强调了总结和复习的重要性，并且非常认真的实践起来，主要原因还是认识到了，复习的性价比太高了，在刚刚实践或者是学习之后，知识和经验的新鲜期尚存的时候，复习起来只需要数十分钟的总结，就能让这部分知识长久的保存起来，似乎就像压缩包一样，把大量的文本内容压缩成很小的一个包，然后分门别类的存在大脑仓库里，而且只占用很小的空间，怎么看都是非常高效的学习手段。这个道理在学生时代其实早就有了成功的实践经验了，只不过切换到工作的时候，渐渐淡忘了，这次也是在边实践边观察中，重新发现了这项工作的重要性，慢慢的开始重视起来了。保持这个习惯，相比在以后的实践中，也会逐渐发现新的东西。\n\n　　上述就是一些收获，此外，还收获了一些教训，就是自己的努力想把当下的事情做的最好的完美主义倾向，让事情进度变得比预想中要更加缓慢，一段时间之后，无论是目标本身进入瓶劲期，还是自己最初的热情消退之后，自己又开始有点对自己某些事情原地踏步的状态产生了不满甚至嫌弃的想法，近乎本能的产生了自卑感，不加思考的打退堂鼓并要开始逃避放弃了。直到把这种思维路径完整的描述出来之后，我才发现，这是一个思维陷阱。自己主观上并没有任何打算放弃的想法，只是因为一点小小的挫折就开始逃避，我觉得是非常不应该的。况且，我想我并不是那么不堪一击的脆弱的人。但是为何总在潜意识里会掉入这样的思维陷阱呢？每次事情没有进展的时候，我总是本能听到了一句对自己斥责：“连这点事情都做不好，还不如当初放弃算了，你可真是个十足的废物！”，让我想到了不开心的童年，从现在的我出发观察小时候的我，发现那时的我，是一个自尊心极强的人，下棋输了会忍不住落泪，做任何事都有要争第一的乐观和自信，这样的行为肯定会经常遭遇失败和挫折，如果只是这样，我想自己会通过不断的尝试和学习来改善，因为我非常明白不服输的人是多么的可怕；但是我却经常在挫折之后，还不断遭遇嘲弄，特别是长辈看到自尊心强的小朋友的时候，经常会被逗笑，而在我看来，这是一种十足的嘲笑，这让随后到来的安慰显得那么虚假，甚至很多时候，嘲笑之后并没有安慰和鼓励，这是长辈心情好的时候发生的事情；一旦心情不好的时候，迎接我的只有无尽的说教，训斥和辱骂以及体罚，愈强的自尊，只会遭致愈强烈的责罚。身体的伤痛会消失，言语的折磨却仍然存在，尽管我早就刻意去忘记那些听到的责骂，但是我总是潜意识里觉得自己是个废物，会为自己没做到的事情失落自责，并且会常常对自己说这句话：“连这点事情都做不好，还不如当初放弃算了，你可真是个十足的废物！”，再加上，至今我都有非常强烈的想要让长辈满意的本能，所以才会特别在意他们的看法和评价，所以开始觉得自己或许真的是个废物，或者真的当初放弃的话会不会更好？然而结果只是成为他们口中更加没用的那个人。 所以，一旦意识到自己在某个方面没什么进展的时候，就会没来由的失落，不断对自己重复这句话，试图激励自己，结果只得到反效果，然后开始逃避直到放弃。到后来，这个套路用的如此之熟练，以至于碰见不想面对的事情，直接逃避就好了，甚至连这个事情大概是什么估计也只有个模糊的印象。~~（每当回忆童年和十年前的事情的时刻，心总是会忍不住抽痛，物理意义上的心脏部位的疼痛，甚至我开始有点儿怀恋这种感觉，因为只有这种或许称之为心碎的感觉，让我非常确定，这些不愉快的经历不是我在回忆里过分夸大这种痛苦，而是无论是在事情发生的时候还是回忆的时候，无差别出现的心疼，告诉我这些过往都是真实可触的，童年的回忆就到此为止吧，因为我已经有点无法承受这样的抽痛了）~~\n\n　　这种因事情进展不顺利，而去选择回避的做法令我吃了很多苦头，明明内心是渴望的挑战，却在需要放手一搏的时候，只是因为可能不是那么重要的挫折便放弃了，韧性不足，对自己缺乏耐心是主因，其次就是没有想要去发掘导致不顺利的主要原因，如果理性去分析的话，会发现很多时候，都是一些外部因素或者是自己没有重视等花点精力去纠正就能轻松解决的事情。比如鼓的练习，每次练到快要有成果的时候，卡在一些稍微有点难度的地方，不想花时间去克服，却又希望着能完整的把曲子演奏出来，毛毛躁躁的尝试强行演奏，并不理想的结果则阻止了自己更进一步；背单词也是，长期受到拖延症的困扰，只要gap了一段时间，就更加不想重新拿起单词书了，只想着破罐子破摔，无法坦然面对自己懒散拖延的事实；写Blog也是，建站初期，兴趣盎然，什么事都想写下来，感觉很有收获，一旦有这种自满自足的感觉，就不由自主的开始gap，直到消耗完这种自满或者遭遇更大的挫折，就开始痛定思痛的发奋起来，抑或是不愿意去提起这种不理想的残缺的拖延状态，无视紧接着就是毫无缘由的放弃。这还不包括那些可能本质上并不适合自己的一些浅尝辄止的尝试。林林总总，诸如此类的问题，其实大部分都是源于，**目的单纯的追求，却用功利的手段去实现**，最后执行力被功利的手段本身给否定了，完全忘记了初衷可能跟功利完全无关，只是为了满足自己的内心而已。比如，前段时间看到一些不错的MAD视频，觉得质量很不错，觉得有机会自己也想把喜欢的动漫做一个类似的剪辑，顺便熟悉一下如何做视频，但是由于优先级比较低，再加上空余时间有点舍不得花在这种心理上觉得是正事的目标上，就在一直拖到现在都没什么太大进展，再回顾这个事情的时候，完全忘记了自己的初衷，所以就算安排了学习剪辑视频之后，也迟迟不愿意开始动手操作，久拖之下，就开始打退堂鼓了，甚至质疑自己为什么要做这个，直到近日看一个新海诚的非常优质的MAD，大为震动，又重燃做视频的冲动。尽管事情本质上是自己没有在最佳时机做这件事，导致另一个时间来执行的时候，出现后续乏力的情况；但是在事情没有进展的时候，总是往放弃的方向靠，则不是解决问题的好办法，起码要对自己对事情多一份耐心，解决拖延并不是只有放弃这一条路可走，发现问题，分析问题，解决问题才是正解；对自己宽容并不是让自己纵容得过且过的行为，而是从失败的灰烬中寻找新的出路。**找到问题的根本的原因，是解决任何问题的核心**，如果在做视频的时候，目标薄弱，意志力涣散，与其在不知道和什么斗争的情况下挣扎，还不如仔细思考一下，为什么会出现这种状况，再做下一步行动，很显然，意识到是没有找到做视频的理由的情况下，看几部厉害的MAD，牵引出心里的创意和想法，而不是盲目的模仿，就跟写代码一样，最重要的部分并不是敲代码，而是对代码的功能的规划和定义。另一方面，其实看到很棒的MAD的时候，会产生一种羡慕嫉妒，感觉自己似乎无论如何也无法想出这样的创意的念头，这也限制了自己的目标，更进一步思考的话，我本来一开始就没打算成为这样的MAD作者，只是看到很厉害的作品的时候，惊叹的同时，进化出的不正常的想要变成跟他一样的的想法，想来跟小时候“学学别人家的孩子”之类的教导不无关系，其实更应该珍视的是自己的想法和创意。 总之，找到初心之后，怎么规划接下来做视频的事情，会更加游刃有余。\n\n　　无论是做视频，写Blog还是学鼓，都是一件很单纯的事情，就是付出了时间和练习，可以很确定能达成的事情，通过回顾自己的写日记的历程就可以发现，我并没有接受任何文字方面的训练，学生时期的作文能力一塌糊涂，直到发现曾经接受的作文训练都是些矫揉造作的无病呻吟，通过王小波的文字，发现了文字的魅力并非词藻的堆彻，而是思想的表达和知识的总结，打开了文字的新世界，用逻辑思维来重新整理每日所思所想以及所为，总是能发现很多新东西，不断的认识自己，同时总结经验和知识，利用费曼学习法，不断增强对知识的理解。能达到这个目的，我就知足了。只是偶尔会出现写Blog卡壳的情况，大部分都是还没想好要写什么的情况下仓促下笔导致的，\n\n\n\n","slug":"业余时间的创造力和执行力","published":0,"comments":1,"layout":"post","photos":[],"link":"","_id":"ckowlff4j0001ye28a2r33w49","content":"<p>　　尽管上一篇Blog，有详尽分析过自己在执行力和自律方面存在的问题，在实践中慢慢摸索，算是找到门道，渐渐在工作时间找到了目标和认真的态度，工作和学习逐渐步入正轨，但是一直有一大块的业余时间并没有充分利用起来，挣扎过数次，但是每次都是无功而返，禁不住让我深思到底是哪方面的原因，让自己迟迟不愿意利用业余时间来做自己想要做的事情。</p>\n<p>　　首先讨论一下自己在执行力上的一些努力和尝试，认识到自己的目标是提升科研能力的时候，工作时间就变得非常充实起来，面对陌生的技术和框架，开始应对的游刃有余起来，特别是用一颗平常心去面对每周的得与失，能很清楚的看到自己做得好的地方和做的差的地方，在周总结中正确的点明之后，就算没有刻意去纠正，意识到做得不好的这种心态也就慢慢的让自己在精进；同时强调了总结和复习的重要性，并且非常认真的实践起来，主要原因还是认识到了，复习的性价比太高了，在刚刚实践或者是学习之后，知识和经验的新鲜期尚存的时候，复习起来只需要数十分钟的总结，就能让这部分知识长久的保存起来，似乎就像压缩包一样，把大量的文本内容压缩成很小的一个包，然后分门别类的存在大脑仓库里，而且只占用很小的空间，怎么看都是非常高效的学习手段。这个道理在学生时代其实早就有了成功的实践经验了，只不过切换到工作的时候，渐渐淡忘了，这次也是在边实践边观察中，重新发现了这项工作的重要性，慢慢的开始重视起来了。保持这个习惯，相比在以后的实践中，也会逐渐发现新的东西。</p>\n<p>　　上述就是一些收获，此外，还收获了一些教训，就是自己的努力想把当下的事情做的最好的完美主义倾向，让事情进度变得比预想中要更加缓慢，一段时间之后，无论是目标本身进入瓶劲期，还是自己最初的热情消退之后，自己又开始有点对自己某些事情原地踏步的状态产生了不满甚至嫌弃的想法，近乎本能的产生了自卑感，不加思考的打退堂鼓并要开始逃避放弃了。直到把这种思维路径完整的描述出来之后，我才发现，这是一个思维陷阱。自己主观上并没有任何打算放弃的想法，只是因为一点小小的挫折就开始逃避，我觉得是非常不应该的。况且，我想我并不是那么不堪一击的脆弱的人。但是为何总在潜意识里会掉入这样的思维陷阱呢？每次事情没有进展的时候，我总是本能听到了一句对自己斥责：“连这点事情都做不好，还不如当初放弃算了，你可真是个十足的废物！”，让我想到了不开心的童年，从现在的我出发观察小时候的我，发现那时的我，是一个自尊心极强的人，下棋输了会忍不住落泪，做任何事都有要争第一的乐观和自信，这样的行为肯定会经常遭遇失败和挫折，如果只是这样，我想自己会通过不断的尝试和学习来改善，因为我非常明白不服输的人是多么的可怕；但是我却经常在挫折之后，还不断遭遇嘲弄，特别是长辈看到自尊心强的小朋友的时候，经常会被逗笑，而在我看来，这是一种十足的嘲笑，这让随后到来的安慰显得那么虚假，甚至很多时候，嘲笑之后并没有安慰和鼓励，这是长辈心情好的时候发生的事情；一旦心情不好的时候，迎接我的只有无尽的说教，训斥和辱骂以及体罚，愈强的自尊，只会遭致愈强烈的责罚。身体的伤痛会消失，言语的折磨却仍然存在，尽管我早就刻意去忘记那些听到的责骂，但是我总是潜意识里觉得自己是个废物，会为自己没做到的事情失落自责，并且会常常对自己说这句话：“连这点事情都做不好，还不如当初放弃算了，你可真是个十足的废物！”，再加上，至今我都有非常强烈的想要让长辈满意的本能，所以才会特别在意他们的看法和评价，所以开始觉得自己或许真的是个废物，或者真的当初放弃的话会不会更好？然而结果只是成为他们口中更加没用的那个人。 所以，一旦意识到自己在某个方面没什么进展的时候，就会没来由的失落，不断对自己重复这句话，试图激励自己，结果只得到反效果，然后开始逃避直到放弃。到后来，这个套路用的如此之熟练，以至于碰见不想面对的事情，直接逃避就好了，甚至连这个事情大概是什么估计也只有个模糊的印象。<del>（每当回忆童年和十年前的事情的时刻，心总是会忍不住抽痛，物理意义上的心脏部位的疼痛，甚至我开始有点儿怀恋这种感觉，因为只有这种或许称之为心碎的感觉，让我非常确定，这些不愉快的经历不是我在回忆里过分夸大这种痛苦，而是无论是在事情发生的时候还是回忆的时候，无差别出现的心疼，告诉我这些过往都是真实可触的，童年的回忆就到此为止吧，因为我已经有点无法承受这样的抽痛了）</del></p>\n<p>　　这种因事情进展不顺利，而去选择回避的做法令我吃了很多苦头，明明内心是渴望的挑战，却在需要放手一搏的时候，只是因为可能不是那么重要的挫折便放弃了，韧性不足，对自己缺乏耐心是主因，其次就是没有想要去发掘导致不顺利的主要原因，如果理性去分析的话，会发现很多时候，都是一些外部因素或者是自己没有重视等花点精力去纠正就能轻松解决的事情。比如鼓的练习，每次练到快要有成果的时候，卡在一些稍微有点难度的地方，不想花时间去克服，却又希望着能完整的把曲子演奏出来，毛毛躁躁的尝试强行演奏，并不理想的结果则阻止了自己更进一步；背单词也是，长期受到拖延症的困扰，只要gap了一段时间，就更加不想重新拿起单词书了，只想着破罐子破摔，无法坦然面对自己懒散拖延的事实；写Blog也是，建站初期，兴趣盎然，什么事都想写下来，感觉很有收获，一旦有这种自满自足的感觉，就不由自主的开始gap，直到消耗完这种自满或者遭遇更大的挫折，就开始痛定思痛的发奋起来，抑或是不愿意去提起这种不理想的残缺的拖延状态，无视紧接着就是毫无缘由的放弃。这还不包括那些可能本质上并不适合自己的一些浅尝辄止的尝试。林林总总，诸如此类的问题，其实大部分都是源于，<strong>目的单纯的追求，却用功利的手段去实现</strong>，最后执行力被功利的手段本身给否定了，完全忘记了初衷可能跟功利完全无关，只是为了满足自己的内心而已。比如，前段时间看到一些不错的MAD视频，觉得质量很不错，觉得有机会自己也想把喜欢的动漫做一个类似的剪辑，顺便熟悉一下如何做视频，但是由于优先级比较低，再加上空余时间有点舍不得花在这种心理上觉得是正事的目标上，就在一直拖到现在都没什么太大进展，再回顾这个事情的时候，完全忘记了自己的初衷，所以就算安排了学习剪辑视频之后，也迟迟不愿意开始动手操作，久拖之下，就开始打退堂鼓了，甚至质疑自己为什么要做这个，直到近日看一个新海诚的非常优质的MAD，大为震动，又重燃做视频的冲动。尽管事情本质上是自己没有在最佳时机做这件事，导致另一个时间来执行的时候，出现后续乏力的情况；但是在事情没有进展的时候，总是往放弃的方向靠，则不是解决问题的好办法，起码要对自己对事情多一份耐心，解决拖延并不是只有放弃这一条路可走，发现问题，分析问题，解决问题才是正解；对自己宽容并不是让自己纵容得过且过的行为，而是从失败的灰烬中寻找新的出路。<strong>找到问题的根本的原因，是解决任何问题的核心</strong>，如果在做视频的时候，目标薄弱，意志力涣散，与其在不知道和什么斗争的情况下挣扎，还不如仔细思考一下，为什么会出现这种状况，再做下一步行动，很显然，意识到是没有找到做视频的理由的情况下，看几部厉害的MAD，牵引出心里的创意和想法，而不是盲目的模仿，就跟写代码一样，最重要的部分并不是敲代码，而是对代码的功能的规划和定义。另一方面，其实看到很棒的MAD的时候，会产生一种羡慕嫉妒，感觉自己似乎无论如何也无法想出这样的创意的念头，这也限制了自己的目标，更进一步思考的话，我本来一开始就没打算成为这样的MAD作者，只是看到很厉害的作品的时候，惊叹的同时，进化出的不正常的想要变成跟他一样的的想法，想来跟小时候“学学别人家的孩子”之类的教导不无关系，其实更应该珍视的是自己的想法和创意。 总之，找到初心之后，怎么规划接下来做视频的事情，会更加游刃有余。</p>\n<p>　　无论是做视频，写Blog还是学鼓，都是一件很单纯的事情，就是付出了时间和练习，可以很确定能达成的事情，通过回顾自己的写日记的历程就可以发现，我并没有接受任何文字方面的训练，学生时期的作文能力一塌糊涂，直到发现曾经接受的作文训练都是些矫揉造作的无病呻吟，通过王小波的文字，发现了文字的魅力并非词藻的堆彻，而是思想的表达和知识的总结，打开了文字的新世界，用逻辑思维来重新整理每日所思所想以及所为，总是能发现很多新东西，不断的认识自己，同时总结经验和知识，利用费曼学习法，不断增强对知识的理解。能达到这个目的，我就知足了。只是偶尔会出现写Blog卡壳的情况，大部分都是还没想好要写什么的情况下仓促下笔导致的，</p>\n","site":{"data":{}},"excerpt":"","more":"<p>　　尽管上一篇Blog，有详尽分析过自己在执行力和自律方面存在的问题，在实践中慢慢摸索，算是找到门道，渐渐在工作时间找到了目标和认真的态度，工作和学习逐渐步入正轨，但是一直有一大块的业余时间并没有充分利用起来，挣扎过数次，但是每次都是无功而返，禁不住让我深思到底是哪方面的原因，让自己迟迟不愿意利用业余时间来做自己想要做的事情。</p>\n<p>　　首先讨论一下自己在执行力上的一些努力和尝试，认识到自己的目标是提升科研能力的时候，工作时间就变得非常充实起来，面对陌生的技术和框架，开始应对的游刃有余起来，特别是用一颗平常心去面对每周的得与失，能很清楚的看到自己做得好的地方和做的差的地方，在周总结中正确的点明之后，就算没有刻意去纠正，意识到做得不好的这种心态也就慢慢的让自己在精进；同时强调了总结和复习的重要性，并且非常认真的实践起来，主要原因还是认识到了，复习的性价比太高了，在刚刚实践或者是学习之后，知识和经验的新鲜期尚存的时候，复习起来只需要数十分钟的总结，就能让这部分知识长久的保存起来，似乎就像压缩包一样，把大量的文本内容压缩成很小的一个包，然后分门别类的存在大脑仓库里，而且只占用很小的空间，怎么看都是非常高效的学习手段。这个道理在学生时代其实早就有了成功的实践经验了，只不过切换到工作的时候，渐渐淡忘了，这次也是在边实践边观察中，重新发现了这项工作的重要性，慢慢的开始重视起来了。保持这个习惯，相比在以后的实践中，也会逐渐发现新的东西。</p>\n<p>　　上述就是一些收获，此外，还收获了一些教训，就是自己的努力想把当下的事情做的最好的完美主义倾向，让事情进度变得比预想中要更加缓慢，一段时间之后，无论是目标本身进入瓶劲期，还是自己最初的热情消退之后，自己又开始有点对自己某些事情原地踏步的状态产生了不满甚至嫌弃的想法，近乎本能的产生了自卑感，不加思考的打退堂鼓并要开始逃避放弃了。直到把这种思维路径完整的描述出来之后，我才发现，这是一个思维陷阱。自己主观上并没有任何打算放弃的想法，只是因为一点小小的挫折就开始逃避，我觉得是非常不应该的。况且，我想我并不是那么不堪一击的脆弱的人。但是为何总在潜意识里会掉入这样的思维陷阱呢？每次事情没有进展的时候，我总是本能听到了一句对自己斥责：“连这点事情都做不好，还不如当初放弃算了，你可真是个十足的废物！”，让我想到了不开心的童年，从现在的我出发观察小时候的我，发现那时的我，是一个自尊心极强的人，下棋输了会忍不住落泪，做任何事都有要争第一的乐观和自信，这样的行为肯定会经常遭遇失败和挫折，如果只是这样，我想自己会通过不断的尝试和学习来改善，因为我非常明白不服输的人是多么的可怕；但是我却经常在挫折之后，还不断遭遇嘲弄，特别是长辈看到自尊心强的小朋友的时候，经常会被逗笑，而在我看来，这是一种十足的嘲笑，这让随后到来的安慰显得那么虚假，甚至很多时候，嘲笑之后并没有安慰和鼓励，这是长辈心情好的时候发生的事情；一旦心情不好的时候，迎接我的只有无尽的说教，训斥和辱骂以及体罚，愈强的自尊，只会遭致愈强烈的责罚。身体的伤痛会消失，言语的折磨却仍然存在，尽管我早就刻意去忘记那些听到的责骂，但是我总是潜意识里觉得自己是个废物，会为自己没做到的事情失落自责，并且会常常对自己说这句话：“连这点事情都做不好，还不如当初放弃算了，你可真是个十足的废物！”，再加上，至今我都有非常强烈的想要让长辈满意的本能，所以才会特别在意他们的看法和评价，所以开始觉得自己或许真的是个废物，或者真的当初放弃的话会不会更好？然而结果只是成为他们口中更加没用的那个人。 所以，一旦意识到自己在某个方面没什么进展的时候，就会没来由的失落，不断对自己重复这句话，试图激励自己，结果只得到反效果，然后开始逃避直到放弃。到后来，这个套路用的如此之熟练，以至于碰见不想面对的事情，直接逃避就好了，甚至连这个事情大概是什么估计也只有个模糊的印象。<del>（每当回忆童年和十年前的事情的时刻，心总是会忍不住抽痛，物理意义上的心脏部位的疼痛，甚至我开始有点儿怀恋这种感觉，因为只有这种或许称之为心碎的感觉，让我非常确定，这些不愉快的经历不是我在回忆里过分夸大这种痛苦，而是无论是在事情发生的时候还是回忆的时候，无差别出现的心疼，告诉我这些过往都是真实可触的，童年的回忆就到此为止吧，因为我已经有点无法承受这样的抽痛了）</del></p>\n<p>　　这种因事情进展不顺利，而去选择回避的做法令我吃了很多苦头，明明内心是渴望的挑战，却在需要放手一搏的时候，只是因为可能不是那么重要的挫折便放弃了，韧性不足，对自己缺乏耐心是主因，其次就是没有想要去发掘导致不顺利的主要原因，如果理性去分析的话，会发现很多时候，都是一些外部因素或者是自己没有重视等花点精力去纠正就能轻松解决的事情。比如鼓的练习，每次练到快要有成果的时候，卡在一些稍微有点难度的地方，不想花时间去克服，却又希望着能完整的把曲子演奏出来，毛毛躁躁的尝试强行演奏，并不理想的结果则阻止了自己更进一步；背单词也是，长期受到拖延症的困扰，只要gap了一段时间，就更加不想重新拿起单词书了，只想着破罐子破摔，无法坦然面对自己懒散拖延的事实；写Blog也是，建站初期，兴趣盎然，什么事都想写下来，感觉很有收获，一旦有这种自满自足的感觉，就不由自主的开始gap，直到消耗完这种自满或者遭遇更大的挫折，就开始痛定思痛的发奋起来，抑或是不愿意去提起这种不理想的残缺的拖延状态，无视紧接着就是毫无缘由的放弃。这还不包括那些可能本质上并不适合自己的一些浅尝辄止的尝试。林林总总，诸如此类的问题，其实大部分都是源于，<strong>目的单纯的追求，却用功利的手段去实现</strong>，最后执行力被功利的手段本身给否定了，完全忘记了初衷可能跟功利完全无关，只是为了满足自己的内心而已。比如，前段时间看到一些不错的MAD视频，觉得质量很不错，觉得有机会自己也想把喜欢的动漫做一个类似的剪辑，顺便熟悉一下如何做视频，但是由于优先级比较低，再加上空余时间有点舍不得花在这种心理上觉得是正事的目标上，就在一直拖到现在都没什么太大进展，再回顾这个事情的时候，完全忘记了自己的初衷，所以就算安排了学习剪辑视频之后，也迟迟不愿意开始动手操作，久拖之下，就开始打退堂鼓了，甚至质疑自己为什么要做这个，直到近日看一个新海诚的非常优质的MAD，大为震动，又重燃做视频的冲动。尽管事情本质上是自己没有在最佳时机做这件事，导致另一个时间来执行的时候，出现后续乏力的情况；但是在事情没有进展的时候，总是往放弃的方向靠，则不是解决问题的好办法，起码要对自己对事情多一份耐心，解决拖延并不是只有放弃这一条路可走，发现问题，分析问题，解决问题才是正解；对自己宽容并不是让自己纵容得过且过的行为，而是从失败的灰烬中寻找新的出路。<strong>找到问题的根本的原因，是解决任何问题的核心</strong>，如果在做视频的时候，目标薄弱，意志力涣散，与其在不知道和什么斗争的情况下挣扎，还不如仔细思考一下，为什么会出现这种状况，再做下一步行动，很显然，意识到是没有找到做视频的理由的情况下，看几部厉害的MAD，牵引出心里的创意和想法，而不是盲目的模仿，就跟写代码一样，最重要的部分并不是敲代码，而是对代码的功能的规划和定义。另一方面，其实看到很棒的MAD的时候，会产生一种羡慕嫉妒，感觉自己似乎无论如何也无法想出这样的创意的念头，这也限制了自己的目标，更进一步思考的话，我本来一开始就没打算成为这样的MAD作者，只是看到很厉害的作品的时候，惊叹的同时，进化出的不正常的想要变成跟他一样的的想法，想来跟小时候“学学别人家的孩子”之类的教导不无关系，其实更应该珍视的是自己的想法和创意。 总之，找到初心之后，怎么规划接下来做视频的事情，会更加游刃有余。</p>\n<p>　　无论是做视频，写Blog还是学鼓，都是一件很单纯的事情，就是付出了时间和练习，可以很确定能达成的事情，通过回顾自己的写日记的历程就可以发现，我并没有接受任何文字方面的训练，学生时期的作文能力一塌糊涂，直到发现曾经接受的作文训练都是些矫揉造作的无病呻吟，通过王小波的文字，发现了文字的魅力并非词藻的堆彻，而是思想的表达和知识的总结，打开了文字的新世界，用逻辑思维来重新整理每日所思所想以及所为，总是能发现很多新东西，不断的认识自己，同时总结经验和知识，利用费曼学习法，不断增强对知识的理解。能达到这个目的，我就知足了。只是偶尔会出现写Blog卡壳的情况，大部分都是还没想好要写什么的情况下仓促下笔导致的，</p>\n"},{"title":"高校教师996:关于内卷的反思","toc":true,"mathjax":true,"top":false,"cover":true,"date":"2021-03-19T06:23:00.000Z","updated":"2021-03-22T10:59:39.687Z","_content":"\n　　因为空有一腔科研热情，而缺乏实际的指导和执行力，所以现在的我只能自称为一名科研爱好者！如果拥有足够的勇气和研究资源，我自认为并确信自己可以获得不错的成果的，仅仅只是出于对自己学习力和创新力的基本自信。但是谈到对学术界的人情世界的认知，我则基本属于一无所知，因为相对于牛人的逸闻轶事，我对牛人之所以为牛人的成果和事件显然更感兴趣一点，所以我对于学术界的关注点仅有成果和论文。恰好暗合了最近刚刚学到的来自Elon Musk 的 First-Principles Thinking ，即第一性原理思维，通过研究问题的最根本层面来寻找解决办法。所以形成了我对学术界的一种理想和刻板的认知，只要有优秀的成果，必然会有赢得世界认同的那一天。现实的科研学术界可能并非如此。\n\n　　由于对学术界缺乏一点基本的认知，所以很多关于学术和科研都凭借自己想象和脑补，造成了很多非常不符合现实的偏差。首先\n\n","source":"_drafts/高校教师996-关于内卷的反思.md","raw":"---\ntitle: 高校教师996:关于内卷的反思\ntoc: true\nmathjax: true\ntop: false\ncover: true\ndate: 2021-03-19 14:23:00\nupdated:\ncategories: 随笔\ntags:\n\t- 随笔\n\t- 反思\n\t- 内卷\n---\n\n　　因为空有一腔科研热情，而缺乏实际的指导和执行力，所以现在的我只能自称为一名科研爱好者！如果拥有足够的勇气和研究资源，我自认为并确信自己可以获得不错的成果的，仅仅只是出于对自己学习力和创新力的基本自信。但是谈到对学术界的人情世界的认知，我则基本属于一无所知，因为相对于牛人的逸闻轶事，我对牛人之所以为牛人的成果和事件显然更感兴趣一点，所以我对于学术界的关注点仅有成果和论文。恰好暗合了最近刚刚学到的来自Elon Musk 的 First-Principles Thinking ，即第一性原理思维，通过研究问题的最根本层面来寻找解决办法。所以形成了我对学术界的一种理想和刻板的认知，只要有优秀的成果，必然会有赢得世界认同的那一天。现实的科研学术界可能并非如此。\n\n　　由于对学术界缺乏一点基本的认知，所以很多关于学术和科研都凭借自己想象和脑补，造成了很多非常不符合现实的偏差。首先\n\n","slug":"高校教师996-关于内卷的反思","published":0,"comments":1,"layout":"post","photos":[],"link":"","_id":"ckowlff4o0003ye289kzd8aq8","content":"<p>　　因为空有一腔科研热情，而缺乏实际的指导和执行力，所以现在的我只能自称为一名科研爱好者！如果拥有足够的勇气和研究资源，我自认为并确信自己可以获得不错的成果的，仅仅只是出于对自己学习力和创新力的基本自信。但是谈到对学术界的人情世界的认知，我则基本属于一无所知，因为相对于牛人的逸闻轶事，我对牛人之所以为牛人的成果和事件显然更感兴趣一点，所以我对于学术界的关注点仅有成果和论文。恰好暗合了最近刚刚学到的来自Elon Musk 的 First-Principles Thinking ，即第一性原理思维，通过研究问题的最根本层面来寻找解决办法。所以形成了我对学术界的一种理想和刻板的认知，只要有优秀的成果，必然会有赢得世界认同的那一天。现实的科研学术界可能并非如此。</p>\n<p>　　由于对学术界缺乏一点基本的认知，所以很多关于学术和科研都凭借自己想象和脑补，造成了很多非常不符合现实的偏差。首先</p>\n","site":{"data":{}},"excerpt":"","more":"<p>　　因为空有一腔科研热情，而缺乏实际的指导和执行力，所以现在的我只能自称为一名科研爱好者！如果拥有足够的勇气和研究资源，我自认为并确信自己可以获得不错的成果的，仅仅只是出于对自己学习力和创新力的基本自信。但是谈到对学术界的人情世界的认知，我则基本属于一无所知，因为相对于牛人的逸闻轶事，我对牛人之所以为牛人的成果和事件显然更感兴趣一点，所以我对于学术界的关注点仅有成果和论文。恰好暗合了最近刚刚学到的来自Elon Musk 的 First-Principles Thinking ，即第一性原理思维，通过研究问题的最根本层面来寻找解决办法。所以形成了我对学术界的一种理想和刻板的认知，只要有优秀的成果，必然会有赢得世界认同的那一天。现实的科研学术界可能并非如此。</p>\n<p>　　由于对学术界缺乏一点基本的认知，所以很多关于学术和科研都凭借自己想象和脑补，造成了很多非常不符合现实的偏差。首先</p>\n"},{"title":"135 Candy","toc":true,"mathjax":true,"top":false,"cover":true,"date":"2021-01-13T07:59:39.000Z","updated":"2021-01-13T10:03:53.128Z","_content":"\n> There are *N* children standing in a line. Each child is assigned a rating value.\n>\n> You are giving candies to these children subjected to the following requirements:\n>\n> - Each child must have at least one candy.\n> - Children with a higher rating get more candies than their neighbors.\n>\n> What is the minimum candies you must give?\n>\n> **Example 1:**\n>\n> ```\n> Input: [1,0,2]\n> Output: 5\n> Explanation: You can allocate to the first, second and third child with 2, 1, 2 candies respectively.\n> ```\n>\n> **Example 2:**\n>\n> ```\n> Input: [1,2,2]\n> Output: 4\n> Explanation: You can allocate to the first, second and third child with 1, 2, 1 candies respectively.\n>              The third child gets 1 candy because it satisfies the above two conditions.\n> ```\n\n　　比较容易理解的题目，但是却是Hard难度。比较好入手，却是卡了好几天，最后依赖Discuss才勉强总结出结果，此为解法一；之后在其他 LeetCode的题解上发现了一个很简单的思路，却无法证明正确性，希望这里可以将其证明出来。\n\n### 解法一\n\n　　此题最先想到的思路就是，利用凹函数/凸函数的极小值和极大值的特性，来搜索上坡下坡，遇到上坡理所当然的将所分配的candy数目相较于左边的candy数目+1，如果是下坡，那么就当作上坡的逆序列，反向上坡，思路并没有问题，但是卡在一个非常奇怪的点，那就是峰值Peak 的选取，回头来看，只需要选择上坡和下坡之中的最大值即可，但是实现过程中，这个思路始终得不到重视，反而遇到了诸多乱七八糟的实现困难，最终卡了数天之后不得不放弃，这其中主要就是迟迟无法实现，导致对这个思路的自我怀疑，总是想投机取巧般的小改一下函数，来尝试新方法，结果有不断产生新问题，最后自己都有点迷失方向了。现在想来，似乎在无意义的实践上浪费了很多时间，相反真正应该关注的思路分析上，甚少得到进步。多少有点试图用战术上的勤奋来掩盖战略上的懒惰，情绪上，思路卡顿的时候，自己过分急躁，有点考场上，答不出来，弃之可惜的感觉，没找到正确的打开方式，却慌不择路，心态濒临崩溃，这种感觉无论是当时还在现在都是不愿想起的，看来我还是无法坦然面对自己，一点儿小挫折，就呼天喊地的，真是缺乏意志的考验。\n\n　　回到上坡/下坡，其实关于上坡可以仅仅根据上坡的长度来计算分配的总candy数目，因为起点是1颗，接下来则是递增为1的等差数列，亦即是起点为1的连续递增数量，可以轻松得到candy_sum,下坡也是一样的，唯一不确定的就是peak_candy，在同时拥有上坡和下坡的长度的时候，也可以推断出是 max(left,right) + 1；最后需要解决的是相邻的rating相同的情况，2个相同的rating可以把序列分割成2个独立的序列，分别计算candy_sum ，最后合并；相同的相邻区间长度大于2的情况，除开首尾部分，中间的candy都可以被置为1。\n\n　　归纳起来，可以把数组模式当作 bottom -> peak -> bottom -> equal 的组合，如果只出现bottom -> peak -> equal ，那么就当作前述模式的特例，这是一个比较有技巧性的归纳，因为针对此题，这个模式基本覆盖所有可能的上坡/下坡情况，特别是equal 可能出现在任何位置的时候，这个模式就被递归的细分下去了，这也是此解法正确性的核心。接下来只需要统计这个模式每一部分的长度即可，根据长度就能轻松计算出此序列的candy_sum\n\n```python\ndef candy(self, ratings: List[int]) -> int:\n    length = len(ratings)\n    if length == 0:\n        return 0\n\n    start = 0\n    sum_candy = 1 # init previous round bottom sum =1, because -1 each round to correct bottom double count problem\n    i = 0\n    while i < length-1: # cur = i, next = i+1\n        # botton -> peak : peak not included\n        while i<length-1 and ratings[i] < ratings[i+1]:\n            i += 1\n        left = i - start\n        start = i\n        # peak -> next bottom : peak not included\n        while i<length-1 and ratings[i] > ratings[i+1]:\n            i += 1\n        right = i - start\n        start = i\n        # count peak_candy and sum_candy\n        peak_candy = max(left, right) + 1\n        sum_candy += (left+1)*left//2 + (right+1)*right//2 + peak_candy - 1 # - 1 because left bottom included by previous round\n        # handle equal \n        while i<length-1 and ratings[i] == ratings[i+1]:\n            i += 1\n            sum_candy += 1\n        start = i\n\n    return sum_candy\n```\n\n　　需要注意的地方是，每次识别bottom -> peak -> bottom -> equal 模式的时候，第一个bottom 会被当作前一个模式的第二个bottom重复计算，所以每轮sum_candy最后都需要减去bottom的candy数目，也就是1，同时初始化的时候，前一轮的模式sum_candy初值为1，就是为了抵消前一轮重复计算的bottom，也就是把第一轮当作只包含一个值rating[0]来计算。\n\n\n\n### 解法二\n\n　　一个非常简单明了的算法是，所有小朋友初始的candy都是1，从左至右，只针对上坡的情况，递增candy；接着从右至左，把原本下坡的情况变成上坡，继续递增candy。这样就轻松的得到了正确的candy分配方案，由于初值都是1，使得相邻相同rating的小朋友candy一直是1。\n\n　　这个算法正确性完全依赖于上坡，而针对此题，最确定的candy分配方案就是上坡，因为下坡可能回面临不知道应该candy应该减到多少的不确定性问题，全部变成上坡，那问题就迎刃而解了。\n\n```python\ndef candy(self, ratings: List[int]) -> int:\n    length = len(ratings)\n    if length == 0:\n        return 0\n\n    candys = [1]*length\n    pre = ratings[0]\n    for i in range(1, length):\n        cur = ratings[i]\n        if cur > pre:\n            candys[i] = candys[i-1]+1\n        pre = cur\n\n    post = ratings[-1]\n    for j in range(length-2,-1,-1):\n        cur = ratings[j]\n        if cur > post and candys[j] <= candys[j+1]:\n            candys[j] = candys[j+1] + 1\n        post = cur\n    return sum(candys)\n```\n\n\n\n### Conclusion\n\n　　此题的关键是发现上坡的计算的确定性，遗憾的是，虽然我发现上坡时稳定正确的，但是却没有好好利用起来，反而将上坡作为一种情况单独讨论，遇到下坡和平坡的时候，单独讨论也还好，只是合并结果的时候遇到了很大的困扰，而且下坡和平坡的时候很多candy无法确定的问题没找到根本原因；没有发现在这多种情况中，上坡的唯一性，错失突破口。\n\n　　此外，在遇到难关的时候，迟迟无法突破的局面，使得自己试图把注意力从思路解析转移到实践的方法，注定是自欺欺人，核心的地方，还是需要多一点耐心，急躁的心态往往是饮鸩止渴；另一方面，自己的这个多少有点急躁的性格，也要多加利用，比如说，预料得到自己在过久的无法打开局面下，极为容易滋生出放弃的习惯，尽量不要让自己在一个进度上卡的太久，积极持续采取措施让自己的进度运转起来，否则放弃的后果是自己难以接受的，也容易白费了之前的一片苦心。","source":"_posts/135-Candy.md","raw":"---\ntitle: 135 Candy\ntoc: true\nmathjax: true\ntop: false\ncover: true\ndate: 2021-01-13 15:59:39\nupdated:\ncategories: Algorithms\ntags:\n\t- Algorithms\n\t- LeetCode\n---\n\n> There are *N* children standing in a line. Each child is assigned a rating value.\n>\n> You are giving candies to these children subjected to the following requirements:\n>\n> - Each child must have at least one candy.\n> - Children with a higher rating get more candies than their neighbors.\n>\n> What is the minimum candies you must give?\n>\n> **Example 1:**\n>\n> ```\n> Input: [1,0,2]\n> Output: 5\n> Explanation: You can allocate to the first, second and third child with 2, 1, 2 candies respectively.\n> ```\n>\n> **Example 2:**\n>\n> ```\n> Input: [1,2,2]\n> Output: 4\n> Explanation: You can allocate to the first, second and third child with 1, 2, 1 candies respectively.\n>              The third child gets 1 candy because it satisfies the above two conditions.\n> ```\n\n　　比较容易理解的题目，但是却是Hard难度。比较好入手，却是卡了好几天，最后依赖Discuss才勉强总结出结果，此为解法一；之后在其他 LeetCode的题解上发现了一个很简单的思路，却无法证明正确性，希望这里可以将其证明出来。\n\n### 解法一\n\n　　此题最先想到的思路就是，利用凹函数/凸函数的极小值和极大值的特性，来搜索上坡下坡，遇到上坡理所当然的将所分配的candy数目相较于左边的candy数目+1，如果是下坡，那么就当作上坡的逆序列，反向上坡，思路并没有问题，但是卡在一个非常奇怪的点，那就是峰值Peak 的选取，回头来看，只需要选择上坡和下坡之中的最大值即可，但是实现过程中，这个思路始终得不到重视，反而遇到了诸多乱七八糟的实现困难，最终卡了数天之后不得不放弃，这其中主要就是迟迟无法实现，导致对这个思路的自我怀疑，总是想投机取巧般的小改一下函数，来尝试新方法，结果有不断产生新问题，最后自己都有点迷失方向了。现在想来，似乎在无意义的实践上浪费了很多时间，相反真正应该关注的思路分析上，甚少得到进步。多少有点试图用战术上的勤奋来掩盖战略上的懒惰，情绪上，思路卡顿的时候，自己过分急躁，有点考场上，答不出来，弃之可惜的感觉，没找到正确的打开方式，却慌不择路，心态濒临崩溃，这种感觉无论是当时还在现在都是不愿想起的，看来我还是无法坦然面对自己，一点儿小挫折，就呼天喊地的，真是缺乏意志的考验。\n\n　　回到上坡/下坡，其实关于上坡可以仅仅根据上坡的长度来计算分配的总candy数目，因为起点是1颗，接下来则是递增为1的等差数列，亦即是起点为1的连续递增数量，可以轻松得到candy_sum,下坡也是一样的，唯一不确定的就是peak_candy，在同时拥有上坡和下坡的长度的时候，也可以推断出是 max(left,right) + 1；最后需要解决的是相邻的rating相同的情况，2个相同的rating可以把序列分割成2个独立的序列，分别计算candy_sum ，最后合并；相同的相邻区间长度大于2的情况，除开首尾部分，中间的candy都可以被置为1。\n\n　　归纳起来，可以把数组模式当作 bottom -> peak -> bottom -> equal 的组合，如果只出现bottom -> peak -> equal ，那么就当作前述模式的特例，这是一个比较有技巧性的归纳，因为针对此题，这个模式基本覆盖所有可能的上坡/下坡情况，特别是equal 可能出现在任何位置的时候，这个模式就被递归的细分下去了，这也是此解法正确性的核心。接下来只需要统计这个模式每一部分的长度即可，根据长度就能轻松计算出此序列的candy_sum\n\n```python\ndef candy(self, ratings: List[int]) -> int:\n    length = len(ratings)\n    if length == 0:\n        return 0\n\n    start = 0\n    sum_candy = 1 # init previous round bottom sum =1, because -1 each round to correct bottom double count problem\n    i = 0\n    while i < length-1: # cur = i, next = i+1\n        # botton -> peak : peak not included\n        while i<length-1 and ratings[i] < ratings[i+1]:\n            i += 1\n        left = i - start\n        start = i\n        # peak -> next bottom : peak not included\n        while i<length-1 and ratings[i] > ratings[i+1]:\n            i += 1\n        right = i - start\n        start = i\n        # count peak_candy and sum_candy\n        peak_candy = max(left, right) + 1\n        sum_candy += (left+1)*left//2 + (right+1)*right//2 + peak_candy - 1 # - 1 because left bottom included by previous round\n        # handle equal \n        while i<length-1 and ratings[i] == ratings[i+1]:\n            i += 1\n            sum_candy += 1\n        start = i\n\n    return sum_candy\n```\n\n　　需要注意的地方是，每次识别bottom -> peak -> bottom -> equal 模式的时候，第一个bottom 会被当作前一个模式的第二个bottom重复计算，所以每轮sum_candy最后都需要减去bottom的candy数目，也就是1，同时初始化的时候，前一轮的模式sum_candy初值为1，就是为了抵消前一轮重复计算的bottom，也就是把第一轮当作只包含一个值rating[0]来计算。\n\n\n\n### 解法二\n\n　　一个非常简单明了的算法是，所有小朋友初始的candy都是1，从左至右，只针对上坡的情况，递增candy；接着从右至左，把原本下坡的情况变成上坡，继续递增candy。这样就轻松的得到了正确的candy分配方案，由于初值都是1，使得相邻相同rating的小朋友candy一直是1。\n\n　　这个算法正确性完全依赖于上坡，而针对此题，最确定的candy分配方案就是上坡，因为下坡可能回面临不知道应该candy应该减到多少的不确定性问题，全部变成上坡，那问题就迎刃而解了。\n\n```python\ndef candy(self, ratings: List[int]) -> int:\n    length = len(ratings)\n    if length == 0:\n        return 0\n\n    candys = [1]*length\n    pre = ratings[0]\n    for i in range(1, length):\n        cur = ratings[i]\n        if cur > pre:\n            candys[i] = candys[i-1]+1\n        pre = cur\n\n    post = ratings[-1]\n    for j in range(length-2,-1,-1):\n        cur = ratings[j]\n        if cur > post and candys[j] <= candys[j+1]:\n            candys[j] = candys[j+1] + 1\n        post = cur\n    return sum(candys)\n```\n\n\n\n### Conclusion\n\n　　此题的关键是发现上坡的计算的确定性，遗憾的是，虽然我发现上坡时稳定正确的，但是却没有好好利用起来，反而将上坡作为一种情况单独讨论，遇到下坡和平坡的时候，单独讨论也还好，只是合并结果的时候遇到了很大的困扰，而且下坡和平坡的时候很多candy无法确定的问题没找到根本原因；没有发现在这多种情况中，上坡的唯一性，错失突破口。\n\n　　此外，在遇到难关的时候，迟迟无法突破的局面，使得自己试图把注意力从思路解析转移到实践的方法，注定是自欺欺人，核心的地方，还是需要多一点耐心，急躁的心态往往是饮鸩止渴；另一方面，自己的这个多少有点急躁的性格，也要多加利用，比如说，预料得到自己在过久的无法打开局面下，极为容易滋生出放弃的习惯，尽量不要让自己在一个进度上卡的太久，积极持续采取措施让自己的进度运转起来，否则放弃的后果是自己难以接受的，也容易白费了之前的一片苦心。","slug":"135-Candy","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"ckowlff4p0005ye28hvhw7t2d","content":"<blockquote>\n<p>There are <em>N</em> children standing in a line. Each child is assigned a rating value.</p>\n<p>You are giving candies to these children subjected to the following requirements:</p>\n<ul>\n<li>Each child must have at least one candy.</li>\n<li>Children with a higher rating get more candies than their neighbors.</li>\n</ul>\n<p>What is the minimum candies you must give?</p>\n<p><strong>Example 1:</strong></p>\n<pre><code>Input: [1,0,2]\nOutput: 5\nExplanation: You can allocate to the first, second and third child with 2, 1, 2 candies respectively.</code></pre>\n<p><strong>Example 2:</strong></p>\n<pre><code>Input: [1,2,2]\nOutput: 4\nExplanation: You can allocate to the first, second and third child with 1, 2, 1 candies respectively.\n             The third child gets 1 candy because it satisfies the above two conditions.</code></pre>\n</blockquote>\n<p>　　比较容易理解的题目，但是却是Hard难度。比较好入手，却是卡了好几天，最后依赖Discuss才勉强总结出结果，此为解法一；之后在其他 LeetCode的题解上发现了一个很简单的思路，却无法证明正确性，希望这里可以将其证明出来。</p>\n<h3 id=\"解法一\"><a href=\"#解法一\" class=\"headerlink\" title=\"解法一\"></a>解法一</h3><p>　　此题最先想到的思路就是，利用凹函数/凸函数的极小值和极大值的特性，来搜索上坡下坡，遇到上坡理所当然的将所分配的candy数目相较于左边的candy数目+1，如果是下坡，那么就当作上坡的逆序列，反向上坡，思路并没有问题，但是卡在一个非常奇怪的点，那就是峰值Peak 的选取，回头来看，只需要选择上坡和下坡之中的最大值即可，但是实现过程中，这个思路始终得不到重视，反而遇到了诸多乱七八糟的实现困难，最终卡了数天之后不得不放弃，这其中主要就是迟迟无法实现，导致对这个思路的自我怀疑，总是想投机取巧般的小改一下函数，来尝试新方法，结果有不断产生新问题，最后自己都有点迷失方向了。现在想来，似乎在无意义的实践上浪费了很多时间，相反真正应该关注的思路分析上，甚少得到进步。多少有点试图用战术上的勤奋来掩盖战略上的懒惰，情绪上，思路卡顿的时候，自己过分急躁，有点考场上，答不出来，弃之可惜的感觉，没找到正确的打开方式，却慌不择路，心态濒临崩溃，这种感觉无论是当时还在现在都是不愿想起的，看来我还是无法坦然面对自己，一点儿小挫折，就呼天喊地的，真是缺乏意志的考验。</p>\n<p>　　回到上坡/下坡，其实关于上坡可以仅仅根据上坡的长度来计算分配的总candy数目，因为起点是1颗，接下来则是递增为1的等差数列，亦即是起点为1的连续递增数量，可以轻松得到candy_sum,下坡也是一样的，唯一不确定的就是peak_candy，在同时拥有上坡和下坡的长度的时候，也可以推断出是 max(left,right) + 1；最后需要解决的是相邻的rating相同的情况，2个相同的rating可以把序列分割成2个独立的序列，分别计算candy_sum ，最后合并；相同的相邻区间长度大于2的情况，除开首尾部分，中间的candy都可以被置为1。</p>\n<p>　　归纳起来，可以把数组模式当作 bottom -&gt; peak -&gt; bottom -&gt; equal 的组合，如果只出现bottom -&gt; peak -&gt; equal ，那么就当作前述模式的特例，这是一个比较有技巧性的归纳，因为针对此题，这个模式基本覆盖所有可能的上坡/下坡情况，特别是equal 可能出现在任何位置的时候，这个模式就被递归的细分下去了，这也是此解法正确性的核心。接下来只需要统计这个模式每一部分的长度即可，根据长度就能轻松计算出此序列的candy_sum</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">candy</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> ratings<span class=\"token punctuation\">:</span> List<span class=\"token punctuation\">[</span>int<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span><span class=\"token operator\">></span> int<span class=\"token punctuation\">:</span>\n    length <span class=\"token operator\">=</span> len<span class=\"token punctuation\">(</span>ratings<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">if</span> length <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> <span class=\"token number\">0</span>\n\n    start <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n    sum_candy <span class=\"token operator\">=</span> <span class=\"token number\">1</span> <span class=\"token comment\" spellcheck=\"true\"># init previous round bottom sum =1, because -1 each round to correct bottom double count problem</span>\n    i <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n    <span class=\"token keyword\">while</span> i <span class=\"token operator\">&lt;</span> length<span class=\"token number\">-1</span><span class=\"token punctuation\">:</span> <span class=\"token comment\" spellcheck=\"true\"># cur = i, next = i+1</span>\n        <span class=\"token comment\" spellcheck=\"true\"># botton -> peak : peak not included</span>\n        <span class=\"token keyword\">while</span> i<span class=\"token operator\">&lt;</span>length<span class=\"token number\">-1</span> <span class=\"token operator\">and</span> ratings<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span> <span class=\"token operator\">&lt;</span> ratings<span class=\"token punctuation\">[</span>i<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n            i <span class=\"token operator\">+=</span> <span class=\"token number\">1</span>\n        left <span class=\"token operator\">=</span> i <span class=\"token operator\">-</span> start\n        start <span class=\"token operator\">=</span> i\n        <span class=\"token comment\" spellcheck=\"true\"># peak -> next bottom : peak not included</span>\n        <span class=\"token keyword\">while</span> i<span class=\"token operator\">&lt;</span>length<span class=\"token number\">-1</span> <span class=\"token operator\">and</span> ratings<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span> <span class=\"token operator\">></span> ratings<span class=\"token punctuation\">[</span>i<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n            i <span class=\"token operator\">+=</span> <span class=\"token number\">1</span>\n        right <span class=\"token operator\">=</span> i <span class=\"token operator\">-</span> start\n        start <span class=\"token operator\">=</span> i\n        <span class=\"token comment\" spellcheck=\"true\"># count peak_candy and sum_candy</span>\n        peak_candy <span class=\"token operator\">=</span> max<span class=\"token punctuation\">(</span>left<span class=\"token punctuation\">,</span> right<span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> <span class=\"token number\">1</span>\n        sum_candy <span class=\"token operator\">+=</span> <span class=\"token punctuation\">(</span>left<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token operator\">*</span>left<span class=\"token operator\">//</span><span class=\"token number\">2</span> <span class=\"token operator\">+</span> <span class=\"token punctuation\">(</span>right<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token operator\">*</span>right<span class=\"token operator\">//</span><span class=\"token number\">2</span> <span class=\"token operator\">+</span> peak_candy <span class=\"token operator\">-</span> <span class=\"token number\">1</span> <span class=\"token comment\" spellcheck=\"true\"># - 1 because left bottom included by previous round</span>\n        <span class=\"token comment\" spellcheck=\"true\"># handle equal </span>\n        <span class=\"token keyword\">while</span> i<span class=\"token operator\">&lt;</span>length<span class=\"token number\">-1</span> <span class=\"token operator\">and</span> ratings<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span> <span class=\"token operator\">==</span> ratings<span class=\"token punctuation\">[</span>i<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n            i <span class=\"token operator\">+=</span> <span class=\"token number\">1</span>\n            sum_candy <span class=\"token operator\">+=</span> <span class=\"token number\">1</span>\n        start <span class=\"token operator\">=</span> i\n\n    <span class=\"token keyword\">return</span> sum_candy</code></pre>\n<p>　　需要注意的地方是，每次识别bottom -&gt; peak -&gt; bottom -&gt; equal 模式的时候，第一个bottom 会被当作前一个模式的第二个bottom重复计算，所以每轮sum_candy最后都需要减去bottom的candy数目，也就是1，同时初始化的时候，前一轮的模式sum_candy初值为1，就是为了抵消前一轮重复计算的bottom，也就是把第一轮当作只包含一个值rating[0]来计算。</p>\n<h3 id=\"解法二\"><a href=\"#解法二\" class=\"headerlink\" title=\"解法二\"></a>解法二</h3><p>　　一个非常简单明了的算法是，所有小朋友初始的candy都是1，从左至右，只针对上坡的情况，递增candy；接着从右至左，把原本下坡的情况变成上坡，继续递增candy。这样就轻松的得到了正确的candy分配方案，由于初值都是1，使得相邻相同rating的小朋友candy一直是1。</p>\n<p>　　这个算法正确性完全依赖于上坡，而针对此题，最确定的candy分配方案就是上坡，因为下坡可能回面临不知道应该candy应该减到多少的不确定性问题，全部变成上坡，那问题就迎刃而解了。</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">candy</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> ratings<span class=\"token punctuation\">:</span> List<span class=\"token punctuation\">[</span>int<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span><span class=\"token operator\">></span> int<span class=\"token punctuation\">:</span>\n    length <span class=\"token operator\">=</span> len<span class=\"token punctuation\">(</span>ratings<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">if</span> length <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> <span class=\"token number\">0</span>\n\n    candys <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token operator\">*</span>length\n    pre <span class=\"token operator\">=</span> ratings<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> length<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        cur <span class=\"token operator\">=</span> ratings<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span>\n        <span class=\"token keyword\">if</span> cur <span class=\"token operator\">></span> pre<span class=\"token punctuation\">:</span>\n            candys<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> candys<span class=\"token punctuation\">[</span>i<span class=\"token number\">-1</span><span class=\"token punctuation\">]</span><span class=\"token operator\">+</span><span class=\"token number\">1</span>\n        pre <span class=\"token operator\">=</span> cur\n\n    post <span class=\"token operator\">=</span> ratings<span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">for</span> j <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span>length<span class=\"token number\">-2</span><span class=\"token punctuation\">,</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        cur <span class=\"token operator\">=</span> ratings<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">]</span>\n        <span class=\"token keyword\">if</span> cur <span class=\"token operator\">></span> post <span class=\"token operator\">and</span> candys<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">]</span> <span class=\"token operator\">&lt;=</span> candys<span class=\"token punctuation\">[</span>j<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n            candys<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> candys<span class=\"token punctuation\">[</span>j<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">+</span> <span class=\"token number\">1</span>\n        post <span class=\"token operator\">=</span> cur\n    <span class=\"token keyword\">return</span> sum<span class=\"token punctuation\">(</span>candys<span class=\"token punctuation\">)</span></code></pre>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>　　此题的关键是发现上坡的计算的确定性，遗憾的是，虽然我发现上坡时稳定正确的，但是却没有好好利用起来，反而将上坡作为一种情况单独讨论，遇到下坡和平坡的时候，单独讨论也还好，只是合并结果的时候遇到了很大的困扰，而且下坡和平坡的时候很多candy无法确定的问题没找到根本原因；没有发现在这多种情况中，上坡的唯一性，错失突破口。</p>\n<p>　　此外，在遇到难关的时候，迟迟无法突破的局面，使得自己试图把注意力从思路解析转移到实践的方法，注定是自欺欺人，核心的地方，还是需要多一点耐心，急躁的心态往往是饮鸩止渴；另一方面，自己的这个多少有点急躁的性格，也要多加利用，比如说，预料得到自己在过久的无法打开局面下，极为容易滋生出放弃的习惯，尽量不要让自己在一个进度上卡的太久，积极持续采取措施让自己的进度运转起来，否则放弃的后果是自己难以接受的，也容易白费了之前的一片苦心。</p>\n","site":{"data":{}},"excerpt":"","more":"<blockquote>\n<p>There are <em>N</em> children standing in a line. Each child is assigned a rating value.</p>\n<p>You are giving candies to these children subjected to the following requirements:</p>\n<ul>\n<li>Each child must have at least one candy.</li>\n<li>Children with a higher rating get more candies than their neighbors.</li>\n</ul>\n<p>What is the minimum candies you must give?</p>\n<p><strong>Example 1:</strong></p>\n<pre><code>Input: [1,0,2]\nOutput: 5\nExplanation: You can allocate to the first, second and third child with 2, 1, 2 candies respectively.</code></pre>\n<p><strong>Example 2:</strong></p>\n<pre><code>Input: [1,2,2]\nOutput: 4\nExplanation: You can allocate to the first, second and third child with 1, 2, 1 candies respectively.\n             The third child gets 1 candy because it satisfies the above two conditions.</code></pre>\n</blockquote>\n<p>　　比较容易理解的题目，但是却是Hard难度。比较好入手，却是卡了好几天，最后依赖Discuss才勉强总结出结果，此为解法一；之后在其他 LeetCode的题解上发现了一个很简单的思路，却无法证明正确性，希望这里可以将其证明出来。</p>\n<h3 id=\"解法一\"><a href=\"#解法一\" class=\"headerlink\" title=\"解法一\"></a>解法一</h3><p>　　此题最先想到的思路就是，利用凹函数/凸函数的极小值和极大值的特性，来搜索上坡下坡，遇到上坡理所当然的将所分配的candy数目相较于左边的candy数目+1，如果是下坡，那么就当作上坡的逆序列，反向上坡，思路并没有问题，但是卡在一个非常奇怪的点，那就是峰值Peak 的选取，回头来看，只需要选择上坡和下坡之中的最大值即可，但是实现过程中，这个思路始终得不到重视，反而遇到了诸多乱七八糟的实现困难，最终卡了数天之后不得不放弃，这其中主要就是迟迟无法实现，导致对这个思路的自我怀疑，总是想投机取巧般的小改一下函数，来尝试新方法，结果有不断产生新问题，最后自己都有点迷失方向了。现在想来，似乎在无意义的实践上浪费了很多时间，相反真正应该关注的思路分析上，甚少得到进步。多少有点试图用战术上的勤奋来掩盖战略上的懒惰，情绪上，思路卡顿的时候，自己过分急躁，有点考场上，答不出来，弃之可惜的感觉，没找到正确的打开方式，却慌不择路，心态濒临崩溃，这种感觉无论是当时还在现在都是不愿想起的，看来我还是无法坦然面对自己，一点儿小挫折，就呼天喊地的，真是缺乏意志的考验。</p>\n<p>　　回到上坡/下坡，其实关于上坡可以仅仅根据上坡的长度来计算分配的总candy数目，因为起点是1颗，接下来则是递增为1的等差数列，亦即是起点为1的连续递增数量，可以轻松得到candy_sum,下坡也是一样的，唯一不确定的就是peak_candy，在同时拥有上坡和下坡的长度的时候，也可以推断出是 max(left,right) + 1；最后需要解决的是相邻的rating相同的情况，2个相同的rating可以把序列分割成2个独立的序列，分别计算candy_sum ，最后合并；相同的相邻区间长度大于2的情况，除开首尾部分，中间的candy都可以被置为1。</p>\n<p>　　归纳起来，可以把数组模式当作 bottom -&gt; peak -&gt; bottom -&gt; equal 的组合，如果只出现bottom -&gt; peak -&gt; equal ，那么就当作前述模式的特例，这是一个比较有技巧性的归纳，因为针对此题，这个模式基本覆盖所有可能的上坡/下坡情况，特别是equal 可能出现在任何位置的时候，这个模式就被递归的细分下去了，这也是此解法正确性的核心。接下来只需要统计这个模式每一部分的长度即可，根据长度就能轻松计算出此序列的candy_sum</p>\n<pre><code class=\"python\">def candy(self, ratings: List[int]) -&gt; int:\n    length = len(ratings)\n    if length == 0:\n        return 0\n\n    start = 0\n    sum_candy = 1 # init previous round bottom sum =1, because -1 each round to correct bottom double count problem\n    i = 0\n    while i &lt; length-1: # cur = i, next = i+1\n        # botton -&gt; peak : peak not included\n        while i&lt;length-1 and ratings[i] &lt; ratings[i+1]:\n            i += 1\n        left = i - start\n        start = i\n        # peak -&gt; next bottom : peak not included\n        while i&lt;length-1 and ratings[i] &gt; ratings[i+1]:\n            i += 1\n        right = i - start\n        start = i\n        # count peak_candy and sum_candy\n        peak_candy = max(left, right) + 1\n        sum_candy += (left+1)*left//2 + (right+1)*right//2 + peak_candy - 1 # - 1 because left bottom included by previous round\n        # handle equal \n        while i&lt;length-1 and ratings[i] == ratings[i+1]:\n            i += 1\n            sum_candy += 1\n        start = i\n\n    return sum_candy</code></pre>\n<p>　　需要注意的地方是，每次识别bottom -&gt; peak -&gt; bottom -&gt; equal 模式的时候，第一个bottom 会被当作前一个模式的第二个bottom重复计算，所以每轮sum_candy最后都需要减去bottom的candy数目，也就是1，同时初始化的时候，前一轮的模式sum_candy初值为1，就是为了抵消前一轮重复计算的bottom，也就是把第一轮当作只包含一个值rating[0]来计算。</p>\n<h3 id=\"解法二\"><a href=\"#解法二\" class=\"headerlink\" title=\"解法二\"></a>解法二</h3><p>　　一个非常简单明了的算法是，所有小朋友初始的candy都是1，从左至右，只针对上坡的情况，递增candy；接着从右至左，把原本下坡的情况变成上坡，继续递增candy。这样就轻松的得到了正确的candy分配方案，由于初值都是1，使得相邻相同rating的小朋友candy一直是1。</p>\n<p>　　这个算法正确性完全依赖于上坡，而针对此题，最确定的candy分配方案就是上坡，因为下坡可能回面临不知道应该candy应该减到多少的不确定性问题，全部变成上坡，那问题就迎刃而解了。</p>\n<pre><code class=\"python\">def candy(self, ratings: List[int]) -&gt; int:\n    length = len(ratings)\n    if length == 0:\n        return 0\n\n    candys = [1]*length\n    pre = ratings[0]\n    for i in range(1, length):\n        cur = ratings[i]\n        if cur &gt; pre:\n            candys[i] = candys[i-1]+1\n        pre = cur\n\n    post = ratings[-1]\n    for j in range(length-2,-1,-1):\n        cur = ratings[j]\n        if cur &gt; post and candys[j] &lt;= candys[j+1]:\n            candys[j] = candys[j+1] + 1\n        post = cur\n    return sum(candys)</code></pre>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>　　此题的关键是发现上坡的计算的确定性，遗憾的是，虽然我发现上坡时稳定正确的，但是却没有好好利用起来，反而将上坡作为一种情况单独讨论，遇到下坡和平坡的时候，单独讨论也还好，只是合并结果的时候遇到了很大的困扰，而且下坡和平坡的时候很多candy无法确定的问题没找到根本原因；没有发现在这多种情况中，上坡的唯一性，错失突破口。</p>\n<p>　　此外，在遇到难关的时候，迟迟无法突破的局面，使得自己试图把注意力从思路解析转移到实践的方法，注定是自欺欺人，核心的地方，还是需要多一点耐心，急躁的心态往往是饮鸩止渴；另一方面，自己的这个多少有点急躁的性格，也要多加利用，比如说，预料得到自己在过久的无法打开局面下，极为容易滋生出放弃的习惯，尽量不要让自己在一个进度上卡的太久，积极持续采取措施让自己的进度运转起来，否则放弃的后果是自己难以接受的，也容易白费了之前的一片苦心。</p>\n"},{"title":"139 Word Break","toc":true,"mathjax":true,"top":false,"cover":true,"date":"2021-01-13T10:04:29.000Z","updated":"2021-01-14T08:29:29.193Z","_content":"\n> Given a **non-empty** string *s* and a dictionary *wordDict*containing a list of **non-empty** words, determine if *s* can be segmented into a space-separated sequence of one or more dictionary words.\n>\n> **Note:**\n>\n> - The same word in the dictionary may be reused multiple times in the segmentation.\n> - You may assume the dictionary does not contain duplicate words.\n>\n> **Example 1:**\n>\n> ```\n> Input: s = \"leetcode\", wordDict = [\"leet\", \"code\"]\n> Output: true\n> Explanation: Return true because \"leetcode\" can be segmented as \"leet code\".\n> ```\n>\n> **Example 2:**\n>\n> ```\n> Input: s = \"applepenapple\", wordDict = [\"apple\", \"pen\"]\n> Output: true\n> Explanation: Return true because \"applepenapple\" can be segmented as \"apple pen apple\".\n>              Note that you are allowed to reuse a dictionary word.\n> ```\n>\n> **Example 3:**\n>\n> ```\n> Input: s = \"catsandog\", wordDict = [\"cats\", \"dog\", \"sand\", \"and\", \"cat\"]\n> Output: false\n> ```\n\n　　Medium难度，入手时第一个想到的是DFS，之前Tree相关的题目做的多了，上手就忍不住用DFS练一下手，虽然正确性没问题，但是性能不行，最终卡在TLE始终无法优化。或许是被DFS卡的有点心烦，也可能是自己懒得从头开始分析此题的其他解法，也没有打算另辟蹊径去分析此题，最后发现简单的DP就可以解决，什么时候DFS好用，什么时候DP更有优势，自己还是没能找到这二者的辨别方法。\n\n### 解法一：DP\n\n　　能用DP的关键要素是最优子结构，但是自己常常跟DFS的递归搞混，不知道哪种递归是可以运用递归子结构，哪种无法用，所以此题虽然DFS解法比较直接，但是并没有立刻想到可以用DP来解的。实际上，此题是可以转化成递归子结构的，而且跟DFS递推式基本一致，只需要根据起点的wordBreak： True or False 以及 后续的substring是否在wordDict来判断即可：\n\n$dp[i] = dp[i-j] \\cap (s[j:i] \\in wordDict), j \\in [0:i]$\n\n起点初始化为True，否则后面没有机会变成True\n\n```python\ndef wordBreak(self, s: str, wordDict: List[str]) -> bool:\n    length = len(s)\n    if length == 0:\n        return False\n    set_words = set(wordDict)\n    set_words.update('')\n    dp = [True] + [False] * length # dp[i] mean s[ :i] can split in wordDict\n\n    for i in range(length+1):\n        for j in range(i):\n            if dp[j] and s[j:i] in set_words:\n                dp[i] = True\n                break\n\n    return dp[-1]\n```\n\n\n\n### 思路二： DFS （TLE）\n\n　　首先想到的是DFS，递归的切分substring，如果substring在wordDict中，那么就继续递归下去，思路简单明了，但是隐隐感觉肯定还是有优化的地方，只是现在还没找到。\n\n```python\ndef wordBreak(self, s: str, wordDict: List[str]) -> bool:\n    if len(set(s)) > len(set(''.join(wordDict))):\n        return False\n    set_words = set(wordDict)\n    r = self.helper(s, set_words)\n    return r\n\ndef helper(self, s, set_words):\n    if s in set_words:\n        return True\n\n    for i in range(len(s),0,-1):\n        sub = s[ :i]\n        if sub in set_words:\n            if  self.helper(s[i: ], set_words):\n                return True\n\n    return False\n```\n\n### \n\n### Conclusion\n\n　　Tree类的题目DFS做的有点多了，甚至都忘了，DFS，分治，贪心算法都是又可能转化为DP的，毕竟DP才是目前算法题的核心且比较有意思的部分。","source":"_posts/139-Word-Break.md","raw":"---\ntitle: 139 Word Break\ntoc: true\nmathjax: true\ntop: false\ncover: true\ndate: 2021-01-13 18:04:29\nupdated:\ncategories: Algorithms\ntags:\n\t- Algorithms\n\t- LeetCode\n---\n\n> Given a **non-empty** string *s* and a dictionary *wordDict*containing a list of **non-empty** words, determine if *s* can be segmented into a space-separated sequence of one or more dictionary words.\n>\n> **Note:**\n>\n> - The same word in the dictionary may be reused multiple times in the segmentation.\n> - You may assume the dictionary does not contain duplicate words.\n>\n> **Example 1:**\n>\n> ```\n> Input: s = \"leetcode\", wordDict = [\"leet\", \"code\"]\n> Output: true\n> Explanation: Return true because \"leetcode\" can be segmented as \"leet code\".\n> ```\n>\n> **Example 2:**\n>\n> ```\n> Input: s = \"applepenapple\", wordDict = [\"apple\", \"pen\"]\n> Output: true\n> Explanation: Return true because \"applepenapple\" can be segmented as \"apple pen apple\".\n>              Note that you are allowed to reuse a dictionary word.\n> ```\n>\n> **Example 3:**\n>\n> ```\n> Input: s = \"catsandog\", wordDict = [\"cats\", \"dog\", \"sand\", \"and\", \"cat\"]\n> Output: false\n> ```\n\n　　Medium难度，入手时第一个想到的是DFS，之前Tree相关的题目做的多了，上手就忍不住用DFS练一下手，虽然正确性没问题，但是性能不行，最终卡在TLE始终无法优化。或许是被DFS卡的有点心烦，也可能是自己懒得从头开始分析此题的其他解法，也没有打算另辟蹊径去分析此题，最后发现简单的DP就可以解决，什么时候DFS好用，什么时候DP更有优势，自己还是没能找到这二者的辨别方法。\n\n### 解法一：DP\n\n　　能用DP的关键要素是最优子结构，但是自己常常跟DFS的递归搞混，不知道哪种递归是可以运用递归子结构，哪种无法用，所以此题虽然DFS解法比较直接，但是并没有立刻想到可以用DP来解的。实际上，此题是可以转化成递归子结构的，而且跟DFS递推式基本一致，只需要根据起点的wordBreak： True or False 以及 后续的substring是否在wordDict来判断即可：\n\n$dp[i] = dp[i-j] \\cap (s[j:i] \\in wordDict), j \\in [0:i]$\n\n起点初始化为True，否则后面没有机会变成True\n\n```python\ndef wordBreak(self, s: str, wordDict: List[str]) -> bool:\n    length = len(s)\n    if length == 0:\n        return False\n    set_words = set(wordDict)\n    set_words.update('')\n    dp = [True] + [False] * length # dp[i] mean s[ :i] can split in wordDict\n\n    for i in range(length+1):\n        for j in range(i):\n            if dp[j] and s[j:i] in set_words:\n                dp[i] = True\n                break\n\n    return dp[-1]\n```\n\n\n\n### 思路二： DFS （TLE）\n\n　　首先想到的是DFS，递归的切分substring，如果substring在wordDict中，那么就继续递归下去，思路简单明了，但是隐隐感觉肯定还是有优化的地方，只是现在还没找到。\n\n```python\ndef wordBreak(self, s: str, wordDict: List[str]) -> bool:\n    if len(set(s)) > len(set(''.join(wordDict))):\n        return False\n    set_words = set(wordDict)\n    r = self.helper(s, set_words)\n    return r\n\ndef helper(self, s, set_words):\n    if s in set_words:\n        return True\n\n    for i in range(len(s),0,-1):\n        sub = s[ :i]\n        if sub in set_words:\n            if  self.helper(s[i: ], set_words):\n                return True\n\n    return False\n```\n\n### \n\n### Conclusion\n\n　　Tree类的题目DFS做的有点多了，甚至都忘了，DFS，分治，贪心算法都是又可能转化为DP的，毕竟DP才是目前算法题的核心且比较有意思的部分。","slug":"139-Word-Break","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"ckowlff4q0007ye28ellm242s","content":"<blockquote>\n<p>Given a <strong>non-empty</strong> string <em>s</em> and a dictionary <em>wordDict</em>containing a list of <strong>non-empty</strong> words, determine if <em>s</em> can be segmented into a space-separated sequence of one or more dictionary words.</p>\n<p><strong>Note:</strong></p>\n<ul>\n<li>The same word in the dictionary may be reused multiple times in the segmentation.</li>\n<li>You may assume the dictionary does not contain duplicate words.</li>\n</ul>\n<p><strong>Example 1:</strong></p>\n<pre><code>Input: s = &quot;leetcode&quot;, wordDict = [&quot;leet&quot;, &quot;code&quot;]\nOutput: true\nExplanation: Return true because &quot;leetcode&quot; can be segmented as &quot;leet code&quot;.</code></pre>\n<p><strong>Example 2:</strong></p>\n<pre><code>Input: s = &quot;applepenapple&quot;, wordDict = [&quot;apple&quot;, &quot;pen&quot;]\nOutput: true\nExplanation: Return true because &quot;applepenapple&quot; can be segmented as &quot;apple pen apple&quot;.\n             Note that you are allowed to reuse a dictionary word.</code></pre>\n<p><strong>Example 3:</strong></p>\n<pre><code>Input: s = &quot;catsandog&quot;, wordDict = [&quot;cats&quot;, &quot;dog&quot;, &quot;sand&quot;, &quot;and&quot;, &quot;cat&quot;]\nOutput: false</code></pre>\n</blockquote>\n<p>　　Medium难度，入手时第一个想到的是DFS，之前Tree相关的题目做的多了，上手就忍不住用DFS练一下手，虽然正确性没问题，但是性能不行，最终卡在TLE始终无法优化。或许是被DFS卡的有点心烦，也可能是自己懒得从头开始分析此题的其他解法，也没有打算另辟蹊径去分析此题，最后发现简单的DP就可以解决，什么时候DFS好用，什么时候DP更有优势，自己还是没能找到这二者的辨别方法。</p>\n<h3 id=\"解法一：DP\"><a href=\"#解法一：DP\" class=\"headerlink\" title=\"解法一：DP\"></a>解法一：DP</h3><p>　　能用DP的关键要素是最优子结构，但是自己常常跟DFS的递归搞混，不知道哪种递归是可以运用递归子结构，哪种无法用，所以此题虽然DFS解法比较直接，但是并没有立刻想到可以用DP来解的。实际上，此题是可以转化成递归子结构的，而且跟DFS递推式基本一致，只需要根据起点的wordBreak： True or False 以及 后续的substring是否在wordDict来判断即可：</p>\n<p>$dp[i] = dp[i-j] \\cap (s[j:i] \\in wordDict), j \\in [0:i]$</p>\n<p>起点初始化为True，否则后面没有机会变成True</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">wordBreak</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> s<span class=\"token punctuation\">:</span> str<span class=\"token punctuation\">,</span> wordDict<span class=\"token punctuation\">:</span> List<span class=\"token punctuation\">[</span>str<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span><span class=\"token operator\">></span> bool<span class=\"token punctuation\">:</span>\n    length <span class=\"token operator\">=</span> len<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">if</span> length <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> <span class=\"token boolean\">False</span>\n    set_words <span class=\"token operator\">=</span> set<span class=\"token punctuation\">(</span>wordDict<span class=\"token punctuation\">)</span>\n    set_words<span class=\"token punctuation\">.</span>update<span class=\"token punctuation\">(</span><span class=\"token string\">''</span><span class=\"token punctuation\">)</span>\n    dp <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">+</span> <span class=\"token punctuation\">[</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">*</span> length <span class=\"token comment\" spellcheck=\"true\"># dp[i] mean s[ :i] can split in wordDict</span>\n\n    <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span>length<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">for</span> j <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">if</span> dp<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">]</span> <span class=\"token operator\">and</span> s<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">:</span>i<span class=\"token punctuation\">]</span> <span class=\"token keyword\">in</span> set_words<span class=\"token punctuation\">:</span>\n                dp<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token boolean\">True</span>\n                <span class=\"token keyword\">break</span>\n\n    <span class=\"token keyword\">return</span> dp<span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span></code></pre>\n<h3 id=\"思路二：-DFS-（TLE）\"><a href=\"#思路二：-DFS-（TLE）\" class=\"headerlink\" title=\"思路二： DFS （TLE）\"></a>思路二： DFS （TLE）</h3><p>　　首先想到的是DFS，递归的切分substring，如果substring在wordDict中，那么就继续递归下去，思路简单明了，但是隐隐感觉肯定还是有优化的地方，只是现在还没找到。</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">wordBreak</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> s<span class=\"token punctuation\">:</span> str<span class=\"token punctuation\">,</span> wordDict<span class=\"token punctuation\">:</span> List<span class=\"token punctuation\">[</span>str<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span><span class=\"token operator\">></span> bool<span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">if</span> len<span class=\"token punctuation\">(</span>set<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">></span> len<span class=\"token punctuation\">(</span>set<span class=\"token punctuation\">(</span><span class=\"token string\">''</span><span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span>wordDict<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> <span class=\"token boolean\">False</span>\n    set_words <span class=\"token operator\">=</span> set<span class=\"token punctuation\">(</span>wordDict<span class=\"token punctuation\">)</span>\n    r <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>helper<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">,</span> set_words<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> r\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">helper</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> s<span class=\"token punctuation\">,</span> set_words<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">if</span> s <span class=\"token keyword\">in</span> set_words<span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> <span class=\"token boolean\">True</span>\n\n    <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span>len<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        sub <span class=\"token operator\">=</span> s<span class=\"token punctuation\">[</span> <span class=\"token punctuation\">:</span>i<span class=\"token punctuation\">]</span>\n        <span class=\"token keyword\">if</span> sub <span class=\"token keyword\">in</span> set_words<span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">if</span>  self<span class=\"token punctuation\">.</span>helper<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">:</span> <span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> set_words<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n                <span class=\"token keyword\">return</span> <span class=\"token boolean\">True</span>\n\n    <span class=\"token keyword\">return</span> <span class=\"token boolean\">False</span></code></pre>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>　　Tree类的题目DFS做的有点多了，甚至都忘了，DFS，分治，贪心算法都是又可能转化为DP的，毕竟DP才是目前算法题的核心且比较有意思的部分。</p>\n","site":{"data":{}},"excerpt":"","more":"<blockquote>\n<p>Given a <strong>non-empty</strong> string <em>s</em> and a dictionary <em>wordDict</em>containing a list of <strong>non-empty</strong> words, determine if <em>s</em> can be segmented into a space-separated sequence of one or more dictionary words.</p>\n<p><strong>Note:</strong></p>\n<ul>\n<li>The same word in the dictionary may be reused multiple times in the segmentation.</li>\n<li>You may assume the dictionary does not contain duplicate words.</li>\n</ul>\n<p><strong>Example 1:</strong></p>\n<pre><code>Input: s = &quot;leetcode&quot;, wordDict = [&quot;leet&quot;, &quot;code&quot;]\nOutput: true\nExplanation: Return true because &quot;leetcode&quot; can be segmented as &quot;leet code&quot;.</code></pre>\n<p><strong>Example 2:</strong></p>\n<pre><code>Input: s = &quot;applepenapple&quot;, wordDict = [&quot;apple&quot;, &quot;pen&quot;]\nOutput: true\nExplanation: Return true because &quot;applepenapple&quot; can be segmented as &quot;apple pen apple&quot;.\n             Note that you are allowed to reuse a dictionary word.</code></pre>\n<p><strong>Example 3:</strong></p>\n<pre><code>Input: s = &quot;catsandog&quot;, wordDict = [&quot;cats&quot;, &quot;dog&quot;, &quot;sand&quot;, &quot;and&quot;, &quot;cat&quot;]\nOutput: false</code></pre>\n</blockquote>\n<p>　　Medium难度，入手时第一个想到的是DFS，之前Tree相关的题目做的多了，上手就忍不住用DFS练一下手，虽然正确性没问题，但是性能不行，最终卡在TLE始终无法优化。或许是被DFS卡的有点心烦，也可能是自己懒得从头开始分析此题的其他解法，也没有打算另辟蹊径去分析此题，最后发现简单的DP就可以解决，什么时候DFS好用，什么时候DP更有优势，自己还是没能找到这二者的辨别方法。</p>\n<h3 id=\"解法一：DP\"><a href=\"#解法一：DP\" class=\"headerlink\" title=\"解法一：DP\"></a>解法一：DP</h3><p>　　能用DP的关键要素是最优子结构，但是自己常常跟DFS的递归搞混，不知道哪种递归是可以运用递归子结构，哪种无法用，所以此题虽然DFS解法比较直接，但是并没有立刻想到可以用DP来解的。实际上，此题是可以转化成递归子结构的，而且跟DFS递推式基本一致，只需要根据起点的wordBreak： True or False 以及 后续的substring是否在wordDict来判断即可：</p>\n<p>$dp[i] = dp[i-j] \\cap (s[j:i] \\in wordDict), j \\in [0:i]$</p>\n<p>起点初始化为True，否则后面没有机会变成True</p>\n<pre><code class=\"python\">def wordBreak(self, s: str, wordDict: List[str]) -&gt; bool:\n    length = len(s)\n    if length == 0:\n        return False\n    set_words = set(wordDict)\n    set_words.update(&#39;&#39;)\n    dp = [True] + [False] * length # dp[i] mean s[ :i] can split in wordDict\n\n    for i in range(length+1):\n        for j in range(i):\n            if dp[j] and s[j:i] in set_words:\n                dp[i] = True\n                break\n\n    return dp[-1]</code></pre>\n<h3 id=\"思路二：-DFS-（TLE）\"><a href=\"#思路二：-DFS-（TLE）\" class=\"headerlink\" title=\"思路二： DFS （TLE）\"></a>思路二： DFS （TLE）</h3><p>　　首先想到的是DFS，递归的切分substring，如果substring在wordDict中，那么就继续递归下去，思路简单明了，但是隐隐感觉肯定还是有优化的地方，只是现在还没找到。</p>\n<pre><code class=\"python\">def wordBreak(self, s: str, wordDict: List[str]) -&gt; bool:\n    if len(set(s)) &gt; len(set(&#39;&#39;.join(wordDict))):\n        return False\n    set_words = set(wordDict)\n    r = self.helper(s, set_words)\n    return r\n\ndef helper(self, s, set_words):\n    if s in set_words:\n        return True\n\n    for i in range(len(s),0,-1):\n        sub = s[ :i]\n        if sub in set_words:\n            if  self.helper(s[i: ], set_words):\n                return True\n\n    return False</code></pre>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>　　Tree类的题目DFS做的有点多了，甚至都忘了，DFS，分治，贪心算法都是又可能转化为DP的，毕竟DP才是目前算法题的核心且比较有意思的部分。</p>\n"},{"title":"146 LRU Cache","toc":true,"mathjax":true,"top":false,"cover":true,"date":"2021-01-13T10:08:17.000Z","updated":"2021-01-21T07:23:17.970Z","_content":"\n> Design a data structure that follows the constraints of a **[Least Recently Used (LRU) cache](https://en.wikipedia.org/wiki/Cache_replacement_policies#LRU)**.\n>\n> Implement the `LRUCache` class:\n>\n> - `LRUCache(int capacity)` Initialize the LRU cache with **positive** size `capacity`.\n> - `int get(int key)` Return the value of the `key` if the key exists, otherwise return `-1`.\n> - `void put(int key, int value)` Update the value of the `key` if the `key` exists. Otherwise, add the `key-value` pair to the cache. If the number of keys exceeds the `capacity` from this operation, **evict** the least recently used key.\n>\n> **Follow up:**\n> Could you do `get` and `put` in `O(1)` time complexity?\n>\n>  \n>\n> **Example 1:**\n>\n> ```\n> Input\n> [\"LRUCache\", \"put\", \"put\", \"get\", \"put\", \"get\", \"put\", \"get\", \"get\", \"get\"]\n> [[2], [1, 1], [2, 2], [1], [3, 3], [2], [4, 4], [1], [3], [4]]\n> Output\n> [null, null, null, 1, null, -1, null, -1, 3, 4]\n> \n> Explanation\n> LRUCache lRUCache = new LRUCache(2);\n> lRUCache.put(1, 1); // cache is {1=1}\n> lRUCache.put(2, 2); // cache is {1=1, 2=2}\n> lRUCache.get(1);    // return 1\n> lRUCache.put(3, 3); // LRU key was 2, evicts key 2, cache is {1=1, 3=3}\n> lRUCache.get(2);    // returns -1 (not found)\n> lRUCache.put(4, 4); // LRU key was 1, evicts key 1, cache is {4=4, 3=3}\n> lRUCache.get(1);    // return -1 (not found)\n> lRUCache.get(3);    // return 3\n> lRUCache.get(4);    // return 4\n> ```\n>\n>  \n>\n> **Constraints:**\n>\n> - `1 <= capacity <= 3000`\n> - `0 <= key <= 3000`\n> - `0 <= value <= 104`\n> - At most `3 * 104` calls will be made to `get` and `put`.\n\n　　Meidum难度，没想到的是用单链表实现起来，很多琐碎的细节需要填补，磕磕碰碰的AC了，查看Discuss发现直接就用双链表实现即可，这才发现题设并没有要求只用单链表来实现，此题用双向链表会省去之前的繁琐操作。\n\n### 解法一： 单链表\n\n　　在AC之前，用单链表实现会有一个比较繁琐的细节就是，获取node的pre索引用作删除node用，最开始懒得扩展结果，准备接口都混在 get/put 内部去实现，最后发现非常不利于debug；最后发现还是需要抽象出更基础的接口，pop,append,getPreNode，保证这几个接口的正确性，后面的get/put实现起来就非常轻松了，结果也证实了这一点，一次就AC\n\n```python\nclass LRUCache:\n\n    def __init__(self, capacity: int):\n        self.key2prenode = {}\n        self.node2key = {}\n        self.root =  ListNode()\n        self.tail = self.root\n        self.size = 0\n        self.capacity = capacity\n\n    def pop(self):\n        if self.size < 1:\n            return\n        cur = self.root.next\n        post = cur.next\n        self.root.next = post\n        if cur in self.node2key:\n            key = self.node2key[cur]\n            self.key2prenode.pop(key)\n            self.node2key.pop(cur)\n\n        if post in self.node2key:\n            post_key = self.node2key[post]\n            self.key2prenode[post_key] = self.root\n\n        return\n\n    def append(self, key, value):\n        node = ListNode(value)\n        self.key2prenode[key] = self.tail\n        self.node2key[node] = key\n        self.tail.next = node\n        self.tail = node\n        return\n\n    def getPreNode(self, key):\n        if key in self.key2prenode:\n            return self.key2prenode[key]\n        return None\n    \n    def update(self, prenode):\n        if prenode == None or prenode.next == None:\n            return \n        cur = prenode.next\n        post = cur.next\n        if cur == self.tail:\n            return\n\n        tail_old = self.tail\n        self.tail.next = cur\n        cur.next = None\n        self.tail = cur\n        if cur in self.node2key:\n            key = self.node2key[cur]\n            self.key2prenode[key] = tail_old\n        prenode.next = post\n        if post in self.node2key:\n            post_key = self.node2key[post]\n            self.key2prenode[post_key] = prenode\n        return\n\n    def get(self, key: int) -> int:\n        pre = self.getPreNode(key)\n        if pre == None or pre.next == None:\n            return -1\n        else:\n            val = pre.next.val\n            self.update(pre)\n            return val\n        \n    def put(self, key: int, value: int) -> None:\n        pre = self.getPreNode(key)\n        if pre == None or pre.next == None:\n            if self.size == self.capacity:\n                self.pop()\n                self.append(key, value)\n            else:\n                self.append(key, value)\n                self.size += 1\n        else:\n            pre.next.val = value\n            self.update(pre)\n            \n        return\n```\n\n由于没想过用双向链表，在单链表的限制下，不得的面临获取pre node索引的繁琐问题，而且这个复杂的比想象中麻烦\n\n### 解法二： 双向链表\n\n　　如果是双向链表的话，问题就很简单了，而且可以在构建双向列表的元素直接从value变成key+value，简化之前的key2Prenode , node2key的复杂索引。由于自己并没有接触过Python下的双向链表，所以这里也没有打算自建双向链表，怕有坑，瞻前顾后不愿意尝试。实际上，双向链表的数据结构只是多了一个pre而已。\n\n```python\nclass BiListNode:\n    def __init__(self, key=None, value=None):\n        self.key = key\n        self.value = value\n        self.pre = None\n        self.next = None\n\nclass LRUCache_Bi:\n    def __init__(self, capacity) :\n        self.capacity = capacity\n        self.size = 0\n        self.hash = dict()\n        self.head = BiListNode()\n        self.tail = BiListNode()\n        self.head.pre, self.head.next = None, self.tail\n        self.tail.pre, self.tail.next = self.head, None\n\n    def get(self, key):\n        value = -1\n        if key in self.hash:\n            node = self.hash[key]\n            value = node.value\n            self.remove_node(node)\n            self.add_to_head(node)\n        return value\n    \n    def put(self, key, value):\n        if self.capacity == 0:\n            return\n        if key in self.hash:\n            node = self.hash[key]\n            self.remove_node(node)\n            self.add_to_head(node)\n            node.value = value\n            return\n        if self.size == self.capacity:\n            node = self.tail.pre\n            self.remove_node(node)\n\n        newNode = BiListNode(key, value)\n        self.add_to_head(newNode)\n        return\n\n    def add_to_head(self, node):\n        post = self.head.next\n        post.pre = node\n        node.next = post\n        self.head.next = node\n        node.pre = self.head\n        self.hash[node.key] = node\n        self.size += 1\n        return\n\n    def remove_node(self, node):\n        pre, post = node.pre, node.next\n        pre.next, post.pre = post, pre\n        self.hash.pop(node.key)\n        self.size -= 1\n        return\n```\n\n这里在get/put之外，重新抽象出add_to_head , remove_node 接口，对应单链表版本的append, remove ，然后在get/put直接调用即可，简单明了，这里需要注意的是，实现过程中，单链表的tail代表的是最新访问的node，而在这里双链表版本插好相反，head才是最新访问的node；此外，这里额外添加了一个hash字段，实际完成的就是key2node的功能。\n\n\n\n### Conclusion\n\n　　双向链表实际上是学生时代学的非常透彻的数据结构了，遗憾的是，当初没能从代码级别去理解，现在只是重新完成当初未完成的事情。奇怪的是，当初的二叉树和树结构理论基础也比较好，很顺畅的就可以切换到现在的代码；反而是更简单的链表，无论是理论还是实践都学的没有树结构好，可能是链表递归并不是常用方法，反而是树结构一般无脑递归就能解决绝大多数问题。最后，通过此题发现，在写数据结构和类的之前，一个好的抽象接口将会带来非常多的收益，这是自己一直不重视的，往往会看完题目就忙着开始动手写代码，这在往常的LeetCode中不是什么大问题，但是在这在类似架构搭建和设计的问题上，很容易自己给自己埋坑，以后自己在程序架构设计上，还需要多下功夫。\n\n","source":"_posts/146-LRU-Cache.md","raw":"---\ntitle: 146 LRU Cache\ntoc: true\nmathjax: true\ntop: false\ncover: true\ndate: 2021-01-13 18:08:17\nupdated:\ncategories:\ntags:\n---\n\n> Design a data structure that follows the constraints of a **[Least Recently Used (LRU) cache](https://en.wikipedia.org/wiki/Cache_replacement_policies#LRU)**.\n>\n> Implement the `LRUCache` class:\n>\n> - `LRUCache(int capacity)` Initialize the LRU cache with **positive** size `capacity`.\n> - `int get(int key)` Return the value of the `key` if the key exists, otherwise return `-1`.\n> - `void put(int key, int value)` Update the value of the `key` if the `key` exists. Otherwise, add the `key-value` pair to the cache. If the number of keys exceeds the `capacity` from this operation, **evict** the least recently used key.\n>\n> **Follow up:**\n> Could you do `get` and `put` in `O(1)` time complexity?\n>\n>  \n>\n> **Example 1:**\n>\n> ```\n> Input\n> [\"LRUCache\", \"put\", \"put\", \"get\", \"put\", \"get\", \"put\", \"get\", \"get\", \"get\"]\n> [[2], [1, 1], [2, 2], [1], [3, 3], [2], [4, 4], [1], [3], [4]]\n> Output\n> [null, null, null, 1, null, -1, null, -1, 3, 4]\n> \n> Explanation\n> LRUCache lRUCache = new LRUCache(2);\n> lRUCache.put(1, 1); // cache is {1=1}\n> lRUCache.put(2, 2); // cache is {1=1, 2=2}\n> lRUCache.get(1);    // return 1\n> lRUCache.put(3, 3); // LRU key was 2, evicts key 2, cache is {1=1, 3=3}\n> lRUCache.get(2);    // returns -1 (not found)\n> lRUCache.put(4, 4); // LRU key was 1, evicts key 1, cache is {4=4, 3=3}\n> lRUCache.get(1);    // return -1 (not found)\n> lRUCache.get(3);    // return 3\n> lRUCache.get(4);    // return 4\n> ```\n>\n>  \n>\n> **Constraints:**\n>\n> - `1 <= capacity <= 3000`\n> - `0 <= key <= 3000`\n> - `0 <= value <= 104`\n> - At most `3 * 104` calls will be made to `get` and `put`.\n\n　　Meidum难度，没想到的是用单链表实现起来，很多琐碎的细节需要填补，磕磕碰碰的AC了，查看Discuss发现直接就用双链表实现即可，这才发现题设并没有要求只用单链表来实现，此题用双向链表会省去之前的繁琐操作。\n\n### 解法一： 单链表\n\n　　在AC之前，用单链表实现会有一个比较繁琐的细节就是，获取node的pre索引用作删除node用，最开始懒得扩展结果，准备接口都混在 get/put 内部去实现，最后发现非常不利于debug；最后发现还是需要抽象出更基础的接口，pop,append,getPreNode，保证这几个接口的正确性，后面的get/put实现起来就非常轻松了，结果也证实了这一点，一次就AC\n\n```python\nclass LRUCache:\n\n    def __init__(self, capacity: int):\n        self.key2prenode = {}\n        self.node2key = {}\n        self.root =  ListNode()\n        self.tail = self.root\n        self.size = 0\n        self.capacity = capacity\n\n    def pop(self):\n        if self.size < 1:\n            return\n        cur = self.root.next\n        post = cur.next\n        self.root.next = post\n        if cur in self.node2key:\n            key = self.node2key[cur]\n            self.key2prenode.pop(key)\n            self.node2key.pop(cur)\n\n        if post in self.node2key:\n            post_key = self.node2key[post]\n            self.key2prenode[post_key] = self.root\n\n        return\n\n    def append(self, key, value):\n        node = ListNode(value)\n        self.key2prenode[key] = self.tail\n        self.node2key[node] = key\n        self.tail.next = node\n        self.tail = node\n        return\n\n    def getPreNode(self, key):\n        if key in self.key2prenode:\n            return self.key2prenode[key]\n        return None\n    \n    def update(self, prenode):\n        if prenode == None or prenode.next == None:\n            return \n        cur = prenode.next\n        post = cur.next\n        if cur == self.tail:\n            return\n\n        tail_old = self.tail\n        self.tail.next = cur\n        cur.next = None\n        self.tail = cur\n        if cur in self.node2key:\n            key = self.node2key[cur]\n            self.key2prenode[key] = tail_old\n        prenode.next = post\n        if post in self.node2key:\n            post_key = self.node2key[post]\n            self.key2prenode[post_key] = prenode\n        return\n\n    def get(self, key: int) -> int:\n        pre = self.getPreNode(key)\n        if pre == None or pre.next == None:\n            return -1\n        else:\n            val = pre.next.val\n            self.update(pre)\n            return val\n        \n    def put(self, key: int, value: int) -> None:\n        pre = self.getPreNode(key)\n        if pre == None or pre.next == None:\n            if self.size == self.capacity:\n                self.pop()\n                self.append(key, value)\n            else:\n                self.append(key, value)\n                self.size += 1\n        else:\n            pre.next.val = value\n            self.update(pre)\n            \n        return\n```\n\n由于没想过用双向链表，在单链表的限制下，不得的面临获取pre node索引的繁琐问题，而且这个复杂的比想象中麻烦\n\n### 解法二： 双向链表\n\n　　如果是双向链表的话，问题就很简单了，而且可以在构建双向列表的元素直接从value变成key+value，简化之前的key2Prenode , node2key的复杂索引。由于自己并没有接触过Python下的双向链表，所以这里也没有打算自建双向链表，怕有坑，瞻前顾后不愿意尝试。实际上，双向链表的数据结构只是多了一个pre而已。\n\n```python\nclass BiListNode:\n    def __init__(self, key=None, value=None):\n        self.key = key\n        self.value = value\n        self.pre = None\n        self.next = None\n\nclass LRUCache_Bi:\n    def __init__(self, capacity) :\n        self.capacity = capacity\n        self.size = 0\n        self.hash = dict()\n        self.head = BiListNode()\n        self.tail = BiListNode()\n        self.head.pre, self.head.next = None, self.tail\n        self.tail.pre, self.tail.next = self.head, None\n\n    def get(self, key):\n        value = -1\n        if key in self.hash:\n            node = self.hash[key]\n            value = node.value\n            self.remove_node(node)\n            self.add_to_head(node)\n        return value\n    \n    def put(self, key, value):\n        if self.capacity == 0:\n            return\n        if key in self.hash:\n            node = self.hash[key]\n            self.remove_node(node)\n            self.add_to_head(node)\n            node.value = value\n            return\n        if self.size == self.capacity:\n            node = self.tail.pre\n            self.remove_node(node)\n\n        newNode = BiListNode(key, value)\n        self.add_to_head(newNode)\n        return\n\n    def add_to_head(self, node):\n        post = self.head.next\n        post.pre = node\n        node.next = post\n        self.head.next = node\n        node.pre = self.head\n        self.hash[node.key] = node\n        self.size += 1\n        return\n\n    def remove_node(self, node):\n        pre, post = node.pre, node.next\n        pre.next, post.pre = post, pre\n        self.hash.pop(node.key)\n        self.size -= 1\n        return\n```\n\n这里在get/put之外，重新抽象出add_to_head , remove_node 接口，对应单链表版本的append, remove ，然后在get/put直接调用即可，简单明了，这里需要注意的是，实现过程中，单链表的tail代表的是最新访问的node，而在这里双链表版本插好相反，head才是最新访问的node；此外，这里额外添加了一个hash字段，实际完成的就是key2node的功能。\n\n\n\n### Conclusion\n\n　　双向链表实际上是学生时代学的非常透彻的数据结构了，遗憾的是，当初没能从代码级别去理解，现在只是重新完成当初未完成的事情。奇怪的是，当初的二叉树和树结构理论基础也比较好，很顺畅的就可以切换到现在的代码；反而是更简单的链表，无论是理论还是实践都学的没有树结构好，可能是链表递归并不是常用方法，反而是树结构一般无脑递归就能解决绝大多数问题。最后，通过此题发现，在写数据结构和类的之前，一个好的抽象接口将会带来非常多的收益，这是自己一直不重视的，往往会看完题目就忙着开始动手写代码，这在往常的LeetCode中不是什么大问题，但是在这在类似架构搭建和设计的问题上，很容易自己给自己埋坑，以后自己在程序架构设计上，还需要多下功夫。\n\n","slug":"146-LRU-Cache","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"ckowlff4r000bye28987c808r","content":"<blockquote>\n<p>Design a data structure that follows the constraints of a <strong><a href=\"https://en.wikipedia.org/wiki/Cache_replacement_policies#LRU\">Least Recently Used (LRU) cache</a></strong>.</p>\n<p>Implement the <code>LRUCache</code> class:</p>\n<ul>\n<li><code>LRUCache(int capacity)</code> Initialize the LRU cache with <strong>positive</strong> size <code>capacity</code>.</li>\n<li><code>int get(int key)</code> Return the value of the <code>key</code> if the key exists, otherwise return <code>-1</code>.</li>\n<li><code>void put(int key, int value)</code> Update the value of the <code>key</code> if the <code>key</code> exists. Otherwise, add the <code>key-value</code> pair to the cache. If the number of keys exceeds the <code>capacity</code> from this operation, <strong>evict</strong> the least recently used key.</li>\n</ul>\n<p><strong>Follow up:</strong><br>Could you do <code>get</code> and <code>put</code> in <code>O(1)</code> time complexity?</p>\n<p><strong>Example 1:</strong></p>\n<pre><code>Input\n[&quot;LRUCache&quot;, &quot;put&quot;, &quot;put&quot;, &quot;get&quot;, &quot;put&quot;, &quot;get&quot;, &quot;put&quot;, &quot;get&quot;, &quot;get&quot;, &quot;get&quot;]\n[[2], [1, 1], [2, 2], [1], [3, 3], [2], [4, 4], [1], [3], [4]]\nOutput\n[null, null, null, 1, null, -1, null, -1, 3, 4]\n\nExplanation\nLRUCache lRUCache = new LRUCache(2);\nlRUCache.put(1, 1); // cache is &#123;1=1&#125;\nlRUCache.put(2, 2); // cache is &#123;1=1, 2=2&#125;\nlRUCache.get(1);    // return 1\nlRUCache.put(3, 3); // LRU key was 2, evicts key 2, cache is &#123;1=1, 3=3&#125;\nlRUCache.get(2);    // returns -1 (not found)\nlRUCache.put(4, 4); // LRU key was 1, evicts key 1, cache is &#123;4=4, 3=3&#125;\nlRUCache.get(1);    // return -1 (not found)\nlRUCache.get(3);    // return 3\nlRUCache.get(4);    // return 4</code></pre>\n<p><strong>Constraints:</strong></p>\n<ul>\n<li><code>1 &lt;= capacity &lt;= 3000</code></li>\n<li><code>0 &lt;= key &lt;= 3000</code></li>\n<li><code>0 &lt;= value &lt;= 104</code></li>\n<li>At most <code>3 * 104</code> calls will be made to <code>get</code> and <code>put</code>.</li>\n</ul>\n</blockquote>\n<p>　　Meidum难度，没想到的是用单链表实现起来，很多琐碎的细节需要填补，磕磕碰碰的AC了，查看Discuss发现直接就用双链表实现即可，这才发现题设并没有要求只用单链表来实现，此题用双向链表会省去之前的繁琐操作。</p>\n<h3 id=\"解法一：-单链表\"><a href=\"#解法一：-单链表\" class=\"headerlink\" title=\"解法一： 单链表\"></a>解法一： 单链表</h3><p>　　在AC之前，用单链表实现会有一个比较繁琐的细节就是，获取node的pre索引用作删除node用，最开始懒得扩展结果，准备接口都混在 get/put 内部去实现，最后发现非常不利于debug；最后发现还是需要抽象出更基础的接口，pop,append,getPreNode，保证这几个接口的正确性，后面的get/put实现起来就非常轻松了，结果也证实了这一点，一次就AC</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">LRUCache</span><span class=\"token punctuation\">:</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> capacity<span class=\"token punctuation\">:</span> int<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        self<span class=\"token punctuation\">.</span>key2prenode <span class=\"token operator\">=</span> <span class=\"token operator\">&amp;</span><span class=\"token comment\" spellcheck=\"true\">#123;&amp;#125;</span>\n        self<span class=\"token punctuation\">.</span>node2key <span class=\"token operator\">=</span> <span class=\"token operator\">&amp;</span><span class=\"token comment\" spellcheck=\"true\">#123;&amp;#125;</span>\n        self<span class=\"token punctuation\">.</span>root <span class=\"token operator\">=</span>  ListNode<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>tail <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>root\n        self<span class=\"token punctuation\">.</span>size <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n        self<span class=\"token punctuation\">.</span>capacity <span class=\"token operator\">=</span> capacity\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">pop</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">if</span> self<span class=\"token punctuation\">.</span>size <span class=\"token operator\">&lt;</span> <span class=\"token number\">1</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">return</span>\n        cur <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>root<span class=\"token punctuation\">.</span>next\n        post <span class=\"token operator\">=</span> cur<span class=\"token punctuation\">.</span>next\n        self<span class=\"token punctuation\">.</span>root<span class=\"token punctuation\">.</span>next <span class=\"token operator\">=</span> post\n        <span class=\"token keyword\">if</span> cur <span class=\"token keyword\">in</span> self<span class=\"token punctuation\">.</span>node2key<span class=\"token punctuation\">:</span>\n            key <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>node2key<span class=\"token punctuation\">[</span>cur<span class=\"token punctuation\">]</span>\n            self<span class=\"token punctuation\">.</span>key2prenode<span class=\"token punctuation\">.</span>pop<span class=\"token punctuation\">(</span>key<span class=\"token punctuation\">)</span>\n            self<span class=\"token punctuation\">.</span>node2key<span class=\"token punctuation\">.</span>pop<span class=\"token punctuation\">(</span>cur<span class=\"token punctuation\">)</span>\n\n        <span class=\"token keyword\">if</span> post <span class=\"token keyword\">in</span> self<span class=\"token punctuation\">.</span>node2key<span class=\"token punctuation\">:</span>\n            post_key <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>node2key<span class=\"token punctuation\">[</span>post<span class=\"token punctuation\">]</span>\n            self<span class=\"token punctuation\">.</span>key2prenode<span class=\"token punctuation\">[</span>post_key<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>root\n\n        <span class=\"token keyword\">return</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">append</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> key<span class=\"token punctuation\">,</span> value<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        node <span class=\"token operator\">=</span> ListNode<span class=\"token punctuation\">(</span>value<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>key2prenode<span class=\"token punctuation\">[</span>key<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>tail\n        self<span class=\"token punctuation\">.</span>node2key<span class=\"token punctuation\">[</span>node<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> key\n        self<span class=\"token punctuation\">.</span>tail<span class=\"token punctuation\">.</span>next <span class=\"token operator\">=</span> node\n        self<span class=\"token punctuation\">.</span>tail <span class=\"token operator\">=</span> node\n        <span class=\"token keyword\">return</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">getPreNode</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> key<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">if</span> key <span class=\"token keyword\">in</span> self<span class=\"token punctuation\">.</span>key2prenode<span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">return</span> self<span class=\"token punctuation\">.</span>key2prenode<span class=\"token punctuation\">[</span>key<span class=\"token punctuation\">]</span>\n        <span class=\"token keyword\">return</span> None\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">update</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> prenode<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">if</span> prenode <span class=\"token operator\">==</span> None <span class=\"token operator\">or</span> prenode<span class=\"token punctuation\">.</span>next <span class=\"token operator\">==</span> None<span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">return</span> \n        cur <span class=\"token operator\">=</span> prenode<span class=\"token punctuation\">.</span>next\n        post <span class=\"token operator\">=</span> cur<span class=\"token punctuation\">.</span>next\n        <span class=\"token keyword\">if</span> cur <span class=\"token operator\">==</span> self<span class=\"token punctuation\">.</span>tail<span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">return</span>\n\n        tail_old <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>tail\n        self<span class=\"token punctuation\">.</span>tail<span class=\"token punctuation\">.</span>next <span class=\"token operator\">=</span> cur\n        cur<span class=\"token punctuation\">.</span>next <span class=\"token operator\">=</span> None\n        self<span class=\"token punctuation\">.</span>tail <span class=\"token operator\">=</span> cur\n        <span class=\"token keyword\">if</span> cur <span class=\"token keyword\">in</span> self<span class=\"token punctuation\">.</span>node2key<span class=\"token punctuation\">:</span>\n            key <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>node2key<span class=\"token punctuation\">[</span>cur<span class=\"token punctuation\">]</span>\n            self<span class=\"token punctuation\">.</span>key2prenode<span class=\"token punctuation\">[</span>key<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> tail_old\n        prenode<span class=\"token punctuation\">.</span>next <span class=\"token operator\">=</span> post\n        <span class=\"token keyword\">if</span> post <span class=\"token keyword\">in</span> self<span class=\"token punctuation\">.</span>node2key<span class=\"token punctuation\">:</span>\n            post_key <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>node2key<span class=\"token punctuation\">[</span>post<span class=\"token punctuation\">]</span>\n            self<span class=\"token punctuation\">.</span>key2prenode<span class=\"token punctuation\">[</span>post_key<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> prenode\n        <span class=\"token keyword\">return</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">get</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> key<span class=\"token punctuation\">:</span> int<span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span><span class=\"token operator\">></span> int<span class=\"token punctuation\">:</span>\n        pre <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>getPreNode<span class=\"token punctuation\">(</span>key<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">if</span> pre <span class=\"token operator\">==</span> None <span class=\"token operator\">or</span> pre<span class=\"token punctuation\">.</span>next <span class=\"token operator\">==</span> None<span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">return</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span>\n        <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n            val <span class=\"token operator\">=</span> pre<span class=\"token punctuation\">.</span>next<span class=\"token punctuation\">.</span>val\n            self<span class=\"token punctuation\">.</span>update<span class=\"token punctuation\">(</span>pre<span class=\"token punctuation\">)</span>\n            <span class=\"token keyword\">return</span> val\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">put</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> key<span class=\"token punctuation\">:</span> int<span class=\"token punctuation\">,</span> value<span class=\"token punctuation\">:</span> int<span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span><span class=\"token operator\">></span> None<span class=\"token punctuation\">:</span>\n        pre <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>getPreNode<span class=\"token punctuation\">(</span>key<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">if</span> pre <span class=\"token operator\">==</span> None <span class=\"token operator\">or</span> pre<span class=\"token punctuation\">.</span>next <span class=\"token operator\">==</span> None<span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">if</span> self<span class=\"token punctuation\">.</span>size <span class=\"token operator\">==</span> self<span class=\"token punctuation\">.</span>capacity<span class=\"token punctuation\">:</span>\n                self<span class=\"token punctuation\">.</span>pop<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n                self<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>key<span class=\"token punctuation\">,</span> value<span class=\"token punctuation\">)</span>\n            <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n                self<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>key<span class=\"token punctuation\">,</span> value<span class=\"token punctuation\">)</span>\n                self<span class=\"token punctuation\">.</span>size <span class=\"token operator\">+=</span> <span class=\"token number\">1</span>\n        <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n            pre<span class=\"token punctuation\">.</span>next<span class=\"token punctuation\">.</span>val <span class=\"token operator\">=</span> value\n            self<span class=\"token punctuation\">.</span>update<span class=\"token punctuation\">(</span>pre<span class=\"token punctuation\">)</span>\n\n        <span class=\"token keyword\">return</span></code></pre>\n<p>由于没想过用双向链表，在单链表的限制下，不得的面临获取pre node索引的繁琐问题，而且这个复杂的比想象中麻烦</p>\n<h3 id=\"解法二：-双向链表\"><a href=\"#解法二：-双向链表\" class=\"headerlink\" title=\"解法二： 双向链表\"></a>解法二： 双向链表</h3><p>　　如果是双向链表的话，问题就很简单了，而且可以在构建双向列表的元素直接从value变成key+value，简化之前的key2Prenode , node2key的复杂索引。由于自己并没有接触过Python下的双向链表，所以这里也没有打算自建双向链表，怕有坑，瞻前顾后不愿意尝试。实际上，双向链表的数据结构只是多了一个pre而已。</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">BiListNode</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> key<span class=\"token operator\">=</span>None<span class=\"token punctuation\">,</span> value<span class=\"token operator\">=</span>None<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        self<span class=\"token punctuation\">.</span>key <span class=\"token operator\">=</span> key\n        self<span class=\"token punctuation\">.</span>value <span class=\"token operator\">=</span> value\n        self<span class=\"token punctuation\">.</span>pre <span class=\"token operator\">=</span> None\n        self<span class=\"token punctuation\">.</span>next <span class=\"token operator\">=</span> None\n\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">LRUCache_Bi</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> capacity<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">:</span>\n        self<span class=\"token punctuation\">.</span>capacity <span class=\"token operator\">=</span> capacity\n        self<span class=\"token punctuation\">.</span>size <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n        self<span class=\"token punctuation\">.</span>hash <span class=\"token operator\">=</span> dict<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>head <span class=\"token operator\">=</span> BiListNode<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>tail <span class=\"token operator\">=</span> BiListNode<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>head<span class=\"token punctuation\">.</span>pre<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>head<span class=\"token punctuation\">.</span>next <span class=\"token operator\">=</span> None<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>tail\n        self<span class=\"token punctuation\">.</span>tail<span class=\"token punctuation\">.</span>pre<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>tail<span class=\"token punctuation\">.</span>next <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>head<span class=\"token punctuation\">,</span> None\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">get</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> key<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        value <span class=\"token operator\">=</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span>\n        <span class=\"token keyword\">if</span> key <span class=\"token keyword\">in</span> self<span class=\"token punctuation\">.</span>hash<span class=\"token punctuation\">:</span>\n            node <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>hash<span class=\"token punctuation\">[</span>key<span class=\"token punctuation\">]</span>\n            value <span class=\"token operator\">=</span> node<span class=\"token punctuation\">.</span>value\n            self<span class=\"token punctuation\">.</span>remove_node<span class=\"token punctuation\">(</span>node<span class=\"token punctuation\">)</span>\n            self<span class=\"token punctuation\">.</span>add_to_head<span class=\"token punctuation\">(</span>node<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> value\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">put</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> key<span class=\"token punctuation\">,</span> value<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">if</span> self<span class=\"token punctuation\">.</span>capacity <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">return</span>\n        <span class=\"token keyword\">if</span> key <span class=\"token keyword\">in</span> self<span class=\"token punctuation\">.</span>hash<span class=\"token punctuation\">:</span>\n            node <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>hash<span class=\"token punctuation\">[</span>key<span class=\"token punctuation\">]</span>\n            self<span class=\"token punctuation\">.</span>remove_node<span class=\"token punctuation\">(</span>node<span class=\"token punctuation\">)</span>\n            self<span class=\"token punctuation\">.</span>add_to_head<span class=\"token punctuation\">(</span>node<span class=\"token punctuation\">)</span>\n            node<span class=\"token punctuation\">.</span>value <span class=\"token operator\">=</span> value\n            <span class=\"token keyword\">return</span>\n        <span class=\"token keyword\">if</span> self<span class=\"token punctuation\">.</span>size <span class=\"token operator\">==</span> self<span class=\"token punctuation\">.</span>capacity<span class=\"token punctuation\">:</span>\n            node <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>tail<span class=\"token punctuation\">.</span>pre\n            self<span class=\"token punctuation\">.</span>remove_node<span class=\"token punctuation\">(</span>node<span class=\"token punctuation\">)</span>\n\n        newNode <span class=\"token operator\">=</span> BiListNode<span class=\"token punctuation\">(</span>key<span class=\"token punctuation\">,</span> value<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>add_to_head<span class=\"token punctuation\">(</span>newNode<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">add_to_head</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> node<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        post <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>head<span class=\"token punctuation\">.</span>next\n        post<span class=\"token punctuation\">.</span>pre <span class=\"token operator\">=</span> node\n        node<span class=\"token punctuation\">.</span>next <span class=\"token operator\">=</span> post\n        self<span class=\"token punctuation\">.</span>head<span class=\"token punctuation\">.</span>next <span class=\"token operator\">=</span> node\n        node<span class=\"token punctuation\">.</span>pre <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>head\n        self<span class=\"token punctuation\">.</span>hash<span class=\"token punctuation\">[</span>node<span class=\"token punctuation\">.</span>key<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> node\n        self<span class=\"token punctuation\">.</span>size <span class=\"token operator\">+=</span> <span class=\"token number\">1</span>\n        <span class=\"token keyword\">return</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">remove_node</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> node<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        pre<span class=\"token punctuation\">,</span> post <span class=\"token operator\">=</span> node<span class=\"token punctuation\">.</span>pre<span class=\"token punctuation\">,</span> node<span class=\"token punctuation\">.</span>next\n        pre<span class=\"token punctuation\">.</span>next<span class=\"token punctuation\">,</span> post<span class=\"token punctuation\">.</span>pre <span class=\"token operator\">=</span> post<span class=\"token punctuation\">,</span> pre\n        self<span class=\"token punctuation\">.</span>hash<span class=\"token punctuation\">.</span>pop<span class=\"token punctuation\">(</span>node<span class=\"token punctuation\">.</span>key<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>size <span class=\"token operator\">-=</span> <span class=\"token number\">1</span>\n        <span class=\"token keyword\">return</span></code></pre>\n<p>这里在get/put之外，重新抽象出add_to_head , remove_node 接口，对应单链表版本的append, remove ，然后在get/put直接调用即可，简单明了，这里需要注意的是，实现过程中，单链表的tail代表的是最新访问的node，而在这里双链表版本插好相反，head才是最新访问的node；此外，这里额外添加了一个hash字段，实际完成的就是key2node的功能。</p>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>　　双向链表实际上是学生时代学的非常透彻的数据结构了，遗憾的是，当初没能从代码级别去理解，现在只是重新完成当初未完成的事情。奇怪的是，当初的二叉树和树结构理论基础也比较好，很顺畅的就可以切换到现在的代码；反而是更简单的链表，无论是理论还是实践都学的没有树结构好，可能是链表递归并不是常用方法，反而是树结构一般无脑递归就能解决绝大多数问题。最后，通过此题发现，在写数据结构和类的之前，一个好的抽象接口将会带来非常多的收益，这是自己一直不重视的，往往会看完题目就忙着开始动手写代码，这在往常的LeetCode中不是什么大问题，但是在这在类似架构搭建和设计的问题上，很容易自己给自己埋坑，以后自己在程序架构设计上，还需要多下功夫。</p>\n","site":{"data":{}},"excerpt":"","more":"<blockquote>\n<p>Design a data structure that follows the constraints of a <strong><a href=\"https://en.wikipedia.org/wiki/Cache_replacement_policies#LRU\">Least Recently Used (LRU) cache</a></strong>.</p>\n<p>Implement the <code>LRUCache</code> class:</p>\n<ul>\n<li><code>LRUCache(int capacity)</code> Initialize the LRU cache with <strong>positive</strong> size <code>capacity</code>.</li>\n<li><code>int get(int key)</code> Return the value of the <code>key</code> if the key exists, otherwise return <code>-1</code>.</li>\n<li><code>void put(int key, int value)</code> Update the value of the <code>key</code> if the <code>key</code> exists. Otherwise, add the <code>key-value</code> pair to the cache. If the number of keys exceeds the <code>capacity</code> from this operation, <strong>evict</strong> the least recently used key.</li>\n</ul>\n<p><strong>Follow up:</strong><br>Could you do <code>get</code> and <code>put</code> in <code>O(1)</code> time complexity?</p>\n<p><strong>Example 1:</strong></p>\n<pre><code>Input\n[&quot;LRUCache&quot;, &quot;put&quot;, &quot;put&quot;, &quot;get&quot;, &quot;put&quot;, &quot;get&quot;, &quot;put&quot;, &quot;get&quot;, &quot;get&quot;, &quot;get&quot;]\n[[2], [1, 1], [2, 2], [1], [3, 3], [2], [4, 4], [1], [3], [4]]\nOutput\n[null, null, null, 1, null, -1, null, -1, 3, 4]\n\nExplanation\nLRUCache lRUCache = new LRUCache(2);\nlRUCache.put(1, 1); // cache is &#123;1=1&#125;\nlRUCache.put(2, 2); // cache is &#123;1=1, 2=2&#125;\nlRUCache.get(1);    // return 1\nlRUCache.put(3, 3); // LRU key was 2, evicts key 2, cache is &#123;1=1, 3=3&#125;\nlRUCache.get(2);    // returns -1 (not found)\nlRUCache.put(4, 4); // LRU key was 1, evicts key 1, cache is &#123;4=4, 3=3&#125;\nlRUCache.get(1);    // return -1 (not found)\nlRUCache.get(3);    // return 3\nlRUCache.get(4);    // return 4</code></pre>\n<p><strong>Constraints:</strong></p>\n<ul>\n<li><code>1 &lt;= capacity &lt;= 3000</code></li>\n<li><code>0 &lt;= key &lt;= 3000</code></li>\n<li><code>0 &lt;= value &lt;= 104</code></li>\n<li>At most <code>3 * 104</code> calls will be made to <code>get</code> and <code>put</code>.</li>\n</ul>\n</blockquote>\n<p>　　Meidum难度，没想到的是用单链表实现起来，很多琐碎的细节需要填补，磕磕碰碰的AC了，查看Discuss发现直接就用双链表实现即可，这才发现题设并没有要求只用单链表来实现，此题用双向链表会省去之前的繁琐操作。</p>\n<h3 id=\"解法一：-单链表\"><a href=\"#解法一：-单链表\" class=\"headerlink\" title=\"解法一： 单链表\"></a>解法一： 单链表</h3><p>　　在AC之前，用单链表实现会有一个比较繁琐的细节就是，获取node的pre索引用作删除node用，最开始懒得扩展结果，准备接口都混在 get/put 内部去实现，最后发现非常不利于debug；最后发现还是需要抽象出更基础的接口，pop,append,getPreNode，保证这几个接口的正确性，后面的get/put实现起来就非常轻松了，结果也证实了这一点，一次就AC</p>\n<pre><code class=\"python\">class LRUCache:\n\n    def __init__(self, capacity: int):\n        self.key2prenode = &#123;&#125;\n        self.node2key = &#123;&#125;\n        self.root =  ListNode()\n        self.tail = self.root\n        self.size = 0\n        self.capacity = capacity\n\n    def pop(self):\n        if self.size &lt; 1:\n            return\n        cur = self.root.next\n        post = cur.next\n        self.root.next = post\n        if cur in self.node2key:\n            key = self.node2key[cur]\n            self.key2prenode.pop(key)\n            self.node2key.pop(cur)\n\n        if post in self.node2key:\n            post_key = self.node2key[post]\n            self.key2prenode[post_key] = self.root\n\n        return\n\n    def append(self, key, value):\n        node = ListNode(value)\n        self.key2prenode[key] = self.tail\n        self.node2key[node] = key\n        self.tail.next = node\n        self.tail = node\n        return\n\n    def getPreNode(self, key):\n        if key in self.key2prenode:\n            return self.key2prenode[key]\n        return None\n\n    def update(self, prenode):\n        if prenode == None or prenode.next == None:\n            return \n        cur = prenode.next\n        post = cur.next\n        if cur == self.tail:\n            return\n\n        tail_old = self.tail\n        self.tail.next = cur\n        cur.next = None\n        self.tail = cur\n        if cur in self.node2key:\n            key = self.node2key[cur]\n            self.key2prenode[key] = tail_old\n        prenode.next = post\n        if post in self.node2key:\n            post_key = self.node2key[post]\n            self.key2prenode[post_key] = prenode\n        return\n\n    def get(self, key: int) -&gt; int:\n        pre = self.getPreNode(key)\n        if pre == None or pre.next == None:\n            return -1\n        else:\n            val = pre.next.val\n            self.update(pre)\n            return val\n\n    def put(self, key: int, value: int) -&gt; None:\n        pre = self.getPreNode(key)\n        if pre == None or pre.next == None:\n            if self.size == self.capacity:\n                self.pop()\n                self.append(key, value)\n            else:\n                self.append(key, value)\n                self.size += 1\n        else:\n            pre.next.val = value\n            self.update(pre)\n\n        return</code></pre>\n<p>由于没想过用双向链表，在单链表的限制下，不得的面临获取pre node索引的繁琐问题，而且这个复杂的比想象中麻烦</p>\n<h3 id=\"解法二：-双向链表\"><a href=\"#解法二：-双向链表\" class=\"headerlink\" title=\"解法二： 双向链表\"></a>解法二： 双向链表</h3><p>　　如果是双向链表的话，问题就很简单了，而且可以在构建双向列表的元素直接从value变成key+value，简化之前的key2Prenode , node2key的复杂索引。由于自己并没有接触过Python下的双向链表，所以这里也没有打算自建双向链表，怕有坑，瞻前顾后不愿意尝试。实际上，双向链表的数据结构只是多了一个pre而已。</p>\n<pre><code class=\"python\">class BiListNode:\n    def __init__(self, key=None, value=None):\n        self.key = key\n        self.value = value\n        self.pre = None\n        self.next = None\n\nclass LRUCache_Bi:\n    def __init__(self, capacity) :\n        self.capacity = capacity\n        self.size = 0\n        self.hash = dict()\n        self.head = BiListNode()\n        self.tail = BiListNode()\n        self.head.pre, self.head.next = None, self.tail\n        self.tail.pre, self.tail.next = self.head, None\n\n    def get(self, key):\n        value = -1\n        if key in self.hash:\n            node = self.hash[key]\n            value = node.value\n            self.remove_node(node)\n            self.add_to_head(node)\n        return value\n\n    def put(self, key, value):\n        if self.capacity == 0:\n            return\n        if key in self.hash:\n            node = self.hash[key]\n            self.remove_node(node)\n            self.add_to_head(node)\n            node.value = value\n            return\n        if self.size == self.capacity:\n            node = self.tail.pre\n            self.remove_node(node)\n\n        newNode = BiListNode(key, value)\n        self.add_to_head(newNode)\n        return\n\n    def add_to_head(self, node):\n        post = self.head.next\n        post.pre = node\n        node.next = post\n        self.head.next = node\n        node.pre = self.head\n        self.hash[node.key] = node\n        self.size += 1\n        return\n\n    def remove_node(self, node):\n        pre, post = node.pre, node.next\n        pre.next, post.pre = post, pre\n        self.hash.pop(node.key)\n        self.size -= 1\n        return</code></pre>\n<p>这里在get/put之外，重新抽象出add_to_head , remove_node 接口，对应单链表版本的append, remove ，然后在get/put直接调用即可，简单明了，这里需要注意的是，实现过程中，单链表的tail代表的是最新访问的node，而在这里双链表版本插好相反，head才是最新访问的node；此外，这里额外添加了一个hash字段，实际完成的就是key2node的功能。</p>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>　　双向链表实际上是学生时代学的非常透彻的数据结构了，遗憾的是，当初没能从代码级别去理解，现在只是重新完成当初未完成的事情。奇怪的是，当初的二叉树和树结构理论基础也比较好，很顺畅的就可以切换到现在的代码；反而是更简单的链表，无论是理论还是实践都学的没有树结构好，可能是链表递归并不是常用方法，反而是树结构一般无脑递归就能解决绝大多数问题。最后，通过此题发现，在写数据结构和类的之前，一个好的抽象接口将会带来非常多的收益，这是自己一直不重视的，往往会看完题目就忙着开始动手写代码，这在往常的LeetCode中不是什么大问题，但是在这在类似架构搭建和设计的问题上，很容易自己给自己埋坑，以后自己在程序架构设计上，还需要多下功夫。</p>\n"},{"title":"147/148 merge sort","toc":true,"mathjax":true,"top":false,"cover":true,"date":"2021-01-21T07:24:50.000Z","updated":"2021-01-21T08:28:05.905Z","_content":"\n> Given the `head` of a linked list, return *the list after sorting it in **ascending order***.\n>\n> **Follow up:** Can you sort the linked list in `O(n logn)` time and `O(1)` memory (i.e. constant space)?\n>\n>  \n>\n> **Example 1:**\n>\n> ![img](https://assets.leetcode.com/uploads/2020/09/14/sort_list_1.jpg)\n>\n> ```\n> Input: head = [4,2,1,3]\n> Output: [1,2,3,4]\n> ```\n>\n> **Example 2:**\n>\n> ![img](https://assets.leetcode.com/uploads/2020/09/14/sort_list_2.jpg)\n>\n> ```\n> Input: head = [-1,5,3,4,0]\n> Output: [-1,0,3,4,5]\n> ```\n>\n> **Example 3:**\n>\n> ```\n> Input: head = []\n> Output: []\n> ```\n>\n>  \n>\n> **Constraints:**\n>\n> - The number of nodes in the list is in the range `[0, 5 * 104]`.\n> - `-105 <= Node.val <= 105`\n\n　　排序类别的题目应该是所有排序算法的基础了，不过用单链表形式出现的还比较新颖，147是插入排序则是比较熟悉的，很轻松的就AC了，然后148则遇到了一点困难，扫完题目就发现当初比较偏爱的三向快速排序法完美适配，论速度应该没有比快排更快的了，但是意外没能通过OJ，可能是跟单链表形式的数组跟快排本身还是存在适配问题，如果是堆排序，明显树结构更合理一点，剩下的就只有归并排序了，奈何，自己从一开始就对归并排序不是特别来电，一直以来并没有太深入研究其算法原理，所以这里实现起来遇到了一点困难，更不知道除了自顶向下的归并排序之外，还有一个自底向上的归并排序，这样不仅算法复杂度满足O(nlogn)，空间复杂度也可以做到O(1)\n\n### 解法一：自顶向下归并排序\n\n　　虽然归并基本是以分治为主，递归的切分数组，最后合并2个已经排好序的数组，所以只需要处理初始化的状态就可以了，但是不是很确定这个初始化应该放在归并中还是放到切分的函数中，理论上应该是在切分函数里，因为主要的递归调用是调用的切分函数的，但是自己无时不刻总是担心的性能问题作祟，在这个问题上犹豫不决，更加犹豫的是，心理上有点怀疑这么粗暴的递归，真的能达到O(nlogn)的性能吗？结果证明是自己多虑了，真实的递归就是如此，枉费当初还从算法复杂的上推理过分治的原理公式，这么快就忘记了。\n\n　　单链表下的分支惟一需要处理的问题就是切分了，需要手动切段单链表，同时处理递归的初始化状态，接下来专心写归并的递归即可，如果合并2个已经排序的单链表，这个实现起来还是比较简单的。\n\n```python\nclass Solution:\n    def sortList(self, head: ListNode) -> ListNode:\n        if head == None or head.next == None:\n            return head\n        mid, tail = head, head.next\n        while tail and tail.next:\n            tail = tail.next.next\n            mid = mid.next\n        start = mid.next\n        mid.next = None\n        left, right = self.sortList(head), self.sortList(start)\n        return self.mergeSort(left, right)\n\n    \n    def mergeSort(self, l1, l2):\n        root = ListNode()\n        tail = root\n        while l1 and l2:\n            if l1.val < l2.val:\n                tail.next = l1\n                l1 = l1.next\n            else:\n                tail.next = l2\n                l2 = l2.next\n            tail = tail.next\n        tail.next = l1 or l2\n        return root.next\n```\n\n### 解法二：自底向上归并排序\n\n　　主要原理基本就是把递归的代码改写成迭代的代码，方法跟希尔排序类似，递增已经排序的size大小，初始size=1，也即是单个元素默认是已排序状态，随后依次合并2个已排序链表，这里主要的改写部分都是和单链表的切分有关，因为跟数组列表可以在O(1)时间内获取任意索引的元素不同，单链表有些繁琐的细节处理，这里的技巧是，根据size大小，获取需要合并的2个链表的头，然后在merge阶段，返回合并后的链表的tail，因为这个tail在下一轮的merge的时候需要用到，因为需要把上一个合并后的tail.next = 合并后的LinkList的head。\n\n```python\nclass Solution:\n    # buttom up merget\n    def getSize(self, head):\n        count = 0\n        while head:\n            count += 1\n            head = head.next\n        return count\n\n    def splitLink(self, head, size):\n        cur = head\n        for _ in range(size):\n            if not cur:\n                break\n            cur = cur.next\n        if not cur:\n            return None\n        next_start = cur.next\n        cur.next = None\n        return next_start\n\n    def mergeSort_bottomup(self, l1, l2, pre_tail):\n        cur = pre_tail\n        while l1 and l2:\n            if l1.val<l2.val:\n                cur.next = l1\n                l1 = l1.next\n            else:\n                cur.next = l2\n                l2 = l2.next\n            cur = cur.next\n        cur.next = l1 or l2\n        while cur.next:\n            cur = cur.next\n        return cur\n\n    # bottom up sort\n    def sortList(self, head: ListNode) -> ListNode:\n        if head == None or head.next == None:\n            return head\n\n        length = self.getSize(head)\n        root = ListNode()\n        root.next = head\n        # pre_tail = None\n        start = None\n        size = 1\n        while size < length:\n            pre_tail = root\n            start = pre_tail.next\n            while start:\n                left = start\n                right = self.splitLink(left, size)\n                start = self.splitLink(right, size)\n                pre_tail = self.mergeSort_bottomup(left, right,pre_tail)\n            size *= 2\n        return root.next\n```\n\n\n\n### Conclusion\n\n　　关于归并的性能还是不太熟悉，所以针对这么精准需要用到归并的时候，遇到了一点困难，不过好在大体的原理还是知道的，只是实现上卡在一些小地方；此外我还是始终认为快排才是最快的，只是无法满足O(1)的空间复杂度，所以在个别测试用例上，很容易TLE，而且快排的代码真的简洁明了很多，相对于归并来说。","source":"_posts/147-148-merge-sort.md","raw":"---\ntitle: 147/148 merge sort\ntoc: true\nmathjax: true\ntop: false\ncover: true\ndate: 2021-01-21 15:24:50\nupdated:\ncategories: Algorithms\ntags:\n\t- Algorithms\n\t- LeetCode\n---\n\n> Given the `head` of a linked list, return *the list after sorting it in **ascending order***.\n>\n> **Follow up:** Can you sort the linked list in `O(n logn)` time and `O(1)` memory (i.e. constant space)?\n>\n>  \n>\n> **Example 1:**\n>\n> ![img](https://assets.leetcode.com/uploads/2020/09/14/sort_list_1.jpg)\n>\n> ```\n> Input: head = [4,2,1,3]\n> Output: [1,2,3,4]\n> ```\n>\n> **Example 2:**\n>\n> ![img](https://assets.leetcode.com/uploads/2020/09/14/sort_list_2.jpg)\n>\n> ```\n> Input: head = [-1,5,3,4,0]\n> Output: [-1,0,3,4,5]\n> ```\n>\n> **Example 3:**\n>\n> ```\n> Input: head = []\n> Output: []\n> ```\n>\n>  \n>\n> **Constraints:**\n>\n> - The number of nodes in the list is in the range `[0, 5 * 104]`.\n> - `-105 <= Node.val <= 105`\n\n　　排序类别的题目应该是所有排序算法的基础了，不过用单链表形式出现的还比较新颖，147是插入排序则是比较熟悉的，很轻松的就AC了，然后148则遇到了一点困难，扫完题目就发现当初比较偏爱的三向快速排序法完美适配，论速度应该没有比快排更快的了，但是意外没能通过OJ，可能是跟单链表形式的数组跟快排本身还是存在适配问题，如果是堆排序，明显树结构更合理一点，剩下的就只有归并排序了，奈何，自己从一开始就对归并排序不是特别来电，一直以来并没有太深入研究其算法原理，所以这里实现起来遇到了一点困难，更不知道除了自顶向下的归并排序之外，还有一个自底向上的归并排序，这样不仅算法复杂度满足O(nlogn)，空间复杂度也可以做到O(1)\n\n### 解法一：自顶向下归并排序\n\n　　虽然归并基本是以分治为主，递归的切分数组，最后合并2个已经排好序的数组，所以只需要处理初始化的状态就可以了，但是不是很确定这个初始化应该放在归并中还是放到切分的函数中，理论上应该是在切分函数里，因为主要的递归调用是调用的切分函数的，但是自己无时不刻总是担心的性能问题作祟，在这个问题上犹豫不决，更加犹豫的是，心理上有点怀疑这么粗暴的递归，真的能达到O(nlogn)的性能吗？结果证明是自己多虑了，真实的递归就是如此，枉费当初还从算法复杂的上推理过分治的原理公式，这么快就忘记了。\n\n　　单链表下的分支惟一需要处理的问题就是切分了，需要手动切段单链表，同时处理递归的初始化状态，接下来专心写归并的递归即可，如果合并2个已经排序的单链表，这个实现起来还是比较简单的。\n\n```python\nclass Solution:\n    def sortList(self, head: ListNode) -> ListNode:\n        if head == None or head.next == None:\n            return head\n        mid, tail = head, head.next\n        while tail and tail.next:\n            tail = tail.next.next\n            mid = mid.next\n        start = mid.next\n        mid.next = None\n        left, right = self.sortList(head), self.sortList(start)\n        return self.mergeSort(left, right)\n\n    \n    def mergeSort(self, l1, l2):\n        root = ListNode()\n        tail = root\n        while l1 and l2:\n            if l1.val < l2.val:\n                tail.next = l1\n                l1 = l1.next\n            else:\n                tail.next = l2\n                l2 = l2.next\n            tail = tail.next\n        tail.next = l1 or l2\n        return root.next\n```\n\n### 解法二：自底向上归并排序\n\n　　主要原理基本就是把递归的代码改写成迭代的代码，方法跟希尔排序类似，递增已经排序的size大小，初始size=1，也即是单个元素默认是已排序状态，随后依次合并2个已排序链表，这里主要的改写部分都是和单链表的切分有关，因为跟数组列表可以在O(1)时间内获取任意索引的元素不同，单链表有些繁琐的细节处理，这里的技巧是，根据size大小，获取需要合并的2个链表的头，然后在merge阶段，返回合并后的链表的tail，因为这个tail在下一轮的merge的时候需要用到，因为需要把上一个合并后的tail.next = 合并后的LinkList的head。\n\n```python\nclass Solution:\n    # buttom up merget\n    def getSize(self, head):\n        count = 0\n        while head:\n            count += 1\n            head = head.next\n        return count\n\n    def splitLink(self, head, size):\n        cur = head\n        for _ in range(size):\n            if not cur:\n                break\n            cur = cur.next\n        if not cur:\n            return None\n        next_start = cur.next\n        cur.next = None\n        return next_start\n\n    def mergeSort_bottomup(self, l1, l2, pre_tail):\n        cur = pre_tail\n        while l1 and l2:\n            if l1.val<l2.val:\n                cur.next = l1\n                l1 = l1.next\n            else:\n                cur.next = l2\n                l2 = l2.next\n            cur = cur.next\n        cur.next = l1 or l2\n        while cur.next:\n            cur = cur.next\n        return cur\n\n    # bottom up sort\n    def sortList(self, head: ListNode) -> ListNode:\n        if head == None or head.next == None:\n            return head\n\n        length = self.getSize(head)\n        root = ListNode()\n        root.next = head\n        # pre_tail = None\n        start = None\n        size = 1\n        while size < length:\n            pre_tail = root\n            start = pre_tail.next\n            while start:\n                left = start\n                right = self.splitLink(left, size)\n                start = self.splitLink(right, size)\n                pre_tail = self.mergeSort_bottomup(left, right,pre_tail)\n            size *= 2\n        return root.next\n```\n\n\n\n### Conclusion\n\n　　关于归并的性能还是不太熟悉，所以针对这么精准需要用到归并的时候，遇到了一点困难，不过好在大体的原理还是知道的，只是实现上卡在一些小地方；此外我还是始终认为快排才是最快的，只是无法满足O(1)的空间复杂度，所以在个别测试用例上，很容易TLE，而且快排的代码真的简洁明了很多，相对于归并来说。","slug":"147-148-merge-sort","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"ckowlff4s000eye28dd924rjd","content":"<blockquote>\n<p>Given the <code>head</code> of a linked list, return <em>the list after sorting it in <strong>ascending order</strong></em>.</p>\n<p><strong>Follow up:</strong> Can you sort the linked list in <code>O(n logn)</code> time and <code>O(1)</code> memory (i.e. constant space)?</p>\n<p><strong>Example 1:</strong></p>\n<p><img src=\"https://assets.leetcode.com/uploads/2020/09/14/sort_list_1.jpg\" alt=\"img\"></p>\n<pre><code>Input: head = [4,2,1,3]\nOutput: [1,2,3,4]</code></pre>\n<p><strong>Example 2:</strong></p>\n<p><img src=\"https://assets.leetcode.com/uploads/2020/09/14/sort_list_2.jpg\" alt=\"img\"></p>\n<pre><code>Input: head = [-1,5,3,4,0]\nOutput: [-1,0,3,4,5]</code></pre>\n<p><strong>Example 3:</strong></p>\n<pre><code>Input: head = []\nOutput: []</code></pre>\n<p><strong>Constraints:</strong></p>\n<ul>\n<li>The number of nodes in the list is in the range <code>[0, 5 * 104]</code>.</li>\n<li><code>-105 &lt;= Node.val &lt;= 105</code></li>\n</ul>\n</blockquote>\n<p>　　排序类别的题目应该是所有排序算法的基础了，不过用单链表形式出现的还比较新颖，147是插入排序则是比较熟悉的，很轻松的就AC了，然后148则遇到了一点困难，扫完题目就发现当初比较偏爱的三向快速排序法完美适配，论速度应该没有比快排更快的了，但是意外没能通过OJ，可能是跟单链表形式的数组跟快排本身还是存在适配问题，如果是堆排序，明显树结构更合理一点，剩下的就只有归并排序了，奈何，自己从一开始就对归并排序不是特别来电，一直以来并没有太深入研究其算法原理，所以这里实现起来遇到了一点困难，更不知道除了自顶向下的归并排序之外，还有一个自底向上的归并排序，这样不仅算法复杂度满足O(nlogn)，空间复杂度也可以做到O(1)</p>\n<h3 id=\"解法一：自顶向下归并排序\"><a href=\"#解法一：自顶向下归并排序\" class=\"headerlink\" title=\"解法一：自顶向下归并排序\"></a>解法一：自顶向下归并排序</h3><p>　　虽然归并基本是以分治为主，递归的切分数组，最后合并2个已经排好序的数组，所以只需要处理初始化的状态就可以了，但是不是很确定这个初始化应该放在归并中还是放到切分的函数中，理论上应该是在切分函数里，因为主要的递归调用是调用的切分函数的，但是自己无时不刻总是担心的性能问题作祟，在这个问题上犹豫不决，更加犹豫的是，心理上有点怀疑这么粗暴的递归，真的能达到O(nlogn)的性能吗？结果证明是自己多虑了，真实的递归就是如此，枉费当初还从算法复杂的上推理过分治的原理公式，这么快就忘记了。</p>\n<p>　　单链表下的分支惟一需要处理的问题就是切分了，需要手动切段单链表，同时处理递归的初始化状态，接下来专心写归并的递归即可，如果合并2个已经排序的单链表，这个实现起来还是比较简单的。</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">Solution</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">sortList</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> head<span class=\"token punctuation\">:</span> ListNode<span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span><span class=\"token operator\">></span> ListNode<span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">if</span> head <span class=\"token operator\">==</span> None <span class=\"token operator\">or</span> head<span class=\"token punctuation\">.</span>next <span class=\"token operator\">==</span> None<span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">return</span> head\n        mid<span class=\"token punctuation\">,</span> tail <span class=\"token operator\">=</span> head<span class=\"token punctuation\">,</span> head<span class=\"token punctuation\">.</span>next\n        <span class=\"token keyword\">while</span> tail <span class=\"token operator\">and</span> tail<span class=\"token punctuation\">.</span>next<span class=\"token punctuation\">:</span>\n            tail <span class=\"token operator\">=</span> tail<span class=\"token punctuation\">.</span>next<span class=\"token punctuation\">.</span>next\n            mid <span class=\"token operator\">=</span> mid<span class=\"token punctuation\">.</span>next\n        start <span class=\"token operator\">=</span> mid<span class=\"token punctuation\">.</span>next\n        mid<span class=\"token punctuation\">.</span>next <span class=\"token operator\">=</span> None\n        left<span class=\"token punctuation\">,</span> right <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>sortList<span class=\"token punctuation\">(</span>head<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>sortList<span class=\"token punctuation\">(</span>start<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> self<span class=\"token punctuation\">.</span>mergeSort<span class=\"token punctuation\">(</span>left<span class=\"token punctuation\">,</span> right<span class=\"token punctuation\">)</span>\n\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">mergeSort</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> l1<span class=\"token punctuation\">,</span> l2<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        root <span class=\"token operator\">=</span> ListNode<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        tail <span class=\"token operator\">=</span> root\n        <span class=\"token keyword\">while</span> l1 <span class=\"token operator\">and</span> l2<span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">if</span> l1<span class=\"token punctuation\">.</span>val <span class=\"token operator\">&lt;</span> l2<span class=\"token punctuation\">.</span>val<span class=\"token punctuation\">:</span>\n                tail<span class=\"token punctuation\">.</span>next <span class=\"token operator\">=</span> l1\n                l1 <span class=\"token operator\">=</span> l1<span class=\"token punctuation\">.</span>next\n            <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n                tail<span class=\"token punctuation\">.</span>next <span class=\"token operator\">=</span> l2\n                l2 <span class=\"token operator\">=</span> l2<span class=\"token punctuation\">.</span>next\n            tail <span class=\"token operator\">=</span> tail<span class=\"token punctuation\">.</span>next\n        tail<span class=\"token punctuation\">.</span>next <span class=\"token operator\">=</span> l1 <span class=\"token operator\">or</span> l2\n        <span class=\"token keyword\">return</span> root<span class=\"token punctuation\">.</span>next</code></pre>\n<h3 id=\"解法二：自底向上归并排序\"><a href=\"#解法二：自底向上归并排序\" class=\"headerlink\" title=\"解法二：自底向上归并排序\"></a>解法二：自底向上归并排序</h3><p>　　主要原理基本就是把递归的代码改写成迭代的代码，方法跟希尔排序类似，递增已经排序的size大小，初始size=1，也即是单个元素默认是已排序状态，随后依次合并2个已排序链表，这里主要的改写部分都是和单链表的切分有关，因为跟数组列表可以在O(1)时间内获取任意索引的元素不同，单链表有些繁琐的细节处理，这里的技巧是，根据size大小，获取需要合并的2个链表的头，然后在merge阶段，返回合并后的链表的tail，因为这个tail在下一轮的merge的时候需要用到，因为需要把上一个合并后的tail.next = 合并后的LinkList的head。</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">Solution</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\" spellcheck=\"true\"># buttom up merget</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">getSize</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> head<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        count <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n        <span class=\"token keyword\">while</span> head<span class=\"token punctuation\">:</span>\n            count <span class=\"token operator\">+=</span> <span class=\"token number\">1</span>\n            head <span class=\"token operator\">=</span> head<span class=\"token punctuation\">.</span>next\n        <span class=\"token keyword\">return</span> count\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">splitLink</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> head<span class=\"token punctuation\">,</span> size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        cur <span class=\"token operator\">=</span> head\n        <span class=\"token keyword\">for</span> _ <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span>size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">if</span> <span class=\"token operator\">not</span> cur<span class=\"token punctuation\">:</span>\n                <span class=\"token keyword\">break</span>\n            cur <span class=\"token operator\">=</span> cur<span class=\"token punctuation\">.</span>next\n        <span class=\"token keyword\">if</span> <span class=\"token operator\">not</span> cur<span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">return</span> None\n        next_start <span class=\"token operator\">=</span> cur<span class=\"token punctuation\">.</span>next\n        cur<span class=\"token punctuation\">.</span>next <span class=\"token operator\">=</span> None\n        <span class=\"token keyword\">return</span> next_start\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">mergeSort_bottomup</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> l1<span class=\"token punctuation\">,</span> l2<span class=\"token punctuation\">,</span> pre_tail<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        cur <span class=\"token operator\">=</span> pre_tail\n        <span class=\"token keyword\">while</span> l1 <span class=\"token operator\">and</span> l2<span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">if</span> l1<span class=\"token punctuation\">.</span>val<span class=\"token operator\">&lt;</span>l2<span class=\"token punctuation\">.</span>val<span class=\"token punctuation\">:</span>\n                cur<span class=\"token punctuation\">.</span>next <span class=\"token operator\">=</span> l1\n                l1 <span class=\"token operator\">=</span> l1<span class=\"token punctuation\">.</span>next\n            <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n                cur<span class=\"token punctuation\">.</span>next <span class=\"token operator\">=</span> l2\n                l2 <span class=\"token operator\">=</span> l2<span class=\"token punctuation\">.</span>next\n            cur <span class=\"token operator\">=</span> cur<span class=\"token punctuation\">.</span>next\n        cur<span class=\"token punctuation\">.</span>next <span class=\"token operator\">=</span> l1 <span class=\"token operator\">or</span> l2\n        <span class=\"token keyword\">while</span> cur<span class=\"token punctuation\">.</span>next<span class=\"token punctuation\">:</span>\n            cur <span class=\"token operator\">=</span> cur<span class=\"token punctuation\">.</span>next\n        <span class=\"token keyword\">return</span> cur\n\n    <span class=\"token comment\" spellcheck=\"true\"># bottom up sort</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">sortList</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> head<span class=\"token punctuation\">:</span> ListNode<span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span><span class=\"token operator\">></span> ListNode<span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">if</span> head <span class=\"token operator\">==</span> None <span class=\"token operator\">or</span> head<span class=\"token punctuation\">.</span>next <span class=\"token operator\">==</span> None<span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">return</span> head\n\n        length <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>getSize<span class=\"token punctuation\">(</span>head<span class=\"token punctuation\">)</span>\n        root <span class=\"token operator\">=</span> ListNode<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        root<span class=\"token punctuation\">.</span>next <span class=\"token operator\">=</span> head\n        <span class=\"token comment\" spellcheck=\"true\"># pre_tail = None</span>\n        start <span class=\"token operator\">=</span> None\n        size <span class=\"token operator\">=</span> <span class=\"token number\">1</span>\n        <span class=\"token keyword\">while</span> size <span class=\"token operator\">&lt;</span> length<span class=\"token punctuation\">:</span>\n            pre_tail <span class=\"token operator\">=</span> root\n            start <span class=\"token operator\">=</span> pre_tail<span class=\"token punctuation\">.</span>next\n            <span class=\"token keyword\">while</span> start<span class=\"token punctuation\">:</span>\n                left <span class=\"token operator\">=</span> start\n                right <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>splitLink<span class=\"token punctuation\">(</span>left<span class=\"token punctuation\">,</span> size<span class=\"token punctuation\">)</span>\n                start <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>splitLink<span class=\"token punctuation\">(</span>right<span class=\"token punctuation\">,</span> size<span class=\"token punctuation\">)</span>\n                pre_tail <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>mergeSort_bottomup<span class=\"token punctuation\">(</span>left<span class=\"token punctuation\">,</span> right<span class=\"token punctuation\">,</span>pre_tail<span class=\"token punctuation\">)</span>\n            size <span class=\"token operator\">*=</span> <span class=\"token number\">2</span>\n        <span class=\"token keyword\">return</span> root<span class=\"token punctuation\">.</span>next</code></pre>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>　　关于归并的性能还是不太熟悉，所以针对这么精准需要用到归并的时候，遇到了一点困难，不过好在大体的原理还是知道的，只是实现上卡在一些小地方；此外我还是始终认为快排才是最快的，只是无法满足O(1)的空间复杂度，所以在个别测试用例上，很容易TLE，而且快排的代码真的简洁明了很多，相对于归并来说。</p>\n","site":{"data":{}},"excerpt":"","more":"<blockquote>\n<p>Given the <code>head</code> of a linked list, return <em>the list after sorting it in <strong>ascending order</strong></em>.</p>\n<p><strong>Follow up:</strong> Can you sort the linked list in <code>O(n logn)</code> time and <code>O(1)</code> memory (i.e. constant space)?</p>\n<p><strong>Example 1:</strong></p>\n<p><img src=\"https://assets.leetcode.com/uploads/2020/09/14/sort_list_1.jpg\" alt=\"img\"></p>\n<pre><code>Input: head = [4,2,1,3]\nOutput: [1,2,3,4]</code></pre>\n<p><strong>Example 2:</strong></p>\n<p><img src=\"https://assets.leetcode.com/uploads/2020/09/14/sort_list_2.jpg\" alt=\"img\"></p>\n<pre><code>Input: head = [-1,5,3,4,0]\nOutput: [-1,0,3,4,5]</code></pre>\n<p><strong>Example 3:</strong></p>\n<pre><code>Input: head = []\nOutput: []</code></pre>\n<p><strong>Constraints:</strong></p>\n<ul>\n<li>The number of nodes in the list is in the range <code>[0, 5 * 104]</code>.</li>\n<li><code>-105 &lt;= Node.val &lt;= 105</code></li>\n</ul>\n</blockquote>\n<p>　　排序类别的题目应该是所有排序算法的基础了，不过用单链表形式出现的还比较新颖，147是插入排序则是比较熟悉的，很轻松的就AC了，然后148则遇到了一点困难，扫完题目就发现当初比较偏爱的三向快速排序法完美适配，论速度应该没有比快排更快的了，但是意外没能通过OJ，可能是跟单链表形式的数组跟快排本身还是存在适配问题，如果是堆排序，明显树结构更合理一点，剩下的就只有归并排序了，奈何，自己从一开始就对归并排序不是特别来电，一直以来并没有太深入研究其算法原理，所以这里实现起来遇到了一点困难，更不知道除了自顶向下的归并排序之外，还有一个自底向上的归并排序，这样不仅算法复杂度满足O(nlogn)，空间复杂度也可以做到O(1)</p>\n<h3 id=\"解法一：自顶向下归并排序\"><a href=\"#解法一：自顶向下归并排序\" class=\"headerlink\" title=\"解法一：自顶向下归并排序\"></a>解法一：自顶向下归并排序</h3><p>　　虽然归并基本是以分治为主，递归的切分数组，最后合并2个已经排好序的数组，所以只需要处理初始化的状态就可以了，但是不是很确定这个初始化应该放在归并中还是放到切分的函数中，理论上应该是在切分函数里，因为主要的递归调用是调用的切分函数的，但是自己无时不刻总是担心的性能问题作祟，在这个问题上犹豫不决，更加犹豫的是，心理上有点怀疑这么粗暴的递归，真的能达到O(nlogn)的性能吗？结果证明是自己多虑了，真实的递归就是如此，枉费当初还从算法复杂的上推理过分治的原理公式，这么快就忘记了。</p>\n<p>　　单链表下的分支惟一需要处理的问题就是切分了，需要手动切段单链表，同时处理递归的初始化状态，接下来专心写归并的递归即可，如果合并2个已经排序的单链表，这个实现起来还是比较简单的。</p>\n<pre><code class=\"python\">class Solution:\n    def sortList(self, head: ListNode) -&gt; ListNode:\n        if head == None or head.next == None:\n            return head\n        mid, tail = head, head.next\n        while tail and tail.next:\n            tail = tail.next.next\n            mid = mid.next\n        start = mid.next\n        mid.next = None\n        left, right = self.sortList(head), self.sortList(start)\n        return self.mergeSort(left, right)\n\n\n    def mergeSort(self, l1, l2):\n        root = ListNode()\n        tail = root\n        while l1 and l2:\n            if l1.val &lt; l2.val:\n                tail.next = l1\n                l1 = l1.next\n            else:\n                tail.next = l2\n                l2 = l2.next\n            tail = tail.next\n        tail.next = l1 or l2\n        return root.next</code></pre>\n<h3 id=\"解法二：自底向上归并排序\"><a href=\"#解法二：自底向上归并排序\" class=\"headerlink\" title=\"解法二：自底向上归并排序\"></a>解法二：自底向上归并排序</h3><p>　　主要原理基本就是把递归的代码改写成迭代的代码，方法跟希尔排序类似，递增已经排序的size大小，初始size=1，也即是单个元素默认是已排序状态，随后依次合并2个已排序链表，这里主要的改写部分都是和单链表的切分有关，因为跟数组列表可以在O(1)时间内获取任意索引的元素不同，单链表有些繁琐的细节处理，这里的技巧是，根据size大小，获取需要合并的2个链表的头，然后在merge阶段，返回合并后的链表的tail，因为这个tail在下一轮的merge的时候需要用到，因为需要把上一个合并后的tail.next = 合并后的LinkList的head。</p>\n<pre><code class=\"python\">class Solution:\n    # buttom up merget\n    def getSize(self, head):\n        count = 0\n        while head:\n            count += 1\n            head = head.next\n        return count\n\n    def splitLink(self, head, size):\n        cur = head\n        for _ in range(size):\n            if not cur:\n                break\n            cur = cur.next\n        if not cur:\n            return None\n        next_start = cur.next\n        cur.next = None\n        return next_start\n\n    def mergeSort_bottomup(self, l1, l2, pre_tail):\n        cur = pre_tail\n        while l1 and l2:\n            if l1.val&lt;l2.val:\n                cur.next = l1\n                l1 = l1.next\n            else:\n                cur.next = l2\n                l2 = l2.next\n            cur = cur.next\n        cur.next = l1 or l2\n        while cur.next:\n            cur = cur.next\n        return cur\n\n    # bottom up sort\n    def sortList(self, head: ListNode) -&gt; ListNode:\n        if head == None or head.next == None:\n            return head\n\n        length = self.getSize(head)\n        root = ListNode()\n        root.next = head\n        # pre_tail = None\n        start = None\n        size = 1\n        while size &lt; length:\n            pre_tail = root\n            start = pre_tail.next\n            while start:\n                left = start\n                right = self.splitLink(left, size)\n                start = self.splitLink(right, size)\n                pre_tail = self.mergeSort_bottomup(left, right,pre_tail)\n            size *= 2\n        return root.next</code></pre>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>　　关于归并的性能还是不太熟悉，所以针对这么精准需要用到归并的时候，遇到了一点困难，不过好在大体的原理还是知道的，只是实现上卡在一些小地方；此外我还是始终认为快排才是最快的，只是无法满足O(1)的空间复杂度，所以在个别测试用例上，很容易TLE，而且快排的代码真的简洁明了很多，相对于归并来说。</p>\n"},{"title":"关于未来的计划","toc":true,"mathjax":true,"top":false,"cover":true,"date":"2021-03-18T02:46:45.000Z","updated":"2021-05-20T07:32:04.510Z","_content":"\n　　经过上一次的风波，我需要为自己的未来做一番规划，避免自己再次陷入被动？\n\n　　宏观来看，我只有一个追求，那就是自由。这是一个很宽泛的概念，叠加上我个人的追求，那具体来看就是自由的探索这个世界，可以是周游世界，也可以是感兴趣的学科领域，不断探索和创造，体验这个过程带来的新奇和乐趣，我想这就是我追求的全部了。只是我并非天之骄子，我的梦想没有翅膀，每前进一步，都需要自己创造条件。\n\n　　首先需要解决的是，金钱问题；曾经在学校的年代，轻易的相信了，“主要努力提升自己的实力，后面的事情自然是水到渠成，金钱也并不例外。” 这套听起来非常有道理的说辞，诚然从人生的时间尺度，这句话并没有什么不对，当然，如果不对，那人生可就是彻底的被一句话给浪费了，但是这样论断有一个比较大的前提条件就是，这个世界是收敛于“大体公平”的。那么，这个世界是朝着越来越公平合理的方向走吗？这个问题过于宏大，而且还是以时代和地域为前提的，我不保证自己把握住了这个世界的宏观整体走向，但是至少在中国这片土地上，我有我自己的看法和论断：相较于不堪回首的过去，中国显然变得更加公平合理了，但是着眼于未来的发展，中国被自己两千年的厚重历史给束缚住了，无法放开手脚，迈向更合理更好的未来，特别是当发展步入深水区，不能正视科学的发展模式和成果，反而用老套的权威思想去接受和包装来自中国之外的科学发展，那么结果自然就是永远都在学习，永远无法超越和创新。 这是不合理的，就如同父辈通过努力和奋斗积累的财力，试图为自己当下的权力背书，在我看来已经完全滑入了傲慢的陷阱，虽然获得了短暂的余晖，但是终将面对来自未来的审判。所以，我并不认为当下的中国是处于一个足够公平合理的年代，特别是近年来，随着房价的上涨，各种奋斗不如买房，努力不如摸鱼的论调横出不穷，也是对于这种基于机遇和运气获得的成功远远高于奋斗这种现象的反抗。毕竟不是每个人都能在人生的时间尺度上沉得住气，很容易被各种偶然的信息和机遇左右，所以我更愿意称这是一个混沌的时代，不要迷信公平和合理，主要精力除了关注发掘自己的优势之外，还应该关注这个时代的优势，努力让这二者的优势更加契合，而非背道而驰。个人在时代面前永远都是渺小的存在，个人的能力和理想难以抵抗时代大潮，虽然听起来仍然很市侩，但是如果能免除一些不必要的烦恼，也不失为一种曲线救国的方法。在当下，困扰我的就是金钱。\n\n　　如果有一夜暴富，或者短时间暴富的机会，我想我会毫不犹豫的去实践，接着便开始肆无忌惮的活着，学习，探索，实践。不知道何时开始，自己产生并逐渐坚定了这种想法，我想主要原因是认识到，通过努力和自身实力来获取成功的道路，变得越来越不可行的时候开始吧。不知不觉，我的思想就被这样的现实逐渐异化。愈加觉得这个世界魔幻了起来，比特币BTC这种全无价值的游戏也能称的上投资，连带类似的各种新创造的电子货币也都水涨船高；股票市场中起名跟区块链技术沾边的都可以顺势而起；通过房产获益的大都是凭着无知者无畏的精神吃到了先行者的红利，而且这个红利居然能持续数十年，让所有人为之疯狂；互联网企业堂而皇之的垄断市场，且有愈演愈烈之势，反而还得到司法部门的特别优待；技术外行的包工头居然能哄来顶尖的技术团队，骗得政府投资，竟也能顺利脱身。所有人都是如此的浮躁，如此的焦虑，却又这样的急功近利。投机者总是能占得先机，获得时代的青睐。只是这种方式，我不大认同罢了，除了因为自己本质上不是一名投机者之外，在认知上，我更愿意相信随着时代的发展，那些更富于思考的优秀产品和模式则会脱颖而出，那些粗制滥造的跟随者则会慢慢消亡。尽管我是如此相信合理的企业运作模式应当如此，而我却又口嫌体直的被异化成一个急功近利，希望短时间暴富的投机者。一方面是因为当下的时代，从无到有的过程中，投机者发挥功能上的作用，过程粗犷一点无妨，只要目的仍然是这种结构性的转变；反之，如果目的是从有到更好，那么投机者仍然试图复制曾经的先发优势，那么等待他的自然只有被淘汰的命运；另一方面，我从来没有把金钱当作一个目的，只是当成一种实现自我追求的手段而己。如果自己实现财务自由，我想我早就开始满世界游山玩水去了，不断体验新生活新事物，学习新知识，过自由自在的人生，甚至去探索未知的宇宙。不仅仅是意淫，是真的渴望这样的生活并愿意付诸实践。\n\n　　那么赚取足够财务自由的资金则是当前首要目的，有2种不同的操作思路，一种是常规且保守的上班，升职加薪，循序渐进，业余时间做点理财的投资，优势是持续有资金入账，不需要担心生活问题，日常的生活也会较为平稳，缺点是，可能需要较长的时间才能实现财务自由，而且常常也会面临暂时的薪酬和能力不能匹配的困境，个人职业发展也并不那么可控；另一种思路则是自己单干创业，缺点很明显就是，打通盈利模式之前，都无任何资金入账，甚至还会亏钱，还存在根本无法盈利的风险，除了技术能力之外，还需要开发管理和沟通相关能力，优点是克服种种可预见或不可预见的困难之后，有比较丰厚的回报，能快速积攒资金，个人发展规划也较为自由。我更倾向于后者，因为无论是个人技术发展，还是综合能力的提升，都会在需求的迫切提升的压力下，得到阶梯式的提升，而且能实现自己专心做技术，不被一些非必须的业务琐事干扰。唯二需要担心的问题是，一方面，自己缺乏坚持下去的信心和勇气，尽管平时自己总潜意识里觉得自己未来肯定能成大事，拥有驾驭更高阶平台的能力，但是自己两面派的特性，当自己无法坐下来思考的时候，自己也是一个彻头彻尾的感性动物，难以看清前路的方向；另一方面，在与父母冲突之前，创业之后没有资金入账，甚至亏钱都可以要父母来买单，但是此役之后，我不能获得没有收入来源之后的资金保障，那么如何生存则成为了一个问题，而且还要面临缺乏资金入账之后，父母本身对我的不齿和鄙视。原本是为自己尊严而战去努力，最后却演变成一开始就可能需要放弃尊严的困境，这无论如何让我都有点难以接受。\n\n　　仔细想想，这2个问题也并不是那么难以克服，找到了原因，这些问题都会迎刃而解！再则，既然想要做更伟大的项目，困难也更多更大，也是理所当然的事情，否则这种只有好处，没有坏处的事情，岂不是人人都可以做的来。我自认为拥有抓住问题核心的能力，只要倾注足够的思考；编程能力是**硬通货**，何必为自己缺乏资金来源而苦恼呢？虽然创业初期，会时常面临不得不接其他项目来补贴生活和项目的困境，这本身对自己也是一种历练，无论是编程能力和沟通能力，其次，这也正是实现了自己能力和酬劳挂钩的目的，为何临到自己身上的时候，对自己抱有侥幸心理呢？基本上来讲，对自己的编程能力和学习能力还是抱有足够的信心的，前提是需要自己合理规划目标和时间，我已经深受之前急功近利之苦，不仅伤害了自信，还浪费许多时间。再则，相比几年前，自己已经足够成熟，来应对自己的慵懒。毕业之后那段时间，确实深受论文折磨，心态上受到的巨大的损伤，只是因为自己表现的足够上进和优秀，就被委以如此重任，时间也非常不充裕，最后落得一副想尽办法偷懒占便宜的毛病。现在想来，仍然有点为当初没搞清楚自己的心态而感到懊恼。经过去年的锤炼，我更好的认识了自己，也能更好的面对自己，未来相信在自律这方面，也无需太多担心。\n\n　　那么剩下的问题就是缺乏豁出去的勇气了，认真审视自己，我开始有点儿讨厌自己的畏首畏尾，犹犹豫豫，缺乏决断力这样的事情了。当初决定要考研的我，当初决定要去拉萨的我，都没有现在的迷惘，不敢直视自己的内心，瞻前顾后，不敢面对失败的结果。都是毕业之后一步一步养成的心理习惯，自从决定要留学开始，行动上就一直自欺欺人，理所当然的还没开始就失败了，之后工作上虽然渐有起色，然而那只是战术上的勤奋的结果，心底上还是懊恼自己有做选择的勇气，却没有承担责任的胆量。尽管毕业前夕的事情，让自己的心态有点失衡，但无疑是自己一步一步亲手葬送自己的未来。不能再逃避现实，逃避自己的内心了，承认自己失败，肯定自己的努力。过去的失误，就由现在来弥补，未来才有机会朝着渴望的方向发展。失败的历史无法更改，原因或许只是我一开始就认怂了，那不是我愿意成为的样子，我知道我强大的一面，但是近年来的失落让我无法依仗这份强大的能力，这样看来只是处于一些心态上的原因没有有效利用自己的才能，想来确实有点不应该。我不愿意成为一个随波逐流的人，却偏偏在近几年不断的想要察言观色，不敢去主动尝试，由于害怕失败，陷入自卑的心态，实际执行起来，发现自己其实可以做的很好的，这个时候却又过分自大自满起来，目的只是为了补偿心中的那份自卑心理而已；这样得过且过的心态循环往复到如今。其实，我非常厌恶这样的自己，因为始终无法理性的看待自己的能力，总是情绪化的面对一些人和事，导致自己难以正确的评估自己的能力，对未来的预期总是有偏差；结果行动上变得越来越保守和不敢前进，长远来看对自己的发展造成了很大的限制和伤害。认真付出的我，其实可以把想要做的事情做的很优秀，只是我或是懒散或是害怕失败，不愿意前进，失败的经验也是很好的收获，这个道理我早应该理解了，为何却自私的忽略了这一点呢？想做的有趣的事情仍然很多，我需要勇敢的迈出第一步，后面的事情就交给自己的天赋和努力了，这一点，我还是有那么一点自信的。\n\n　　除了金钱之外，我还有一个很重要的个人追求不得不提，那就是追逐P.H.D，不知道是本性使然，还是小时候的执念，内心中总是有股对学位的追逐，当偶然间意识到，自己其实无论是性格还是能力，其实都非常适合做科研，只是经常会由于自己心态上的因素和周遭环境的影响，无法坚定自己的想法，经常跑偏。比如说，最近诸多科研人员996，辛苦的科研努力沦为高校政绩，科研追求化为泡影，都比较让人寒心，时常会动摇自己的信念。关于这一点的具体探讨，将在另外一篇讨论高校青年教师和内卷相关话题中再做考察。","source":"_posts/关于未来的计划.md","raw":"---\ntitle: 关于未来的计划\ntoc: true\nmathjax: true\ntop: false\ncover: true\ndate: 2021-03-18 10:46:45\nupdated:\ncategories: 随笔\ntags:\n\t- 随笔\n---\n\n　　经过上一次的风波，我需要为自己的未来做一番规划，避免自己再次陷入被动？\n\n　　宏观来看，我只有一个追求，那就是自由。这是一个很宽泛的概念，叠加上我个人的追求，那具体来看就是自由的探索这个世界，可以是周游世界，也可以是感兴趣的学科领域，不断探索和创造，体验这个过程带来的新奇和乐趣，我想这就是我追求的全部了。只是我并非天之骄子，我的梦想没有翅膀，每前进一步，都需要自己创造条件。\n\n　　首先需要解决的是，金钱问题；曾经在学校的年代，轻易的相信了，“主要努力提升自己的实力，后面的事情自然是水到渠成，金钱也并不例外。” 这套听起来非常有道理的说辞，诚然从人生的时间尺度，这句话并没有什么不对，当然，如果不对，那人生可就是彻底的被一句话给浪费了，但是这样论断有一个比较大的前提条件就是，这个世界是收敛于“大体公平”的。那么，这个世界是朝着越来越公平合理的方向走吗？这个问题过于宏大，而且还是以时代和地域为前提的，我不保证自己把握住了这个世界的宏观整体走向，但是至少在中国这片土地上，我有我自己的看法和论断：相较于不堪回首的过去，中国显然变得更加公平合理了，但是着眼于未来的发展，中国被自己两千年的厚重历史给束缚住了，无法放开手脚，迈向更合理更好的未来，特别是当发展步入深水区，不能正视科学的发展模式和成果，反而用老套的权威思想去接受和包装来自中国之外的科学发展，那么结果自然就是永远都在学习，永远无法超越和创新。 这是不合理的，就如同父辈通过努力和奋斗积累的财力，试图为自己当下的权力背书，在我看来已经完全滑入了傲慢的陷阱，虽然获得了短暂的余晖，但是终将面对来自未来的审判。所以，我并不认为当下的中国是处于一个足够公平合理的年代，特别是近年来，随着房价的上涨，各种奋斗不如买房，努力不如摸鱼的论调横出不穷，也是对于这种基于机遇和运气获得的成功远远高于奋斗这种现象的反抗。毕竟不是每个人都能在人生的时间尺度上沉得住气，很容易被各种偶然的信息和机遇左右，所以我更愿意称这是一个混沌的时代，不要迷信公平和合理，主要精力除了关注发掘自己的优势之外，还应该关注这个时代的优势，努力让这二者的优势更加契合，而非背道而驰。个人在时代面前永远都是渺小的存在，个人的能力和理想难以抵抗时代大潮，虽然听起来仍然很市侩，但是如果能免除一些不必要的烦恼，也不失为一种曲线救国的方法。在当下，困扰我的就是金钱。\n\n　　如果有一夜暴富，或者短时间暴富的机会，我想我会毫不犹豫的去实践，接着便开始肆无忌惮的活着，学习，探索，实践。不知道何时开始，自己产生并逐渐坚定了这种想法，我想主要原因是认识到，通过努力和自身实力来获取成功的道路，变得越来越不可行的时候开始吧。不知不觉，我的思想就被这样的现实逐渐异化。愈加觉得这个世界魔幻了起来，比特币BTC这种全无价值的游戏也能称的上投资，连带类似的各种新创造的电子货币也都水涨船高；股票市场中起名跟区块链技术沾边的都可以顺势而起；通过房产获益的大都是凭着无知者无畏的精神吃到了先行者的红利，而且这个红利居然能持续数十年，让所有人为之疯狂；互联网企业堂而皇之的垄断市场，且有愈演愈烈之势，反而还得到司法部门的特别优待；技术外行的包工头居然能哄来顶尖的技术团队，骗得政府投资，竟也能顺利脱身。所有人都是如此的浮躁，如此的焦虑，却又这样的急功近利。投机者总是能占得先机，获得时代的青睐。只是这种方式，我不大认同罢了，除了因为自己本质上不是一名投机者之外，在认知上，我更愿意相信随着时代的发展，那些更富于思考的优秀产品和模式则会脱颖而出，那些粗制滥造的跟随者则会慢慢消亡。尽管我是如此相信合理的企业运作模式应当如此，而我却又口嫌体直的被异化成一个急功近利，希望短时间暴富的投机者。一方面是因为当下的时代，从无到有的过程中，投机者发挥功能上的作用，过程粗犷一点无妨，只要目的仍然是这种结构性的转变；反之，如果目的是从有到更好，那么投机者仍然试图复制曾经的先发优势，那么等待他的自然只有被淘汰的命运；另一方面，我从来没有把金钱当作一个目的，只是当成一种实现自我追求的手段而己。如果自己实现财务自由，我想我早就开始满世界游山玩水去了，不断体验新生活新事物，学习新知识，过自由自在的人生，甚至去探索未知的宇宙。不仅仅是意淫，是真的渴望这样的生活并愿意付诸实践。\n\n　　那么赚取足够财务自由的资金则是当前首要目的，有2种不同的操作思路，一种是常规且保守的上班，升职加薪，循序渐进，业余时间做点理财的投资，优势是持续有资金入账，不需要担心生活问题，日常的生活也会较为平稳，缺点是，可能需要较长的时间才能实现财务自由，而且常常也会面临暂时的薪酬和能力不能匹配的困境，个人职业发展也并不那么可控；另一种思路则是自己单干创业，缺点很明显就是，打通盈利模式之前，都无任何资金入账，甚至还会亏钱，还存在根本无法盈利的风险，除了技术能力之外，还需要开发管理和沟通相关能力，优点是克服种种可预见或不可预见的困难之后，有比较丰厚的回报，能快速积攒资金，个人发展规划也较为自由。我更倾向于后者，因为无论是个人技术发展，还是综合能力的提升，都会在需求的迫切提升的压力下，得到阶梯式的提升，而且能实现自己专心做技术，不被一些非必须的业务琐事干扰。唯二需要担心的问题是，一方面，自己缺乏坚持下去的信心和勇气，尽管平时自己总潜意识里觉得自己未来肯定能成大事，拥有驾驭更高阶平台的能力，但是自己两面派的特性，当自己无法坐下来思考的时候，自己也是一个彻头彻尾的感性动物，难以看清前路的方向；另一方面，在与父母冲突之前，创业之后没有资金入账，甚至亏钱都可以要父母来买单，但是此役之后，我不能获得没有收入来源之后的资金保障，那么如何生存则成为了一个问题，而且还要面临缺乏资金入账之后，父母本身对我的不齿和鄙视。原本是为自己尊严而战去努力，最后却演变成一开始就可能需要放弃尊严的困境，这无论如何让我都有点难以接受。\n\n　　仔细想想，这2个问题也并不是那么难以克服，找到了原因，这些问题都会迎刃而解！再则，既然想要做更伟大的项目，困难也更多更大，也是理所当然的事情，否则这种只有好处，没有坏处的事情，岂不是人人都可以做的来。我自认为拥有抓住问题核心的能力，只要倾注足够的思考；编程能力是**硬通货**，何必为自己缺乏资金来源而苦恼呢？虽然创业初期，会时常面临不得不接其他项目来补贴生活和项目的困境，这本身对自己也是一种历练，无论是编程能力和沟通能力，其次，这也正是实现了自己能力和酬劳挂钩的目的，为何临到自己身上的时候，对自己抱有侥幸心理呢？基本上来讲，对自己的编程能力和学习能力还是抱有足够的信心的，前提是需要自己合理规划目标和时间，我已经深受之前急功近利之苦，不仅伤害了自信，还浪费许多时间。再则，相比几年前，自己已经足够成熟，来应对自己的慵懒。毕业之后那段时间，确实深受论文折磨，心态上受到的巨大的损伤，只是因为自己表现的足够上进和优秀，就被委以如此重任，时间也非常不充裕，最后落得一副想尽办法偷懒占便宜的毛病。现在想来，仍然有点为当初没搞清楚自己的心态而感到懊恼。经过去年的锤炼，我更好的认识了自己，也能更好的面对自己，未来相信在自律这方面，也无需太多担心。\n\n　　那么剩下的问题就是缺乏豁出去的勇气了，认真审视自己，我开始有点儿讨厌自己的畏首畏尾，犹犹豫豫，缺乏决断力这样的事情了。当初决定要考研的我，当初决定要去拉萨的我，都没有现在的迷惘，不敢直视自己的内心，瞻前顾后，不敢面对失败的结果。都是毕业之后一步一步养成的心理习惯，自从决定要留学开始，行动上就一直自欺欺人，理所当然的还没开始就失败了，之后工作上虽然渐有起色，然而那只是战术上的勤奋的结果，心底上还是懊恼自己有做选择的勇气，却没有承担责任的胆量。尽管毕业前夕的事情，让自己的心态有点失衡，但无疑是自己一步一步亲手葬送自己的未来。不能再逃避现实，逃避自己的内心了，承认自己失败，肯定自己的努力。过去的失误，就由现在来弥补，未来才有机会朝着渴望的方向发展。失败的历史无法更改，原因或许只是我一开始就认怂了，那不是我愿意成为的样子，我知道我强大的一面，但是近年来的失落让我无法依仗这份强大的能力，这样看来只是处于一些心态上的原因没有有效利用自己的才能，想来确实有点不应该。我不愿意成为一个随波逐流的人，却偏偏在近几年不断的想要察言观色，不敢去主动尝试，由于害怕失败，陷入自卑的心态，实际执行起来，发现自己其实可以做的很好的，这个时候却又过分自大自满起来，目的只是为了补偿心中的那份自卑心理而已；这样得过且过的心态循环往复到如今。其实，我非常厌恶这样的自己，因为始终无法理性的看待自己的能力，总是情绪化的面对一些人和事，导致自己难以正确的评估自己的能力，对未来的预期总是有偏差；结果行动上变得越来越保守和不敢前进，长远来看对自己的发展造成了很大的限制和伤害。认真付出的我，其实可以把想要做的事情做的很优秀，只是我或是懒散或是害怕失败，不愿意前进，失败的经验也是很好的收获，这个道理我早应该理解了，为何却自私的忽略了这一点呢？想做的有趣的事情仍然很多，我需要勇敢的迈出第一步，后面的事情就交给自己的天赋和努力了，这一点，我还是有那么一点自信的。\n\n　　除了金钱之外，我还有一个很重要的个人追求不得不提，那就是追逐P.H.D，不知道是本性使然，还是小时候的执念，内心中总是有股对学位的追逐，当偶然间意识到，自己其实无论是性格还是能力，其实都非常适合做科研，只是经常会由于自己心态上的因素和周遭环境的影响，无法坚定自己的想法，经常跑偏。比如说，最近诸多科研人员996，辛苦的科研努力沦为高校政绩，科研追求化为泡影，都比较让人寒心，时常会动摇自己的信念。关于这一点的具体探讨，将在另外一篇讨论高校青年教师和内卷相关话题中再做考察。","slug":"关于未来的计划","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"ckowlff4u000iye288xs01w1p","content":"<p>　　经过上一次的风波，我需要为自己的未来做一番规划，避免自己再次陷入被动？</p>\n<p>　　宏观来看，我只有一个追求，那就是自由。这是一个很宽泛的概念，叠加上我个人的追求，那具体来看就是自由的探索这个世界，可以是周游世界，也可以是感兴趣的学科领域，不断探索和创造，体验这个过程带来的新奇和乐趣，我想这就是我追求的全部了。只是我并非天之骄子，我的梦想没有翅膀，每前进一步，都需要自己创造条件。</p>\n<p>　　首先需要解决的是，金钱问题；曾经在学校的年代，轻易的相信了，“主要努力提升自己的实力，后面的事情自然是水到渠成，金钱也并不例外。” 这套听起来非常有道理的说辞，诚然从人生的时间尺度，这句话并没有什么不对，当然，如果不对，那人生可就是彻底的被一句话给浪费了，但是这样论断有一个比较大的前提条件就是，这个世界是收敛于“大体公平”的。那么，这个世界是朝着越来越公平合理的方向走吗？这个问题过于宏大，而且还是以时代和地域为前提的，我不保证自己把握住了这个世界的宏观整体走向，但是至少在中国这片土地上，我有我自己的看法和论断：相较于不堪回首的过去，中国显然变得更加公平合理了，但是着眼于未来的发展，中国被自己两千年的厚重历史给束缚住了，无法放开手脚，迈向更合理更好的未来，特别是当发展步入深水区，不能正视科学的发展模式和成果，反而用老套的权威思想去接受和包装来自中国之外的科学发展，那么结果自然就是永远都在学习，永远无法超越和创新。 这是不合理的，就如同父辈通过努力和奋斗积累的财力，试图为自己当下的权力背书，在我看来已经完全滑入了傲慢的陷阱，虽然获得了短暂的余晖，但是终将面对来自未来的审判。所以，我并不认为当下的中国是处于一个足够公平合理的年代，特别是近年来，随着房价的上涨，各种奋斗不如买房，努力不如摸鱼的论调横出不穷，也是对于这种基于机遇和运气获得的成功远远高于奋斗这种现象的反抗。毕竟不是每个人都能在人生的时间尺度上沉得住气，很容易被各种偶然的信息和机遇左右，所以我更愿意称这是一个混沌的时代，不要迷信公平和合理，主要精力除了关注发掘自己的优势之外，还应该关注这个时代的优势，努力让这二者的优势更加契合，而非背道而驰。个人在时代面前永远都是渺小的存在，个人的能力和理想难以抵抗时代大潮，虽然听起来仍然很市侩，但是如果能免除一些不必要的烦恼，也不失为一种曲线救国的方法。在当下，困扰我的就是金钱。</p>\n<p>　　如果有一夜暴富，或者短时间暴富的机会，我想我会毫不犹豫的去实践，接着便开始肆无忌惮的活着，学习，探索，实践。不知道何时开始，自己产生并逐渐坚定了这种想法，我想主要原因是认识到，通过努力和自身实力来获取成功的道路，变得越来越不可行的时候开始吧。不知不觉，我的思想就被这样的现实逐渐异化。愈加觉得这个世界魔幻了起来，比特币BTC这种全无价值的游戏也能称的上投资，连带类似的各种新创造的电子货币也都水涨船高；股票市场中起名跟区块链技术沾边的都可以顺势而起；通过房产获益的大都是凭着无知者无畏的精神吃到了先行者的红利，而且这个红利居然能持续数十年，让所有人为之疯狂；互联网企业堂而皇之的垄断市场，且有愈演愈烈之势，反而还得到司法部门的特别优待；技术外行的包工头居然能哄来顶尖的技术团队，骗得政府投资，竟也能顺利脱身。所有人都是如此的浮躁，如此的焦虑，却又这样的急功近利。投机者总是能占得先机，获得时代的青睐。只是这种方式，我不大认同罢了，除了因为自己本质上不是一名投机者之外，在认知上，我更愿意相信随着时代的发展，那些更富于思考的优秀产品和模式则会脱颖而出，那些粗制滥造的跟随者则会慢慢消亡。尽管我是如此相信合理的企业运作模式应当如此，而我却又口嫌体直的被异化成一个急功近利，希望短时间暴富的投机者。一方面是因为当下的时代，从无到有的过程中，投机者发挥功能上的作用，过程粗犷一点无妨，只要目的仍然是这种结构性的转变；反之，如果目的是从有到更好，那么投机者仍然试图复制曾经的先发优势，那么等待他的自然只有被淘汰的命运；另一方面，我从来没有把金钱当作一个目的，只是当成一种实现自我追求的手段而己。如果自己实现财务自由，我想我早就开始满世界游山玩水去了，不断体验新生活新事物，学习新知识，过自由自在的人生，甚至去探索未知的宇宙。不仅仅是意淫，是真的渴望这样的生活并愿意付诸实践。</p>\n<p>　　那么赚取足够财务自由的资金则是当前首要目的，有2种不同的操作思路，一种是常规且保守的上班，升职加薪，循序渐进，业余时间做点理财的投资，优势是持续有资金入账，不需要担心生活问题，日常的生活也会较为平稳，缺点是，可能需要较长的时间才能实现财务自由，而且常常也会面临暂时的薪酬和能力不能匹配的困境，个人职业发展也并不那么可控；另一种思路则是自己单干创业，缺点很明显就是，打通盈利模式之前，都无任何资金入账，甚至还会亏钱，还存在根本无法盈利的风险，除了技术能力之外，还需要开发管理和沟通相关能力，优点是克服种种可预见或不可预见的困难之后，有比较丰厚的回报，能快速积攒资金，个人发展规划也较为自由。我更倾向于后者，因为无论是个人技术发展，还是综合能力的提升，都会在需求的迫切提升的压力下，得到阶梯式的提升，而且能实现自己专心做技术，不被一些非必须的业务琐事干扰。唯二需要担心的问题是，一方面，自己缺乏坚持下去的信心和勇气，尽管平时自己总潜意识里觉得自己未来肯定能成大事，拥有驾驭更高阶平台的能力，但是自己两面派的特性，当自己无法坐下来思考的时候，自己也是一个彻头彻尾的感性动物，难以看清前路的方向；另一方面，在与父母冲突之前，创业之后没有资金入账，甚至亏钱都可以要父母来买单，但是此役之后，我不能获得没有收入来源之后的资金保障，那么如何生存则成为了一个问题，而且还要面临缺乏资金入账之后，父母本身对我的不齿和鄙视。原本是为自己尊严而战去努力，最后却演变成一开始就可能需要放弃尊严的困境，这无论如何让我都有点难以接受。</p>\n<p>　　仔细想想，这2个问题也并不是那么难以克服，找到了原因，这些问题都会迎刃而解！再则，既然想要做更伟大的项目，困难也更多更大，也是理所当然的事情，否则这种只有好处，没有坏处的事情，岂不是人人都可以做的来。我自认为拥有抓住问题核心的能力，只要倾注足够的思考；编程能力是<strong>硬通货</strong>，何必为自己缺乏资金来源而苦恼呢？虽然创业初期，会时常面临不得不接其他项目来补贴生活和项目的困境，这本身对自己也是一种历练，无论是编程能力和沟通能力，其次，这也正是实现了自己能力和酬劳挂钩的目的，为何临到自己身上的时候，对自己抱有侥幸心理呢？基本上来讲，对自己的编程能力和学习能力还是抱有足够的信心的，前提是需要自己合理规划目标和时间，我已经深受之前急功近利之苦，不仅伤害了自信，还浪费许多时间。再则，相比几年前，自己已经足够成熟，来应对自己的慵懒。毕业之后那段时间，确实深受论文折磨，心态上受到的巨大的损伤，只是因为自己表现的足够上进和优秀，就被委以如此重任，时间也非常不充裕，最后落得一副想尽办法偷懒占便宜的毛病。现在想来，仍然有点为当初没搞清楚自己的心态而感到懊恼。经过去年的锤炼，我更好的认识了自己，也能更好的面对自己，未来相信在自律这方面，也无需太多担心。</p>\n<p>　　那么剩下的问题就是缺乏豁出去的勇气了，认真审视自己，我开始有点儿讨厌自己的畏首畏尾，犹犹豫豫，缺乏决断力这样的事情了。当初决定要考研的我，当初决定要去拉萨的我，都没有现在的迷惘，不敢直视自己的内心，瞻前顾后，不敢面对失败的结果。都是毕业之后一步一步养成的心理习惯，自从决定要留学开始，行动上就一直自欺欺人，理所当然的还没开始就失败了，之后工作上虽然渐有起色，然而那只是战术上的勤奋的结果，心底上还是懊恼自己有做选择的勇气，却没有承担责任的胆量。尽管毕业前夕的事情，让自己的心态有点失衡，但无疑是自己一步一步亲手葬送自己的未来。不能再逃避现实，逃避自己的内心了，承认自己失败，肯定自己的努力。过去的失误，就由现在来弥补，未来才有机会朝着渴望的方向发展。失败的历史无法更改，原因或许只是我一开始就认怂了，那不是我愿意成为的样子，我知道我强大的一面，但是近年来的失落让我无法依仗这份强大的能力，这样看来只是处于一些心态上的原因没有有效利用自己的才能，想来确实有点不应该。我不愿意成为一个随波逐流的人，却偏偏在近几年不断的想要察言观色，不敢去主动尝试，由于害怕失败，陷入自卑的心态，实际执行起来，发现自己其实可以做的很好的，这个时候却又过分自大自满起来，目的只是为了补偿心中的那份自卑心理而已；这样得过且过的心态循环往复到如今。其实，我非常厌恶这样的自己，因为始终无法理性的看待自己的能力，总是情绪化的面对一些人和事，导致自己难以正确的评估自己的能力，对未来的预期总是有偏差；结果行动上变得越来越保守和不敢前进，长远来看对自己的发展造成了很大的限制和伤害。认真付出的我，其实可以把想要做的事情做的很优秀，只是我或是懒散或是害怕失败，不愿意前进，失败的经验也是很好的收获，这个道理我早应该理解了，为何却自私的忽略了这一点呢？想做的有趣的事情仍然很多，我需要勇敢的迈出第一步，后面的事情就交给自己的天赋和努力了，这一点，我还是有那么一点自信的。</p>\n<p>　　除了金钱之外，我还有一个很重要的个人追求不得不提，那就是追逐P.H.D，不知道是本性使然，还是小时候的执念，内心中总是有股对学位的追逐，当偶然间意识到，自己其实无论是性格还是能力，其实都非常适合做科研，只是经常会由于自己心态上的因素和周遭环境的影响，无法坚定自己的想法，经常跑偏。比如说，最近诸多科研人员996，辛苦的科研努力沦为高校政绩，科研追求化为泡影，都比较让人寒心，时常会动摇自己的信念。关于这一点的具体探讨，将在另外一篇讨论高校青年教师和内卷相关话题中再做考察。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>　　经过上一次的风波，我需要为自己的未来做一番规划，避免自己再次陷入被动？</p>\n<p>　　宏观来看，我只有一个追求，那就是自由。这是一个很宽泛的概念，叠加上我个人的追求，那具体来看就是自由的探索这个世界，可以是周游世界，也可以是感兴趣的学科领域，不断探索和创造，体验这个过程带来的新奇和乐趣，我想这就是我追求的全部了。只是我并非天之骄子，我的梦想没有翅膀，每前进一步，都需要自己创造条件。</p>\n<p>　　首先需要解决的是，金钱问题；曾经在学校的年代，轻易的相信了，“主要努力提升自己的实力，后面的事情自然是水到渠成，金钱也并不例外。” 这套听起来非常有道理的说辞，诚然从人生的时间尺度，这句话并没有什么不对，当然，如果不对，那人生可就是彻底的被一句话给浪费了，但是这样论断有一个比较大的前提条件就是，这个世界是收敛于“大体公平”的。那么，这个世界是朝着越来越公平合理的方向走吗？这个问题过于宏大，而且还是以时代和地域为前提的，我不保证自己把握住了这个世界的宏观整体走向，但是至少在中国这片土地上，我有我自己的看法和论断：相较于不堪回首的过去，中国显然变得更加公平合理了，但是着眼于未来的发展，中国被自己两千年的厚重历史给束缚住了，无法放开手脚，迈向更合理更好的未来，特别是当发展步入深水区，不能正视科学的发展模式和成果，反而用老套的权威思想去接受和包装来自中国之外的科学发展，那么结果自然就是永远都在学习，永远无法超越和创新。 这是不合理的，就如同父辈通过努力和奋斗积累的财力，试图为自己当下的权力背书，在我看来已经完全滑入了傲慢的陷阱，虽然获得了短暂的余晖，但是终将面对来自未来的审判。所以，我并不认为当下的中国是处于一个足够公平合理的年代，特别是近年来，随着房价的上涨，各种奋斗不如买房，努力不如摸鱼的论调横出不穷，也是对于这种基于机遇和运气获得的成功远远高于奋斗这种现象的反抗。毕竟不是每个人都能在人生的时间尺度上沉得住气，很容易被各种偶然的信息和机遇左右，所以我更愿意称这是一个混沌的时代，不要迷信公平和合理，主要精力除了关注发掘自己的优势之外，还应该关注这个时代的优势，努力让这二者的优势更加契合，而非背道而驰。个人在时代面前永远都是渺小的存在，个人的能力和理想难以抵抗时代大潮，虽然听起来仍然很市侩，但是如果能免除一些不必要的烦恼，也不失为一种曲线救国的方法。在当下，困扰我的就是金钱。</p>\n<p>　　如果有一夜暴富，或者短时间暴富的机会，我想我会毫不犹豫的去实践，接着便开始肆无忌惮的活着，学习，探索，实践。不知道何时开始，自己产生并逐渐坚定了这种想法，我想主要原因是认识到，通过努力和自身实力来获取成功的道路，变得越来越不可行的时候开始吧。不知不觉，我的思想就被这样的现实逐渐异化。愈加觉得这个世界魔幻了起来，比特币BTC这种全无价值的游戏也能称的上投资，连带类似的各种新创造的电子货币也都水涨船高；股票市场中起名跟区块链技术沾边的都可以顺势而起；通过房产获益的大都是凭着无知者无畏的精神吃到了先行者的红利，而且这个红利居然能持续数十年，让所有人为之疯狂；互联网企业堂而皇之的垄断市场，且有愈演愈烈之势，反而还得到司法部门的特别优待；技术外行的包工头居然能哄来顶尖的技术团队，骗得政府投资，竟也能顺利脱身。所有人都是如此的浮躁，如此的焦虑，却又这样的急功近利。投机者总是能占得先机，获得时代的青睐。只是这种方式，我不大认同罢了，除了因为自己本质上不是一名投机者之外，在认知上，我更愿意相信随着时代的发展，那些更富于思考的优秀产品和模式则会脱颖而出，那些粗制滥造的跟随者则会慢慢消亡。尽管我是如此相信合理的企业运作模式应当如此，而我却又口嫌体直的被异化成一个急功近利，希望短时间暴富的投机者。一方面是因为当下的时代，从无到有的过程中，投机者发挥功能上的作用，过程粗犷一点无妨，只要目的仍然是这种结构性的转变；反之，如果目的是从有到更好，那么投机者仍然试图复制曾经的先发优势，那么等待他的自然只有被淘汰的命运；另一方面，我从来没有把金钱当作一个目的，只是当成一种实现自我追求的手段而己。如果自己实现财务自由，我想我早就开始满世界游山玩水去了，不断体验新生活新事物，学习新知识，过自由自在的人生，甚至去探索未知的宇宙。不仅仅是意淫，是真的渴望这样的生活并愿意付诸实践。</p>\n<p>　　那么赚取足够财务自由的资金则是当前首要目的，有2种不同的操作思路，一种是常规且保守的上班，升职加薪，循序渐进，业余时间做点理财的投资，优势是持续有资金入账，不需要担心生活问题，日常的生活也会较为平稳，缺点是，可能需要较长的时间才能实现财务自由，而且常常也会面临暂时的薪酬和能力不能匹配的困境，个人职业发展也并不那么可控；另一种思路则是自己单干创业，缺点很明显就是，打通盈利模式之前，都无任何资金入账，甚至还会亏钱，还存在根本无法盈利的风险，除了技术能力之外，还需要开发管理和沟通相关能力，优点是克服种种可预见或不可预见的困难之后，有比较丰厚的回报，能快速积攒资金，个人发展规划也较为自由。我更倾向于后者，因为无论是个人技术发展，还是综合能力的提升，都会在需求的迫切提升的压力下，得到阶梯式的提升，而且能实现自己专心做技术，不被一些非必须的业务琐事干扰。唯二需要担心的问题是，一方面，自己缺乏坚持下去的信心和勇气，尽管平时自己总潜意识里觉得自己未来肯定能成大事，拥有驾驭更高阶平台的能力，但是自己两面派的特性，当自己无法坐下来思考的时候，自己也是一个彻头彻尾的感性动物，难以看清前路的方向；另一方面，在与父母冲突之前，创业之后没有资金入账，甚至亏钱都可以要父母来买单，但是此役之后，我不能获得没有收入来源之后的资金保障，那么如何生存则成为了一个问题，而且还要面临缺乏资金入账之后，父母本身对我的不齿和鄙视。原本是为自己尊严而战去努力，最后却演变成一开始就可能需要放弃尊严的困境，这无论如何让我都有点难以接受。</p>\n<p>　　仔细想想，这2个问题也并不是那么难以克服，找到了原因，这些问题都会迎刃而解！再则，既然想要做更伟大的项目，困难也更多更大，也是理所当然的事情，否则这种只有好处，没有坏处的事情，岂不是人人都可以做的来。我自认为拥有抓住问题核心的能力，只要倾注足够的思考；编程能力是<strong>硬通货</strong>，何必为自己缺乏资金来源而苦恼呢？虽然创业初期，会时常面临不得不接其他项目来补贴生活和项目的困境，这本身对自己也是一种历练，无论是编程能力和沟通能力，其次，这也正是实现了自己能力和酬劳挂钩的目的，为何临到自己身上的时候，对自己抱有侥幸心理呢？基本上来讲，对自己的编程能力和学习能力还是抱有足够的信心的，前提是需要自己合理规划目标和时间，我已经深受之前急功近利之苦，不仅伤害了自信，还浪费许多时间。再则，相比几年前，自己已经足够成熟，来应对自己的慵懒。毕业之后那段时间，确实深受论文折磨，心态上受到的巨大的损伤，只是因为自己表现的足够上进和优秀，就被委以如此重任，时间也非常不充裕，最后落得一副想尽办法偷懒占便宜的毛病。现在想来，仍然有点为当初没搞清楚自己的心态而感到懊恼。经过去年的锤炼，我更好的认识了自己，也能更好的面对自己，未来相信在自律这方面，也无需太多担心。</p>\n<p>　　那么剩下的问题就是缺乏豁出去的勇气了，认真审视自己，我开始有点儿讨厌自己的畏首畏尾，犹犹豫豫，缺乏决断力这样的事情了。当初决定要考研的我，当初决定要去拉萨的我，都没有现在的迷惘，不敢直视自己的内心，瞻前顾后，不敢面对失败的结果。都是毕业之后一步一步养成的心理习惯，自从决定要留学开始，行动上就一直自欺欺人，理所当然的还没开始就失败了，之后工作上虽然渐有起色，然而那只是战术上的勤奋的结果，心底上还是懊恼自己有做选择的勇气，却没有承担责任的胆量。尽管毕业前夕的事情，让自己的心态有点失衡，但无疑是自己一步一步亲手葬送自己的未来。不能再逃避现实，逃避自己的内心了，承认自己失败，肯定自己的努力。过去的失误，就由现在来弥补，未来才有机会朝着渴望的方向发展。失败的历史无法更改，原因或许只是我一开始就认怂了，那不是我愿意成为的样子，我知道我强大的一面，但是近年来的失落让我无法依仗这份强大的能力，这样看来只是处于一些心态上的原因没有有效利用自己的才能，想来确实有点不应该。我不愿意成为一个随波逐流的人，却偏偏在近几年不断的想要察言观色，不敢去主动尝试，由于害怕失败，陷入自卑的心态，实际执行起来，发现自己其实可以做的很好的，这个时候却又过分自大自满起来，目的只是为了补偿心中的那份自卑心理而已；这样得过且过的心态循环往复到如今。其实，我非常厌恶这样的自己，因为始终无法理性的看待自己的能力，总是情绪化的面对一些人和事，导致自己难以正确的评估自己的能力，对未来的预期总是有偏差；结果行动上变得越来越保守和不敢前进，长远来看对自己的发展造成了很大的限制和伤害。认真付出的我，其实可以把想要做的事情做的很优秀，只是我或是懒散或是害怕失败，不愿意前进，失败的经验也是很好的收获，这个道理我早应该理解了，为何却自私的忽略了这一点呢？想做的有趣的事情仍然很多，我需要勇敢的迈出第一步，后面的事情就交给自己的天赋和努力了，这一点，我还是有那么一点自信的。</p>\n<p>　　除了金钱之外，我还有一个很重要的个人追求不得不提，那就是追逐P.H.D，不知道是本性使然，还是小时候的执念，内心中总是有股对学位的追逐，当偶然间意识到，自己其实无论是性格还是能力，其实都非常适合做科研，只是经常会由于自己心态上的因素和周遭环境的影响，无法坚定自己的想法，经常跑偏。比如说，最近诸多科研人员996，辛苦的科研努力沦为高校政绩，科研追求化为泡影，都比较让人寒心，时常会动摇自己的信念。关于这一点的具体探讨，将在另外一篇讨论高校青年教师和内卷相关话题中再做考察。</p>\n"},{"title":"双重思想（转载）","toc":true,"mathjax":true,"top":false,"cover":true,"date":"2021-03-19T03:02:29.000Z","updated":"2021-03-19T03:06:50.320Z","_content":"\n> 一遍非常详细的解释双重思想定义的文章\n>\n> From : https://yonglezh.github.io/2017/01/21/doublethink/\n\n\n\n大概2011年的时候，读了《1984》，对书中提到的“双重思想”很感兴趣，却一直觉得不能彻底理解。\n\n书中定义双重思想为“一个人的脑子里同时具有两种相互矛盾的信念，而且两种都接受。”最初读到这里我非常迷惑：何谓“同时接受矛盾的信念”？人真的可以做到吗？比如，为什么我觉得无法想象自己同时接受地心说和日心说呢？\n\n### 澄清定义 - “同时接受”带来的误解\n\n有一个非常经典的例子可以用来帮助理解“同时接受矛盾的信念”：母亲和女友同时掉入水中，且两人都不会游泳，你会救哪一个？\n\n很多人会选择在面对母亲的时候说救母亲、面对女友的时候说救女友，但同时面对母亲和女友两人的时候就不知该如何回答了。\n\n所以人是没法同时按照两种相互矛盾的信念来行为的。那么，这是说没有“双重思想”这种东西吗？\n\n不，书中描述的双重思想还是存在的，但是其定义中的“同时接受”却容易造成误解。“同时”指的是“两种想法同时存在在脑中”，但并非指“两种想法同时表现于意识的表层并指导此人的行为”。\n\nWikipedia上对于双重思想的定义在《1984》书中原句的基础上增加了一个描述，跟我的理解很相似：\n\n> Doublethink is the act of simultaneously accepting two mutually contradictory beliefs as correct, often **in distinct social contexts**.\n\n这个定义特别指出了双重思想是指“**在不同的情景下**接受矛盾的信念”。\n\n牛津英语字典（The Oxford Companion to the English Language）中对于双重思想的解释也着重说明了这一点：\n\n> The term is widely used to describe a capacity to engage in one line of thought in one situation (at work, in a certain group, in business, etc.) and another line in another situation (at home, in another group, in private life), without necessarily sensing any conflict between the two.\n\n那么，这就是双重思想的全部了么？只要我面对母亲的时候相信该救母亲、面对女友的时候相信该救女友，就是双重思想了么？\n\n### 双重思想到底是什么？\n\n《1984》中对于双重思想的定义作了进一步的解释：\n\n> “知与不知，知道全部真实情况而却扯些滴水不漏的谎话，同时持两种互相抵消的观点，明知它们互相矛盾而仍都相信，用逻辑来反逻辑，一边表示拥护道德一边又否定道德，一边相信民主是办不到的一边又相信党是民主的捍卫者，忘掉一切必须忘掉的东西而又在需要的时候想起它来，然后又马上忘掉它，而尤其是，把这样的做法应用到做法本身上面——这可谓绝妙透顶了：有意识地进入无意识，而后又并不意识到你刚才完成的催眠。即使要了解“双重思想”的含义你也得使用双重思想。”\n\n这段话的核心是：\n\n1. 相信两种互相矛盾的信念。（并非同时，而是脑袋里具备两种互相矛盾的信念。）\n2. 根据情境来切换自己相信的信念。\n3. 忽略矛盾，认为自己当下的信念就是自己一直以来坚持的信念。\n4. 忽略自己在忽略矛盾，忽略自己在不断地切换信念。（意识不到自己按不同情境做了不同的选择 —— 这是明知两个信念存在矛盾却不认为自己的行为有矛盾的关键。）\n\n依然用经典例子来帮助理解：如果（1、2）一个人面对母亲的时候相信应该救母亲，面对女友的时候相信应该救女友，并且（3）意识不到自己在面对另一个人时的矛盾信念和行为，而且（4）意识不到自己在使用这种思想方法忽略矛盾。这就是双重思想了。\n\n### 什么不是双重思想\n\n仅有（1）、（2）是不够的，（3）的重要性在于：如果一个人面对母亲的时候说应该救母亲，面对女友的时候说应该救女友，但是意识得到自己在两种情况下的矛盾行为，那么这个人并没有双重思想。因为他没有两种相互矛盾的信念，而是只有一个一致的信念：相信应该哄母亲和女友开心。\n\n《1984》中老大哥的缔造者们正属于此类，他们意识到只有一个唯一不变的目的，那就是权力本身。为了维护权力，在说谎有利的时候说谎、在使用双重思想有利的时候使用双重思想。这并非双重思想！因为他们一直只有一个一致的信念：追求权力。\n\n仅有（1）、（2）和（3）还是不够的，（4）的重要性在于：即使一个人有在不同情景下矛盾的信念和行为，但是一旦被提醒就意识到了自己的前后矛盾，并且调整自己的信念来化解该矛盾，那么此人也不是双重思想，而仅仅是可以改正的不一致信念。即，仅有“忽略矛盾”是不够的，必须要有“忽略自己在忽略矛盾”。\n\n（3）“忽略矛盾”和（4）“忽略自己在忽略矛盾”正是双重思想最关键之处，也是《1984》中党用来培养双重思想的特殊训练的重点。\n\n### 双重思想是如何造就的\n\n1. 犯罪停止(crimestop)\n\n   > “犯罪停止(crimestop)的意思就是指在产生任何危险思想之前出于本能地悬崖勒马的能力。这种能力还包括不能理解类比，不能看到逻辑错误，不能正确了解与党的原则不一致的最简单论点、对于任何可以朝异端方向发展的思路感到厌倦、厌恶。总而言之，犯罪停止(crimestop)意味着起保护作用的愚蠢。”\n\n   这是训练一个人针对党的观点“忽略矛盾”的能力。\n\n   其实这种不加思考的、对某些观点本能的反感的能力人类本身就具备，如果一个观点让人觉得不舒服（比如被人当面指出错误），即使它逻辑上正确，人情感上也很难接受。反而是依据客观事实的逻辑思考和分析的能力才是需要辛苦训练才能获得的。\n\n   此“犯罪停止”训练，一方面增强人类本身的这种缺点，另一方面确保这种能力应用在党的观点上。\n\n2. 黑白(blackwhite)\n\n   > “这意味着不顾明显事实硬说黑就是白的无耻习惯。用在党员身上，这意味着在党的纪律要求你说黑就是白时，你就有这样自觉的忠诚。但这也意味着相信黑就是白的能力，甚至是知道黑就是白和忘掉过去曾经有过相反认识的能力。”\n\n   这是训练针对党的要求“忽略自己在忽略矛盾”的能力。其实无需特意训练，人类本身就具有这种忽略矛盾并厚着脸皮死不承认的能力……人类实在是是个善于欺骗自己的物种。\n\n   所以，即使不经历书中的训练，人也能形成双重思想。只要不鼓励人们学习进行深入客观的思考、形成自己的意见即可。\n\n### 人的思想可能一致吗\n\n每个人大概都有在不同情景下思想不一致的时候，意识到这一点并非特别困难的事情：尝试回想一下若干年之前的自己就很容易能发现，现在的自己对很多事物都有了不一样的看法。\n\n在某一时刻，人只能按照一个信念行动，但同时脑后可能存在着暂时被抛在脑后的与其矛盾的信念。正如同时面对母亲和女友很多人无法提供一个该救谁的答案：人脑可以把矛盾的想法暂时放在脑后，但无法在同一时刻按照两个矛盾的想法行动。\n\n思想在不同时刻的不一致是无法被消除的。人的思想总是在变化，有的时候是在前进，即随着对事物的理解更深刻，更加接近真理，有的时候是在倒退，就连科学界对于真理的认识也是反反复复螺旋式前进的。\n\n思想在同一时刻的不一致可能也是很难被消除的。一个人的思想纷繁复杂，可能总有什么矛盾的想法深埋在脑海深处、不到某一特殊的时刻不会被激发出来并被意识到。\n\n不过，只要意识到自己可能忽略自己思想中的矛盾，并在发现自己的思想存在矛盾时尽量调整，就打破了双重思想最重要的一环。努力发现自己的不一致，尽量避免被自己的本能或情绪蒙蔽，也接受自己与过去的自己的不一致，尽力朝着真理前进。这大概是我们能做的事了罢。\n\n### 为什么要避免双重思想\n\n双重思想的特点是对不一致的忽略和对事实的扭曲，这意味着双重思想是真理的敌人。而对真理的追求大概是人类从历史上习得的最重要的教训之一了。\n\n> “When you are studying any matter, or considering any philosophy, ask yourself only what are the facts and what is the truth that the facts bear out. Never let yourself be diverted either by what you wish to believe, or by what you think would have beneficent social effects if it were believed. But look only, and solely, at what are the facts.” – Bertrand Russell - Message to Future Generations\n\n### 写在文后：双重思想 VS. 双重标准\n\n最初写这篇文章的原因之一是在思考双重标准与双重思想有什么区别。\n\n双重思想的核心是忽略矛盾和忽略自己忽略了矛盾，双重标准的核心是在我者和他者的立场转换时的不一致的态度（倾向于美化自己丑化他人）。\n\n双重思想的“不同情景”在范围上涵盖（⊃）双重标准的“我者和他者的立场转换”，双重思想的“矛盾”在范围上也涵盖（⊃）双重标准的“不一致的态度”，但双重思想相比于双重标准额外要求“忽略矛盾”和“忽略自己忽略矛盾”。所以双重思想和双重标准其实有重叠之处。\n\n![alt text](https://upload.wikimedia.org/wikipedia/commons/d/da/Set_intersection.svg)\n\n1. A：双重思想\n2. B：双重标准\n3. A∩B：“我者和他者的立场转换”+“不一致的态度”+“忽略矛盾”+“忽略自己忽略矛盾”\n4. A-A∩B：(“不同情景” - “我者和他者的立场转换”)+(“矛盾” - “不一致的态度”)+“忽略矛盾”+“忽略自己忽略矛盾”\n5. B-A∩B： “我者和他者的立场转换”+“不一致的态度” - “忽略矛盾” - “忽略自己忽略矛盾”\n\n前面提到的一个人选择救母亲还是女友的双重思想的例子属于（4）双重思想但非双重标准，因为它不涉及“我者和他者的立场转换”。\n\n如果一个人，在别人选择救母亲的时候谴责他渣男，在别人选择救女友的时候谴责他不孝，自己选的时候却选什么都觉得自己选得好。这是典型的双重标准，因为存在“我者和他者的立场转换”和“不一致的态度”。\n\n如果这个人意识得到自己在这件事上的观点前后不一致（不满足“忽略矛盾”）、或者被人提醒后认识到自己的观点前后不一致（不满足“忽略自己在忽略矛盾”），那么这个人的行为就仅仅属于（5）双重标准而非双重思想。\n\n如果他忽略自己对自己和别人观点不一致，并且即使经人提醒也不承认，那么这个人的行为就属于（3）既是双重标准又是双重思想了。","source":"_posts/双重思想（转载）.md","raw":"---\ntitle: 双重思想（转载）\ntoc: true\nmathjax: true\ntop: false\ncover: true\ndate: 2021-03-19 11:02:29\nupdated:\ncategories: 随笔\ntags:\n\t- 随笔\n\t- 1984\n---\n\n> 一遍非常详细的解释双重思想定义的文章\n>\n> From : https://yonglezh.github.io/2017/01/21/doublethink/\n\n\n\n大概2011年的时候，读了《1984》，对书中提到的“双重思想”很感兴趣，却一直觉得不能彻底理解。\n\n书中定义双重思想为“一个人的脑子里同时具有两种相互矛盾的信念，而且两种都接受。”最初读到这里我非常迷惑：何谓“同时接受矛盾的信念”？人真的可以做到吗？比如，为什么我觉得无法想象自己同时接受地心说和日心说呢？\n\n### 澄清定义 - “同时接受”带来的误解\n\n有一个非常经典的例子可以用来帮助理解“同时接受矛盾的信念”：母亲和女友同时掉入水中，且两人都不会游泳，你会救哪一个？\n\n很多人会选择在面对母亲的时候说救母亲、面对女友的时候说救女友，但同时面对母亲和女友两人的时候就不知该如何回答了。\n\n所以人是没法同时按照两种相互矛盾的信念来行为的。那么，这是说没有“双重思想”这种东西吗？\n\n不，书中描述的双重思想还是存在的，但是其定义中的“同时接受”却容易造成误解。“同时”指的是“两种想法同时存在在脑中”，但并非指“两种想法同时表现于意识的表层并指导此人的行为”。\n\nWikipedia上对于双重思想的定义在《1984》书中原句的基础上增加了一个描述，跟我的理解很相似：\n\n> Doublethink is the act of simultaneously accepting two mutually contradictory beliefs as correct, often **in distinct social contexts**.\n\n这个定义特别指出了双重思想是指“**在不同的情景下**接受矛盾的信念”。\n\n牛津英语字典（The Oxford Companion to the English Language）中对于双重思想的解释也着重说明了这一点：\n\n> The term is widely used to describe a capacity to engage in one line of thought in one situation (at work, in a certain group, in business, etc.) and another line in another situation (at home, in another group, in private life), without necessarily sensing any conflict between the two.\n\n那么，这就是双重思想的全部了么？只要我面对母亲的时候相信该救母亲、面对女友的时候相信该救女友，就是双重思想了么？\n\n### 双重思想到底是什么？\n\n《1984》中对于双重思想的定义作了进一步的解释：\n\n> “知与不知，知道全部真实情况而却扯些滴水不漏的谎话，同时持两种互相抵消的观点，明知它们互相矛盾而仍都相信，用逻辑来反逻辑，一边表示拥护道德一边又否定道德，一边相信民主是办不到的一边又相信党是民主的捍卫者，忘掉一切必须忘掉的东西而又在需要的时候想起它来，然后又马上忘掉它，而尤其是，把这样的做法应用到做法本身上面——这可谓绝妙透顶了：有意识地进入无意识，而后又并不意识到你刚才完成的催眠。即使要了解“双重思想”的含义你也得使用双重思想。”\n\n这段话的核心是：\n\n1. 相信两种互相矛盾的信念。（并非同时，而是脑袋里具备两种互相矛盾的信念。）\n2. 根据情境来切换自己相信的信念。\n3. 忽略矛盾，认为自己当下的信念就是自己一直以来坚持的信念。\n4. 忽略自己在忽略矛盾，忽略自己在不断地切换信念。（意识不到自己按不同情境做了不同的选择 —— 这是明知两个信念存在矛盾却不认为自己的行为有矛盾的关键。）\n\n依然用经典例子来帮助理解：如果（1、2）一个人面对母亲的时候相信应该救母亲，面对女友的时候相信应该救女友，并且（3）意识不到自己在面对另一个人时的矛盾信念和行为，而且（4）意识不到自己在使用这种思想方法忽略矛盾。这就是双重思想了。\n\n### 什么不是双重思想\n\n仅有（1）、（2）是不够的，（3）的重要性在于：如果一个人面对母亲的时候说应该救母亲，面对女友的时候说应该救女友，但是意识得到自己在两种情况下的矛盾行为，那么这个人并没有双重思想。因为他没有两种相互矛盾的信念，而是只有一个一致的信念：相信应该哄母亲和女友开心。\n\n《1984》中老大哥的缔造者们正属于此类，他们意识到只有一个唯一不变的目的，那就是权力本身。为了维护权力，在说谎有利的时候说谎、在使用双重思想有利的时候使用双重思想。这并非双重思想！因为他们一直只有一个一致的信念：追求权力。\n\n仅有（1）、（2）和（3）还是不够的，（4）的重要性在于：即使一个人有在不同情景下矛盾的信念和行为，但是一旦被提醒就意识到了自己的前后矛盾，并且调整自己的信念来化解该矛盾，那么此人也不是双重思想，而仅仅是可以改正的不一致信念。即，仅有“忽略矛盾”是不够的，必须要有“忽略自己在忽略矛盾”。\n\n（3）“忽略矛盾”和（4）“忽略自己在忽略矛盾”正是双重思想最关键之处，也是《1984》中党用来培养双重思想的特殊训练的重点。\n\n### 双重思想是如何造就的\n\n1. 犯罪停止(crimestop)\n\n   > “犯罪停止(crimestop)的意思就是指在产生任何危险思想之前出于本能地悬崖勒马的能力。这种能力还包括不能理解类比，不能看到逻辑错误，不能正确了解与党的原则不一致的最简单论点、对于任何可以朝异端方向发展的思路感到厌倦、厌恶。总而言之，犯罪停止(crimestop)意味着起保护作用的愚蠢。”\n\n   这是训练一个人针对党的观点“忽略矛盾”的能力。\n\n   其实这种不加思考的、对某些观点本能的反感的能力人类本身就具备，如果一个观点让人觉得不舒服（比如被人当面指出错误），即使它逻辑上正确，人情感上也很难接受。反而是依据客观事实的逻辑思考和分析的能力才是需要辛苦训练才能获得的。\n\n   此“犯罪停止”训练，一方面增强人类本身的这种缺点，另一方面确保这种能力应用在党的观点上。\n\n2. 黑白(blackwhite)\n\n   > “这意味着不顾明显事实硬说黑就是白的无耻习惯。用在党员身上，这意味着在党的纪律要求你说黑就是白时，你就有这样自觉的忠诚。但这也意味着相信黑就是白的能力，甚至是知道黑就是白和忘掉过去曾经有过相反认识的能力。”\n\n   这是训练针对党的要求“忽略自己在忽略矛盾”的能力。其实无需特意训练，人类本身就具有这种忽略矛盾并厚着脸皮死不承认的能力……人类实在是是个善于欺骗自己的物种。\n\n   所以，即使不经历书中的训练，人也能形成双重思想。只要不鼓励人们学习进行深入客观的思考、形成自己的意见即可。\n\n### 人的思想可能一致吗\n\n每个人大概都有在不同情景下思想不一致的时候，意识到这一点并非特别困难的事情：尝试回想一下若干年之前的自己就很容易能发现，现在的自己对很多事物都有了不一样的看法。\n\n在某一时刻，人只能按照一个信念行动，但同时脑后可能存在着暂时被抛在脑后的与其矛盾的信念。正如同时面对母亲和女友很多人无法提供一个该救谁的答案：人脑可以把矛盾的想法暂时放在脑后，但无法在同一时刻按照两个矛盾的想法行动。\n\n思想在不同时刻的不一致是无法被消除的。人的思想总是在变化，有的时候是在前进，即随着对事物的理解更深刻，更加接近真理，有的时候是在倒退，就连科学界对于真理的认识也是反反复复螺旋式前进的。\n\n思想在同一时刻的不一致可能也是很难被消除的。一个人的思想纷繁复杂，可能总有什么矛盾的想法深埋在脑海深处、不到某一特殊的时刻不会被激发出来并被意识到。\n\n不过，只要意识到自己可能忽略自己思想中的矛盾，并在发现自己的思想存在矛盾时尽量调整，就打破了双重思想最重要的一环。努力发现自己的不一致，尽量避免被自己的本能或情绪蒙蔽，也接受自己与过去的自己的不一致，尽力朝着真理前进。这大概是我们能做的事了罢。\n\n### 为什么要避免双重思想\n\n双重思想的特点是对不一致的忽略和对事实的扭曲，这意味着双重思想是真理的敌人。而对真理的追求大概是人类从历史上习得的最重要的教训之一了。\n\n> “When you are studying any matter, or considering any philosophy, ask yourself only what are the facts and what is the truth that the facts bear out. Never let yourself be diverted either by what you wish to believe, or by what you think would have beneficent social effects if it were believed. But look only, and solely, at what are the facts.” – Bertrand Russell - Message to Future Generations\n\n### 写在文后：双重思想 VS. 双重标准\n\n最初写这篇文章的原因之一是在思考双重标准与双重思想有什么区别。\n\n双重思想的核心是忽略矛盾和忽略自己忽略了矛盾，双重标准的核心是在我者和他者的立场转换时的不一致的态度（倾向于美化自己丑化他人）。\n\n双重思想的“不同情景”在范围上涵盖（⊃）双重标准的“我者和他者的立场转换”，双重思想的“矛盾”在范围上也涵盖（⊃）双重标准的“不一致的态度”，但双重思想相比于双重标准额外要求“忽略矛盾”和“忽略自己忽略矛盾”。所以双重思想和双重标准其实有重叠之处。\n\n![alt text](https://upload.wikimedia.org/wikipedia/commons/d/da/Set_intersection.svg)\n\n1. A：双重思想\n2. B：双重标准\n3. A∩B：“我者和他者的立场转换”+“不一致的态度”+“忽略矛盾”+“忽略自己忽略矛盾”\n4. A-A∩B：(“不同情景” - “我者和他者的立场转换”)+(“矛盾” - “不一致的态度”)+“忽略矛盾”+“忽略自己忽略矛盾”\n5. B-A∩B： “我者和他者的立场转换”+“不一致的态度” - “忽略矛盾” - “忽略自己忽略矛盾”\n\n前面提到的一个人选择救母亲还是女友的双重思想的例子属于（4）双重思想但非双重标准，因为它不涉及“我者和他者的立场转换”。\n\n如果一个人，在别人选择救母亲的时候谴责他渣男，在别人选择救女友的时候谴责他不孝，自己选的时候却选什么都觉得自己选得好。这是典型的双重标准，因为存在“我者和他者的立场转换”和“不一致的态度”。\n\n如果这个人意识得到自己在这件事上的观点前后不一致（不满足“忽略矛盾”）、或者被人提醒后认识到自己的观点前后不一致（不满足“忽略自己在忽略矛盾”），那么这个人的行为就仅仅属于（5）双重标准而非双重思想。\n\n如果他忽略自己对自己和别人观点不一致，并且即使经人提醒也不承认，那么这个人的行为就属于（3）既是双重标准又是双重思想了。","slug":"双重思想（转载）","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"ckowlff4u000lye28c1bi3rap","content":"<blockquote>\n<p>一遍非常详细的解释双重思想定义的文章</p>\n<p>From : <a href=\"https://yonglezh.github.io/2017/01/21/doublethink/\">https://yonglezh.github.io/2017/01/21/doublethink/</a></p>\n</blockquote>\n<p>大概2011年的时候，读了《1984》，对书中提到的“双重思想”很感兴趣，却一直觉得不能彻底理解。</p>\n<p>书中定义双重思想为“一个人的脑子里同时具有两种相互矛盾的信念，而且两种都接受。”最初读到这里我非常迷惑：何谓“同时接受矛盾的信念”？人真的可以做到吗？比如，为什么我觉得无法想象自己同时接受地心说和日心说呢？</p>\n<h3 id=\"澄清定义-“同时接受”带来的误解\"><a href=\"#澄清定义-“同时接受”带来的误解\" class=\"headerlink\" title=\"澄清定义 - “同时接受”带来的误解\"></a>澄清定义 - “同时接受”带来的误解</h3><p>有一个非常经典的例子可以用来帮助理解“同时接受矛盾的信念”：母亲和女友同时掉入水中，且两人都不会游泳，你会救哪一个？</p>\n<p>很多人会选择在面对母亲的时候说救母亲、面对女友的时候说救女友，但同时面对母亲和女友两人的时候就不知该如何回答了。</p>\n<p>所以人是没法同时按照两种相互矛盾的信念来行为的。那么，这是说没有“双重思想”这种东西吗？</p>\n<p>不，书中描述的双重思想还是存在的，但是其定义中的“同时接受”却容易造成误解。“同时”指的是“两种想法同时存在在脑中”，但并非指“两种想法同时表现于意识的表层并指导此人的行为”。</p>\n<p>Wikipedia上对于双重思想的定义在《1984》书中原句的基础上增加了一个描述，跟我的理解很相似：</p>\n<blockquote>\n<p>Doublethink is the act of simultaneously accepting two mutually contradictory beliefs as correct, often <strong>in distinct social contexts</strong>.</p>\n</blockquote>\n<p>这个定义特别指出了双重思想是指“<strong>在不同的情景下</strong>接受矛盾的信念”。</p>\n<p>牛津英语字典（The Oxford Companion to the English Language）中对于双重思想的解释也着重说明了这一点：</p>\n<blockquote>\n<p>The term is widely used to describe a capacity to engage in one line of thought in one situation (at work, in a certain group, in business, etc.) and another line in another situation (at home, in another group, in private life), without necessarily sensing any conflict between the two.</p>\n</blockquote>\n<p>那么，这就是双重思想的全部了么？只要我面对母亲的时候相信该救母亲、面对女友的时候相信该救女友，就是双重思想了么？</p>\n<h3 id=\"双重思想到底是什么？\"><a href=\"#双重思想到底是什么？\" class=\"headerlink\" title=\"双重思想到底是什么？\"></a>双重思想到底是什么？</h3><p>《1984》中对于双重思想的定义作了进一步的解释：</p>\n<blockquote>\n<p>“知与不知，知道全部真实情况而却扯些滴水不漏的谎话，同时持两种互相抵消的观点，明知它们互相矛盾而仍都相信，用逻辑来反逻辑，一边表示拥护道德一边又否定道德，一边相信民主是办不到的一边又相信党是民主的捍卫者，忘掉一切必须忘掉的东西而又在需要的时候想起它来，然后又马上忘掉它，而尤其是，把这样的做法应用到做法本身上面——这可谓绝妙透顶了：有意识地进入无意识，而后又并不意识到你刚才完成的催眠。即使要了解“双重思想”的含义你也得使用双重思想。”</p>\n</blockquote>\n<p>这段话的核心是：</p>\n<ol>\n<li>相信两种互相矛盾的信念。（并非同时，而是脑袋里具备两种互相矛盾的信念。）</li>\n<li>根据情境来切换自己相信的信念。</li>\n<li>忽略矛盾，认为自己当下的信念就是自己一直以来坚持的信念。</li>\n<li>忽略自己在忽略矛盾，忽略自己在不断地切换信念。（意识不到自己按不同情境做了不同的选择 —— 这是明知两个信念存在矛盾却不认为自己的行为有矛盾的关键。）</li>\n</ol>\n<p>依然用经典例子来帮助理解：如果（1、2）一个人面对母亲的时候相信应该救母亲，面对女友的时候相信应该救女友，并且（3）意识不到自己在面对另一个人时的矛盾信念和行为，而且（4）意识不到自己在使用这种思想方法忽略矛盾。这就是双重思想了。</p>\n<h3 id=\"什么不是双重思想\"><a href=\"#什么不是双重思想\" class=\"headerlink\" title=\"什么不是双重思想\"></a>什么不是双重思想</h3><p>仅有（1）、（2）是不够的，（3）的重要性在于：如果一个人面对母亲的时候说应该救母亲，面对女友的时候说应该救女友，但是意识得到自己在两种情况下的矛盾行为，那么这个人并没有双重思想。因为他没有两种相互矛盾的信念，而是只有一个一致的信念：相信应该哄母亲和女友开心。</p>\n<p>《1984》中老大哥的缔造者们正属于此类，他们意识到只有一个唯一不变的目的，那就是权力本身。为了维护权力，在说谎有利的时候说谎、在使用双重思想有利的时候使用双重思想。这并非双重思想！因为他们一直只有一个一致的信念：追求权力。</p>\n<p>仅有（1）、（2）和（3）还是不够的，（4）的重要性在于：即使一个人有在不同情景下矛盾的信念和行为，但是一旦被提醒就意识到了自己的前后矛盾，并且调整自己的信念来化解该矛盾，那么此人也不是双重思想，而仅仅是可以改正的不一致信念。即，仅有“忽略矛盾”是不够的，必须要有“忽略自己在忽略矛盾”。</p>\n<p>（3）“忽略矛盾”和（4）“忽略自己在忽略矛盾”正是双重思想最关键之处，也是《1984》中党用来培养双重思想的特殊训练的重点。</p>\n<h3 id=\"双重思想是如何造就的\"><a href=\"#双重思想是如何造就的\" class=\"headerlink\" title=\"双重思想是如何造就的\"></a>双重思想是如何造就的</h3><ol>\n<li><p>犯罪停止(crimestop)</p>\n<blockquote>\n<p>“犯罪停止(crimestop)的意思就是指在产生任何危险思想之前出于本能地悬崖勒马的能力。这种能力还包括不能理解类比，不能看到逻辑错误，不能正确了解与党的原则不一致的最简单论点、对于任何可以朝异端方向发展的思路感到厌倦、厌恶。总而言之，犯罪停止(crimestop)意味着起保护作用的愚蠢。”</p>\n</blockquote>\n<p>这是训练一个人针对党的观点“忽略矛盾”的能力。</p>\n<p>其实这种不加思考的、对某些观点本能的反感的能力人类本身就具备，如果一个观点让人觉得不舒服（比如被人当面指出错误），即使它逻辑上正确，人情感上也很难接受。反而是依据客观事实的逻辑思考和分析的能力才是需要辛苦训练才能获得的。</p>\n<p>此“犯罪停止”训练，一方面增强人类本身的这种缺点，另一方面确保这种能力应用在党的观点上。</p>\n</li>\n<li><p>黑白(blackwhite)</p>\n<blockquote>\n<p>“这意味着不顾明显事实硬说黑就是白的无耻习惯。用在党员身上，这意味着在党的纪律要求你说黑就是白时，你就有这样自觉的忠诚。但这也意味着相信黑就是白的能力，甚至是知道黑就是白和忘掉过去曾经有过相反认识的能力。”</p>\n</blockquote>\n<p>这是训练针对党的要求“忽略自己在忽略矛盾”的能力。其实无需特意训练，人类本身就具有这种忽略矛盾并厚着脸皮死不承认的能力……人类实在是是个善于欺骗自己的物种。</p>\n<p>所以，即使不经历书中的训练，人也能形成双重思想。只要不鼓励人们学习进行深入客观的思考、形成自己的意见即可。</p>\n</li>\n</ol>\n<h3 id=\"人的思想可能一致吗\"><a href=\"#人的思想可能一致吗\" class=\"headerlink\" title=\"人的思想可能一致吗\"></a>人的思想可能一致吗</h3><p>每个人大概都有在不同情景下思想不一致的时候，意识到这一点并非特别困难的事情：尝试回想一下若干年之前的自己就很容易能发现，现在的自己对很多事物都有了不一样的看法。</p>\n<p>在某一时刻，人只能按照一个信念行动，但同时脑后可能存在着暂时被抛在脑后的与其矛盾的信念。正如同时面对母亲和女友很多人无法提供一个该救谁的答案：人脑可以把矛盾的想法暂时放在脑后，但无法在同一时刻按照两个矛盾的想法行动。</p>\n<p>思想在不同时刻的不一致是无法被消除的。人的思想总是在变化，有的时候是在前进，即随着对事物的理解更深刻，更加接近真理，有的时候是在倒退，就连科学界对于真理的认识也是反反复复螺旋式前进的。</p>\n<p>思想在同一时刻的不一致可能也是很难被消除的。一个人的思想纷繁复杂，可能总有什么矛盾的想法深埋在脑海深处、不到某一特殊的时刻不会被激发出来并被意识到。</p>\n<p>不过，只要意识到自己可能忽略自己思想中的矛盾，并在发现自己的思想存在矛盾时尽量调整，就打破了双重思想最重要的一环。努力发现自己的不一致，尽量避免被自己的本能或情绪蒙蔽，也接受自己与过去的自己的不一致，尽力朝着真理前进。这大概是我们能做的事了罢。</p>\n<h3 id=\"为什么要避免双重思想\"><a href=\"#为什么要避免双重思想\" class=\"headerlink\" title=\"为什么要避免双重思想\"></a>为什么要避免双重思想</h3><p>双重思想的特点是对不一致的忽略和对事实的扭曲，这意味着双重思想是真理的敌人。而对真理的追求大概是人类从历史上习得的最重要的教训之一了。</p>\n<blockquote>\n<p>“When you are studying any matter, or considering any philosophy, ask yourself only what are the facts and what is the truth that the facts bear out. Never let yourself be diverted either by what you wish to believe, or by what you think would have beneficent social effects if it were believed. But look only, and solely, at what are the facts.” – Bertrand Russell - Message to Future Generations</p>\n</blockquote>\n<h3 id=\"写在文后：双重思想-VS-双重标准\"><a href=\"#写在文后：双重思想-VS-双重标准\" class=\"headerlink\" title=\"写在文后：双重思想 VS. 双重标准\"></a>写在文后：双重思想 VS. 双重标准</h3><p>最初写这篇文章的原因之一是在思考双重标准与双重思想有什么区别。</p>\n<p>双重思想的核心是忽略矛盾和忽略自己忽略了矛盾，双重标准的核心是在我者和他者的立场转换时的不一致的态度（倾向于美化自己丑化他人）。</p>\n<p>双重思想的“不同情景”在范围上涵盖（⊃）双重标准的“我者和他者的立场转换”，双重思想的“矛盾”在范围上也涵盖（⊃）双重标准的“不一致的态度”，但双重思想相比于双重标准额外要求“忽略矛盾”和“忽略自己忽略矛盾”。所以双重思想和双重标准其实有重叠之处。</p>\n<p><img src=\"https://upload.wikimedia.org/wikipedia/commons/d/da/Set_intersection.svg\" alt=\"alt text\"></p>\n<ol>\n<li>A：双重思想</li>\n<li>B：双重标准</li>\n<li>A∩B：“我者和他者的立场转换”+“不一致的态度”+“忽略矛盾”+“忽略自己忽略矛盾”</li>\n<li>A-A∩B：(“不同情景” - “我者和他者的立场转换”)+(“矛盾” - “不一致的态度”)+“忽略矛盾”+“忽略自己忽略矛盾”</li>\n<li>B-A∩B： “我者和他者的立场转换”+“不一致的态度” - “忽略矛盾” - “忽略自己忽略矛盾”</li>\n</ol>\n<p>前面提到的一个人选择救母亲还是女友的双重思想的例子属于（4）双重思想但非双重标准，因为它不涉及“我者和他者的立场转换”。</p>\n<p>如果一个人，在别人选择救母亲的时候谴责他渣男，在别人选择救女友的时候谴责他不孝，自己选的时候却选什么都觉得自己选得好。这是典型的双重标准，因为存在“我者和他者的立场转换”和“不一致的态度”。</p>\n<p>如果这个人意识得到自己在这件事上的观点前后不一致（不满足“忽略矛盾”）、或者被人提醒后认识到自己的观点前后不一致（不满足“忽略自己在忽略矛盾”），那么这个人的行为就仅仅属于（5）双重标准而非双重思想。</p>\n<p>如果他忽略自己对自己和别人观点不一致，并且即使经人提醒也不承认，那么这个人的行为就属于（3）既是双重标准又是双重思想了。</p>\n","site":{"data":{}},"excerpt":"","more":"<blockquote>\n<p>一遍非常详细的解释双重思想定义的文章</p>\n<p>From : <a href=\"https://yonglezh.github.io/2017/01/21/doublethink/\">https://yonglezh.github.io/2017/01/21/doublethink/</a></p>\n</blockquote>\n<p>大概2011年的时候，读了《1984》，对书中提到的“双重思想”很感兴趣，却一直觉得不能彻底理解。</p>\n<p>书中定义双重思想为“一个人的脑子里同时具有两种相互矛盾的信念，而且两种都接受。”最初读到这里我非常迷惑：何谓“同时接受矛盾的信念”？人真的可以做到吗？比如，为什么我觉得无法想象自己同时接受地心说和日心说呢？</p>\n<h3 id=\"澄清定义-“同时接受”带来的误解\"><a href=\"#澄清定义-“同时接受”带来的误解\" class=\"headerlink\" title=\"澄清定义 - “同时接受”带来的误解\"></a>澄清定义 - “同时接受”带来的误解</h3><p>有一个非常经典的例子可以用来帮助理解“同时接受矛盾的信念”：母亲和女友同时掉入水中，且两人都不会游泳，你会救哪一个？</p>\n<p>很多人会选择在面对母亲的时候说救母亲、面对女友的时候说救女友，但同时面对母亲和女友两人的时候就不知该如何回答了。</p>\n<p>所以人是没法同时按照两种相互矛盾的信念来行为的。那么，这是说没有“双重思想”这种东西吗？</p>\n<p>不，书中描述的双重思想还是存在的，但是其定义中的“同时接受”却容易造成误解。“同时”指的是“两种想法同时存在在脑中”，但并非指“两种想法同时表现于意识的表层并指导此人的行为”。</p>\n<p>Wikipedia上对于双重思想的定义在《1984》书中原句的基础上增加了一个描述，跟我的理解很相似：</p>\n<blockquote>\n<p>Doublethink is the act of simultaneously accepting two mutually contradictory beliefs as correct, often <strong>in distinct social contexts</strong>.</p>\n</blockquote>\n<p>这个定义特别指出了双重思想是指“<strong>在不同的情景下</strong>接受矛盾的信念”。</p>\n<p>牛津英语字典（The Oxford Companion to the English Language）中对于双重思想的解释也着重说明了这一点：</p>\n<blockquote>\n<p>The term is widely used to describe a capacity to engage in one line of thought in one situation (at work, in a certain group, in business, etc.) and another line in another situation (at home, in another group, in private life), without necessarily sensing any conflict between the two.</p>\n</blockquote>\n<p>那么，这就是双重思想的全部了么？只要我面对母亲的时候相信该救母亲、面对女友的时候相信该救女友，就是双重思想了么？</p>\n<h3 id=\"双重思想到底是什么？\"><a href=\"#双重思想到底是什么？\" class=\"headerlink\" title=\"双重思想到底是什么？\"></a>双重思想到底是什么？</h3><p>《1984》中对于双重思想的定义作了进一步的解释：</p>\n<blockquote>\n<p>“知与不知，知道全部真实情况而却扯些滴水不漏的谎话，同时持两种互相抵消的观点，明知它们互相矛盾而仍都相信，用逻辑来反逻辑，一边表示拥护道德一边又否定道德，一边相信民主是办不到的一边又相信党是民主的捍卫者，忘掉一切必须忘掉的东西而又在需要的时候想起它来，然后又马上忘掉它，而尤其是，把这样的做法应用到做法本身上面——这可谓绝妙透顶了：有意识地进入无意识，而后又并不意识到你刚才完成的催眠。即使要了解“双重思想”的含义你也得使用双重思想。”</p>\n</blockquote>\n<p>这段话的核心是：</p>\n<ol>\n<li>相信两种互相矛盾的信念。（并非同时，而是脑袋里具备两种互相矛盾的信念。）</li>\n<li>根据情境来切换自己相信的信念。</li>\n<li>忽略矛盾，认为自己当下的信念就是自己一直以来坚持的信念。</li>\n<li>忽略自己在忽略矛盾，忽略自己在不断地切换信念。（意识不到自己按不同情境做了不同的选择 —— 这是明知两个信念存在矛盾却不认为自己的行为有矛盾的关键。）</li>\n</ol>\n<p>依然用经典例子来帮助理解：如果（1、2）一个人面对母亲的时候相信应该救母亲，面对女友的时候相信应该救女友，并且（3）意识不到自己在面对另一个人时的矛盾信念和行为，而且（4）意识不到自己在使用这种思想方法忽略矛盾。这就是双重思想了。</p>\n<h3 id=\"什么不是双重思想\"><a href=\"#什么不是双重思想\" class=\"headerlink\" title=\"什么不是双重思想\"></a>什么不是双重思想</h3><p>仅有（1）、（2）是不够的，（3）的重要性在于：如果一个人面对母亲的时候说应该救母亲，面对女友的时候说应该救女友，但是意识得到自己在两种情况下的矛盾行为，那么这个人并没有双重思想。因为他没有两种相互矛盾的信念，而是只有一个一致的信念：相信应该哄母亲和女友开心。</p>\n<p>《1984》中老大哥的缔造者们正属于此类，他们意识到只有一个唯一不变的目的，那就是权力本身。为了维护权力，在说谎有利的时候说谎、在使用双重思想有利的时候使用双重思想。这并非双重思想！因为他们一直只有一个一致的信念：追求权力。</p>\n<p>仅有（1）、（2）和（3）还是不够的，（4）的重要性在于：即使一个人有在不同情景下矛盾的信念和行为，但是一旦被提醒就意识到了自己的前后矛盾，并且调整自己的信念来化解该矛盾，那么此人也不是双重思想，而仅仅是可以改正的不一致信念。即，仅有“忽略矛盾”是不够的，必须要有“忽略自己在忽略矛盾”。</p>\n<p>（3）“忽略矛盾”和（4）“忽略自己在忽略矛盾”正是双重思想最关键之处，也是《1984》中党用来培养双重思想的特殊训练的重点。</p>\n<h3 id=\"双重思想是如何造就的\"><a href=\"#双重思想是如何造就的\" class=\"headerlink\" title=\"双重思想是如何造就的\"></a>双重思想是如何造就的</h3><ol>\n<li><p>犯罪停止(crimestop)</p>\n<blockquote>\n<p>“犯罪停止(crimestop)的意思就是指在产生任何危险思想之前出于本能地悬崖勒马的能力。这种能力还包括不能理解类比，不能看到逻辑错误，不能正确了解与党的原则不一致的最简单论点、对于任何可以朝异端方向发展的思路感到厌倦、厌恶。总而言之，犯罪停止(crimestop)意味着起保护作用的愚蠢。”</p>\n</blockquote>\n<p>这是训练一个人针对党的观点“忽略矛盾”的能力。</p>\n<p>其实这种不加思考的、对某些观点本能的反感的能力人类本身就具备，如果一个观点让人觉得不舒服（比如被人当面指出错误），即使它逻辑上正确，人情感上也很难接受。反而是依据客观事实的逻辑思考和分析的能力才是需要辛苦训练才能获得的。</p>\n<p>此“犯罪停止”训练，一方面增强人类本身的这种缺点，另一方面确保这种能力应用在党的观点上。</p>\n</li>\n<li><p>黑白(blackwhite)</p>\n<blockquote>\n<p>“这意味着不顾明显事实硬说黑就是白的无耻习惯。用在党员身上，这意味着在党的纪律要求你说黑就是白时，你就有这样自觉的忠诚。但这也意味着相信黑就是白的能力，甚至是知道黑就是白和忘掉过去曾经有过相反认识的能力。”</p>\n</blockquote>\n<p>这是训练针对党的要求“忽略自己在忽略矛盾”的能力。其实无需特意训练，人类本身就具有这种忽略矛盾并厚着脸皮死不承认的能力……人类实在是是个善于欺骗自己的物种。</p>\n<p>所以，即使不经历书中的训练，人也能形成双重思想。只要不鼓励人们学习进行深入客观的思考、形成自己的意见即可。</p>\n</li>\n</ol>\n<h3 id=\"人的思想可能一致吗\"><a href=\"#人的思想可能一致吗\" class=\"headerlink\" title=\"人的思想可能一致吗\"></a>人的思想可能一致吗</h3><p>每个人大概都有在不同情景下思想不一致的时候，意识到这一点并非特别困难的事情：尝试回想一下若干年之前的自己就很容易能发现，现在的自己对很多事物都有了不一样的看法。</p>\n<p>在某一时刻，人只能按照一个信念行动，但同时脑后可能存在着暂时被抛在脑后的与其矛盾的信念。正如同时面对母亲和女友很多人无法提供一个该救谁的答案：人脑可以把矛盾的想法暂时放在脑后，但无法在同一时刻按照两个矛盾的想法行动。</p>\n<p>思想在不同时刻的不一致是无法被消除的。人的思想总是在变化，有的时候是在前进，即随着对事物的理解更深刻，更加接近真理，有的时候是在倒退，就连科学界对于真理的认识也是反反复复螺旋式前进的。</p>\n<p>思想在同一时刻的不一致可能也是很难被消除的。一个人的思想纷繁复杂，可能总有什么矛盾的想法深埋在脑海深处、不到某一特殊的时刻不会被激发出来并被意识到。</p>\n<p>不过，只要意识到自己可能忽略自己思想中的矛盾，并在发现自己的思想存在矛盾时尽量调整，就打破了双重思想最重要的一环。努力发现自己的不一致，尽量避免被自己的本能或情绪蒙蔽，也接受自己与过去的自己的不一致，尽力朝着真理前进。这大概是我们能做的事了罢。</p>\n<h3 id=\"为什么要避免双重思想\"><a href=\"#为什么要避免双重思想\" class=\"headerlink\" title=\"为什么要避免双重思想\"></a>为什么要避免双重思想</h3><p>双重思想的特点是对不一致的忽略和对事实的扭曲，这意味着双重思想是真理的敌人。而对真理的追求大概是人类从历史上习得的最重要的教训之一了。</p>\n<blockquote>\n<p>“When you are studying any matter, or considering any philosophy, ask yourself only what are the facts and what is the truth that the facts bear out. Never let yourself be diverted either by what you wish to believe, or by what you think would have beneficent social effects if it were believed. But look only, and solely, at what are the facts.” – Bertrand Russell - Message to Future Generations</p>\n</blockquote>\n<h3 id=\"写在文后：双重思想-VS-双重标准\"><a href=\"#写在文后：双重思想-VS-双重标准\" class=\"headerlink\" title=\"写在文后：双重思想 VS. 双重标准\"></a>写在文后：双重思想 VS. 双重标准</h3><p>最初写这篇文章的原因之一是在思考双重标准与双重思想有什么区别。</p>\n<p>双重思想的核心是忽略矛盾和忽略自己忽略了矛盾，双重标准的核心是在我者和他者的立场转换时的不一致的态度（倾向于美化自己丑化他人）。</p>\n<p>双重思想的“不同情景”在范围上涵盖（⊃）双重标准的“我者和他者的立场转换”，双重思想的“矛盾”在范围上也涵盖（⊃）双重标准的“不一致的态度”，但双重思想相比于双重标准额外要求“忽略矛盾”和“忽略自己忽略矛盾”。所以双重思想和双重标准其实有重叠之处。</p>\n<p><img src=\"https://upload.wikimedia.org/wikipedia/commons/d/da/Set_intersection.svg\" alt=\"alt text\"></p>\n<ol>\n<li>A：双重思想</li>\n<li>B：双重标准</li>\n<li>A∩B：“我者和他者的立场转换”+“不一致的态度”+“忽略矛盾”+“忽略自己忽略矛盾”</li>\n<li>A-A∩B：(“不同情景” - “我者和他者的立场转换”)+(“矛盾” - “不一致的态度”)+“忽略矛盾”+“忽略自己忽略矛盾”</li>\n<li>B-A∩B： “我者和他者的立场转换”+“不一致的态度” - “忽略矛盾” - “忽略自己忽略矛盾”</li>\n</ol>\n<p>前面提到的一个人选择救母亲还是女友的双重思想的例子属于（4）双重思想但非双重标准，因为它不涉及“我者和他者的立场转换”。</p>\n<p>如果一个人，在别人选择救母亲的时候谴责他渣男，在别人选择救女友的时候谴责他不孝，自己选的时候却选什么都觉得自己选得好。这是典型的双重标准，因为存在“我者和他者的立场转换”和“不一致的态度”。</p>\n<p>如果这个人意识得到自己在这件事上的观点前后不一致（不满足“忽略矛盾”）、或者被人提醒后认识到自己的观点前后不一致（不满足“忽略自己在忽略矛盾”），那么这个人的行为就仅仅属于（5）双重标准而非双重思想。</p>\n<p>如果他忽略自己对自己和别人观点不一致，并且即使经人提醒也不承认，那么这个人的行为就属于（3）既是双重标准又是双重思想了。</p>\n"},{"title":"权力的意志如何影响我的生活","toc":true,"mathjax":true,"top":false,"cover":true,"date":"2021-03-15T10:51:14.000Z","updated":"2021-05-20T07:48:13.153Z","_content":"\n>  Maybe I'm a Reprobate\n\n　　每年开年时刻，都希望自己在新的一年能改头换面，越来愈好，今年是牛年，除了盼望自己愈来愈牛之外，经过去年疫情的洗礼，这次多少还带着一点行动来支持自己的追求，为此，自己紧锣密鼓的做了很多事情，很快就得到了相应的报应。\n\n　　我想，若干年后回首今朝，或许这件事对我懒散的人生来说，是一场重要的转折点，因为发现有些事情，并没有看上去那么轻松，特别是关于家庭伦理这方面的事情，总是暗藏一些难以预见的巨大阻力，阻止我改变自己，尽管我在理论上对这些问题耳熟能详，以为自己能轻松应对，结果却遭遇巨大失败，最后不得不回到最初的状态。\n\n　　简单来说，在自己逐步想要改变家庭内部那种死气沉沉的努力中，与父母间爆发了激烈的冲突，不仅自己情绪失控，完全没能做到理性的应对这些问题，还留下一个烂摊子，根本无力收场。最后禁不住怀疑自己，我这究竟是在做什么？\n\n　　现在回想起来，自己最大的误判就是，以为自己能通过讲道理的方式来纠正一些父辈们的陈旧观念。经此一役，我想自己应该再也不会做这样天真的妄想吧！也许，营造出他们希望看到的那份假象，才是性价比最高的事情。\n\n> 正直诚恳的人，看着别人往错的路上走，出力反对，容易坏了感情。而人精什么言行都不反对，和谁都关系好，别人在错误上栽了，自己拂衣而去，又省力又讨好。Nick Carraway 的父亲是老江湖，教他做后一种人，美其名曰宽容，但父子心照不宣，这是市侩。\n>\n> ​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t了不起的盖茨比书评\n\n　　我本性即使心直口快，追求真实，却在还没认识到这一点的时候，接受了市侩的压迫。此后便开始了漫长的自我怀疑和自我斗争。常常会想，自己是不是在错误的时候和错误的地点来到这个世界。这样分裂的经历直接导致我成为了一个不折不扣的两面派，结果就是两样好处一处都没有占到，一直跌跌撞撞，犹犹豫豫的活到了今天。\n\n　　就在不久前，我还坚信，相信自己更重要，就算失败，起码不留遗憾，为自己活过。然后等来了真实的来自父母的谩骂和嘲讽，我才发现自己是如此的脆弱和不堪一击，并不是因为父母的话语多么朴实和偏僻入理；而是自己就算在如此对立的场合，仍然试图想要无视这些情绪，想要沟通和对话，结果事与愿违的结果加深了自己的挫败感，随之而来的就是那些言语的无法弥合伤害。宏观来看，这些问题是源于父母的认知水平无法匹配与生俱来的权威和尊严，即德不配位，或者称作识不配位更准确一点，随着我们的成长，这个问题愈来愈明显，我本来的设想是通过平等沟通的方式，将这个事实传达出去；然而在对方的认知中，这个一开始的前提条件也是不存在的。这也难怪事情演变成鸡同鸭讲，对牛弹琴的局面。更甚之处是，**父母依靠的财力优势，仍然牢牢把握着这种绝对的权力**，而这是我暂时无法应对了，直到愤怒的父母扬言要回收之前给予的房产时，我才感受到自己的无能和失落。\n\n　　我并非天才，也绝非废物，只是目前为止，并没有认真对待工作，只是用玩乐的态度去享受工作和生活，所以并没有追求收入上的绝对提升，只求轻松和开心，毕竟在学术和编程上，自己也是有点个人追求的，只是这种懒散和无欲无求的心态下，自己也只是得过且过的过着每一天，对新事物很有兴趣，尽管有着三分钟热度的问题，但还算是维持着不断学习的习惯，只是执行力欠缺，很多事情做着做着就无疾而终了。错过了一些还算不错的创业的机会，都是因为没有认真对待的原因，连带着错过毕业的就业机会，自己并没有进入大厂，也就失去了填补光鲜履历的机会，也都是归功于我的无谓的浪费青春。偶尔半夜惊醒，懊恼的悔恨莫及，但也无可奈何。我不是那种，意志坚强不屈的，认准了自己的强项，就会闷头前进的人，尽管我无比渴望如此。总是会受到周遭的变化影响，时而自信满满，时而暗自神伤。想来也是多亏了毕业论文的折磨下，我总是会在力所能及的地方隐藏自己的实力，来获得偷闲的机会，为此沾沾自喜。这种恶习一直保留到现在了吧 ！ 无论如何，兜兜转转的几年之后，最终还是认识到，这样固步自封对自己只是一种更加长期的折磨和伤害，我想，现在的我已经做好了成为一个意志坚强的人的准备了。\n\n　　但是挥霍无度的青春，并不会因此而消失，那些浪费的时间和机会仍然持续对我造成影响，直接的体现就是，我并没有什么积蓄，要么就是为了买心仪的设备和产品，要么就是经常出去旅游。也许，从现在开始存钱，以目前的收入水平，也还可以积攒一些可观的收入；而且随着自己能力的提升，获得一份更好的收入的工作也不是难事。只是这些并不足以抵抗父母的财力优势，我坚信未来的自己能创立属于自己的公司和事业，获得更符合自身能力水平的收入。只是在当下，当父母跟我做出切割，虽然生活水平不会受到太大影响，但是我并无法做出足以让他们反思的同等回应，特别是父亲简单直白的说，你并不如我。试图证明没有他们，我就是一个彻头彻尾的废物。让我承认父母的尊严和权威的时候，我只能选择沉默。\n\n　　尽管我知道，这样一种极端的对话是不理智的，就算来自父母的财力优势再明显，也无法证明我的人生是失败的，特别是在当下这个暗流涌动的混乱年代，我顶多算是一个浪费了年华的可怜人。只是如若是我拥有足以匹配父母的财力，我想父母也不会妄自动用这个伤敌一千，自损八百的杀招吧！再则，起码能有平等沟通的可能性！只是在面对这样一个核武器打击的时候，我感觉自己的自尊受到了极大的伤害，我没想到我试图沟通的举动，居然会演变为绝对权力的竞争，赢的那一个就能贯彻自己的意志。这看起来分明就是一场追逐权力的政治斗争。而我明显准备不足，妄图用道理，独自应战，却忘了，在这片土地上，诞生的从来只有，胜者为王，败者为寇 的原始戏码，就如同霍布斯所描述的利维坦下的自然状态，只有追逐胜利的斗争，没有文明该有的思考。由于赢者通吃，所以这个斗争过程就显得那么原始，直接，且无所不用其极。\n\n　　我从来没有相信过父辈那一套虚伪的儒家理论，不是因为其陈旧而腐朽，只是因为其对人性的无端压抑，至于如何个局限法，以目前我的水平，只能做到意会，却难以言传的地步，主要原因还是学识不深，无法把所思所想的核心都融会贯通起来，以为能理解王小波杂文之中的吐槽国学和儒家的妙处，就能走南闯北，天不怕地不怕。殊不知，临到关系到自己利害关系的时刻，也免不了瞻前顾后起来。更何况还要考虑到未来的家庭关系，我并没有办法做到无所畏惧，毕竟就算自己豁出去，无论最终是自己证明自己是对了，还是最终承认自己是个彻头彻尾的废物，对整个家庭而言，都难言有什么好处。\n\n　　此事虽然到此为止，但是未来的我该何去何从，这是我最应该关心的事情。\n\n　　这件事除了对我的自尊造成了长期的影响之外，还让我有那么一会儿开始怀疑起自己所谓的自信和坚持的根基，特别是当我发现自己没办法在行为上践行自我的时候，也就是发现自己没办法豁出去的时候；但转念一想，这种固执的坚持知行合一的想法有点幼稚，说起来虽然有点市侩，但是自己的行为虽然是践行自己的意志，但是如果是在无甚多收效的地方浪费太多执行力的话，自己还能一如既往的坚持自我吗？我深表怀疑。虽然无需做到哥白尼那种左右逢源，但是在关键点上默默为自己的坚持积攒实力还是完全做得到的。剩下的就交给时代本身来解决了。\n\n　　为什么是时代呢？重温王小波的《知识分子的不幸》，我才终于意识到我心中的那份不甘的来源：\n\n> 什么是知识分子最害怕的事？\n>\n> 最怕活在不理智的时代\n>\n> 所谓不理智的时代，就是伽利略低头认罪，承认地球不转的年代；也是拉瓦锡上断头台的年代；是茨威格服毒自杀的年代，也是老舍跳进太平湖的年代。\n>\n> 知识分子只是会以理服人，假如不讲理，他就没有长处，只有短处，活着没意思，不如死掉\n\n　　至于为什么会不理智？只因为信仰而已，因为狂信，人就不想讲理。这个世界上并不存在可以狂信而无丧失理智危险的信仰。更甚之处在于，\n\n> 道理这个东西有一定复杂性，不一定是君王所能理解的。再说，假如能和他讲理，他就不是君王。君王总是对的，臣民总是不对。君王的品性不可更改，臣民就得适应这种现实\n\n　　我如梦初醒，明白自己败在何处了！\n\n　　原来答案我早就曾经看过了，只是被我理解之后就被抛诸脑后给忘记了。无论是思考不够深刻，还是临到发表意见时被环境左右导致无法自如的表达自我，总之，前方的路还有很多困难需要克服，希望未来自己仍然能不忘初心，坚定自己的道路，切不可因为一时的得失而裹足不前，应该着眼于更加长远的理想。\n\n　　关于未来，我应该做些什么，才能避免自己再次陷入被动呢？\n\n　　关于这一点，将是下一篇文章的主要内容。","source":"_posts/权力的意志如何影响我的生活.md","raw":"---\ntitle: 权力的意志如何影响我的生活\ntoc: true\nmathjax: true\ntop: false\ncover: true\ndate: 2021-03-15 18:51:14\nupdated:\ncategories: 随笔\ntags:\n\t- 随笔\n\t- 反思\t\n\t- 哲学\n\t- 王小波\n---\n\n>  Maybe I'm a Reprobate\n\n　　每年开年时刻，都希望自己在新的一年能改头换面，越来愈好，今年是牛年，除了盼望自己愈来愈牛之外，经过去年疫情的洗礼，这次多少还带着一点行动来支持自己的追求，为此，自己紧锣密鼓的做了很多事情，很快就得到了相应的报应。\n\n　　我想，若干年后回首今朝，或许这件事对我懒散的人生来说，是一场重要的转折点，因为发现有些事情，并没有看上去那么轻松，特别是关于家庭伦理这方面的事情，总是暗藏一些难以预见的巨大阻力，阻止我改变自己，尽管我在理论上对这些问题耳熟能详，以为自己能轻松应对，结果却遭遇巨大失败，最后不得不回到最初的状态。\n\n　　简单来说，在自己逐步想要改变家庭内部那种死气沉沉的努力中，与父母间爆发了激烈的冲突，不仅自己情绪失控，完全没能做到理性的应对这些问题，还留下一个烂摊子，根本无力收场。最后禁不住怀疑自己，我这究竟是在做什么？\n\n　　现在回想起来，自己最大的误判就是，以为自己能通过讲道理的方式来纠正一些父辈们的陈旧观念。经此一役，我想自己应该再也不会做这样天真的妄想吧！也许，营造出他们希望看到的那份假象，才是性价比最高的事情。\n\n> 正直诚恳的人，看着别人往错的路上走，出力反对，容易坏了感情。而人精什么言行都不反对，和谁都关系好，别人在错误上栽了，自己拂衣而去，又省力又讨好。Nick Carraway 的父亲是老江湖，教他做后一种人，美其名曰宽容，但父子心照不宣，这是市侩。\n>\n> ​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t了不起的盖茨比书评\n\n　　我本性即使心直口快，追求真实，却在还没认识到这一点的时候，接受了市侩的压迫。此后便开始了漫长的自我怀疑和自我斗争。常常会想，自己是不是在错误的时候和错误的地点来到这个世界。这样分裂的经历直接导致我成为了一个不折不扣的两面派，结果就是两样好处一处都没有占到，一直跌跌撞撞，犹犹豫豫的活到了今天。\n\n　　就在不久前，我还坚信，相信自己更重要，就算失败，起码不留遗憾，为自己活过。然后等来了真实的来自父母的谩骂和嘲讽，我才发现自己是如此的脆弱和不堪一击，并不是因为父母的话语多么朴实和偏僻入理；而是自己就算在如此对立的场合，仍然试图想要无视这些情绪，想要沟通和对话，结果事与愿违的结果加深了自己的挫败感，随之而来的就是那些言语的无法弥合伤害。宏观来看，这些问题是源于父母的认知水平无法匹配与生俱来的权威和尊严，即德不配位，或者称作识不配位更准确一点，随着我们的成长，这个问题愈来愈明显，我本来的设想是通过平等沟通的方式，将这个事实传达出去；然而在对方的认知中，这个一开始的前提条件也是不存在的。这也难怪事情演变成鸡同鸭讲，对牛弹琴的局面。更甚之处是，**父母依靠的财力优势，仍然牢牢把握着这种绝对的权力**，而这是我暂时无法应对了，直到愤怒的父母扬言要回收之前给予的房产时，我才感受到自己的无能和失落。\n\n　　我并非天才，也绝非废物，只是目前为止，并没有认真对待工作，只是用玩乐的态度去享受工作和生活，所以并没有追求收入上的绝对提升，只求轻松和开心，毕竟在学术和编程上，自己也是有点个人追求的，只是这种懒散和无欲无求的心态下，自己也只是得过且过的过着每一天，对新事物很有兴趣，尽管有着三分钟热度的问题，但还算是维持着不断学习的习惯，只是执行力欠缺，很多事情做着做着就无疾而终了。错过了一些还算不错的创业的机会，都是因为没有认真对待的原因，连带着错过毕业的就业机会，自己并没有进入大厂，也就失去了填补光鲜履历的机会，也都是归功于我的无谓的浪费青春。偶尔半夜惊醒，懊恼的悔恨莫及，但也无可奈何。我不是那种，意志坚强不屈的，认准了自己的强项，就会闷头前进的人，尽管我无比渴望如此。总是会受到周遭的变化影响，时而自信满满，时而暗自神伤。想来也是多亏了毕业论文的折磨下，我总是会在力所能及的地方隐藏自己的实力，来获得偷闲的机会，为此沾沾自喜。这种恶习一直保留到现在了吧 ！ 无论如何，兜兜转转的几年之后，最终还是认识到，这样固步自封对自己只是一种更加长期的折磨和伤害，我想，现在的我已经做好了成为一个意志坚强的人的准备了。\n\n　　但是挥霍无度的青春，并不会因此而消失，那些浪费的时间和机会仍然持续对我造成影响，直接的体现就是，我并没有什么积蓄，要么就是为了买心仪的设备和产品，要么就是经常出去旅游。也许，从现在开始存钱，以目前的收入水平，也还可以积攒一些可观的收入；而且随着自己能力的提升，获得一份更好的收入的工作也不是难事。只是这些并不足以抵抗父母的财力优势，我坚信未来的自己能创立属于自己的公司和事业，获得更符合自身能力水平的收入。只是在当下，当父母跟我做出切割，虽然生活水平不会受到太大影响，但是我并无法做出足以让他们反思的同等回应，特别是父亲简单直白的说，你并不如我。试图证明没有他们，我就是一个彻头彻尾的废物。让我承认父母的尊严和权威的时候，我只能选择沉默。\n\n　　尽管我知道，这样一种极端的对话是不理智的，就算来自父母的财力优势再明显，也无法证明我的人生是失败的，特别是在当下这个暗流涌动的混乱年代，我顶多算是一个浪费了年华的可怜人。只是如若是我拥有足以匹配父母的财力，我想父母也不会妄自动用这个伤敌一千，自损八百的杀招吧！再则，起码能有平等沟通的可能性！只是在面对这样一个核武器打击的时候，我感觉自己的自尊受到了极大的伤害，我没想到我试图沟通的举动，居然会演变为绝对权力的竞争，赢的那一个就能贯彻自己的意志。这看起来分明就是一场追逐权力的政治斗争。而我明显准备不足，妄图用道理，独自应战，却忘了，在这片土地上，诞生的从来只有，胜者为王，败者为寇 的原始戏码，就如同霍布斯所描述的利维坦下的自然状态，只有追逐胜利的斗争，没有文明该有的思考。由于赢者通吃，所以这个斗争过程就显得那么原始，直接，且无所不用其极。\n\n　　我从来没有相信过父辈那一套虚伪的儒家理论，不是因为其陈旧而腐朽，只是因为其对人性的无端压抑，至于如何个局限法，以目前我的水平，只能做到意会，却难以言传的地步，主要原因还是学识不深，无法把所思所想的核心都融会贯通起来，以为能理解王小波杂文之中的吐槽国学和儒家的妙处，就能走南闯北，天不怕地不怕。殊不知，临到关系到自己利害关系的时刻，也免不了瞻前顾后起来。更何况还要考虑到未来的家庭关系，我并没有办法做到无所畏惧，毕竟就算自己豁出去，无论最终是自己证明自己是对了，还是最终承认自己是个彻头彻尾的废物，对整个家庭而言，都难言有什么好处。\n\n　　此事虽然到此为止，但是未来的我该何去何从，这是我最应该关心的事情。\n\n　　这件事除了对我的自尊造成了长期的影响之外，还让我有那么一会儿开始怀疑起自己所谓的自信和坚持的根基，特别是当我发现自己没办法在行为上践行自我的时候，也就是发现自己没办法豁出去的时候；但转念一想，这种固执的坚持知行合一的想法有点幼稚，说起来虽然有点市侩，但是自己的行为虽然是践行自己的意志，但是如果是在无甚多收效的地方浪费太多执行力的话，自己还能一如既往的坚持自我吗？我深表怀疑。虽然无需做到哥白尼那种左右逢源，但是在关键点上默默为自己的坚持积攒实力还是完全做得到的。剩下的就交给时代本身来解决了。\n\n　　为什么是时代呢？重温王小波的《知识分子的不幸》，我才终于意识到我心中的那份不甘的来源：\n\n> 什么是知识分子最害怕的事？\n>\n> 最怕活在不理智的时代\n>\n> 所谓不理智的时代，就是伽利略低头认罪，承认地球不转的年代；也是拉瓦锡上断头台的年代；是茨威格服毒自杀的年代，也是老舍跳进太平湖的年代。\n>\n> 知识分子只是会以理服人，假如不讲理，他就没有长处，只有短处，活着没意思，不如死掉\n\n　　至于为什么会不理智？只因为信仰而已，因为狂信，人就不想讲理。这个世界上并不存在可以狂信而无丧失理智危险的信仰。更甚之处在于，\n\n> 道理这个东西有一定复杂性，不一定是君王所能理解的。再说，假如能和他讲理，他就不是君王。君王总是对的，臣民总是不对。君王的品性不可更改，臣民就得适应这种现实\n\n　　我如梦初醒，明白自己败在何处了！\n\n　　原来答案我早就曾经看过了，只是被我理解之后就被抛诸脑后给忘记了。无论是思考不够深刻，还是临到发表意见时被环境左右导致无法自如的表达自我，总之，前方的路还有很多困难需要克服，希望未来自己仍然能不忘初心，坚定自己的道路，切不可因为一时的得失而裹足不前，应该着眼于更加长远的理想。\n\n　　关于未来，我应该做些什么，才能避免自己再次陷入被动呢？\n\n　　关于这一点，将是下一篇文章的主要内容。","slug":"权力的意志如何影响我的生活","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"ckowlff4x000nye2839rj25e2","content":"<blockquote>\n<p> Maybe I’m a Reprobate</p>\n</blockquote>\n<p>　　每年开年时刻，都希望自己在新的一年能改头换面，越来愈好，今年是牛年，除了盼望自己愈来愈牛之外，经过去年疫情的洗礼，这次多少还带着一点行动来支持自己的追求，为此，自己紧锣密鼓的做了很多事情，很快就得到了相应的报应。</p>\n<p>　　我想，若干年后回首今朝，或许这件事对我懒散的人生来说，是一场重要的转折点，因为发现有些事情，并没有看上去那么轻松，特别是关于家庭伦理这方面的事情，总是暗藏一些难以预见的巨大阻力，阻止我改变自己，尽管我在理论上对这些问题耳熟能详，以为自己能轻松应对，结果却遭遇巨大失败，最后不得不回到最初的状态。</p>\n<p>　　简单来说，在自己逐步想要改变家庭内部那种死气沉沉的努力中，与父母间爆发了激烈的冲突，不仅自己情绪失控，完全没能做到理性的应对这些问题，还留下一个烂摊子，根本无力收场。最后禁不住怀疑自己，我这究竟是在做什么？</p>\n<p>　　现在回想起来，自己最大的误判就是，以为自己能通过讲道理的方式来纠正一些父辈们的陈旧观念。经此一役，我想自己应该再也不会做这样天真的妄想吧！也许，营造出他们希望看到的那份假象，才是性价比最高的事情。</p>\n<blockquote>\n<p>正直诚恳的人，看着别人往错的路上走，出力反对，容易坏了感情。而人精什么言行都不反对，和谁都关系好，别人在错误上栽了，自己拂衣而去，又省力又讨好。Nick Carraway 的父亲是老江湖，教他做后一种人，美其名曰宽容，但父子心照不宣，这是市侩。</p>\n<p>​                                                                            了不起的盖茨比书评</p>\n</blockquote>\n<p>　　我本性即使心直口快，追求真实，却在还没认识到这一点的时候，接受了市侩的压迫。此后便开始了漫长的自我怀疑和自我斗争。常常会想，自己是不是在错误的时候和错误的地点来到这个世界。这样分裂的经历直接导致我成为了一个不折不扣的两面派，结果就是两样好处一处都没有占到，一直跌跌撞撞，犹犹豫豫的活到了今天。</p>\n<p>　　就在不久前，我还坚信，相信自己更重要，就算失败，起码不留遗憾，为自己活过。然后等来了真实的来自父母的谩骂和嘲讽，我才发现自己是如此的脆弱和不堪一击，并不是因为父母的话语多么朴实和偏僻入理；而是自己就算在如此对立的场合，仍然试图想要无视这些情绪，想要沟通和对话，结果事与愿违的结果加深了自己的挫败感，随之而来的就是那些言语的无法弥合伤害。宏观来看，这些问题是源于父母的认知水平无法匹配与生俱来的权威和尊严，即德不配位，或者称作识不配位更准确一点，随着我们的成长，这个问题愈来愈明显，我本来的设想是通过平等沟通的方式，将这个事实传达出去；然而在对方的认知中，这个一开始的前提条件也是不存在的。这也难怪事情演变成鸡同鸭讲，对牛弹琴的局面。更甚之处是，<strong>父母依靠的财力优势，仍然牢牢把握着这种绝对的权力</strong>，而这是我暂时无法应对了，直到愤怒的父母扬言要回收之前给予的房产时，我才感受到自己的无能和失落。</p>\n<p>　　我并非天才，也绝非废物，只是目前为止，并没有认真对待工作，只是用玩乐的态度去享受工作和生活，所以并没有追求收入上的绝对提升，只求轻松和开心，毕竟在学术和编程上，自己也是有点个人追求的，只是这种懒散和无欲无求的心态下，自己也只是得过且过的过着每一天，对新事物很有兴趣，尽管有着三分钟热度的问题，但还算是维持着不断学习的习惯，只是执行力欠缺，很多事情做着做着就无疾而终了。错过了一些还算不错的创业的机会，都是因为没有认真对待的原因，连带着错过毕业的就业机会，自己并没有进入大厂，也就失去了填补光鲜履历的机会，也都是归功于我的无谓的浪费青春。偶尔半夜惊醒，懊恼的悔恨莫及，但也无可奈何。我不是那种，意志坚强不屈的，认准了自己的强项，就会闷头前进的人，尽管我无比渴望如此。总是会受到周遭的变化影响，时而自信满满，时而暗自神伤。想来也是多亏了毕业论文的折磨下，我总是会在力所能及的地方隐藏自己的实力，来获得偷闲的机会，为此沾沾自喜。这种恶习一直保留到现在了吧 ！ 无论如何，兜兜转转的几年之后，最终还是认识到，这样固步自封对自己只是一种更加长期的折磨和伤害，我想，现在的我已经做好了成为一个意志坚强的人的准备了。</p>\n<p>　　但是挥霍无度的青春，并不会因此而消失，那些浪费的时间和机会仍然持续对我造成影响，直接的体现就是，我并没有什么积蓄，要么就是为了买心仪的设备和产品，要么就是经常出去旅游。也许，从现在开始存钱，以目前的收入水平，也还可以积攒一些可观的收入；而且随着自己能力的提升，获得一份更好的收入的工作也不是难事。只是这些并不足以抵抗父母的财力优势，我坚信未来的自己能创立属于自己的公司和事业，获得更符合自身能力水平的收入。只是在当下，当父母跟我做出切割，虽然生活水平不会受到太大影响，但是我并无法做出足以让他们反思的同等回应，特别是父亲简单直白的说，你并不如我。试图证明没有他们，我就是一个彻头彻尾的废物。让我承认父母的尊严和权威的时候，我只能选择沉默。</p>\n<p>　　尽管我知道，这样一种极端的对话是不理智的，就算来自父母的财力优势再明显，也无法证明我的人生是失败的，特别是在当下这个暗流涌动的混乱年代，我顶多算是一个浪费了年华的可怜人。只是如若是我拥有足以匹配父母的财力，我想父母也不会妄自动用这个伤敌一千，自损八百的杀招吧！再则，起码能有平等沟通的可能性！只是在面对这样一个核武器打击的时候，我感觉自己的自尊受到了极大的伤害，我没想到我试图沟通的举动，居然会演变为绝对权力的竞争，赢的那一个就能贯彻自己的意志。这看起来分明就是一场追逐权力的政治斗争。而我明显准备不足，妄图用道理，独自应战，却忘了，在这片土地上，诞生的从来只有，胜者为王，败者为寇 的原始戏码，就如同霍布斯所描述的利维坦下的自然状态，只有追逐胜利的斗争，没有文明该有的思考。由于赢者通吃，所以这个斗争过程就显得那么原始，直接，且无所不用其极。</p>\n<p>　　我从来没有相信过父辈那一套虚伪的儒家理论，不是因为其陈旧而腐朽，只是因为其对人性的无端压抑，至于如何个局限法，以目前我的水平，只能做到意会，却难以言传的地步，主要原因还是学识不深，无法把所思所想的核心都融会贯通起来，以为能理解王小波杂文之中的吐槽国学和儒家的妙处，就能走南闯北，天不怕地不怕。殊不知，临到关系到自己利害关系的时刻，也免不了瞻前顾后起来。更何况还要考虑到未来的家庭关系，我并没有办法做到无所畏惧，毕竟就算自己豁出去，无论最终是自己证明自己是对了，还是最终承认自己是个彻头彻尾的废物，对整个家庭而言，都难言有什么好处。</p>\n<p>　　此事虽然到此为止，但是未来的我该何去何从，这是我最应该关心的事情。</p>\n<p>　　这件事除了对我的自尊造成了长期的影响之外，还让我有那么一会儿开始怀疑起自己所谓的自信和坚持的根基，特别是当我发现自己没办法在行为上践行自我的时候，也就是发现自己没办法豁出去的时候；但转念一想，这种固执的坚持知行合一的想法有点幼稚，说起来虽然有点市侩，但是自己的行为虽然是践行自己的意志，但是如果是在无甚多收效的地方浪费太多执行力的话，自己还能一如既往的坚持自我吗？我深表怀疑。虽然无需做到哥白尼那种左右逢源，但是在关键点上默默为自己的坚持积攒实力还是完全做得到的。剩下的就交给时代本身来解决了。</p>\n<p>　　为什么是时代呢？重温王小波的《知识分子的不幸》，我才终于意识到我心中的那份不甘的来源：</p>\n<blockquote>\n<p>什么是知识分子最害怕的事？</p>\n<p>最怕活在不理智的时代</p>\n<p>所谓不理智的时代，就是伽利略低头认罪，承认地球不转的年代；也是拉瓦锡上断头台的年代；是茨威格服毒自杀的年代，也是老舍跳进太平湖的年代。</p>\n<p>知识分子只是会以理服人，假如不讲理，他就没有长处，只有短处，活着没意思，不如死掉</p>\n</blockquote>\n<p>　　至于为什么会不理智？只因为信仰而已，因为狂信，人就不想讲理。这个世界上并不存在可以狂信而无丧失理智危险的信仰。更甚之处在于，</p>\n<blockquote>\n<p>道理这个东西有一定复杂性，不一定是君王所能理解的。再说，假如能和他讲理，他就不是君王。君王总是对的，臣民总是不对。君王的品性不可更改，臣民就得适应这种现实</p>\n</blockquote>\n<p>　　我如梦初醒，明白自己败在何处了！</p>\n<p>　　原来答案我早就曾经看过了，只是被我理解之后就被抛诸脑后给忘记了。无论是思考不够深刻，还是临到发表意见时被环境左右导致无法自如的表达自我，总之，前方的路还有很多困难需要克服，希望未来自己仍然能不忘初心，坚定自己的道路，切不可因为一时的得失而裹足不前，应该着眼于更加长远的理想。</p>\n<p>　　关于未来，我应该做些什么，才能避免自己再次陷入被动呢？</p>\n<p>　　关于这一点，将是下一篇文章的主要内容。</p>\n","site":{"data":{}},"excerpt":"","more":"<blockquote>\n<p> Maybe I’m a Reprobate</p>\n</blockquote>\n<p>　　每年开年时刻，都希望自己在新的一年能改头换面，越来愈好，今年是牛年，除了盼望自己愈来愈牛之外，经过去年疫情的洗礼，这次多少还带着一点行动来支持自己的追求，为此，自己紧锣密鼓的做了很多事情，很快就得到了相应的报应。</p>\n<p>　　我想，若干年后回首今朝，或许这件事对我懒散的人生来说，是一场重要的转折点，因为发现有些事情，并没有看上去那么轻松，特别是关于家庭伦理这方面的事情，总是暗藏一些难以预见的巨大阻力，阻止我改变自己，尽管我在理论上对这些问题耳熟能详，以为自己能轻松应对，结果却遭遇巨大失败，最后不得不回到最初的状态。</p>\n<p>　　简单来说，在自己逐步想要改变家庭内部那种死气沉沉的努力中，与父母间爆发了激烈的冲突，不仅自己情绪失控，完全没能做到理性的应对这些问题，还留下一个烂摊子，根本无力收场。最后禁不住怀疑自己，我这究竟是在做什么？</p>\n<p>　　现在回想起来，自己最大的误判就是，以为自己能通过讲道理的方式来纠正一些父辈们的陈旧观念。经此一役，我想自己应该再也不会做这样天真的妄想吧！也许，营造出他们希望看到的那份假象，才是性价比最高的事情。</p>\n<blockquote>\n<p>正直诚恳的人，看着别人往错的路上走，出力反对，容易坏了感情。而人精什么言行都不反对，和谁都关系好，别人在错误上栽了，自己拂衣而去，又省力又讨好。Nick Carraway 的父亲是老江湖，教他做后一种人，美其名曰宽容，但父子心照不宣，这是市侩。</p>\n<p>​                                                                            了不起的盖茨比书评</p>\n</blockquote>\n<p>　　我本性即使心直口快，追求真实，却在还没认识到这一点的时候，接受了市侩的压迫。此后便开始了漫长的自我怀疑和自我斗争。常常会想，自己是不是在错误的时候和错误的地点来到这个世界。这样分裂的经历直接导致我成为了一个不折不扣的两面派，结果就是两样好处一处都没有占到，一直跌跌撞撞，犹犹豫豫的活到了今天。</p>\n<p>　　就在不久前，我还坚信，相信自己更重要，就算失败，起码不留遗憾，为自己活过。然后等来了真实的来自父母的谩骂和嘲讽，我才发现自己是如此的脆弱和不堪一击，并不是因为父母的话语多么朴实和偏僻入理；而是自己就算在如此对立的场合，仍然试图想要无视这些情绪，想要沟通和对话，结果事与愿违的结果加深了自己的挫败感，随之而来的就是那些言语的无法弥合伤害。宏观来看，这些问题是源于父母的认知水平无法匹配与生俱来的权威和尊严，即德不配位，或者称作识不配位更准确一点，随着我们的成长，这个问题愈来愈明显，我本来的设想是通过平等沟通的方式，将这个事实传达出去；然而在对方的认知中，这个一开始的前提条件也是不存在的。这也难怪事情演变成鸡同鸭讲，对牛弹琴的局面。更甚之处是，<strong>父母依靠的财力优势，仍然牢牢把握着这种绝对的权力</strong>，而这是我暂时无法应对了，直到愤怒的父母扬言要回收之前给予的房产时，我才感受到自己的无能和失落。</p>\n<p>　　我并非天才，也绝非废物，只是目前为止，并没有认真对待工作，只是用玩乐的态度去享受工作和生活，所以并没有追求收入上的绝对提升，只求轻松和开心，毕竟在学术和编程上，自己也是有点个人追求的，只是这种懒散和无欲无求的心态下，自己也只是得过且过的过着每一天，对新事物很有兴趣，尽管有着三分钟热度的问题，但还算是维持着不断学习的习惯，只是执行力欠缺，很多事情做着做着就无疾而终了。错过了一些还算不错的创业的机会，都是因为没有认真对待的原因，连带着错过毕业的就业机会，自己并没有进入大厂，也就失去了填补光鲜履历的机会，也都是归功于我的无谓的浪费青春。偶尔半夜惊醒，懊恼的悔恨莫及，但也无可奈何。我不是那种，意志坚强不屈的，认准了自己的强项，就会闷头前进的人，尽管我无比渴望如此。总是会受到周遭的变化影响，时而自信满满，时而暗自神伤。想来也是多亏了毕业论文的折磨下，我总是会在力所能及的地方隐藏自己的实力，来获得偷闲的机会，为此沾沾自喜。这种恶习一直保留到现在了吧 ！ 无论如何，兜兜转转的几年之后，最终还是认识到，这样固步自封对自己只是一种更加长期的折磨和伤害，我想，现在的我已经做好了成为一个意志坚强的人的准备了。</p>\n<p>　　但是挥霍无度的青春，并不会因此而消失，那些浪费的时间和机会仍然持续对我造成影响，直接的体现就是，我并没有什么积蓄，要么就是为了买心仪的设备和产品，要么就是经常出去旅游。也许，从现在开始存钱，以目前的收入水平，也还可以积攒一些可观的收入；而且随着自己能力的提升，获得一份更好的收入的工作也不是难事。只是这些并不足以抵抗父母的财力优势，我坚信未来的自己能创立属于自己的公司和事业，获得更符合自身能力水平的收入。只是在当下，当父母跟我做出切割，虽然生活水平不会受到太大影响，但是我并无法做出足以让他们反思的同等回应，特别是父亲简单直白的说，你并不如我。试图证明没有他们，我就是一个彻头彻尾的废物。让我承认父母的尊严和权威的时候，我只能选择沉默。</p>\n<p>　　尽管我知道，这样一种极端的对话是不理智的，就算来自父母的财力优势再明显，也无法证明我的人生是失败的，特别是在当下这个暗流涌动的混乱年代，我顶多算是一个浪费了年华的可怜人。只是如若是我拥有足以匹配父母的财力，我想父母也不会妄自动用这个伤敌一千，自损八百的杀招吧！再则，起码能有平等沟通的可能性！只是在面对这样一个核武器打击的时候，我感觉自己的自尊受到了极大的伤害，我没想到我试图沟通的举动，居然会演变为绝对权力的竞争，赢的那一个就能贯彻自己的意志。这看起来分明就是一场追逐权力的政治斗争。而我明显准备不足，妄图用道理，独自应战，却忘了，在这片土地上，诞生的从来只有，胜者为王，败者为寇 的原始戏码，就如同霍布斯所描述的利维坦下的自然状态，只有追逐胜利的斗争，没有文明该有的思考。由于赢者通吃，所以这个斗争过程就显得那么原始，直接，且无所不用其极。</p>\n<p>　　我从来没有相信过父辈那一套虚伪的儒家理论，不是因为其陈旧而腐朽，只是因为其对人性的无端压抑，至于如何个局限法，以目前我的水平，只能做到意会，却难以言传的地步，主要原因还是学识不深，无法把所思所想的核心都融会贯通起来，以为能理解王小波杂文之中的吐槽国学和儒家的妙处，就能走南闯北，天不怕地不怕。殊不知，临到关系到自己利害关系的时刻，也免不了瞻前顾后起来。更何况还要考虑到未来的家庭关系，我并没有办法做到无所畏惧，毕竟就算自己豁出去，无论最终是自己证明自己是对了，还是最终承认自己是个彻头彻尾的废物，对整个家庭而言，都难言有什么好处。</p>\n<p>　　此事虽然到此为止，但是未来的我该何去何从，这是我最应该关心的事情。</p>\n<p>　　这件事除了对我的自尊造成了长期的影响之外，还让我有那么一会儿开始怀疑起自己所谓的自信和坚持的根基，特别是当我发现自己没办法在行为上践行自我的时候，也就是发现自己没办法豁出去的时候；但转念一想，这种固执的坚持知行合一的想法有点幼稚，说起来虽然有点市侩，但是自己的行为虽然是践行自己的意志，但是如果是在无甚多收效的地方浪费太多执行力的话，自己还能一如既往的坚持自我吗？我深表怀疑。虽然无需做到哥白尼那种左右逢源，但是在关键点上默默为自己的坚持积攒实力还是完全做得到的。剩下的就交给时代本身来解决了。</p>\n<p>　　为什么是时代呢？重温王小波的《知识分子的不幸》，我才终于意识到我心中的那份不甘的来源：</p>\n<blockquote>\n<p>什么是知识分子最害怕的事？</p>\n<p>最怕活在不理智的时代</p>\n<p>所谓不理智的时代，就是伽利略低头认罪，承认地球不转的年代；也是拉瓦锡上断头台的年代；是茨威格服毒自杀的年代，也是老舍跳进太平湖的年代。</p>\n<p>知识分子只是会以理服人，假如不讲理，他就没有长处，只有短处，活着没意思，不如死掉</p>\n</blockquote>\n<p>　　至于为什么会不理智？只因为信仰而已，因为狂信，人就不想讲理。这个世界上并不存在可以狂信而无丧失理智危险的信仰。更甚之处在于，</p>\n<blockquote>\n<p>道理这个东西有一定复杂性，不一定是君王所能理解的。再说，假如能和他讲理，他就不是君王。君王总是对的，臣民总是不对。君王的品性不可更改，臣民就得适应这种现实</p>\n</blockquote>\n<p>　　我如梦初醒，明白自己败在何处了！</p>\n<p>　　原来答案我早就曾经看过了，只是被我理解之后就被抛诸脑后给忘记了。无论是思考不够深刻，还是临到发表意见时被环境左右导致无法自如的表达自我，总之，前方的路还有很多困难需要克服，希望未来自己仍然能不忘初心，坚定自己的道路，切不可因为一时的得失而裹足不前，应该着眼于更加长远的理想。</p>\n<p>　　关于未来，我应该做些什么，才能避免自己再次陷入被动呢？</p>\n<p>　　关于这一点，将是下一篇文章的主要内容。</p>\n"},{"title":"跳出刷题的自我怀疑(转载)","toc":true,"mathjax":true,"top":false,"cover":true,"date":"2021-03-10T03:08:08.000Z","updated":"2021-03-10T03:10:56.692Z","_content":"\n> 刷题中总感觉自己是个废物，终于在这篇文章中找到原因\n>\n> From https://zhuanlan.zhihu.com/p/351560331\n\n刚接触 Leetcode 的时候，我经常边刷题边陷入自我怀疑，通常有几个原因：1）想不到最优解：一些简单题目的最优解，我觉得自己不可能想出来，也不太能理解。2）看不懂解法：论坛中被赞最多的解法往往为了追求代码的简短性而忽略可读性，在刷题初期要理解解法都需要耗费大量时间。3）差距太大：网上有不少[竞赛直播的视频](https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3D4ALB5m_Idkk)，他们在 20 分钟之内就能解答四道题目，对比之下，实在自愧弗如。\n\n这几个原因导致的自我怀疑不仅打压了我刷题的热情，耗费了大量时间，也影响了我对自己真实算法水平的判断。如今刷过一些题之后，我开始了解到一些更深层的原因，希望在此能帮助到刷题中迷茫的各位：\n\n## 简单题不简单\n\n我推测 Leetcode 的题目难度并不是根据最优解的难度来设定的，也就是说一道标记为简单的题目它的次优解可能非常直观，但是最优解却很难想出来。这一点对于当初是刷题新手的我尤其致命，因为在被最优解的美妙所震撼之后，我强迫自己理解和证明它，但是如果在一定时间内还没有成功，我就会陷入深深的自我怀疑，举几道题目为例：\n\n[最大子序和](https://link.zhihu.com/?target=https%3A//leetcode-cn.com/problems/maximum-subarray/)（简单）\n\n[官方解答](https://link.zhihu.com/?target=https%3A//leetcode-cn.com/problems/maximum-subarray/solution/zui-da-zi-xu-he-by-leetcode-solution/)轻描淡写地给出了分治解法以及动态规划解法。这两个解法容易想到吗？《编程珠玑》第八章讨论了这个问题：\n\n> “... 1977年的时候，他将该问题叙述给 Michael Shamos 听，结果 Shamos 花一个通宵就设计出了算法3（注：分治解法）。过了没多久，Shamos 向我介绍这个问题，我们一致认为这很可能是最好的算法了，因为研究人员刚刚证明了几个类似的问题需要正比于 O(*n*log*n)* 的时间。几天之后，Shamos 在卡内基—梅隆大学研讨会上介绍了该问题及其历史，结果与会的统计学家 Jay Kadane 在一分钟之内就勾勒出了算法4（注：动态规划解法）。好在我们知道不会有更快的算法了：任何正确的算法都必须至少花费 *O*(*n*) 的时间“\n\n数学家 Michael Shamos ，花费一个通宵才设计出分治解法。而且他和计算机科学家 Jon Bentley 都没有想到动态规划最优解，我又何必要求自己在两个小时内想到。\n\n[环形链表](https://link.zhihu.com/?target=https%3A//leetcode-cn.com/problems/linked-list-cycle/)（简单）\n\n最优解使用了 Floyd 判圈算法，一般来说，算法带人名的都不是凡夫俗子可以想到的。\n\n[多数元素](https://link.zhihu.com/?target=https%3A//leetcode-cn.com/problems/majority-element/)（简单）\n\n最优解使用了 Boyer-Moore 投票算法，它的一般形式发表了一篇论文，具体可以浏览[这里的介绍](https://link.zhihu.com/?target=https%3A//www.cs.ou.edu/~rlpage/dmtools/mjrty.pdf)\n\n[寻找重复数](https://link.zhihu.com/?target=https%3A//leetcode-cn.com/problems/find-the-duplicate-number/)（中等）\n\n最优解是 O(N) 的快慢指针解法，我当初看到这个解法的时候感觉难以置信，不禁让我去查找这个解法的相关资料，最终找到了[这篇文章](https://link.zhihu.com/?target=https%3A//keithschwarz.com/interesting/code/%3Fdir%3Dfind-duplicate)\n\n> “This problem (reportedly) took CS legend Don Knuth twenty-four hours to solve and I have only met one person (Keith Amling) who could solve it in less time than this.“\n\nDon Knuth ，算法界的传奇，著作包括《计算机程序设计艺术》这本巨著，花了 24 小时想到快慢指针的解法。\n\n这样的例子还有不少，我举这些例子并不是让你自我安慰或满足于次优解。而是说某些题目的最优解是很难想出来的，了解这些事实才可以对自己的水平进行更加准确地评估，不会因为花一个小时想不出最优解而自怨自哀。另外，做题过程中若被题目的难度标签所影响，高估或者低估自己的真实水平都对于面试并没有任何帮助，所以我现在都使用自己开发的 Leetcode invisible 插件隐藏了题目的难度。我建议 Leetcode 可以参考 Google Kickstart 进行分级，一道题目可以分两个难度标签和两个测试集，例如，最大子序和这道题目的标签是 **简单 | 困难**，代表次优解比较简单，但是最优解需要一些巧思才能解决。\n\n## 熟能生巧\n\n我在 Leetcode 大概做过 900 题，熟悉了题目的套路之后，仅仅从题目名字就能推测到解法。例如 “石子游戏“ 一看就是 minmax 策略，“ XXX 子序列” 的话可以试试动态规划，“ XXX 最大的最小值” 是二分或者滑动窗口。**但是这并不代表我的算法水平多么厉害，让我去做其他平台的题目我绝不可能从题目名字想到解法。**同样地，竞赛的选手也有这样的优势，在做了大量题目，参加了大量竞赛之后，他们可以从题目名字或者描述中就能找到之前做过题目的影子，快速找到解法。\n\n## 大神也会卡题\n\n[Lee215](https://link.zhihu.com/?target=https%3A//leetcode.com/lee215/) 是 Leetcode 美区 reputation 分数最高的用户，竞赛成绩也很前，一些题解更是令我醍醐灌顶。[cuiaoxiang](https://link.zhihu.com/?target=https%3A//leetcode.com/cuiaoxiang/) 是 Leetcode 国区竞赛前列，经常在 20 分钟能解答四道题目。但是即使优秀如他们，偶尔也会遇到卡题的情况，[推荐这个 Lee215 第一人称解说的竞赛视频](https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DZIhhoFQp8H4%26t%3D1030s)，视频中 cuiaoxiang 难得一见使用 30 分钟解答一道中等题。可见，算法涉及的东西非常多，大家熟悉的领域也不同，即使我在 30 分钟内解答了那道题，也绝不代表我比 cuiaoxiang 厉害。\n\n## 竞赛不是面试\n\n大部分人刷题都是为了面试而不是竞赛，而面试和竞赛其实差别很大，William 是非常强的竞赛选手，Leetcode 竞赛拿过几次第一，而且他发布过一个连续 12 小时解答 120 道算法题的视频，直接把我看呆。不过竞赛选手沟通不一定强，他和一位谷歌工程师进行过[模拟面试](https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3D-tNMxwWSN_M%26t%3D0s)，沟通并不比其他人强很多。同样的，面试选手竞赛不一定厉害。竞赛的时候题目会给定所有限制条件，只需要写代码提交，不需要向其他人解释自己的思路。但是面试需要让面试官理解自己的思路和代码，也可以让面试官提供一些帮助，对于沟通的要求要更高。\n\n## 总结\n\n刷题离不开坚持和努力，更需要保证良好的心态，我在刷题初期的时候，经常会心态崩溃。现在回过头看，实在是没有必要，最后分享一句话给大家，共勉。\n\n\"If you are not enjoying it, you are doing it wrong.\"","source":"_posts/跳出刷题的自我怀疑-转载.md","raw":"---\ntitle: 跳出刷题的自我怀疑(转载)\ntoc: true\nmathjax: true\ntop: false\ncover: true\ndate: 2021-03-10 11:08:08\nupdated:\ncategories: Algorithms\ntags:\n\t- Algorithms\n\t- LeetCode\n---\n\n> 刷题中总感觉自己是个废物，终于在这篇文章中找到原因\n>\n> From https://zhuanlan.zhihu.com/p/351560331\n\n刚接触 Leetcode 的时候，我经常边刷题边陷入自我怀疑，通常有几个原因：1）想不到最优解：一些简单题目的最优解，我觉得自己不可能想出来，也不太能理解。2）看不懂解法：论坛中被赞最多的解法往往为了追求代码的简短性而忽略可读性，在刷题初期要理解解法都需要耗费大量时间。3）差距太大：网上有不少[竞赛直播的视频](https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3D4ALB5m_Idkk)，他们在 20 分钟之内就能解答四道题目，对比之下，实在自愧弗如。\n\n这几个原因导致的自我怀疑不仅打压了我刷题的热情，耗费了大量时间，也影响了我对自己真实算法水平的判断。如今刷过一些题之后，我开始了解到一些更深层的原因，希望在此能帮助到刷题中迷茫的各位：\n\n## 简单题不简单\n\n我推测 Leetcode 的题目难度并不是根据最优解的难度来设定的，也就是说一道标记为简单的题目它的次优解可能非常直观，但是最优解却很难想出来。这一点对于当初是刷题新手的我尤其致命，因为在被最优解的美妙所震撼之后，我强迫自己理解和证明它，但是如果在一定时间内还没有成功，我就会陷入深深的自我怀疑，举几道题目为例：\n\n[最大子序和](https://link.zhihu.com/?target=https%3A//leetcode-cn.com/problems/maximum-subarray/)（简单）\n\n[官方解答](https://link.zhihu.com/?target=https%3A//leetcode-cn.com/problems/maximum-subarray/solution/zui-da-zi-xu-he-by-leetcode-solution/)轻描淡写地给出了分治解法以及动态规划解法。这两个解法容易想到吗？《编程珠玑》第八章讨论了这个问题：\n\n> “... 1977年的时候，他将该问题叙述给 Michael Shamos 听，结果 Shamos 花一个通宵就设计出了算法3（注：分治解法）。过了没多久，Shamos 向我介绍这个问题，我们一致认为这很可能是最好的算法了，因为研究人员刚刚证明了几个类似的问题需要正比于 O(*n*log*n)* 的时间。几天之后，Shamos 在卡内基—梅隆大学研讨会上介绍了该问题及其历史，结果与会的统计学家 Jay Kadane 在一分钟之内就勾勒出了算法4（注：动态规划解法）。好在我们知道不会有更快的算法了：任何正确的算法都必须至少花费 *O*(*n*) 的时间“\n\n数学家 Michael Shamos ，花费一个通宵才设计出分治解法。而且他和计算机科学家 Jon Bentley 都没有想到动态规划最优解，我又何必要求自己在两个小时内想到。\n\n[环形链表](https://link.zhihu.com/?target=https%3A//leetcode-cn.com/problems/linked-list-cycle/)（简单）\n\n最优解使用了 Floyd 判圈算法，一般来说，算法带人名的都不是凡夫俗子可以想到的。\n\n[多数元素](https://link.zhihu.com/?target=https%3A//leetcode-cn.com/problems/majority-element/)（简单）\n\n最优解使用了 Boyer-Moore 投票算法，它的一般形式发表了一篇论文，具体可以浏览[这里的介绍](https://link.zhihu.com/?target=https%3A//www.cs.ou.edu/~rlpage/dmtools/mjrty.pdf)\n\n[寻找重复数](https://link.zhihu.com/?target=https%3A//leetcode-cn.com/problems/find-the-duplicate-number/)（中等）\n\n最优解是 O(N) 的快慢指针解法，我当初看到这个解法的时候感觉难以置信，不禁让我去查找这个解法的相关资料，最终找到了[这篇文章](https://link.zhihu.com/?target=https%3A//keithschwarz.com/interesting/code/%3Fdir%3Dfind-duplicate)\n\n> “This problem (reportedly) took CS legend Don Knuth twenty-four hours to solve and I have only met one person (Keith Amling) who could solve it in less time than this.“\n\nDon Knuth ，算法界的传奇，著作包括《计算机程序设计艺术》这本巨著，花了 24 小时想到快慢指针的解法。\n\n这样的例子还有不少，我举这些例子并不是让你自我安慰或满足于次优解。而是说某些题目的最优解是很难想出来的，了解这些事实才可以对自己的水平进行更加准确地评估，不会因为花一个小时想不出最优解而自怨自哀。另外，做题过程中若被题目的难度标签所影响，高估或者低估自己的真实水平都对于面试并没有任何帮助，所以我现在都使用自己开发的 Leetcode invisible 插件隐藏了题目的难度。我建议 Leetcode 可以参考 Google Kickstart 进行分级，一道题目可以分两个难度标签和两个测试集，例如，最大子序和这道题目的标签是 **简单 | 困难**，代表次优解比较简单，但是最优解需要一些巧思才能解决。\n\n## 熟能生巧\n\n我在 Leetcode 大概做过 900 题，熟悉了题目的套路之后，仅仅从题目名字就能推测到解法。例如 “石子游戏“ 一看就是 minmax 策略，“ XXX 子序列” 的话可以试试动态规划，“ XXX 最大的最小值” 是二分或者滑动窗口。**但是这并不代表我的算法水平多么厉害，让我去做其他平台的题目我绝不可能从题目名字想到解法。**同样地，竞赛的选手也有这样的优势，在做了大量题目，参加了大量竞赛之后，他们可以从题目名字或者描述中就能找到之前做过题目的影子，快速找到解法。\n\n## 大神也会卡题\n\n[Lee215](https://link.zhihu.com/?target=https%3A//leetcode.com/lee215/) 是 Leetcode 美区 reputation 分数最高的用户，竞赛成绩也很前，一些题解更是令我醍醐灌顶。[cuiaoxiang](https://link.zhihu.com/?target=https%3A//leetcode.com/cuiaoxiang/) 是 Leetcode 国区竞赛前列，经常在 20 分钟能解答四道题目。但是即使优秀如他们，偶尔也会遇到卡题的情况，[推荐这个 Lee215 第一人称解说的竞赛视频](https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DZIhhoFQp8H4%26t%3D1030s)，视频中 cuiaoxiang 难得一见使用 30 分钟解答一道中等题。可见，算法涉及的东西非常多，大家熟悉的领域也不同，即使我在 30 分钟内解答了那道题，也绝不代表我比 cuiaoxiang 厉害。\n\n## 竞赛不是面试\n\n大部分人刷题都是为了面试而不是竞赛，而面试和竞赛其实差别很大，William 是非常强的竞赛选手，Leetcode 竞赛拿过几次第一，而且他发布过一个连续 12 小时解答 120 道算法题的视频，直接把我看呆。不过竞赛选手沟通不一定强，他和一位谷歌工程师进行过[模拟面试](https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3D-tNMxwWSN_M%26t%3D0s)，沟通并不比其他人强很多。同样的，面试选手竞赛不一定厉害。竞赛的时候题目会给定所有限制条件，只需要写代码提交，不需要向其他人解释自己的思路。但是面试需要让面试官理解自己的思路和代码，也可以让面试官提供一些帮助，对于沟通的要求要更高。\n\n## 总结\n\n刷题离不开坚持和努力，更需要保证良好的心态，我在刷题初期的时候，经常会心态崩溃。现在回过头看，实在是没有必要，最后分享一句话给大家，共勉。\n\n\"If you are not enjoying it, you are doing it wrong.\"","slug":"跳出刷题的自我怀疑-转载","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"ckowlff560018ye28068hhosu","content":"<blockquote>\n<p>刷题中总感觉自己是个废物，终于在这篇文章中找到原因</p>\n<p>From <a href=\"https://zhuanlan.zhihu.com/p/351560331\">https://zhuanlan.zhihu.com/p/351560331</a></p>\n</blockquote>\n<p>刚接触 Leetcode 的时候，我经常边刷题边陷入自我怀疑，通常有几个原因：1）想不到最优解：一些简单题目的最优解，我觉得自己不可能想出来，也不太能理解。2）看不懂解法：论坛中被赞最多的解法往往为了追求代码的简短性而忽略可读性，在刷题初期要理解解法都需要耗费大量时间。3）差距太大：网上有不少<a href=\"https://link.zhihu.com/?target=https://www.youtube.com/watch?v=4ALB5m_Idkk\">竞赛直播的视频</a>，他们在 20 分钟之内就能解答四道题目，对比之下，实在自愧弗如。</p>\n<p>这几个原因导致的自我怀疑不仅打压了我刷题的热情，耗费了大量时间，也影响了我对自己真实算法水平的判断。如今刷过一些题之后，我开始了解到一些更深层的原因，希望在此能帮助到刷题中迷茫的各位：</p>\n<h2 id=\"简单题不简单\"><a href=\"#简单题不简单\" class=\"headerlink\" title=\"简单题不简单\"></a>简单题不简单</h2><p>我推测 Leetcode 的题目难度并不是根据最优解的难度来设定的，也就是说一道标记为简单的题目它的次优解可能非常直观，但是最优解却很难想出来。这一点对于当初是刷题新手的我尤其致命，因为在被最优解的美妙所震撼之后，我强迫自己理解和证明它，但是如果在一定时间内还没有成功，我就会陷入深深的自我怀疑，举几道题目为例：</p>\n<p><a href=\"https://link.zhihu.com/?target=https://leetcode-cn.com/problems/maximum-subarray/\">最大子序和</a>（简单）</p>\n<p><a href=\"https://link.zhihu.com/?target=https://leetcode-cn.com/problems/maximum-subarray/solution/zui-da-zi-xu-he-by-leetcode-solution/\">官方解答</a>轻描淡写地给出了分治解法以及动态规划解法。这两个解法容易想到吗？《编程珠玑》第八章讨论了这个问题：</p>\n<blockquote>\n<p>“… 1977年的时候，他将该问题叙述给 Michael Shamos 听，结果 Shamos 花一个通宵就设计出了算法3（注：分治解法）。过了没多久，Shamos 向我介绍这个问题，我们一致认为这很可能是最好的算法了，因为研究人员刚刚证明了几个类似的问题需要正比于 O(<em>n</em>log<em>n)</em> 的时间。几天之后，Shamos 在卡内基—梅隆大学研讨会上介绍了该问题及其历史，结果与会的统计学家 Jay Kadane 在一分钟之内就勾勒出了算法4（注：动态规划解法）。好在我们知道不会有更快的算法了：任何正确的算法都必须至少花费 <em>O</em>(<em>n</em>) 的时间“</p>\n</blockquote>\n<p>数学家 Michael Shamos ，花费一个通宵才设计出分治解法。而且他和计算机科学家 Jon Bentley 都没有想到动态规划最优解，我又何必要求自己在两个小时内想到。</p>\n<p><a href=\"https://link.zhihu.com/?target=https://leetcode-cn.com/problems/linked-list-cycle/\">环形链表</a>（简单）</p>\n<p>最优解使用了 Floyd 判圈算法，一般来说，算法带人名的都不是凡夫俗子可以想到的。</p>\n<p><a href=\"https://link.zhihu.com/?target=https://leetcode-cn.com/problems/majority-element/\">多数元素</a>（简单）</p>\n<p>最优解使用了 Boyer-Moore 投票算法，它的一般形式发表了一篇论文，具体可以浏览<a href=\"https://link.zhihu.com/?target=https://www.cs.ou.edu/~rlpage/dmtools/mjrty.pdf\">这里的介绍</a></p>\n<p><a href=\"https://link.zhihu.com/?target=https://leetcode-cn.com/problems/find-the-duplicate-number/\">寻找重复数</a>（中等）</p>\n<p>最优解是 O(N) 的快慢指针解法，我当初看到这个解法的时候感觉难以置信，不禁让我去查找这个解法的相关资料，最终找到了<a href=\"https://link.zhihu.com/?target=https://keithschwarz.com/interesting/code/?dir=find-duplicate\">这篇文章</a></p>\n<blockquote>\n<p>“This problem (reportedly) took CS legend Don Knuth twenty-four hours to solve and I have only met one person (Keith Amling) who could solve it in less time than this.“</p>\n</blockquote>\n<p>Don Knuth ，算法界的传奇，著作包括《计算机程序设计艺术》这本巨著，花了 24 小时想到快慢指针的解法。</p>\n<p>这样的例子还有不少，我举这些例子并不是让你自我安慰或满足于次优解。而是说某些题目的最优解是很难想出来的，了解这些事实才可以对自己的水平进行更加准确地评估，不会因为花一个小时想不出最优解而自怨自哀。另外，做题过程中若被题目的难度标签所影响，高估或者低估自己的真实水平都对于面试并没有任何帮助，所以我现在都使用自己开发的 Leetcode invisible 插件隐藏了题目的难度。我建议 Leetcode 可以参考 Google Kickstart 进行分级，一道题目可以分两个难度标签和两个测试集，例如，最大子序和这道题目的标签是 <strong>简单 | 困难</strong>，代表次优解比较简单，但是最优解需要一些巧思才能解决。</p>\n<h2 id=\"熟能生巧\"><a href=\"#熟能生巧\" class=\"headerlink\" title=\"熟能生巧\"></a>熟能生巧</h2><p>我在 Leetcode 大概做过 900 题，熟悉了题目的套路之后，仅仅从题目名字就能推测到解法。例如 “石子游戏“ 一看就是 minmax 策略，“ XXX 子序列” 的话可以试试动态规划，“ XXX 最大的最小值” 是二分或者滑动窗口。<strong>但是这并不代表我的算法水平多么厉害，让我去做其他平台的题目我绝不可能从题目名字想到解法。</strong>同样地，竞赛的选手也有这样的优势，在做了大量题目，参加了大量竞赛之后，他们可以从题目名字或者描述中就能找到之前做过题目的影子，快速找到解法。</p>\n<h2 id=\"大神也会卡题\"><a href=\"#大神也会卡题\" class=\"headerlink\" title=\"大神也会卡题\"></a>大神也会卡题</h2><p><a href=\"https://link.zhihu.com/?target=https://leetcode.com/lee215/\">Lee215</a> 是 Leetcode 美区 reputation 分数最高的用户，竞赛成绩也很前，一些题解更是令我醍醐灌顶。<a href=\"https://link.zhihu.com/?target=https://leetcode.com/cuiaoxiang/\">cuiaoxiang</a> 是 Leetcode 国区竞赛前列，经常在 20 分钟能解答四道题目。但是即使优秀如他们，偶尔也会遇到卡题的情况，<a href=\"https://link.zhihu.com/?target=https://www.youtube.com/watch?v=ZIhhoFQp8H4&t=1030s\">推荐这个 Lee215 第一人称解说的竞赛视频</a>，视频中 cuiaoxiang 难得一见使用 30 分钟解答一道中等题。可见，算法涉及的东西非常多，大家熟悉的领域也不同，即使我在 30 分钟内解答了那道题，也绝不代表我比 cuiaoxiang 厉害。</p>\n<h2 id=\"竞赛不是面试\"><a href=\"#竞赛不是面试\" class=\"headerlink\" title=\"竞赛不是面试\"></a>竞赛不是面试</h2><p>大部分人刷题都是为了面试而不是竞赛，而面试和竞赛其实差别很大，William 是非常强的竞赛选手，Leetcode 竞赛拿过几次第一，而且他发布过一个连续 12 小时解答 120 道算法题的视频，直接把我看呆。不过竞赛选手沟通不一定强，他和一位谷歌工程师进行过<a href=\"https://link.zhihu.com/?target=https://www.youtube.com/watch?v=-tNMxwWSN_M&t=0s\">模拟面试</a>，沟通并不比其他人强很多。同样的，面试选手竞赛不一定厉害。竞赛的时候题目会给定所有限制条件，只需要写代码提交，不需要向其他人解释自己的思路。但是面试需要让面试官理解自己的思路和代码，也可以让面试官提供一些帮助，对于沟通的要求要更高。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>刷题离不开坚持和努力，更需要保证良好的心态，我在刷题初期的时候，经常会心态崩溃。现在回过头看，实在是没有必要，最后分享一句话给大家，共勉。</p>\n<p>“If you are not enjoying it, you are doing it wrong.”</p>\n","site":{"data":{}},"excerpt":"","more":"<blockquote>\n<p>刷题中总感觉自己是个废物，终于在这篇文章中找到原因</p>\n<p>From <a href=\"https://zhuanlan.zhihu.com/p/351560331\">https://zhuanlan.zhihu.com/p/351560331</a></p>\n</blockquote>\n<p>刚接触 Leetcode 的时候，我经常边刷题边陷入自我怀疑，通常有几个原因：1）想不到最优解：一些简单题目的最优解，我觉得自己不可能想出来，也不太能理解。2）看不懂解法：论坛中被赞最多的解法往往为了追求代码的简短性而忽略可读性，在刷题初期要理解解法都需要耗费大量时间。3）差距太大：网上有不少<a href=\"https://link.zhihu.com/?target=https://www.youtube.com/watch?v=4ALB5m_Idkk\">竞赛直播的视频</a>，他们在 20 分钟之内就能解答四道题目，对比之下，实在自愧弗如。</p>\n<p>这几个原因导致的自我怀疑不仅打压了我刷题的热情，耗费了大量时间，也影响了我对自己真实算法水平的判断。如今刷过一些题之后，我开始了解到一些更深层的原因，希望在此能帮助到刷题中迷茫的各位：</p>\n<h2 id=\"简单题不简单\"><a href=\"#简单题不简单\" class=\"headerlink\" title=\"简单题不简单\"></a>简单题不简单</h2><p>我推测 Leetcode 的题目难度并不是根据最优解的难度来设定的，也就是说一道标记为简单的题目它的次优解可能非常直观，但是最优解却很难想出来。这一点对于当初是刷题新手的我尤其致命，因为在被最优解的美妙所震撼之后，我强迫自己理解和证明它，但是如果在一定时间内还没有成功，我就会陷入深深的自我怀疑，举几道题目为例：</p>\n<p><a href=\"https://link.zhihu.com/?target=https://leetcode-cn.com/problems/maximum-subarray/\">最大子序和</a>（简单）</p>\n<p><a href=\"https://link.zhihu.com/?target=https://leetcode-cn.com/problems/maximum-subarray/solution/zui-da-zi-xu-he-by-leetcode-solution/\">官方解答</a>轻描淡写地给出了分治解法以及动态规划解法。这两个解法容易想到吗？《编程珠玑》第八章讨论了这个问题：</p>\n<blockquote>\n<p>“… 1977年的时候，他将该问题叙述给 Michael Shamos 听，结果 Shamos 花一个通宵就设计出了算法3（注：分治解法）。过了没多久，Shamos 向我介绍这个问题，我们一致认为这很可能是最好的算法了，因为研究人员刚刚证明了几个类似的问题需要正比于 O(<em>n</em>log<em>n)</em> 的时间。几天之后，Shamos 在卡内基—梅隆大学研讨会上介绍了该问题及其历史，结果与会的统计学家 Jay Kadane 在一分钟之内就勾勒出了算法4（注：动态规划解法）。好在我们知道不会有更快的算法了：任何正确的算法都必须至少花费 <em>O</em>(<em>n</em>) 的时间“</p>\n</blockquote>\n<p>数学家 Michael Shamos ，花费一个通宵才设计出分治解法。而且他和计算机科学家 Jon Bentley 都没有想到动态规划最优解，我又何必要求自己在两个小时内想到。</p>\n<p><a href=\"https://link.zhihu.com/?target=https://leetcode-cn.com/problems/linked-list-cycle/\">环形链表</a>（简单）</p>\n<p>最优解使用了 Floyd 判圈算法，一般来说，算法带人名的都不是凡夫俗子可以想到的。</p>\n<p><a href=\"https://link.zhihu.com/?target=https://leetcode-cn.com/problems/majority-element/\">多数元素</a>（简单）</p>\n<p>最优解使用了 Boyer-Moore 投票算法，它的一般形式发表了一篇论文，具体可以浏览<a href=\"https://link.zhihu.com/?target=https://www.cs.ou.edu/~rlpage/dmtools/mjrty.pdf\">这里的介绍</a></p>\n<p><a href=\"https://link.zhihu.com/?target=https://leetcode-cn.com/problems/find-the-duplicate-number/\">寻找重复数</a>（中等）</p>\n<p>最优解是 O(N) 的快慢指针解法，我当初看到这个解法的时候感觉难以置信，不禁让我去查找这个解法的相关资料，最终找到了<a href=\"https://link.zhihu.com/?target=https://keithschwarz.com/interesting/code/?dir=find-duplicate\">这篇文章</a></p>\n<blockquote>\n<p>“This problem (reportedly) took CS legend Don Knuth twenty-four hours to solve and I have only met one person (Keith Amling) who could solve it in less time than this.“</p>\n</blockquote>\n<p>Don Knuth ，算法界的传奇，著作包括《计算机程序设计艺术》这本巨著，花了 24 小时想到快慢指针的解法。</p>\n<p>这样的例子还有不少，我举这些例子并不是让你自我安慰或满足于次优解。而是说某些题目的最优解是很难想出来的，了解这些事实才可以对自己的水平进行更加准确地评估，不会因为花一个小时想不出最优解而自怨自哀。另外，做题过程中若被题目的难度标签所影响，高估或者低估自己的真实水平都对于面试并没有任何帮助，所以我现在都使用自己开发的 Leetcode invisible 插件隐藏了题目的难度。我建议 Leetcode 可以参考 Google Kickstart 进行分级，一道题目可以分两个难度标签和两个测试集，例如，最大子序和这道题目的标签是 <strong>简单 | 困难</strong>，代表次优解比较简单，但是最优解需要一些巧思才能解决。</p>\n<h2 id=\"熟能生巧\"><a href=\"#熟能生巧\" class=\"headerlink\" title=\"熟能生巧\"></a>熟能生巧</h2><p>我在 Leetcode 大概做过 900 题，熟悉了题目的套路之后，仅仅从题目名字就能推测到解法。例如 “石子游戏“ 一看就是 minmax 策略，“ XXX 子序列” 的话可以试试动态规划，“ XXX 最大的最小值” 是二分或者滑动窗口。<strong>但是这并不代表我的算法水平多么厉害，让我去做其他平台的题目我绝不可能从题目名字想到解法。</strong>同样地，竞赛的选手也有这样的优势，在做了大量题目，参加了大量竞赛之后，他们可以从题目名字或者描述中就能找到之前做过题目的影子，快速找到解法。</p>\n<h2 id=\"大神也会卡题\"><a href=\"#大神也会卡题\" class=\"headerlink\" title=\"大神也会卡题\"></a>大神也会卡题</h2><p><a href=\"https://link.zhihu.com/?target=https://leetcode.com/lee215/\">Lee215</a> 是 Leetcode 美区 reputation 分数最高的用户，竞赛成绩也很前，一些题解更是令我醍醐灌顶。<a href=\"https://link.zhihu.com/?target=https://leetcode.com/cuiaoxiang/\">cuiaoxiang</a> 是 Leetcode 国区竞赛前列，经常在 20 分钟能解答四道题目。但是即使优秀如他们，偶尔也会遇到卡题的情况，<a href=\"https://link.zhihu.com/?target=https://www.youtube.com/watch?v=ZIhhoFQp8H4&t=1030s\">推荐这个 Lee215 第一人称解说的竞赛视频</a>，视频中 cuiaoxiang 难得一见使用 30 分钟解答一道中等题。可见，算法涉及的东西非常多，大家熟悉的领域也不同，即使我在 30 分钟内解答了那道题，也绝不代表我比 cuiaoxiang 厉害。</p>\n<h2 id=\"竞赛不是面试\"><a href=\"#竞赛不是面试\" class=\"headerlink\" title=\"竞赛不是面试\"></a>竞赛不是面试</h2><p>大部分人刷题都是为了面试而不是竞赛，而面试和竞赛其实差别很大，William 是非常强的竞赛选手，Leetcode 竞赛拿过几次第一，而且他发布过一个连续 12 小时解答 120 道算法题的视频，直接把我看呆。不过竞赛选手沟通不一定强，他和一位谷歌工程师进行过<a href=\"https://link.zhihu.com/?target=https://www.youtube.com/watch?v=-tNMxwWSN_M&t=0s\">模拟面试</a>，沟通并不比其他人强很多。同样的，面试选手竞赛不一定厉害。竞赛的时候题目会给定所有限制条件，只需要写代码提交，不需要向其他人解释自己的思路。但是面试需要让面试官理解自己的思路和代码，也可以让面试官提供一些帮助，对于沟通的要求要更高。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>刷题离不开坚持和努力，更需要保证良好的心态，我在刷题初期的时候，经常会心态崩溃。现在回过头看，实在是没有必要，最后分享一句话给大家，共勉。</p>\n<p>“If you are not enjoying it, you are doing it wrong.”</p>\n"},{"title":"时断时续的执行力反思","toc":true,"mathjax":true,"top":false,"cover":true,"date":"2021-03-02T10:50:09.000Z","updated":"2021-04-02T02:53:21.397Z","_content":"\n　　自从去年国庆开始写blog之后，更新过程尤为艰难，兴趣点的转移，执行力时断时续，尽管深知自己有这样三分钟热度的嗜好，但是仍然难以有效利用或者纠正，任由其发展。这与当初设想的定期更新的理想大相径庭，其实要论原因，每一篇随笔里都或多或少有提及，结果上来看，还是执行力不强的问题，心态上看，还是决心和勇气不足。总结出了很多原因，依然无法有效的解决问题，现在打算把这些断断续续的分析成果认真的探讨一下，希望能发现自身存在的根本性的问题。\n\n> Desultory :  going constantly from one subject to another in a halfhearted way \n\n　　最近背单词的时候发现了一个单词非常好的描述了发生在我身上的这种现象，不知道是不是中了年少时候教导的，“要么不做，要么就做到最好”，这个毒鸡汤的影响，再加上总是妄想武侠小说或者各种爽文网剧中，那种不鸣则已，一鸣惊人的那种戏剧性的影响，总是幻想着能厚积薄发，一鼓作气的逆袭。结果就是形成了这股略显中二的行事风格，在年少气盛的时候，没看出哪里有什么不对；只是到了重要的分水岭的时候，便开始畏首畏尾了。一般这种情况碰上拖延症，就这么一直搁置下去了。最后积累到一起，让自己无所适从，不知道如何面对。其实我一直都知道，这种人生重要的选择，都需要倾注足够的精力去分析和思考，只是都被我逃避开了。所以，当一些烦恼和麻烦接二连三的到来时，最终归根溯源，都会追查到这份逃避和懒惰上来。这种心态就像是这样：\n\n> 我在考试该复习前，有时特别想看书闲玩总之是干一切其他的事情。\n>\n> 考试结束后真到了有时间做上述那些事的时候，往往作个几天就又想搞别的了\n\n　　因为思考这种利弊，很费脑力，尽管这些脑力不用也没干什么有意义的其他的什么事情？我又是个比较怕麻烦的人，所以为了暂且的安定，则选择了逃避并搁置这种疑惑。结果来看，则是带来更多更复杂的烦心事。这让我想起最近看到了关于[双重思想](https://yonglezh.github.io/2017/01/21/doublethink/)的文章，关于双重思想形成的4大要素：\n\n>1. 相信两种互相矛盾的信念。（并非同时，而是脑袋里具备两种互相矛盾的信念。）\n>2. 根据情境来切换自己相信的信念。\n>3. 忽略矛盾，认为自己当下的信念就是自己一直以来坚持的信念。\n>4. 忽略自己在忽略矛盾，忽略自己在不断地切换信念。（意识不到自己按不同情境做了不同的选择 —— 这是明知两个信念存在矛盾却不认为自己的行为有矛盾的关键。）\n\n　　我相信暂时的搁置能换来更少的麻烦，同时也相信现在投入一定的思考能让自己未来烦恼变得更少。所以秉承着怕麻烦的总体思路，尽管我能隐约察觉到这两种选择的矛盾之处，但是烦恼这种东西，难以量化，如果不去深思这二者的区别的话，就更加难以分辨。所以，我在快要发现矛盾的时候，及时的选择的忽略矛盾，这便是上述要素3；再则，在不断的切换兴趣点和日常琐事中，完全意识不到自己是在忽略矛盾，因为彻底了忘记了这些问题了，也正好达成了要素4。\n\n　　现实是公平且残酷的，当我都快要忘记这些不想面对的抉择的时候，这些乱七八糟的事情像是约好似的集中朝我走来。小到上班摸鱼，摸到自己都开始厌恶自己，不知道整天在做些什么，开始后悔不如一开始就认真做事，还能有所成长和收获，落得个习惯性的无所事事，还要花费很多精力去纠正这种不良习惯，何况还有没能成长的自信还需要去弥补；大到面对人生的道路的时候，犹犹豫豫，不敢前进，最后拖拖拉拉，搞得自己都没勇气去踏出应该走出的那一步，还不如一开始就走出去，白白积累一段不愉快的自卑的经历。\n\n　　经过这些问题和失败才感同身受的明白了，认真合理的规划自己的人生，才是最省心和麻烦最少的一种方式。就跟写代码一样，以前做OJ的时候，总是爱盲目的开始敲代码，经常发现凭借直觉的代码总是会犯一些最基础的错误，看到这些稚嫩的代码，连自己都为之汗颜；后来才明白，做OJ最主要的部分其实并不是敲代码，反而是思考的过程才是最需要去实践和锻炼的。这么简单的道理，怎么自己却不明白呢？合理的构思代码结构，划分功能模块，化繁为简，分区而治才是简洁高效并优雅的代码的基本保障，明明在学习《算法4》的时候，对这种优雅的API结构化写代码流程极为推崇，为什么到了自己实践的 时候，却也犯糊涂了呢？之后无论是看设计模式，还是学习一些知名的开源项目，都是把这种模块化的思维贯彻到底的一种实践方式，曾经觉得，这些开源项目都是可望不可及的存在，必定是每一行代码都是非常牛逼的存在，学习后才发现，其实功能被切分到非常具体的时候，这些代码就算是我也能轻松写出来；厉害的地方其实在项目架构的模块切分和接口的定义。自己照着这个设计思路一走就会发现，这样的架构其实有很多其他的变种，都能实现大体相同的功能，之所以成为目前的这个架构，都是有很多细节和稳定性以及扩展性的考量的，朝着这个方向去理解，就会发现很多结构和接口的设计就显得合理多了。总之，无论是代码架构还是人生规划，随意的搁置和逃避是非常不划算的选择，因为相比决策阶段的投入，失败成本显然高太多，况且自己并非不愿意去做决策，只是非常单纯的懒继而拖延症愈发严重而导致的，认真去思考，其实自己还是愿意去做的；况且我还是非常享受生活朝着自己规划的方向走的过程的，一点点收益和成果，都会让自己非常满足；就跟写出很棒的代码一样，规划得当，小小的努力就能获得非常巨大的AC快感。\n\n　　所以，发现矛盾要正视，投入一点分析和思考去解决，认真执行，接下来的一点点收获和成果就能指导自己朝着希望的方向前进。任何时候都不要吝惜自己的思考，认识自己，发现自己，认知这个世界，释放自己能量，那才是我想要成为的自我。\n\n　　规划之外，还有执行力的一些习惯问题需要纠正，总的来说，就是自己不够自律。\n\n　　三联生活周刊公众号的一篇关于[自律的文章](https://mp.weixin.qq.com/s/EjPk_1csVjxQFr9PWJABxQ)启发了我，里面所描述的种种现象完美的击中了我，我之所以不敢轻易开启我的自由职业，最大的忧虑就是担心自己不够自律，无论是学英文，学架子鼓，学新的编程语言，学新的开源架构，无一例外都是刚接触的时候信心满满，没坚持几天就开始逃避并放弃，学的好一点，只是因为某些领域天赋还行，短时间内就可以达到可堪够用的程度，但是最后也是由于自满自足，逃不过被搁置的命运。不能否认的是，这些事物中，在经历最初的新鲜感之后，很容易进入一个枯燥无味的瓶颈期，我想，我的兴趣基本都没有逃过瓶颈阶段的围追堵截，除了形势所迫被逼无奈不得不去面对，其他时候基本都停在了这个阶段。回想起来，其实就算在学鼓的瓶颈期，我也并不是很反感重复的基本功练习，顶多就是不那么好玩，只是需要训练的时候，自己习惯性的逃避起来，就开始为不练习寻找理由，就只想到了练鼓的不好玩的那一面，结果就没能坚持下去。对于架子鼓，我还是抱有持续的热情的。只是被更高一级的懒拖给掩盖了。从这个角度来看，保持一幅清醒理智的头脑才是这之中最重要的，也许在电脑前，我更想要去玩游戏，更想要去看电影，吃饭的时候，也愿意无聊的看看游戏直播，虽然我其实并没有那么喜欢看直播。但是，**我本质上，还是喜欢一段时间内，只专注做一件事的人**，所以这段时间内的成果，就显得至关重要。做的很好，那么就有坚持继续下去的动力，如果做得不好，那么很可能就会被搁置下去。以前都是漫不经心的执行着，所以只有一些零星的事情才坚持下来了，现在看来，着实有点可惜，因为这些坚持的下来的事情，要么就是偶然做的很好，要么就是天赋很好，轻松就搞定了。根本还没有轮到努力发挥的空间，特别是对于自己非常渴望的事情，就没能通过努力有所精进。\n\n> 在和乔布斯聊了5分钟后，库克就决定跳槽到苹果。2010年，库克在母校奥本大学的毕业典礼以嘉宾身份发表讲话，他说“我的直觉告诉我，加入苹果是一个千载难逢的机会，可以为这位创意天才工作，也可以加入一个能够重振的伟大的管理团队。”\n>\n> 如果不是对想做的事真心热爱，很难放弃自己稳妥到手的利益，投身到一家岌岌可危的公司。\n>\n> **不是自律让库克成功，而是找到了自己热爱的事情，自己愿意用“苦行僧”式的自律方式把热爱的事变得更好。**\n\n　　对于热爱的事情，我不乏对其想要认真做好的热情，只是没能有效利用自律来把热爱的事情做的更好，只是单纯的觉得热爱的事情会自然的变好。这种天真的想法，似曾相识，恰如当年觉得爱情，只要有满腔热情，后面的事情就自然而然的就会变好。事实并非如此，对于热爱的事情，不仅要有热情，还要合理利用自律的方式来让其变得更好，否则只是无根之木，无源之水，难以为继，白白消耗一番心血。\n\n　　这样想来，其实自己做事一贯如此，总是以为关注最主要的部分，其他的次要问题就会迎刃而解。但是或许我对迎刃而解这个理解是有偏差的，我以为的情况是，什么都不做，这个事情自己会随着主要问题一并被解决；而实际情况，需要为次要问题做一些基本的准备和关注，甚至为了万无一失，需要投入同样重要的资源。似乎我在做事的时候，从来没有思考过，这件事情要做好，需要牵涉到哪几方面的内容，以及需要在这几方面应该如何分配资源做准备。所有的精力完全只放到事情本身上，如果是比较单纯的事情，那么自然就没必要为其他的事情分心；但是，目前面临的事情，很少是比较单纯的，很多事情都会牵涉到很多方面， 这还不包括自己在分析问题的关键点上存在偏差这个可能，所以，成熟可靠的做法，是仔细分析事情的各个方面，判断重点，合理的划分资源。在热爱的事情这个问题上，还需要自律来为其添砖加瓦。\n\n　　在看王小波的杂文的时候，在看哲学经典的时候，在看费曼的《物理学讲义》的时候，在看《算法4》的时候，在看3Blue1Brown的数学视频的时候，在看优雅的开源代码的时候，心中都会涌现出对热爱的事情的渴望，这些内容其实在理解之后，都变得不那么晦涩难懂，反而激发出自己想要做出这样成果的热情。因为我觉得，这些都是非常优秀的成就，我希望自己能在这个方向上走出那怕一小步，我也就心满意足了。\n\n> 当下我们很多人的自律纯属一时兴起，因为缺乏目标感、因为想要逃避真正的困难、或者仅仅觉得自己的生活单调，所以才妄图抓住自律这根救命稻草来把自己拉出泥泞。\n>\n> 结果就是陷入恶性循环：下定决心自律→因为真正目标不清晰而难以坚持不下去→鄙视自己意志力脆弱、厌恶自己→妄图再靠“一时兴起”的自律来拯救自己\n\n　　之前在自律方面一直陷入这样一时兴起的恶性循环中，目标感虽然还算比较明确，但是缺乏一定要去达成的使命感，所以才会想方设法的逃避困难，虽然真正做起来，没有那么复杂，至于长期以往的摸鱼，确实会导致生活单调，想要反弹，跟习惯的斗争，确实是旷日持久，这也当然属于失败成本，而且是属于非常难以纠正的那种。也只能怪自己过分骄傲自大，明明是自己失败了，却还妄图自欺欺人的来在虚荣心上来进行补偿，真是本末倒置，首尾都没能兼顾。\n\n　　摆平心态，自己一手造成的恶果，就要自己努力去修正。自律不是一时兴起，不需要心血来潮的补救，找到自己热爱的事情，发现自己愿意为这份渴望付出到什么程度，用相对应的自律方式将这份热爱的事变的更好，我也就知足了。\n\n　　相信自己，坚定自我。不达目的，誓不罢休。\n\n　　使命感之外，还有一个来自于之前毒鸡汤的后续影响不那么容易察觉，就是内心深处的完美主义。我不知道应该如何正确的看待完美主义这个特质， 有时候觉得完美主义的强迫症会让事情变得更好；有时候却又发现，自己只是无形之中被完美主义给束缚住，感觉自己无法做到理想的高度，就会不停的打退堂鼓。本质上来讲，这应该是自己做事没有明确目标导致的，因为如果自己预定的目标并不是做到最好，那么就算不是完美达成，也不至于丧气，虽然心里多少还是对自己没尽全力感到不快。大部分时候做事，都是看到日程表里有这件事，或者是发现此时此刻应该要做这个事情，糊里糊涂的毫无目的的就上阵了。最明显的案例是，自己以前每次学习新的框架的时候，总是想事无巨细，从框架结构到代码细节都想掌握，再开始使用；结果发现，捣鼓一阵子之后，似乎很久之后都不会再用到这部分知识，直到遇到问题，再回头来看的时候，发现这个框架已经陌生得仿佛之前从来没有用过一样。再回想起当初的目的，只不过是想要用上这个框架而已，深究原理和细节似乎并没有提供太多有用的帮助，这是影响一；其次，有了多次这样的无用功的经验之后，自己明显学乖了，一碰到可能要遇到这样工作量的事情，就本能的开始回避起来，这是影响二。\n\n　　如果影响一还可以通过合理规划目标来解决，那么影响二则是现在困扰自己最大的难题。不知道是不是因为年龄的增长，还是失败的经历让自己情绪过分低落，面对一些挑战的时候，自己开始变得保守和本能的回避起来。比如，每次想要跳槽的时候，总是会想复习算法题，复习深度学习的概念，尽管自己对算法题平时也有在积累，深度学习的工具平时也有在用，但是总是感觉没有全面的学习，总是会有一些遗漏，难以覆盖所有可能的情况。把面试当作综合性测试一样来看待，所以每次准备都会耗费大量的精力，然而却只是经过几次面试就没再派上用场了。这其中自然有自己多少还是有点社交恐惧症，面对陌生的面试者，拼命的想要通过丰富的知识储备来对抗内心的胆怯，但是我似乎有点迷失了重点，因为我开始发现，技术上的储备并不是面试的全部，特别是发现很多技术面，自认为回答的很好，却仍然没能通过，而这后来深思之下，可能只是职位的需求和自身的能力不一定匹配有关，比如说这个职位可能只需要普通能力的即可，或者是薪酬或者是HR等非技术因素导致的。我太执着于技术反而会认为没通过的原因多少还是自己不够完美，尽管我目前这种得过且过的状态，似乎也难言技术上是无可挑剔的。但是胜任工作，我自以为还是绰绰有余的。不过这其中还是有内卷的因素，导致面试造火箭，上班拧螺钉的情况愈来愈严重。总之，种种因素，让自己产生了面试一定要完美准备的倾向，结果反而让自己对频繁的面试产生了抗拒感。这也是为什么，当发现不错的职位的时候，自己并没有忙着投简历，反而只是观望，结果只是平白无故的搁置这件事而已。\n\n　　从这个角度，解决这个问题的方式很明显，其一，就是把浮躁的技术能力进行本质上的提升，无论是工作还是科研，这都是非常核心的能力，不需要通过临时抱佛脚的方式来死记硬背一些深度学习的概念，尽管面试的时候还是经常碰到这些类似八股文的程式化的问题，但是自己并不需要纠结一定要全面的回答出来，毕竟兴趣点和精力决定了，自己只能对某些特定的领域精通，专注于自己曾经做过的项目和内容即可。抓住理论模型和算法实践这两个核心能力，理解为主，实践为辅，不骄不躁，荣辱不惊；无论曾经做的多么半途而废，多么浮躁不堪，现在开始，踏踏实实的看理论，看源码，写代码，获取实实在在的编程能力才是最核心的部分。其次，就是算法题的复习的时间误区，基于平时都在刷题的基础，其实复习起来只需要较短的时间，特别在很多轮的算法题的复习之后，更多的精力需要放到总结上，而非盲目的刷题。最后，除开技术储备，就是正确的看待面试这个问题，除了职位技术的匹配之外，还有公司本身的需求和机遇等因素的影响，很多时候技术以外的影响反而更大一点，虽然我并非那种天赋异禀的人才，能逢面必过。所以我也没必要纠结于那些没有拿到offer的面试，试图在自身上寻找原因，然后再暗自神伤。首先，技术的盲点，面试环节就能马上知道，之后再补上就可以了；其次非技术因素，就算自己怎么苦思冥想也不会有一个可靠的结论，所以与其在这里胡思乱想，还不如积极的投入到下一次面试中去；最后，面试其实并没办法全面的考察能力，有很多偶然的因素在其中，没必要太纠结于自己无法办到的事情，从更加宏观的层面来要求自己，有足够的底气做支撑，做到，不卑不亢，荣辱不惊。我想自己也能心平气和面试了，那样的自己也才算是真正的成长和进步了。\n\n 　　最后，关于做事情的正确心态的问题，前面也说到过，糊里糊涂的蛮干的方式多少还是不行的，起码无法掌握做事情的度，也缺乏一个实际的目标感。更有效率的做事心态应该是在开始做事前，考虑清楚，这件事的目的，需要做到什么效果，牵涉到哪些方面，这些资源是否都准备就绪。。。等等之类的做一番简单的思考，觉得条件都具备了，再开始执行。总的来说，是需要有一个宏观的思考之后，再开始做事，我想那样执行力应该更加游刃有余吧。尽管不一定能每件事都做到这样，但是至少重要的事情，希望自己能有这样一个思考，也算有一个不错的开端。这也正印证了前面的关于规划和执行的讨论。**倾注足够的思考的执行，才是更有效率的达成目标的方式**","source":"_posts/时断时续的执行力反思.md","raw":"---\ntitle: 时断时续的执行力反思\ntoc: true\nmathjax: true\ntop: false\ncover: true\ndate: 2021-03-02 18:50:09\nupdated:\ncategories: 随笔\ntags:\n\t- 随笔\n\t- 反思\n\t- 总结\n---\n\n　　自从去年国庆开始写blog之后，更新过程尤为艰难，兴趣点的转移，执行力时断时续，尽管深知自己有这样三分钟热度的嗜好，但是仍然难以有效利用或者纠正，任由其发展。这与当初设想的定期更新的理想大相径庭，其实要论原因，每一篇随笔里都或多或少有提及，结果上来看，还是执行力不强的问题，心态上看，还是决心和勇气不足。总结出了很多原因，依然无法有效的解决问题，现在打算把这些断断续续的分析成果认真的探讨一下，希望能发现自身存在的根本性的问题。\n\n> Desultory :  going constantly from one subject to another in a halfhearted way \n\n　　最近背单词的时候发现了一个单词非常好的描述了发生在我身上的这种现象，不知道是不是中了年少时候教导的，“要么不做，要么就做到最好”，这个毒鸡汤的影响，再加上总是妄想武侠小说或者各种爽文网剧中，那种不鸣则已，一鸣惊人的那种戏剧性的影响，总是幻想着能厚积薄发，一鼓作气的逆袭。结果就是形成了这股略显中二的行事风格，在年少气盛的时候，没看出哪里有什么不对；只是到了重要的分水岭的时候，便开始畏首畏尾了。一般这种情况碰上拖延症，就这么一直搁置下去了。最后积累到一起，让自己无所适从，不知道如何面对。其实我一直都知道，这种人生重要的选择，都需要倾注足够的精力去分析和思考，只是都被我逃避开了。所以，当一些烦恼和麻烦接二连三的到来时，最终归根溯源，都会追查到这份逃避和懒惰上来。这种心态就像是这样：\n\n> 我在考试该复习前，有时特别想看书闲玩总之是干一切其他的事情。\n>\n> 考试结束后真到了有时间做上述那些事的时候，往往作个几天就又想搞别的了\n\n　　因为思考这种利弊，很费脑力，尽管这些脑力不用也没干什么有意义的其他的什么事情？我又是个比较怕麻烦的人，所以为了暂且的安定，则选择了逃避并搁置这种疑惑。结果来看，则是带来更多更复杂的烦心事。这让我想起最近看到了关于[双重思想](https://yonglezh.github.io/2017/01/21/doublethink/)的文章，关于双重思想形成的4大要素：\n\n>1. 相信两种互相矛盾的信念。（并非同时，而是脑袋里具备两种互相矛盾的信念。）\n>2. 根据情境来切换自己相信的信念。\n>3. 忽略矛盾，认为自己当下的信念就是自己一直以来坚持的信念。\n>4. 忽略自己在忽略矛盾，忽略自己在不断地切换信念。（意识不到自己按不同情境做了不同的选择 —— 这是明知两个信念存在矛盾却不认为自己的行为有矛盾的关键。）\n\n　　我相信暂时的搁置能换来更少的麻烦，同时也相信现在投入一定的思考能让自己未来烦恼变得更少。所以秉承着怕麻烦的总体思路，尽管我能隐约察觉到这两种选择的矛盾之处，但是烦恼这种东西，难以量化，如果不去深思这二者的区别的话，就更加难以分辨。所以，我在快要发现矛盾的时候，及时的选择的忽略矛盾，这便是上述要素3；再则，在不断的切换兴趣点和日常琐事中，完全意识不到自己是在忽略矛盾，因为彻底了忘记了这些问题了，也正好达成了要素4。\n\n　　现实是公平且残酷的，当我都快要忘记这些不想面对的抉择的时候，这些乱七八糟的事情像是约好似的集中朝我走来。小到上班摸鱼，摸到自己都开始厌恶自己，不知道整天在做些什么，开始后悔不如一开始就认真做事，还能有所成长和收获，落得个习惯性的无所事事，还要花费很多精力去纠正这种不良习惯，何况还有没能成长的自信还需要去弥补；大到面对人生的道路的时候，犹犹豫豫，不敢前进，最后拖拖拉拉，搞得自己都没勇气去踏出应该走出的那一步，还不如一开始就走出去，白白积累一段不愉快的自卑的经历。\n\n　　经过这些问题和失败才感同身受的明白了，认真合理的规划自己的人生，才是最省心和麻烦最少的一种方式。就跟写代码一样，以前做OJ的时候，总是爱盲目的开始敲代码，经常发现凭借直觉的代码总是会犯一些最基础的错误，看到这些稚嫩的代码，连自己都为之汗颜；后来才明白，做OJ最主要的部分其实并不是敲代码，反而是思考的过程才是最需要去实践和锻炼的。这么简单的道理，怎么自己却不明白呢？合理的构思代码结构，划分功能模块，化繁为简，分区而治才是简洁高效并优雅的代码的基本保障，明明在学习《算法4》的时候，对这种优雅的API结构化写代码流程极为推崇，为什么到了自己实践的 时候，却也犯糊涂了呢？之后无论是看设计模式，还是学习一些知名的开源项目，都是把这种模块化的思维贯彻到底的一种实践方式，曾经觉得，这些开源项目都是可望不可及的存在，必定是每一行代码都是非常牛逼的存在，学习后才发现，其实功能被切分到非常具体的时候，这些代码就算是我也能轻松写出来；厉害的地方其实在项目架构的模块切分和接口的定义。自己照着这个设计思路一走就会发现，这样的架构其实有很多其他的变种，都能实现大体相同的功能，之所以成为目前的这个架构，都是有很多细节和稳定性以及扩展性的考量的，朝着这个方向去理解，就会发现很多结构和接口的设计就显得合理多了。总之，无论是代码架构还是人生规划，随意的搁置和逃避是非常不划算的选择，因为相比决策阶段的投入，失败成本显然高太多，况且自己并非不愿意去做决策，只是非常单纯的懒继而拖延症愈发严重而导致的，认真去思考，其实自己还是愿意去做的；况且我还是非常享受生活朝着自己规划的方向走的过程的，一点点收益和成果，都会让自己非常满足；就跟写出很棒的代码一样，规划得当，小小的努力就能获得非常巨大的AC快感。\n\n　　所以，发现矛盾要正视，投入一点分析和思考去解决，认真执行，接下来的一点点收获和成果就能指导自己朝着希望的方向前进。任何时候都不要吝惜自己的思考，认识自己，发现自己，认知这个世界，释放自己能量，那才是我想要成为的自我。\n\n　　规划之外，还有执行力的一些习惯问题需要纠正，总的来说，就是自己不够自律。\n\n　　三联生活周刊公众号的一篇关于[自律的文章](https://mp.weixin.qq.com/s/EjPk_1csVjxQFr9PWJABxQ)启发了我，里面所描述的种种现象完美的击中了我，我之所以不敢轻易开启我的自由职业，最大的忧虑就是担心自己不够自律，无论是学英文，学架子鼓，学新的编程语言，学新的开源架构，无一例外都是刚接触的时候信心满满，没坚持几天就开始逃避并放弃，学的好一点，只是因为某些领域天赋还行，短时间内就可以达到可堪够用的程度，但是最后也是由于自满自足，逃不过被搁置的命运。不能否认的是，这些事物中，在经历最初的新鲜感之后，很容易进入一个枯燥无味的瓶颈期，我想，我的兴趣基本都没有逃过瓶颈阶段的围追堵截，除了形势所迫被逼无奈不得不去面对，其他时候基本都停在了这个阶段。回想起来，其实就算在学鼓的瓶颈期，我也并不是很反感重复的基本功练习，顶多就是不那么好玩，只是需要训练的时候，自己习惯性的逃避起来，就开始为不练习寻找理由，就只想到了练鼓的不好玩的那一面，结果就没能坚持下去。对于架子鼓，我还是抱有持续的热情的。只是被更高一级的懒拖给掩盖了。从这个角度来看，保持一幅清醒理智的头脑才是这之中最重要的，也许在电脑前，我更想要去玩游戏，更想要去看电影，吃饭的时候，也愿意无聊的看看游戏直播，虽然我其实并没有那么喜欢看直播。但是，**我本质上，还是喜欢一段时间内，只专注做一件事的人**，所以这段时间内的成果，就显得至关重要。做的很好，那么就有坚持继续下去的动力，如果做得不好，那么很可能就会被搁置下去。以前都是漫不经心的执行着，所以只有一些零星的事情才坚持下来了，现在看来，着实有点可惜，因为这些坚持的下来的事情，要么就是偶然做的很好，要么就是天赋很好，轻松就搞定了。根本还没有轮到努力发挥的空间，特别是对于自己非常渴望的事情，就没能通过努力有所精进。\n\n> 在和乔布斯聊了5分钟后，库克就决定跳槽到苹果。2010年，库克在母校奥本大学的毕业典礼以嘉宾身份发表讲话，他说“我的直觉告诉我，加入苹果是一个千载难逢的机会，可以为这位创意天才工作，也可以加入一个能够重振的伟大的管理团队。”\n>\n> 如果不是对想做的事真心热爱，很难放弃自己稳妥到手的利益，投身到一家岌岌可危的公司。\n>\n> **不是自律让库克成功，而是找到了自己热爱的事情，自己愿意用“苦行僧”式的自律方式把热爱的事变得更好。**\n\n　　对于热爱的事情，我不乏对其想要认真做好的热情，只是没能有效利用自律来把热爱的事情做的更好，只是单纯的觉得热爱的事情会自然的变好。这种天真的想法，似曾相识，恰如当年觉得爱情，只要有满腔热情，后面的事情就自然而然的就会变好。事实并非如此，对于热爱的事情，不仅要有热情，还要合理利用自律的方式来让其变得更好，否则只是无根之木，无源之水，难以为继，白白消耗一番心血。\n\n　　这样想来，其实自己做事一贯如此，总是以为关注最主要的部分，其他的次要问题就会迎刃而解。但是或许我对迎刃而解这个理解是有偏差的，我以为的情况是，什么都不做，这个事情自己会随着主要问题一并被解决；而实际情况，需要为次要问题做一些基本的准备和关注，甚至为了万无一失，需要投入同样重要的资源。似乎我在做事的时候，从来没有思考过，这件事情要做好，需要牵涉到哪几方面的内容，以及需要在这几方面应该如何分配资源做准备。所有的精力完全只放到事情本身上，如果是比较单纯的事情，那么自然就没必要为其他的事情分心；但是，目前面临的事情，很少是比较单纯的，很多事情都会牵涉到很多方面， 这还不包括自己在分析问题的关键点上存在偏差这个可能，所以，成熟可靠的做法，是仔细分析事情的各个方面，判断重点，合理的划分资源。在热爱的事情这个问题上，还需要自律来为其添砖加瓦。\n\n　　在看王小波的杂文的时候，在看哲学经典的时候，在看费曼的《物理学讲义》的时候，在看《算法4》的时候，在看3Blue1Brown的数学视频的时候，在看优雅的开源代码的时候，心中都会涌现出对热爱的事情的渴望，这些内容其实在理解之后，都变得不那么晦涩难懂，反而激发出自己想要做出这样成果的热情。因为我觉得，这些都是非常优秀的成就，我希望自己能在这个方向上走出那怕一小步，我也就心满意足了。\n\n> 当下我们很多人的自律纯属一时兴起，因为缺乏目标感、因为想要逃避真正的困难、或者仅仅觉得自己的生活单调，所以才妄图抓住自律这根救命稻草来把自己拉出泥泞。\n>\n> 结果就是陷入恶性循环：下定决心自律→因为真正目标不清晰而难以坚持不下去→鄙视自己意志力脆弱、厌恶自己→妄图再靠“一时兴起”的自律来拯救自己\n\n　　之前在自律方面一直陷入这样一时兴起的恶性循环中，目标感虽然还算比较明确，但是缺乏一定要去达成的使命感，所以才会想方设法的逃避困难，虽然真正做起来，没有那么复杂，至于长期以往的摸鱼，确实会导致生活单调，想要反弹，跟习惯的斗争，确实是旷日持久，这也当然属于失败成本，而且是属于非常难以纠正的那种。也只能怪自己过分骄傲自大，明明是自己失败了，却还妄图自欺欺人的来在虚荣心上来进行补偿，真是本末倒置，首尾都没能兼顾。\n\n　　摆平心态，自己一手造成的恶果，就要自己努力去修正。自律不是一时兴起，不需要心血来潮的补救，找到自己热爱的事情，发现自己愿意为这份渴望付出到什么程度，用相对应的自律方式将这份热爱的事变的更好，我也就知足了。\n\n　　相信自己，坚定自我。不达目的，誓不罢休。\n\n　　使命感之外，还有一个来自于之前毒鸡汤的后续影响不那么容易察觉，就是内心深处的完美主义。我不知道应该如何正确的看待完美主义这个特质， 有时候觉得完美主义的强迫症会让事情变得更好；有时候却又发现，自己只是无形之中被完美主义给束缚住，感觉自己无法做到理想的高度，就会不停的打退堂鼓。本质上来讲，这应该是自己做事没有明确目标导致的，因为如果自己预定的目标并不是做到最好，那么就算不是完美达成，也不至于丧气，虽然心里多少还是对自己没尽全力感到不快。大部分时候做事，都是看到日程表里有这件事，或者是发现此时此刻应该要做这个事情，糊里糊涂的毫无目的的就上阵了。最明显的案例是，自己以前每次学习新的框架的时候，总是想事无巨细，从框架结构到代码细节都想掌握，再开始使用；结果发现，捣鼓一阵子之后，似乎很久之后都不会再用到这部分知识，直到遇到问题，再回头来看的时候，发现这个框架已经陌生得仿佛之前从来没有用过一样。再回想起当初的目的，只不过是想要用上这个框架而已，深究原理和细节似乎并没有提供太多有用的帮助，这是影响一；其次，有了多次这样的无用功的经验之后，自己明显学乖了，一碰到可能要遇到这样工作量的事情，就本能的开始回避起来，这是影响二。\n\n　　如果影响一还可以通过合理规划目标来解决，那么影响二则是现在困扰自己最大的难题。不知道是不是因为年龄的增长，还是失败的经历让自己情绪过分低落，面对一些挑战的时候，自己开始变得保守和本能的回避起来。比如，每次想要跳槽的时候，总是会想复习算法题，复习深度学习的概念，尽管自己对算法题平时也有在积累，深度学习的工具平时也有在用，但是总是感觉没有全面的学习，总是会有一些遗漏，难以覆盖所有可能的情况。把面试当作综合性测试一样来看待，所以每次准备都会耗费大量的精力，然而却只是经过几次面试就没再派上用场了。这其中自然有自己多少还是有点社交恐惧症，面对陌生的面试者，拼命的想要通过丰富的知识储备来对抗内心的胆怯，但是我似乎有点迷失了重点，因为我开始发现，技术上的储备并不是面试的全部，特别是发现很多技术面，自认为回答的很好，却仍然没能通过，而这后来深思之下，可能只是职位的需求和自身的能力不一定匹配有关，比如说这个职位可能只需要普通能力的即可，或者是薪酬或者是HR等非技术因素导致的。我太执着于技术反而会认为没通过的原因多少还是自己不够完美，尽管我目前这种得过且过的状态，似乎也难言技术上是无可挑剔的。但是胜任工作，我自以为还是绰绰有余的。不过这其中还是有内卷的因素，导致面试造火箭，上班拧螺钉的情况愈来愈严重。总之，种种因素，让自己产生了面试一定要完美准备的倾向，结果反而让自己对频繁的面试产生了抗拒感。这也是为什么，当发现不错的职位的时候，自己并没有忙着投简历，反而只是观望，结果只是平白无故的搁置这件事而已。\n\n　　从这个角度，解决这个问题的方式很明显，其一，就是把浮躁的技术能力进行本质上的提升，无论是工作还是科研，这都是非常核心的能力，不需要通过临时抱佛脚的方式来死记硬背一些深度学习的概念，尽管面试的时候还是经常碰到这些类似八股文的程式化的问题，但是自己并不需要纠结一定要全面的回答出来，毕竟兴趣点和精力决定了，自己只能对某些特定的领域精通，专注于自己曾经做过的项目和内容即可。抓住理论模型和算法实践这两个核心能力，理解为主，实践为辅，不骄不躁，荣辱不惊；无论曾经做的多么半途而废，多么浮躁不堪，现在开始，踏踏实实的看理论，看源码，写代码，获取实实在在的编程能力才是最核心的部分。其次，就是算法题的复习的时间误区，基于平时都在刷题的基础，其实复习起来只需要较短的时间，特别在很多轮的算法题的复习之后，更多的精力需要放到总结上，而非盲目的刷题。最后，除开技术储备，就是正确的看待面试这个问题，除了职位技术的匹配之外，还有公司本身的需求和机遇等因素的影响，很多时候技术以外的影响反而更大一点，虽然我并非那种天赋异禀的人才，能逢面必过。所以我也没必要纠结于那些没有拿到offer的面试，试图在自身上寻找原因，然后再暗自神伤。首先，技术的盲点，面试环节就能马上知道，之后再补上就可以了；其次非技术因素，就算自己怎么苦思冥想也不会有一个可靠的结论，所以与其在这里胡思乱想，还不如积极的投入到下一次面试中去；最后，面试其实并没办法全面的考察能力，有很多偶然的因素在其中，没必要太纠结于自己无法办到的事情，从更加宏观的层面来要求自己，有足够的底气做支撑，做到，不卑不亢，荣辱不惊。我想自己也能心平气和面试了，那样的自己也才算是真正的成长和进步了。\n\n 　　最后，关于做事情的正确心态的问题，前面也说到过，糊里糊涂的蛮干的方式多少还是不行的，起码无法掌握做事情的度，也缺乏一个实际的目标感。更有效率的做事心态应该是在开始做事前，考虑清楚，这件事的目的，需要做到什么效果，牵涉到哪些方面，这些资源是否都准备就绪。。。等等之类的做一番简单的思考，觉得条件都具备了，再开始执行。总的来说，是需要有一个宏观的思考之后，再开始做事，我想那样执行力应该更加游刃有余吧。尽管不一定能每件事都做到这样，但是至少重要的事情，希望自己能有这样一个思考，也算有一个不错的开端。这也正印证了前面的关于规划和执行的讨论。**倾注足够的思考的执行，才是更有效率的达成目标的方式**","slug":"时断时续的执行力反思","published":1,"comments":1,"layout":"post","photos":[],"link":"","_id":"ckowlff59001bye28ctkqepy1","content":"<p>　　自从去年国庆开始写blog之后，更新过程尤为艰难，兴趣点的转移，执行力时断时续，尽管深知自己有这样三分钟热度的嗜好，但是仍然难以有效利用或者纠正，任由其发展。这与当初设想的定期更新的理想大相径庭，其实要论原因，每一篇随笔里都或多或少有提及，结果上来看，还是执行力不强的问题，心态上看，还是决心和勇气不足。总结出了很多原因，依然无法有效的解决问题，现在打算把这些断断续续的分析成果认真的探讨一下，希望能发现自身存在的根本性的问题。</p>\n<blockquote>\n<p>Desultory :  going constantly from one subject to another in a halfhearted way </p>\n</blockquote>\n<p>　　最近背单词的时候发现了一个单词非常好的描述了发生在我身上的这种现象，不知道是不是中了年少时候教导的，“要么不做，要么就做到最好”，这个毒鸡汤的影响，再加上总是妄想武侠小说或者各种爽文网剧中，那种不鸣则已，一鸣惊人的那种戏剧性的影响，总是幻想着能厚积薄发，一鼓作气的逆袭。结果就是形成了这股略显中二的行事风格，在年少气盛的时候，没看出哪里有什么不对；只是到了重要的分水岭的时候，便开始畏首畏尾了。一般这种情况碰上拖延症，就这么一直搁置下去了。最后积累到一起，让自己无所适从，不知道如何面对。其实我一直都知道，这种人生重要的选择，都需要倾注足够的精力去分析和思考，只是都被我逃避开了。所以，当一些烦恼和麻烦接二连三的到来时，最终归根溯源，都会追查到这份逃避和懒惰上来。这种心态就像是这样：</p>\n<blockquote>\n<p>我在考试该复习前，有时特别想看书闲玩总之是干一切其他的事情。</p>\n<p>考试结束后真到了有时间做上述那些事的时候，往往作个几天就又想搞别的了</p>\n</blockquote>\n<p>　　因为思考这种利弊，很费脑力，尽管这些脑力不用也没干什么有意义的其他的什么事情？我又是个比较怕麻烦的人，所以为了暂且的安定，则选择了逃避并搁置这种疑惑。结果来看，则是带来更多更复杂的烦心事。这让我想起最近看到了关于<a href=\"https://yonglezh.github.io/2017/01/21/doublethink/\">双重思想</a>的文章，关于双重思想形成的4大要素：</p>\n<blockquote>\n<ol>\n<li>相信两种互相矛盾的信念。（并非同时，而是脑袋里具备两种互相矛盾的信念。）</li>\n<li>根据情境来切换自己相信的信念。</li>\n<li>忽略矛盾，认为自己当下的信念就是自己一直以来坚持的信念。</li>\n<li>忽略自己在忽略矛盾，忽略自己在不断地切换信念。（意识不到自己按不同情境做了不同的选择 —— 这是明知两个信念存在矛盾却不认为自己的行为有矛盾的关键。）</li>\n</ol>\n</blockquote>\n<p>　　我相信暂时的搁置能换来更少的麻烦，同时也相信现在投入一定的思考能让自己未来烦恼变得更少。所以秉承着怕麻烦的总体思路，尽管我能隐约察觉到这两种选择的矛盾之处，但是烦恼这种东西，难以量化，如果不去深思这二者的区别的话，就更加难以分辨。所以，我在快要发现矛盾的时候，及时的选择的忽略矛盾，这便是上述要素3；再则，在不断的切换兴趣点和日常琐事中，完全意识不到自己是在忽略矛盾，因为彻底了忘记了这些问题了，也正好达成了要素4。</p>\n<p>　　现实是公平且残酷的，当我都快要忘记这些不想面对的抉择的时候，这些乱七八糟的事情像是约好似的集中朝我走来。小到上班摸鱼，摸到自己都开始厌恶自己，不知道整天在做些什么，开始后悔不如一开始就认真做事，还能有所成长和收获，落得个习惯性的无所事事，还要花费很多精力去纠正这种不良习惯，何况还有没能成长的自信还需要去弥补；大到面对人生的道路的时候，犹犹豫豫，不敢前进，最后拖拖拉拉，搞得自己都没勇气去踏出应该走出的那一步，还不如一开始就走出去，白白积累一段不愉快的自卑的经历。</p>\n<p>　　经过这些问题和失败才感同身受的明白了，认真合理的规划自己的人生，才是最省心和麻烦最少的一种方式。就跟写代码一样，以前做OJ的时候，总是爱盲目的开始敲代码，经常发现凭借直觉的代码总是会犯一些最基础的错误，看到这些稚嫩的代码，连自己都为之汗颜；后来才明白，做OJ最主要的部分其实并不是敲代码，反而是思考的过程才是最需要去实践和锻炼的。这么简单的道理，怎么自己却不明白呢？合理的构思代码结构，划分功能模块，化繁为简，分区而治才是简洁高效并优雅的代码的基本保障，明明在学习《算法4》的时候，对这种优雅的API结构化写代码流程极为推崇，为什么到了自己实践的 时候，却也犯糊涂了呢？之后无论是看设计模式，还是学习一些知名的开源项目，都是把这种模块化的思维贯彻到底的一种实践方式，曾经觉得，这些开源项目都是可望不可及的存在，必定是每一行代码都是非常牛逼的存在，学习后才发现，其实功能被切分到非常具体的时候，这些代码就算是我也能轻松写出来；厉害的地方其实在项目架构的模块切分和接口的定义。自己照着这个设计思路一走就会发现，这样的架构其实有很多其他的变种，都能实现大体相同的功能，之所以成为目前的这个架构，都是有很多细节和稳定性以及扩展性的考量的，朝着这个方向去理解，就会发现很多结构和接口的设计就显得合理多了。总之，无论是代码架构还是人生规划，随意的搁置和逃避是非常不划算的选择，因为相比决策阶段的投入，失败成本显然高太多，况且自己并非不愿意去做决策，只是非常单纯的懒继而拖延症愈发严重而导致的，认真去思考，其实自己还是愿意去做的；况且我还是非常享受生活朝着自己规划的方向走的过程的，一点点收益和成果，都会让自己非常满足；就跟写出很棒的代码一样，规划得当，小小的努力就能获得非常巨大的AC快感。</p>\n<p>　　所以，发现矛盾要正视，投入一点分析和思考去解决，认真执行，接下来的一点点收获和成果就能指导自己朝着希望的方向前进。任何时候都不要吝惜自己的思考，认识自己，发现自己，认知这个世界，释放自己能量，那才是我想要成为的自我。</p>\n<p>　　规划之外，还有执行力的一些习惯问题需要纠正，总的来说，就是自己不够自律。</p>\n<p>　　三联生活周刊公众号的一篇关于<a href=\"https://mp.weixin.qq.com/s/EjPk_1csVjxQFr9PWJABxQ\">自律的文章</a>启发了我，里面所描述的种种现象完美的击中了我，我之所以不敢轻易开启我的自由职业，最大的忧虑就是担心自己不够自律，无论是学英文，学架子鼓，学新的编程语言，学新的开源架构，无一例外都是刚接触的时候信心满满，没坚持几天就开始逃避并放弃，学的好一点，只是因为某些领域天赋还行，短时间内就可以达到可堪够用的程度，但是最后也是由于自满自足，逃不过被搁置的命运。不能否认的是，这些事物中，在经历最初的新鲜感之后，很容易进入一个枯燥无味的瓶颈期，我想，我的兴趣基本都没有逃过瓶颈阶段的围追堵截，除了形势所迫被逼无奈不得不去面对，其他时候基本都停在了这个阶段。回想起来，其实就算在学鼓的瓶颈期，我也并不是很反感重复的基本功练习，顶多就是不那么好玩，只是需要训练的时候，自己习惯性的逃避起来，就开始为不练习寻找理由，就只想到了练鼓的不好玩的那一面，结果就没能坚持下去。对于架子鼓，我还是抱有持续的热情的。只是被更高一级的懒拖给掩盖了。从这个角度来看，保持一幅清醒理智的头脑才是这之中最重要的，也许在电脑前，我更想要去玩游戏，更想要去看电影，吃饭的时候，也愿意无聊的看看游戏直播，虽然我其实并没有那么喜欢看直播。但是，<strong>我本质上，还是喜欢一段时间内，只专注做一件事的人</strong>，所以这段时间内的成果，就显得至关重要。做的很好，那么就有坚持继续下去的动力，如果做得不好，那么很可能就会被搁置下去。以前都是漫不经心的执行着，所以只有一些零星的事情才坚持下来了，现在看来，着实有点可惜，因为这些坚持的下来的事情，要么就是偶然做的很好，要么就是天赋很好，轻松就搞定了。根本还没有轮到努力发挥的空间，特别是对于自己非常渴望的事情，就没能通过努力有所精进。</p>\n<blockquote>\n<p>在和乔布斯聊了5分钟后，库克就决定跳槽到苹果。2010年，库克在母校奥本大学的毕业典礼以嘉宾身份发表讲话，他说“我的直觉告诉我，加入苹果是一个千载难逢的机会，可以为这位创意天才工作，也可以加入一个能够重振的伟大的管理团队。”</p>\n<p>如果不是对想做的事真心热爱，很难放弃自己稳妥到手的利益，投身到一家岌岌可危的公司。</p>\n<p><strong>不是自律让库克成功，而是找到了自己热爱的事情，自己愿意用“苦行僧”式的自律方式把热爱的事变得更好。</strong></p>\n</blockquote>\n<p>　　对于热爱的事情，我不乏对其想要认真做好的热情，只是没能有效利用自律来把热爱的事情做的更好，只是单纯的觉得热爱的事情会自然的变好。这种天真的想法，似曾相识，恰如当年觉得爱情，只要有满腔热情，后面的事情就自然而然的就会变好。事实并非如此，对于热爱的事情，不仅要有热情，还要合理利用自律的方式来让其变得更好，否则只是无根之木，无源之水，难以为继，白白消耗一番心血。</p>\n<p>　　这样想来，其实自己做事一贯如此，总是以为关注最主要的部分，其他的次要问题就会迎刃而解。但是或许我对迎刃而解这个理解是有偏差的，我以为的情况是，什么都不做，这个事情自己会随着主要问题一并被解决；而实际情况，需要为次要问题做一些基本的准备和关注，甚至为了万无一失，需要投入同样重要的资源。似乎我在做事的时候，从来没有思考过，这件事情要做好，需要牵涉到哪几方面的内容，以及需要在这几方面应该如何分配资源做准备。所有的精力完全只放到事情本身上，如果是比较单纯的事情，那么自然就没必要为其他的事情分心；但是，目前面临的事情，很少是比较单纯的，很多事情都会牵涉到很多方面， 这还不包括自己在分析问题的关键点上存在偏差这个可能，所以，成熟可靠的做法，是仔细分析事情的各个方面，判断重点，合理的划分资源。在热爱的事情这个问题上，还需要自律来为其添砖加瓦。</p>\n<p>　　在看王小波的杂文的时候，在看哲学经典的时候，在看费曼的《物理学讲义》的时候，在看《算法4》的时候，在看3Blue1Brown的数学视频的时候，在看优雅的开源代码的时候，心中都会涌现出对热爱的事情的渴望，这些内容其实在理解之后，都变得不那么晦涩难懂，反而激发出自己想要做出这样成果的热情。因为我觉得，这些都是非常优秀的成就，我希望自己能在这个方向上走出那怕一小步，我也就心满意足了。</p>\n<blockquote>\n<p>当下我们很多人的自律纯属一时兴起，因为缺乏目标感、因为想要逃避真正的困难、或者仅仅觉得自己的生活单调，所以才妄图抓住自律这根救命稻草来把自己拉出泥泞。</p>\n<p>结果就是陷入恶性循环：下定决心自律→因为真正目标不清晰而难以坚持不下去→鄙视自己意志力脆弱、厌恶自己→妄图再靠“一时兴起”的自律来拯救自己</p>\n</blockquote>\n<p>　　之前在自律方面一直陷入这样一时兴起的恶性循环中，目标感虽然还算比较明确，但是缺乏一定要去达成的使命感，所以才会想方设法的逃避困难，虽然真正做起来，没有那么复杂，至于长期以往的摸鱼，确实会导致生活单调，想要反弹，跟习惯的斗争，确实是旷日持久，这也当然属于失败成本，而且是属于非常难以纠正的那种。也只能怪自己过分骄傲自大，明明是自己失败了，却还妄图自欺欺人的来在虚荣心上来进行补偿，真是本末倒置，首尾都没能兼顾。</p>\n<p>　　摆平心态，自己一手造成的恶果，就要自己努力去修正。自律不是一时兴起，不需要心血来潮的补救，找到自己热爱的事情，发现自己愿意为这份渴望付出到什么程度，用相对应的自律方式将这份热爱的事变的更好，我也就知足了。</p>\n<p>　　相信自己，坚定自我。不达目的，誓不罢休。</p>\n<p>　　使命感之外，还有一个来自于之前毒鸡汤的后续影响不那么容易察觉，就是内心深处的完美主义。我不知道应该如何正确的看待完美主义这个特质， 有时候觉得完美主义的强迫症会让事情变得更好；有时候却又发现，自己只是无形之中被完美主义给束缚住，感觉自己无法做到理想的高度，就会不停的打退堂鼓。本质上来讲，这应该是自己做事没有明确目标导致的，因为如果自己预定的目标并不是做到最好，那么就算不是完美达成，也不至于丧气，虽然心里多少还是对自己没尽全力感到不快。大部分时候做事，都是看到日程表里有这件事，或者是发现此时此刻应该要做这个事情，糊里糊涂的毫无目的的就上阵了。最明显的案例是，自己以前每次学习新的框架的时候，总是想事无巨细，从框架结构到代码细节都想掌握，再开始使用；结果发现，捣鼓一阵子之后，似乎很久之后都不会再用到这部分知识，直到遇到问题，再回头来看的时候，发现这个框架已经陌生得仿佛之前从来没有用过一样。再回想起当初的目的，只不过是想要用上这个框架而已，深究原理和细节似乎并没有提供太多有用的帮助，这是影响一；其次，有了多次这样的无用功的经验之后，自己明显学乖了，一碰到可能要遇到这样工作量的事情，就本能的开始回避起来，这是影响二。</p>\n<p>　　如果影响一还可以通过合理规划目标来解决，那么影响二则是现在困扰自己最大的难题。不知道是不是因为年龄的增长，还是失败的经历让自己情绪过分低落，面对一些挑战的时候，自己开始变得保守和本能的回避起来。比如，每次想要跳槽的时候，总是会想复习算法题，复习深度学习的概念，尽管自己对算法题平时也有在积累，深度学习的工具平时也有在用，但是总是感觉没有全面的学习，总是会有一些遗漏，难以覆盖所有可能的情况。把面试当作综合性测试一样来看待，所以每次准备都会耗费大量的精力，然而却只是经过几次面试就没再派上用场了。这其中自然有自己多少还是有点社交恐惧症，面对陌生的面试者，拼命的想要通过丰富的知识储备来对抗内心的胆怯，但是我似乎有点迷失了重点，因为我开始发现，技术上的储备并不是面试的全部，特别是发现很多技术面，自认为回答的很好，却仍然没能通过，而这后来深思之下，可能只是职位的需求和自身的能力不一定匹配有关，比如说这个职位可能只需要普通能力的即可，或者是薪酬或者是HR等非技术因素导致的。我太执着于技术反而会认为没通过的原因多少还是自己不够完美，尽管我目前这种得过且过的状态，似乎也难言技术上是无可挑剔的。但是胜任工作，我自以为还是绰绰有余的。不过这其中还是有内卷的因素，导致面试造火箭，上班拧螺钉的情况愈来愈严重。总之，种种因素，让自己产生了面试一定要完美准备的倾向，结果反而让自己对频繁的面试产生了抗拒感。这也是为什么，当发现不错的职位的时候，自己并没有忙着投简历，反而只是观望，结果只是平白无故的搁置这件事而已。</p>\n<p>　　从这个角度，解决这个问题的方式很明显，其一，就是把浮躁的技术能力进行本质上的提升，无论是工作还是科研，这都是非常核心的能力，不需要通过临时抱佛脚的方式来死记硬背一些深度学习的概念，尽管面试的时候还是经常碰到这些类似八股文的程式化的问题，但是自己并不需要纠结一定要全面的回答出来，毕竟兴趣点和精力决定了，自己只能对某些特定的领域精通，专注于自己曾经做过的项目和内容即可。抓住理论模型和算法实践这两个核心能力，理解为主，实践为辅，不骄不躁，荣辱不惊；无论曾经做的多么半途而废，多么浮躁不堪，现在开始，踏踏实实的看理论，看源码，写代码，获取实实在在的编程能力才是最核心的部分。其次，就是算法题的复习的时间误区，基于平时都在刷题的基础，其实复习起来只需要较短的时间，特别在很多轮的算法题的复习之后，更多的精力需要放到总结上，而非盲目的刷题。最后，除开技术储备，就是正确的看待面试这个问题，除了职位技术的匹配之外，还有公司本身的需求和机遇等因素的影响，很多时候技术以外的影响反而更大一点，虽然我并非那种天赋异禀的人才，能逢面必过。所以我也没必要纠结于那些没有拿到offer的面试，试图在自身上寻找原因，然后再暗自神伤。首先，技术的盲点，面试环节就能马上知道，之后再补上就可以了；其次非技术因素，就算自己怎么苦思冥想也不会有一个可靠的结论，所以与其在这里胡思乱想，还不如积极的投入到下一次面试中去；最后，面试其实并没办法全面的考察能力，有很多偶然的因素在其中，没必要太纠结于自己无法办到的事情，从更加宏观的层面来要求自己，有足够的底气做支撑，做到，不卑不亢，荣辱不惊。我想自己也能心平气和面试了，那样的自己也才算是真正的成长和进步了。</p>\n<p> 　　最后，关于做事情的正确心态的问题，前面也说到过，糊里糊涂的蛮干的方式多少还是不行的，起码无法掌握做事情的度，也缺乏一个实际的目标感。更有效率的做事心态应该是在开始做事前，考虑清楚，这件事的目的，需要做到什么效果，牵涉到哪些方面，这些资源是否都准备就绪。。。等等之类的做一番简单的思考，觉得条件都具备了，再开始执行。总的来说，是需要有一个宏观的思考之后，再开始做事，我想那样执行力应该更加游刃有余吧。尽管不一定能每件事都做到这样，但是至少重要的事情，希望自己能有这样一个思考，也算有一个不错的开端。这也正印证了前面的关于规划和执行的讨论。<strong>倾注足够的思考的执行，才是更有效率的达成目标的方式</strong></p>\n","site":{"data":{}},"excerpt":"","more":"<p>　　自从去年国庆开始写blog之后，更新过程尤为艰难，兴趣点的转移，执行力时断时续，尽管深知自己有这样三分钟热度的嗜好，但是仍然难以有效利用或者纠正，任由其发展。这与当初设想的定期更新的理想大相径庭，其实要论原因，每一篇随笔里都或多或少有提及，结果上来看，还是执行力不强的问题，心态上看，还是决心和勇气不足。总结出了很多原因，依然无法有效的解决问题，现在打算把这些断断续续的分析成果认真的探讨一下，希望能发现自身存在的根本性的问题。</p>\n<blockquote>\n<p>Desultory :  going constantly from one subject to another in a halfhearted way </p>\n</blockquote>\n<p>　　最近背单词的时候发现了一个单词非常好的描述了发生在我身上的这种现象，不知道是不是中了年少时候教导的，“要么不做，要么就做到最好”，这个毒鸡汤的影响，再加上总是妄想武侠小说或者各种爽文网剧中，那种不鸣则已，一鸣惊人的那种戏剧性的影响，总是幻想着能厚积薄发，一鼓作气的逆袭。结果就是形成了这股略显中二的行事风格，在年少气盛的时候，没看出哪里有什么不对；只是到了重要的分水岭的时候，便开始畏首畏尾了。一般这种情况碰上拖延症，就这么一直搁置下去了。最后积累到一起，让自己无所适从，不知道如何面对。其实我一直都知道，这种人生重要的选择，都需要倾注足够的精力去分析和思考，只是都被我逃避开了。所以，当一些烦恼和麻烦接二连三的到来时，最终归根溯源，都会追查到这份逃避和懒惰上来。这种心态就像是这样：</p>\n<blockquote>\n<p>我在考试该复习前，有时特别想看书闲玩总之是干一切其他的事情。</p>\n<p>考试结束后真到了有时间做上述那些事的时候，往往作个几天就又想搞别的了</p>\n</blockquote>\n<p>　　因为思考这种利弊，很费脑力，尽管这些脑力不用也没干什么有意义的其他的什么事情？我又是个比较怕麻烦的人，所以为了暂且的安定，则选择了逃避并搁置这种疑惑。结果来看，则是带来更多更复杂的烦心事。这让我想起最近看到了关于<a href=\"https://yonglezh.github.io/2017/01/21/doublethink/\">双重思想</a>的文章，关于双重思想形成的4大要素：</p>\n<blockquote>\n<ol>\n<li>相信两种互相矛盾的信念。（并非同时，而是脑袋里具备两种互相矛盾的信念。）</li>\n<li>根据情境来切换自己相信的信念。</li>\n<li>忽略矛盾，认为自己当下的信念就是自己一直以来坚持的信念。</li>\n<li>忽略自己在忽略矛盾，忽略自己在不断地切换信念。（意识不到自己按不同情境做了不同的选择 —— 这是明知两个信念存在矛盾却不认为自己的行为有矛盾的关键。）</li>\n</ol>\n</blockquote>\n<p>　　我相信暂时的搁置能换来更少的麻烦，同时也相信现在投入一定的思考能让自己未来烦恼变得更少。所以秉承着怕麻烦的总体思路，尽管我能隐约察觉到这两种选择的矛盾之处，但是烦恼这种东西，难以量化，如果不去深思这二者的区别的话，就更加难以分辨。所以，我在快要发现矛盾的时候，及时的选择的忽略矛盾，这便是上述要素3；再则，在不断的切换兴趣点和日常琐事中，完全意识不到自己是在忽略矛盾，因为彻底了忘记了这些问题了，也正好达成了要素4。</p>\n<p>　　现实是公平且残酷的，当我都快要忘记这些不想面对的抉择的时候，这些乱七八糟的事情像是约好似的集中朝我走来。小到上班摸鱼，摸到自己都开始厌恶自己，不知道整天在做些什么，开始后悔不如一开始就认真做事，还能有所成长和收获，落得个习惯性的无所事事，还要花费很多精力去纠正这种不良习惯，何况还有没能成长的自信还需要去弥补；大到面对人生的道路的时候，犹犹豫豫，不敢前进，最后拖拖拉拉，搞得自己都没勇气去踏出应该走出的那一步，还不如一开始就走出去，白白积累一段不愉快的自卑的经历。</p>\n<p>　　经过这些问题和失败才感同身受的明白了，认真合理的规划自己的人生，才是最省心和麻烦最少的一种方式。就跟写代码一样，以前做OJ的时候，总是爱盲目的开始敲代码，经常发现凭借直觉的代码总是会犯一些最基础的错误，看到这些稚嫩的代码，连自己都为之汗颜；后来才明白，做OJ最主要的部分其实并不是敲代码，反而是思考的过程才是最需要去实践和锻炼的。这么简单的道理，怎么自己却不明白呢？合理的构思代码结构，划分功能模块，化繁为简，分区而治才是简洁高效并优雅的代码的基本保障，明明在学习《算法4》的时候，对这种优雅的API结构化写代码流程极为推崇，为什么到了自己实践的 时候，却也犯糊涂了呢？之后无论是看设计模式，还是学习一些知名的开源项目，都是把这种模块化的思维贯彻到底的一种实践方式，曾经觉得，这些开源项目都是可望不可及的存在，必定是每一行代码都是非常牛逼的存在，学习后才发现，其实功能被切分到非常具体的时候，这些代码就算是我也能轻松写出来；厉害的地方其实在项目架构的模块切分和接口的定义。自己照着这个设计思路一走就会发现，这样的架构其实有很多其他的变种，都能实现大体相同的功能，之所以成为目前的这个架构，都是有很多细节和稳定性以及扩展性的考量的，朝着这个方向去理解，就会发现很多结构和接口的设计就显得合理多了。总之，无论是代码架构还是人生规划，随意的搁置和逃避是非常不划算的选择，因为相比决策阶段的投入，失败成本显然高太多，况且自己并非不愿意去做决策，只是非常单纯的懒继而拖延症愈发严重而导致的，认真去思考，其实自己还是愿意去做的；况且我还是非常享受生活朝着自己规划的方向走的过程的，一点点收益和成果，都会让自己非常满足；就跟写出很棒的代码一样，规划得当，小小的努力就能获得非常巨大的AC快感。</p>\n<p>　　所以，发现矛盾要正视，投入一点分析和思考去解决，认真执行，接下来的一点点收获和成果就能指导自己朝着希望的方向前进。任何时候都不要吝惜自己的思考，认识自己，发现自己，认知这个世界，释放自己能量，那才是我想要成为的自我。</p>\n<p>　　规划之外，还有执行力的一些习惯问题需要纠正，总的来说，就是自己不够自律。</p>\n<p>　　三联生活周刊公众号的一篇关于<a href=\"https://mp.weixin.qq.com/s/EjPk_1csVjxQFr9PWJABxQ\">自律的文章</a>启发了我，里面所描述的种种现象完美的击中了我，我之所以不敢轻易开启我的自由职业，最大的忧虑就是担心自己不够自律，无论是学英文，学架子鼓，学新的编程语言，学新的开源架构，无一例外都是刚接触的时候信心满满，没坚持几天就开始逃避并放弃，学的好一点，只是因为某些领域天赋还行，短时间内就可以达到可堪够用的程度，但是最后也是由于自满自足，逃不过被搁置的命运。不能否认的是，这些事物中，在经历最初的新鲜感之后，很容易进入一个枯燥无味的瓶颈期，我想，我的兴趣基本都没有逃过瓶颈阶段的围追堵截，除了形势所迫被逼无奈不得不去面对，其他时候基本都停在了这个阶段。回想起来，其实就算在学鼓的瓶颈期，我也并不是很反感重复的基本功练习，顶多就是不那么好玩，只是需要训练的时候，自己习惯性的逃避起来，就开始为不练习寻找理由，就只想到了练鼓的不好玩的那一面，结果就没能坚持下去。对于架子鼓，我还是抱有持续的热情的。只是被更高一级的懒拖给掩盖了。从这个角度来看，保持一幅清醒理智的头脑才是这之中最重要的，也许在电脑前，我更想要去玩游戏，更想要去看电影，吃饭的时候，也愿意无聊的看看游戏直播，虽然我其实并没有那么喜欢看直播。但是，<strong>我本质上，还是喜欢一段时间内，只专注做一件事的人</strong>，所以这段时间内的成果，就显得至关重要。做的很好，那么就有坚持继续下去的动力，如果做得不好，那么很可能就会被搁置下去。以前都是漫不经心的执行着，所以只有一些零星的事情才坚持下来了，现在看来，着实有点可惜，因为这些坚持的下来的事情，要么就是偶然做的很好，要么就是天赋很好，轻松就搞定了。根本还没有轮到努力发挥的空间，特别是对于自己非常渴望的事情，就没能通过努力有所精进。</p>\n<blockquote>\n<p>在和乔布斯聊了5分钟后，库克就决定跳槽到苹果。2010年，库克在母校奥本大学的毕业典礼以嘉宾身份发表讲话，他说“我的直觉告诉我，加入苹果是一个千载难逢的机会，可以为这位创意天才工作，也可以加入一个能够重振的伟大的管理团队。”</p>\n<p>如果不是对想做的事真心热爱，很难放弃自己稳妥到手的利益，投身到一家岌岌可危的公司。</p>\n<p><strong>不是自律让库克成功，而是找到了自己热爱的事情，自己愿意用“苦行僧”式的自律方式把热爱的事变得更好。</strong></p>\n</blockquote>\n<p>　　对于热爱的事情，我不乏对其想要认真做好的热情，只是没能有效利用自律来把热爱的事情做的更好，只是单纯的觉得热爱的事情会自然的变好。这种天真的想法，似曾相识，恰如当年觉得爱情，只要有满腔热情，后面的事情就自然而然的就会变好。事实并非如此，对于热爱的事情，不仅要有热情，还要合理利用自律的方式来让其变得更好，否则只是无根之木，无源之水，难以为继，白白消耗一番心血。</p>\n<p>　　这样想来，其实自己做事一贯如此，总是以为关注最主要的部分，其他的次要问题就会迎刃而解。但是或许我对迎刃而解这个理解是有偏差的，我以为的情况是，什么都不做，这个事情自己会随着主要问题一并被解决；而实际情况，需要为次要问题做一些基本的准备和关注，甚至为了万无一失，需要投入同样重要的资源。似乎我在做事的时候，从来没有思考过，这件事情要做好，需要牵涉到哪几方面的内容，以及需要在这几方面应该如何分配资源做准备。所有的精力完全只放到事情本身上，如果是比较单纯的事情，那么自然就没必要为其他的事情分心；但是，目前面临的事情，很少是比较单纯的，很多事情都会牵涉到很多方面， 这还不包括自己在分析问题的关键点上存在偏差这个可能，所以，成熟可靠的做法，是仔细分析事情的各个方面，判断重点，合理的划分资源。在热爱的事情这个问题上，还需要自律来为其添砖加瓦。</p>\n<p>　　在看王小波的杂文的时候，在看哲学经典的时候，在看费曼的《物理学讲义》的时候，在看《算法4》的时候，在看3Blue1Brown的数学视频的时候，在看优雅的开源代码的时候，心中都会涌现出对热爱的事情的渴望，这些内容其实在理解之后，都变得不那么晦涩难懂，反而激发出自己想要做出这样成果的热情。因为我觉得，这些都是非常优秀的成就，我希望自己能在这个方向上走出那怕一小步，我也就心满意足了。</p>\n<blockquote>\n<p>当下我们很多人的自律纯属一时兴起，因为缺乏目标感、因为想要逃避真正的困难、或者仅仅觉得自己的生活单调，所以才妄图抓住自律这根救命稻草来把自己拉出泥泞。</p>\n<p>结果就是陷入恶性循环：下定决心自律→因为真正目标不清晰而难以坚持不下去→鄙视自己意志力脆弱、厌恶自己→妄图再靠“一时兴起”的自律来拯救自己</p>\n</blockquote>\n<p>　　之前在自律方面一直陷入这样一时兴起的恶性循环中，目标感虽然还算比较明确，但是缺乏一定要去达成的使命感，所以才会想方设法的逃避困难，虽然真正做起来，没有那么复杂，至于长期以往的摸鱼，确实会导致生活单调，想要反弹，跟习惯的斗争，确实是旷日持久，这也当然属于失败成本，而且是属于非常难以纠正的那种。也只能怪自己过分骄傲自大，明明是自己失败了，却还妄图自欺欺人的来在虚荣心上来进行补偿，真是本末倒置，首尾都没能兼顾。</p>\n<p>　　摆平心态，自己一手造成的恶果，就要自己努力去修正。自律不是一时兴起，不需要心血来潮的补救，找到自己热爱的事情，发现自己愿意为这份渴望付出到什么程度，用相对应的自律方式将这份热爱的事变的更好，我也就知足了。</p>\n<p>　　相信自己，坚定自我。不达目的，誓不罢休。</p>\n<p>　　使命感之外，还有一个来自于之前毒鸡汤的后续影响不那么容易察觉，就是内心深处的完美主义。我不知道应该如何正确的看待完美主义这个特质， 有时候觉得完美主义的强迫症会让事情变得更好；有时候却又发现，自己只是无形之中被完美主义给束缚住，感觉自己无法做到理想的高度，就会不停的打退堂鼓。本质上来讲，这应该是自己做事没有明确目标导致的，因为如果自己预定的目标并不是做到最好，那么就算不是完美达成，也不至于丧气，虽然心里多少还是对自己没尽全力感到不快。大部分时候做事，都是看到日程表里有这件事，或者是发现此时此刻应该要做这个事情，糊里糊涂的毫无目的的就上阵了。最明显的案例是，自己以前每次学习新的框架的时候，总是想事无巨细，从框架结构到代码细节都想掌握，再开始使用；结果发现，捣鼓一阵子之后，似乎很久之后都不会再用到这部分知识，直到遇到问题，再回头来看的时候，发现这个框架已经陌生得仿佛之前从来没有用过一样。再回想起当初的目的，只不过是想要用上这个框架而已，深究原理和细节似乎并没有提供太多有用的帮助，这是影响一；其次，有了多次这样的无用功的经验之后，自己明显学乖了，一碰到可能要遇到这样工作量的事情，就本能的开始回避起来，这是影响二。</p>\n<p>　　如果影响一还可以通过合理规划目标来解决，那么影响二则是现在困扰自己最大的难题。不知道是不是因为年龄的增长，还是失败的经历让自己情绪过分低落，面对一些挑战的时候，自己开始变得保守和本能的回避起来。比如，每次想要跳槽的时候，总是会想复习算法题，复习深度学习的概念，尽管自己对算法题平时也有在积累，深度学习的工具平时也有在用，但是总是感觉没有全面的学习，总是会有一些遗漏，难以覆盖所有可能的情况。把面试当作综合性测试一样来看待，所以每次准备都会耗费大量的精力，然而却只是经过几次面试就没再派上用场了。这其中自然有自己多少还是有点社交恐惧症，面对陌生的面试者，拼命的想要通过丰富的知识储备来对抗内心的胆怯，但是我似乎有点迷失了重点，因为我开始发现，技术上的储备并不是面试的全部，特别是发现很多技术面，自认为回答的很好，却仍然没能通过，而这后来深思之下，可能只是职位的需求和自身的能力不一定匹配有关，比如说这个职位可能只需要普通能力的即可，或者是薪酬或者是HR等非技术因素导致的。我太执着于技术反而会认为没通过的原因多少还是自己不够完美，尽管我目前这种得过且过的状态，似乎也难言技术上是无可挑剔的。但是胜任工作，我自以为还是绰绰有余的。不过这其中还是有内卷的因素，导致面试造火箭，上班拧螺钉的情况愈来愈严重。总之，种种因素，让自己产生了面试一定要完美准备的倾向，结果反而让自己对频繁的面试产生了抗拒感。这也是为什么，当发现不错的职位的时候，自己并没有忙着投简历，反而只是观望，结果只是平白无故的搁置这件事而已。</p>\n<p>　　从这个角度，解决这个问题的方式很明显，其一，就是把浮躁的技术能力进行本质上的提升，无论是工作还是科研，这都是非常核心的能力，不需要通过临时抱佛脚的方式来死记硬背一些深度学习的概念，尽管面试的时候还是经常碰到这些类似八股文的程式化的问题，但是自己并不需要纠结一定要全面的回答出来，毕竟兴趣点和精力决定了，自己只能对某些特定的领域精通，专注于自己曾经做过的项目和内容即可。抓住理论模型和算法实践这两个核心能力，理解为主，实践为辅，不骄不躁，荣辱不惊；无论曾经做的多么半途而废，多么浮躁不堪，现在开始，踏踏实实的看理论，看源码，写代码，获取实实在在的编程能力才是最核心的部分。其次，就是算法题的复习的时间误区，基于平时都在刷题的基础，其实复习起来只需要较短的时间，特别在很多轮的算法题的复习之后，更多的精力需要放到总结上，而非盲目的刷题。最后，除开技术储备，就是正确的看待面试这个问题，除了职位技术的匹配之外，还有公司本身的需求和机遇等因素的影响，很多时候技术以外的影响反而更大一点，虽然我并非那种天赋异禀的人才，能逢面必过。所以我也没必要纠结于那些没有拿到offer的面试，试图在自身上寻找原因，然后再暗自神伤。首先，技术的盲点，面试环节就能马上知道，之后再补上就可以了；其次非技术因素，就算自己怎么苦思冥想也不会有一个可靠的结论，所以与其在这里胡思乱想，还不如积极的投入到下一次面试中去；最后，面试其实并没办法全面的考察能力，有很多偶然的因素在其中，没必要太纠结于自己无法办到的事情，从更加宏观的层面来要求自己，有足够的底气做支撑，做到，不卑不亢，荣辱不惊。我想自己也能心平气和面试了，那样的自己也才算是真正的成长和进步了。</p>\n<p> 　　最后，关于做事情的正确心态的问题，前面也说到过，糊里糊涂的蛮干的方式多少还是不行的，起码无法掌握做事情的度，也缺乏一个实际的目标感。更有效率的做事心态应该是在开始做事前，考虑清楚，这件事的目的，需要做到什么效果，牵涉到哪些方面，这些资源是否都准备就绪。。。等等之类的做一番简单的思考，觉得条件都具备了，再开始执行。总的来说，是需要有一个宏观的思考之后，再开始做事，我想那样执行力应该更加游刃有余吧。尽管不一定能每件事都做到这样，但是至少重要的事情，希望自己能有这样一个思考，也算有一个不错的开端。这也正印证了前面的关于规划和执行的讨论。<strong>倾注足够的思考的执行，才是更有效率的达成目标的方式</strong></p>\n"}],"PostAsset":[{"_id":"source/_posts/my-first-blog/Euphonium_Movie_2nd_KV.jpg","slug":"Euphonium_Movie_2nd_KV.jpg","post":"ckg7qjivj00037j28eqt5h8o6","modified":0,"renderable":0},{"_id":"source/_posts/my-first-blog/Euphonium_Movie_Finale_KV2.jpg","slug":"Euphonium_Movie_Finale_KV2.jpg","post":"ckg7qjivj00037j28eqt5h8o6","modified":0,"renderable":0},{"_id":"source/_posts/my-first-blog/relife-1.png","slug":"relife-1.png","post":"ckg7qjivj00037j28eqt5h8o6","modified":0,"renderable":0},{"_id":"source/_posts/my-first-blog/relife-2.jpg","slug":"relife-2.jpg","post":"ckg7qjivj00037j28eqt5h8o6","modified":0,"renderable":0},{"_id":"source/_posts/改变与懒惰/2020-07-31.jpg","slug":"2020-07-31.jpg","post":"ckgt6w3by0000wx280fap2hbl","modified":0,"renderable":0},{"_id":"source/_posts/改变与懒惰/IMG_9373.jpeg","slug":"IMG_9373.jpeg","post":"ckgt6w3by0000wx280fap2hbl","modified":0,"renderable":0},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img1.png","slug":"img1.png","post":"ckhlu9lv60000w328appuchpc","modified":0,"renderable":0},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img2.jpg","slug":"img2.jpg","post":"ckhlu9lv60000w328appuchpc","modified":0,"renderable":0},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img3.jpg","slug":"img3.jpg","post":"ckhlu9lv60000w328appuchpc","modified":0,"renderable":0},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img6.jpg","slug":"img6.jpg","post":"ckhlu9lv60000w328appuchpc","modified":0,"renderable":0},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img7.jpg","slug":"img7.jpg","post":"ckhlu9lv60000w328appuchpc","modified":0,"renderable":0},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img8.jpg","slug":"img8.jpg","post":"ckhlu9lv60000w328appuchpc","modified":0,"renderable":0},{"_id":"source/_posts/同步和异步编程-如何并行写同一个日志文件实践/1.jpeg","slug":"1.jpeg","post":"ckhvqydbn00006z28dot614mu","modified":0,"renderable":0},{"_id":"source/_posts/同步和异步编程-如何并行写同一个日志文件实践/2.jpeg","slug":"2.jpeg","post":"ckhvqydbn00006z28dot614mu","modified":0,"renderable":0},{"_id":"source/_posts/GPU-in-Pytorch-并行和分布式实践/1.png","slug":"1.png","post":"ckhyotdja0000l9289pxq20m8","modified":0,"renderable":0},{"_id":"source/_posts/132-Palindrome-Partitioning-II/1.png","slug":"1.png","post":"ckisn8crf0000vy28agpvc03h","modified":0,"renderable":0},{"_id":"source/_posts/132-Palindrome-Partitioning-II/2.png","slug":"2.png","post":"ckisn8crf0000vy28agpvc03h","modified":0,"renderable":0},{"_id":"source/_posts/132-Palindrome-Partitioning-II/3.png","slug":"3.png","post":"ckisn8crf0000vy28agpvc03h","modified":0,"renderable":0},{"_id":"source/_posts/132-Palindrome-Partitioning-II/4.png","slug":"4.png","post":"ckisn8crf0000vy28agpvc03h","modified":0,"renderable":0}],"PostCategory":[{"post_id":"ckg7lu78w0001bz2813ba5wh9","category_id":"ckg7piwa500006a2813an1zrb","_id":"ckg7piwa700016a285sew2411"},{"post_id":"ckg7qjivj00037j28eqt5h8o6","category_id":"ckg7qrrd000047j280hei7e4u","_id":"ckg7qrrd200077j2815sh1w8k"},{"post_id":"ckgt6w3by0000wx280fap2hbl","category_id":"ckg7qrrd000047j280hei7e4u","_id":"ckgt6w3c20002wx28byyididl"},{"post_id":"ckh35zvbu0000u7282nma27nl","category_id":"ckh35zvby0001u72804px1qo7","_id":"ckh35zvc10004u7286ara9hbw"},{"post_id":"ckhlu9lv60000w328appuchpc","category_id":"ckhlubjrc0000y9282so58qf8","_id":"ckhludw3200011e288weserfg"},{"post_id":"ckhvqydbn00006z28dot614mu","category_id":"ckhvqydbp00016z281mnxdhf3","_id":"ckhvqydbs00046z28hojpcuoy"},{"post_id":"ckhyotdja0000l9289pxq20m8","category_id":"ckhyotdje0002l928gyqa3gxs","_id":"ckhyotdji0006l928chmj50lm"},{"post_id":"ckiedp6df0000ue287z2u4kkg","category_id":"ckg7qrrd000047j280hei7e4u","_id":"ckiedp6dh0003ue28bxwgcd1k"},{"post_id":"ckiijvpg20000ce28c4jz8x92","category_id":"ckhvqydbp00016z281mnxdhf3","_id":"ckiioltfh0002mm281hyw65zr"},{"post_id":"ckisn8crf0000vy28agpvc03h","category_id":"ckhvqydbp00016z281mnxdhf3","_id":"ckisn8crr0003vy28b4m3ence"},{"post_id":"ckipop2xt0000ej28e6qn6mng","category_id":"ckhvqydbp00016z281mnxdhf3","_id":"ckisn8crs0005vy281q3i35ba"},{"post_id":"ckjb842170000pl28gqvdaaxk","category_id":"ckhvqydbp00016z281mnxdhf3","_id":"ckjb842180003pl28dfq44609"},{"post_id":"ckowlff4h0000ye2852873zj8","category_id":"ckg7qrrd000047j280hei7e4u","_id":"ckowlff4q0006ye2899dwgjcm"},{"post_id":"ckowlff4j0001ye28a2r33w49","category_id":"ckg7qrrd000047j280hei7e4u","_id":"ckowlff4r0008ye280z2g1w0d"},{"post_id":"ckowlff4o0003ye289kzd8aq8","category_id":"ckg7qrrd000047j280hei7e4u","_id":"ckowlff4s000cye285m277t5t"},{"post_id":"ckowlff4p0005ye28hvhw7t2d","category_id":"ckhvqydbp00016z281mnxdhf3","_id":"ckowlff4t000fye285lkdef1y"},{"post_id":"ckowlff4q0007ye28ellm242s","category_id":"ckhvqydbp00016z281mnxdhf3","_id":"ckowlff4u000jye287m7v3hgy"},{"post_id":"ckowlff4s000eye28dd924rjd","category_id":"ckhvqydbp00016z281mnxdhf3","_id":"ckowlff4y000oye2880aweale"},{"post_id":"ckowlff4u000iye288xs01w1p","category_id":"ckg7qrrd000047j280hei7e4u","_id":"ckowlff50000rye281yobgjd6"},{"post_id":"ckowlff4u000lye28c1bi3rap","category_id":"ckg7qrrd000047j280hei7e4u","_id":"ckowlff50000tye28bgo54k6g"},{"post_id":"ckowlff4x000nye2839rj25e2","category_id":"ckg7qrrd000047j280hei7e4u","_id":"ckowlff51000wye280o1o6xv3"},{"post_id":"ckowlff560018ye28068hhosu","category_id":"ckhvqydbp00016z281mnxdhf3","_id":"ckowlff5a001cye282x2988xk"},{"post_id":"ckowlff59001bye28ctkqepy1","category_id":"ckg7qrrd000047j280hei7e4u","_id":"ckowlff5d001fye28e2qn0xgw"}],"PostTag":[{"post_id":"ckg7lu78w0001bz2813ba5wh9","tag_id":"ckg7piagr00003v286tc5gsox","_id":"ckg7piagv00013v284nc01o56"},{"post_id":"ckg7qjivj00037j28eqt5h8o6","tag_id":"ckg7qrrd100057j289azi5q0k","_id":"ckg7qrrd100067j285lt81jqx"},{"post_id":"ckgt6w3by0000wx280fap2hbl","tag_id":"ckg7qrrd100057j289azi5q0k","_id":"ckgt6w3c10001wx28eo595uf9"},{"post_id":"ckh35zvbu0000u7282nma27nl","tag_id":"ckh35zvc00002u728a8q656mh","_id":"ckh35zvc20007u728fve34h0a"},{"post_id":"ckh35zvbu0000u7282nma27nl","tag_id":"ckh35zvc10003u7282616cu2s","_id":"ckh35zvc20008u7288k0q907w"},{"post_id":"ckh35zvbu0000u7282nma27nl","tag_id":"ckh35zvc10005u7281p3i2ffq","_id":"ckh35zvc20009u728720xdpaf"},{"post_id":"ckh35zvbu0000u7282nma27nl","tag_id":"ckh35zvc20006u7284k31d8oe","_id":"ckh35zvc2000au728gaocav7r"},{"post_id":"ckhlu9lv60000w328appuchpc","tag_id":"ckhlubjrc0001y928d4gv9u9l","_id":"ckhlubjrd0002y928dmdedxmp"},{"post_id":"ckhlu9lv60000w328appuchpc","tag_id":"ckhlue30200021e28486k1p79","_id":"ckhlue30300031e285vhycy46"},{"post_id":"ckhvqydbn00006z28dot614mu","tag_id":"ckhvqydbr00026z288h0ue571","_id":"ckhvqydbs00056z28fvjve3ff"},{"post_id":"ckhvqydbn00006z28dot614mu","tag_id":"ckhvqydbr00036z28g66r7hsq","_id":"ckhvqydbs00066z287q3d8g0m"},{"post_id":"ckhyotdja0000l9289pxq20m8","tag_id":"ckhyotdjg0003l928djuaby6g","_id":"ckhyotdjj000al928fghx5blk"},{"post_id":"ckhyotdja0000l9289pxq20m8","tag_id":"ckhyotdjh0005l928hv1rcwfl","_id":"ckhyotdjj000bl9280y7t97t2"},{"post_id":"ckhyotdja0000l9289pxq20m8","tag_id":"ckhyotdjj0007l928cxr4e0vg","_id":"ckhyotdjk000dl9284vfag7vy"},{"post_id":"ckiedp6df0000ue287z2u4kkg","tag_id":"ckhvqydbr00026z288h0ue571","_id":"ckiedp6dh0001ue28glzp1ror"},{"post_id":"ckiedp6df0000ue287z2u4kkg","tag_id":"ckhlue30200021e28486k1p79","_id":"ckiedp6dh0002ue28cbft5tkf"},{"post_id":"ckiedp6df0000ue287z2u4kkg","tag_id":"ckg7qrrd100057j289azi5q0k","_id":"ckiedp6dh0004ue2829v986c9"},{"post_id":"ckiijvpg20000ce28c4jz8x92","tag_id":"ckhvqydbr00026z288h0ue571","_id":"ckiioltfc0000mm28gddsbz0v"},{"post_id":"ckiijvpg20000ce28c4jz8x92","tag_id":"ckhlue30200021e28486k1p79","_id":"ckiioltfh0001mm2839g48sbz"},{"post_id":"ckisn8crf0000vy28agpvc03h","tag_id":"ckhvqydbr00026z288h0ue571","_id":"ckisn8crq0001vy282horerud"},{"post_id":"ckisn8crf0000vy28agpvc03h","tag_id":"ckhlue30200021e28486k1p79","_id":"ckisn8crr0002vy28a7ljcfyd"},{"post_id":"ckipop2xt0000ej28e6qn6mng","tag_id":"ckhvqydbr00026z288h0ue571","_id":"ckisn8crr0004vy287usp93bl"},{"post_id":"ckipop2xt0000ej28e6qn6mng","tag_id":"ckhlue30200021e28486k1p79","_id":"ckisn8crt0006vy2851tb3tdp"},{"post_id":"ckjb842170000pl28gqvdaaxk","tag_id":"ckhvqydbr00026z288h0ue571","_id":"ckjb842180001pl28212r6fpn"},{"post_id":"ckjb842170000pl28gqvdaaxk","tag_id":"ckhvqydbr00036z28g66r7hsq","_id":"ckjb842180002pl282e3rgz6c"},{"post_id":"ckowlff4j0001ye28a2r33w49","tag_id":"ckg7qrrd100057j289azi5q0k","_id":"ckowlff4p0004ye288z23440a"},{"post_id":"ckowlff4p0005ye28hvhw7t2d","tag_id":"ckhvqydbr00026z288h0ue571","_id":"ckowlff4r000aye284sbifesn"},{"post_id":"ckowlff4p0005ye28hvhw7t2d","tag_id":"ckhlue30200021e28486k1p79","_id":"ckowlff4s000dye28f4db6bcg"},{"post_id":"ckowlff4h0000ye2852873zj8","tag_id":"ckg7qrrd100057j289azi5q0k","_id":"ckowlff4u000hye289zli0c2z"},{"post_id":"ckowlff4h0000ye2852873zj8","tag_id":"ckowlff4l0002ye285qyw7hji","_id":"ckowlff4u000kye288jlk6h1h"},{"post_id":"ckowlff4q0007ye28ellm242s","tag_id":"ckhvqydbr00026z288h0ue571","_id":"ckowlff4w000mye28hrxgdf0r"},{"post_id":"ckowlff4q0007ye28ellm242s","tag_id":"ckhlue30200021e28486k1p79","_id":"ckowlff4z000qye284q1ag6zr"},{"post_id":"ckowlff4s000eye28dd924rjd","tag_id":"ckhvqydbr00026z288h0ue571","_id":"ckowlff50000sye282f1h4tb3"},{"post_id":"ckowlff4s000eye28dd924rjd","tag_id":"ckhlue30200021e28486k1p79","_id":"ckowlff50000uye288s6m6m5k"},{"post_id":"ckowlff4u000iye288xs01w1p","tag_id":"ckg7qrrd100057j289azi5q0k","_id":"ckowlff51000xye283qmk4419"},{"post_id":"ckowlff4o0003ye289kzd8aq8","tag_id":"ckg7qrrd100057j289azi5q0k","_id":"ckowlff51000yye284wls22og"},{"post_id":"ckowlff4o0003ye289kzd8aq8","tag_id":"ckowlff4r0009ye28502fahz6","_id":"ckowlff510010ye28c7fzh7aq"},{"post_id":"ckowlff4o0003ye289kzd8aq8","tag_id":"ckowlff4t000gye28fwmgcem9","_id":"ckowlff510011ye284b0aejg1"},{"post_id":"ckowlff4u000lye28c1bi3rap","tag_id":"ckg7qrrd100057j289azi5q0k","_id":"ckowlff520012ye28edbvb49e"},{"post_id":"ckowlff4u000lye28c1bi3rap","tag_id":"ckowlff4z000pye288tmjhtsj","_id":"ckowlff520013ye2887cs46ih"},{"post_id":"ckowlff4x000nye2839rj25e2","tag_id":"ckg7qrrd100057j289azi5q0k","_id":"ckowlff520014ye289k9la1zz"},{"post_id":"ckowlff4x000nye2839rj25e2","tag_id":"ckowlff4r0009ye28502fahz6","_id":"ckowlff520015ye287s3bbbek"},{"post_id":"ckowlff4x000nye2839rj25e2","tag_id":"ckowlff50000vye287pdpgozk","_id":"ckowlff520016ye284pb70suy"},{"post_id":"ckowlff4x000nye2839rj25e2","tag_id":"ckowlff51000zye2856h8flno","_id":"ckowlff530017ye28ftem3fcn"},{"post_id":"ckowlff560018ye28068hhosu","tag_id":"ckhvqydbr00026z288h0ue571","_id":"ckowlff580019ye2897f041k8"},{"post_id":"ckowlff560018ye28068hhosu","tag_id":"ckhlue30200021e28486k1p79","_id":"ckowlff59001aye2836zbg8fp"},{"post_id":"ckowlff59001bye28ctkqepy1","tag_id":"ckg7qrrd100057j289azi5q0k","_id":"ckowlff5c001dye280e5jd13y"},{"post_id":"ckowlff59001bye28ctkqepy1","tag_id":"ckowlff4r0009ye28502fahz6","_id":"ckowlff5d001eye282xkmf3qf"},{"post_id":"ckowlff59001bye28ctkqepy1","tag_id":"ckowlff4l0002ye285qyw7hji","_id":"ckowlff5d001gye28dfakhfcf"}],"Tag":[{"name":"test","_id":"ckg7piagr00003v286tc5gsox"},{"name":"随笔","_id":"ckg7qrrd100057j289azi5q0k"},{"name":"ubuntu","_id":"ckh35zvc00002u728a8q656mh"},{"name":"故障排查","_id":"ckh35zvc10003u7282616cu2s"},{"name":"linux内核","_id":"ckh35zvc10005u7281p3i2ffq"},{"name":"grub","_id":"ckh35zvc20006u7284k31d8oe"},{"name":"-Algorithm","_id":"ckhlu9lvk0002w328hz4x9glf"},{"name":"Algorithm","_id":"ckhlubjrc0001y928d4gv9u9l"},{"name":"LeetCode","_id":"ckhlue30200021e28486k1p79"},{"name":"Algorithms","_id":"ckhvqydbr00026z288h0ue571"},{"name":"OS","_id":"ckhvqydbr00036z28g66r7hsq"},{"name":"Pytorch","_id":"ckhyotdjg0003l928djuaby6g"},{"name":"CUDA","_id":"ckhyotdjh0005l928hv1rcwfl"},{"name":"GPU","_id":"ckhyotdjj0007l928cxr4e0vg"},{"name":"NLP","_id":"ckhyotdjj000cl928cahc99hr"},{"name":"总结","_id":"ckowlff4l0002ye285qyw7hji"},{"name":"反思","_id":"ckowlff4r0009ye28502fahz6"},{"name":"内卷","_id":"ckowlff4t000gye28fwmgcem9"},{"name":"1984","_id":"ckowlff4z000pye288tmjhtsj"},{"name":"哲学","_id":"ckowlff50000vye287pdpgozk"},{"name":"王小波","_id":"ckowlff51000zye2856h8flno"}]}}