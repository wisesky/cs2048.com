{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"themes/hexo-theme-matery/source/favicon.png","path":"favicon.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/css/gitment.css","path":"css/gitment.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/css/my-gitalk.css","path":"css/my-gitalk.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/css/matery.css","path":"css/matery.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/css/my.css","path":"css/my.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/js/matery.js","path":"js/matery.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/js/search.js","path":"js/search.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/avatar.jpg","path":"medias/avatar.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/comment_bg.png","path":"medias/comment_bg.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/cover.jpg","path":"medias/cover.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/icp.png","path":"medias/icp.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/logo.png","path":"medias/logo.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/animate/animate.min.css","path":"libs/animate/animate.min.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/aos/aos.css","path":"libs/aos/aos.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/aos/aos.js","path":"libs/aos/aos.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/aplayer/APlayer.min.css","path":"libs/aplayer/APlayer.min.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/aplayer/APlayer.min.js","path":"libs/aplayer/APlayer.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/codeBlock/codeBlockFuction.js","path":"libs/codeBlock/codeBlockFuction.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/codeBlock/codeCopy.js","path":"libs/codeBlock/codeCopy.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/codeBlock/codeLang.js","path":"libs/codeBlock/codeLang.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/codeBlock/codeShrink.js","path":"libs/codeBlock/codeShrink.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/background/canvas-nest.js","path":"libs/background/canvas-nest.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/background/ribbon-dynamic.js","path":"libs/background/ribbon-dynamic.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/background/ribbon-refresh.min.js","path":"libs/background/ribbon-refresh.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/background/ribbon.min.js","path":"libs/background/ribbon.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/cryptojs/crypto-js.min.js","path":"libs/cryptojs/crypto-js.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/dplayer/DPlayer.min.css","path":"libs/dplayer/DPlayer.min.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/dplayer/DPlayer.min.js","path":"libs/dplayer/DPlayer.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/echarts/echarts.min.js","path":"libs/echarts/echarts.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/jqcloud/jqcloud-1.0.4.min.js","path":"libs/jqcloud/jqcloud-1.0.4.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/jqcloud/jqcloud.css","path":"libs/jqcloud/jqcloud.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/gitalk/gitalk.css","path":"libs/gitalk/gitalk.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/gitalk/gitalk.min.js","path":"libs/gitalk/gitalk.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/gitment/gitment-default.css","path":"libs/gitment/gitment-default.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/gitment/gitment.js","path":"libs/gitment/gitment.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/instantpage/instantpage.js","path":"libs/instantpage/instantpage.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/jquery/jquery.min.js","path":"libs/jquery/jquery.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/masonry/masonry.pkgd.min.js","path":"libs/masonry/masonry.pkgd.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/materialize/materialize.min.css","path":"libs/materialize/materialize.min.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/materialize/materialize.min.js","path":"libs/materialize/materialize.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/others/busuanzi.pure.mini.js","path":"libs/others/busuanzi.pure.mini.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/others/clicklove.js","path":"libs/others/clicklove.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/scrollprogress/scrollProgress.min.js","path":"libs/scrollprogress/scrollProgress.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/tocbot/tocbot.css","path":"libs/tocbot/tocbot.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/tocbot/tocbot.min.js","path":"libs/tocbot/tocbot.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/valine/Valine.min.js","path":"libs/valine/Valine.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/valine/av-min.js","path":"libs/valine/av-min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/banner/0.jpg","path":"medias/banner/0.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/banner/1.jpg","path":"medias/banner/1.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/banner/2.jpg","path":"medias/banner/2.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/banner/3.jpg","path":"medias/banner/3.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/banner/4.jpg","path":"medias/banner/4.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/banner/5.jpg","path":"medias/banner/5.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/banner/6.jpg","path":"medias/banner/6.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/0.jpg","path":"medias/featureimages/0.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/1.jpg","path":"medias/featureimages/1.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/10.jpg","path":"medias/featureimages/10.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/11.jpg","path":"medias/featureimages/11.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/12.jpg","path":"medias/featureimages/12.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/13.jpg","path":"medias/featureimages/13.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/14.jpg","path":"medias/featureimages/14.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/15.jpg","path":"medias/featureimages/15.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/16.jpg","path":"medias/featureimages/16.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/17.jpg","path":"medias/featureimages/17.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/18.jpg","path":"medias/featureimages/18.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/19.jpg","path":"medias/featureimages/19.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/2.jpg","path":"medias/featureimages/2.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/20.jpg","path":"medias/featureimages/20.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/21.jpg","path":"medias/featureimages/21.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/22.jpg","path":"medias/featureimages/22.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/23.jpg","path":"medias/featureimages/23.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/3.jpg","path":"medias/featureimages/3.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/4.jpg","path":"medias/featureimages/4.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/5.jpg","path":"medias/featureimages/5.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/6.jpg","path":"medias/featureimages/6.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/7.jpg","path":"medias/featureimages/7.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/8.jpg","path":"medias/featureimages/8.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/9.jpg","path":"medias/featureimages/9.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/reward/wechat.png","path":"medias/reward/wechat.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/css/all.css","path":"libs/awesome/css/all.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-brands-400.eot","path":"libs/awesome/webfonts/fa-brands-400.eot","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-brands-400.svg","path":"libs/awesome/webfonts/fa-brands-400.svg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-brands-400.ttf","path":"libs/awesome/webfonts/fa-brands-400.ttf","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-brands-400.woff","path":"libs/awesome/webfonts/fa-brands-400.woff","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-brands-400.woff2","path":"libs/awesome/webfonts/fa-brands-400.woff2","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-regular-400.svg","path":"libs/awesome/webfonts/fa-regular-400.svg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-regular-400.eot","path":"libs/awesome/webfonts/fa-regular-400.eot","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-regular-400.woff","path":"libs/awesome/webfonts/fa-regular-400.woff","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-regular-400.ttf","path":"libs/awesome/webfonts/fa-regular-400.ttf","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-regular-400.woff2","path":"libs/awesome/webfonts/fa-regular-400.woff2","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-solid-900.svg","path":"libs/awesome/webfonts/fa-solid-900.svg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-solid-900.ttf","path":"libs/awesome/webfonts/fa-solid-900.ttf","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-solid-900.eot","path":"libs/awesome/webfonts/fa-solid-900.eot","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-solid-900.woff","path":"libs/awesome/webfonts/fa-solid-900.woff","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-solid-900.woff2","path":"libs/awesome/webfonts/fa-solid-900.woff2","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/css/lightgallery.min.css","path":"libs/lightGallery/css/lightgallery.min.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/fonts/lg.eot","path":"libs/lightGallery/fonts/lg.eot","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/fonts/lg.svg","path":"libs/lightGallery/fonts/lg.svg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/fonts/lg.ttf","path":"libs/lightGallery/fonts/lg.ttf","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/fonts/lg.woff","path":"libs/lightGallery/fonts/lg.woff","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/img/loading.gif","path":"libs/lightGallery/img/loading.gif","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/img/video-play.png","path":"libs/lightGallery/img/video-play.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/img/vimeo-play.png","path":"libs/lightGallery/img/vimeo-play.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/img/youtube-play.png","path":"libs/lightGallery/img/youtube-play.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/js/lightgallery-all.min.js","path":"libs/lightGallery/js/lightgallery-all.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/share/css/share.min.css","path":"libs/share/css/share.min.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/share/fonts/iconfont.svg","path":"libs/share/fonts/iconfont.svg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/share/fonts/iconfont.eot","path":"libs/share/fonts/iconfont.eot","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/share/fonts/iconfont.woff","path":"libs/share/fonts/iconfont.woff","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/share/fonts/iconfont.ttf","path":"libs/share/fonts/iconfont.ttf","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/share/js/social-share.min.js","path":"libs/share/js/social-share.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/share/js/jquery.share.min.js","path":"libs/share/js/jquery.share.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/wisesky.jpg","path":"medias/wisesky.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/reward/alipay.png","path":"medias/reward/alipay.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/ACG/1.jpg","path":"medias/ACG/1.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/ACG/2.jpg","path":"medias/ACG/2.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/ACG/3.jpg","path":"medias/ACG/3.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/ACG/4.jpg","path":"medias/ACG/4.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/ACG/5.jpg","path":"medias/ACG/5.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/ACG/6.jpg","path":"medias/ACG/6.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/ACG/7.jpg","path":"medias/ACG/7.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/ACG/8.jpg","path":"medias/ACG/8.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/ACG/9.jpg","path":"medias/ACG/9.jpg","modified":0,"renderable":1}],"Cache":[{"_id":"source/_posts/hello-world.md","hash":"8b7386961fcf8a46dc6cc34fa9d7422c423b08aa","modified":1602577934454},{"_id":"source/tags/index.md","hash":"6c223043ba32b7c76c33eafceeff9ed879520bfb","modified":1602576798016},{"_id":"source/about/index.md","hash":"48622c880dbd4a56f3346b15c7dc5c36d5e4239b","modified":1602570937056},{"_id":"source/categories/index.md","hash":"03ecf6ae4723c42f6eef84663eb50841513e4b62","modified":1602576755198},{"_id":"themes/hexo-theme-matery/.gitignore","hash":"727607929a51db7ea10968f547c26041eee9cfff","modified":1602570653689},{"_id":"themes/hexo-theme-matery/LICENSE","hash":"7df059597099bb7dcf25d2a9aedfaf4465f72d8d","modified":1602570653689},{"_id":"themes/hexo-theme-matery/README.md","hash":"56299cf1fe60a11fef61b3948fe148f995df747e","modified":1602570653690},{"_id":"themes/hexo-theme-matery/layout/404.ejs","hash":"9c8ca67377211e5d60fdde272a975faa9a91a22a","modified":1602570653691},{"_id":"themes/hexo-theme-matery/README_CN.md","hash":"0fdf818476a444663cc8ffa2f194199d9fd93508","modified":1602570653690},{"_id":"themes/hexo-theme-matery/_config.yml","hash":"9440d60860d7823cb6193934528caedb8ace2939","modified":1606210030449},{"_id":"themes/hexo-theme-matery/layout/archive.ejs","hash":"cdac701de8370f9f3794a0eed4165983993a1ca7","modified":1602570653697},{"_id":"themes/hexo-theme-matery/layout/about.ejs","hash":"41849f9300b8dc47048333fcf4a897dd8a2a13ca","modified":1602570653697},{"_id":"themes/hexo-theme-matery/layout/contact.ejs","hash":"72fb5af3fc2f8955e2eb10926bbe4532a04ccd1b","modified":1602570653698},{"_id":"themes/hexo-theme-matery/layout/category.ejs","hash":"00019bca11fb46477f22017cb1f5ad8444da0580","modified":1602570653698},{"_id":"themes/hexo-theme-matery/layout/categories.ejs","hash":"8e54665cc25d7c333da7d9f312987190be6215da","modified":1602570653697},{"_id":"themes/hexo-theme-matery/layout/friends.ejs","hash":"f5d6459bed0f4ecb214f2dbff5b2207a80c44f66","modified":1602570653698},{"_id":"themes/hexo-theme-matery/layout/index.ejs","hash":"4dc6f08e7709cc04e886be72dbf0d06469f0effc","modified":1602570653698},{"_id":"themes/hexo-theme-matery/layout/tag.ejs","hash":"85a4b05bd8a6ad0f17ff2e97dae56949b379c204","modified":1602570653699},{"_id":"themes/hexo-theme-matery/layout/post.ejs","hash":"90b5a4c1f70e4756db569c15a7c6cad0c77c4500","modified":1602570653699},{"_id":"themes/hexo-theme-matery/layout/tags.ejs","hash":"cf9517aa6a0111355121f44615d6923e312283c7","modified":1602570653699},{"_id":"themes/hexo-theme-matery/languages/default.yml","hash":"54ccc01b097c5bf6820f0edfcece1a87b78ab32d","modified":1602570653690},{"_id":"themes/hexo-theme-matery/layout/layout.ejs","hash":"746abd6bec5ed42bfeb54575fa613d38fb19fe96","modified":1602570653698},{"_id":"themes/hexo-theme-matery/languages/zh-CN.yml","hash":"ec0c18fb0e3ab3ee44268dc2b44fc832cffe3c1b","modified":1602580129662},{"_id":"themes/hexo-theme-matery/languages/zh-HK.yml","hash":"ae34ac0e175c3037675722e436637efbceea32f0","modified":1602570653691},{"_id":"themes/hexo-theme-matery/layout/_partial/back-top.ejs","hash":"47ee36a042bb6d52bbe1d0f329637e8ffcf1d0aa","modified":1602570653691},{"_id":"themes/hexo-theme-matery/source/favicon.png","hash":"774fee8c6d0be9dbb010b20f36c06848d06e3da0","modified":1602570653700},{"_id":"themes/hexo-theme-matery/layout/_partial/background.ejs","hash":"aef6edeeb11209831a11d8c7f5d59992e2573335","modified":1602570653691},{"_id":"themes/hexo-theme-matery/layout/_partial/baidu-push.ejs","hash":"2cebcc5ea3614d7f76ec36670e68050cbe611202","modified":1602570653691},{"_id":"themes/hexo-theme-matery/layout/_partial/bg-cover.ejs","hash":"02191109712f61c0e487b8f0b8466597181a9004","modified":1602570653692},{"_id":"themes/hexo-theme-matery/layout/_partial/bg-cover-content.ejs","hash":"28617bf2a35a4269eba6df466acd174e416d2d1e","modified":1602570653691},{"_id":"themes/hexo-theme-matery/layout/_partial/baidu-analytics.ejs","hash":"3bbcdb474ca1dcad514bdc4b7763e17c55df04fd","modified":1602570653691},{"_id":"themes/hexo-theme-matery/layout/_partial/disqus.ejs","hash":"b2dc2c8b5ed56815e55cc2ea54a6dc4eeba2375d","modified":1602570653692},{"_id":"themes/hexo-theme-matery/layout/_partial/gitalk.ejs","hash":"2aa8fbb04b046fa7679092a48372d7e036835dff","modified":1602570653692},{"_id":"themes/hexo-theme-matery/layout/_partial/github-link.ejs","hash":"3aeb581bd78ab8e15b858e4c44c03bcf92f20b9e","modified":1602570653692},{"_id":"themes/hexo-theme-matery/layout/_partial/gitment.ejs","hash":"90f6218512ef2eab63ada7ad2fc766ae635a2297","modified":1602570653692},{"_id":"themes/hexo-theme-matery/layout/_partial/footer.ejs","hash":"4b5476478ba12183b7c97a33d5545fc53be362a8","modified":1602570653692},{"_id":"themes/hexo-theme-matery/layout/_partial/google-analytics.ejs","hash":"5f4992205617da5f8cc5863c62b5ec46e414e2fb","modified":1602570653692},{"_id":"themes/hexo-theme-matery/layout/_partial/head.ejs","hash":"8d263ebccccd0f9e69539f402955296de6f24a62","modified":1602570653692},{"_id":"themes/hexo-theme-matery/layout/_partial/header.ejs","hash":"59e38c70f3d8e7165e686e5e84a627835f4321b0","modified":1602570653693},{"_id":"themes/hexo-theme-matery/layout/_partial/index-cover.ejs","hash":"76b4a37e0364380b143fdf94bf1a5e6941564414","modified":1602570653693},{"_id":"themes/hexo-theme-matery/layout/_partial/livere.ejs","hash":"9c3401b42ea7f26410a5593bae93ada7e57b43be","modified":1602570653693},{"_id":"themes/hexo-theme-matery/layout/_partial/minivaline.ejs","hash":"5f09386aece8f9cf31f6059bbde79cd6c5171493","modified":1602570653693},{"_id":"themes/hexo-theme-matery/layout/_partial/navigation.ejs","hash":"78b70ff24b3039c871331ebec114b936c1756cc8","modified":1602570653693},{"_id":"themes/hexo-theme-matery/layout/_partial/paging.ejs","hash":"e2df12cf92a82b1a7a7add2eac1db1d954bc5511","modified":1602570653693},{"_id":"themes/hexo-theme-matery/layout/_partial/post-cover.ejs","hash":"d1c873c5de54498c722e155aadb8c0ec39485dfa","modified":1602570653693},{"_id":"themes/hexo-theme-matery/layout/_partial/post-detail-toc.ejs","hash":"a8c9abd8cf806235cadb087a5acca3f9182b76ea","modified":1602570653694},{"_id":"themes/hexo-theme-matery/layout/_partial/post-statis.ejs","hash":"04889f9031743c6b081d02fa4027b0dbfcc45ecf","modified":1602570653694},{"_id":"themes/hexo-theme-matery/layout/_partial/mobile-nav.ejs","hash":"cb0cb452be1cd1857ba600f04025b506f3b6fc79","modified":1602570653693},{"_id":"themes/hexo-theme-matery/layout/_partial/prev-next.ejs","hash":"c76b78782ea82340104fccc089417572e0adece4","modified":1602570653694},{"_id":"themes/hexo-theme-matery/layout/_partial/reprint-statement.ejs","hash":"0ce3f9361f558b99cc2f059c5e50b0e2a152ae38","modified":1602570653694},{"_id":"themes/hexo-theme-matery/layout/_partial/post-detail.ejs","hash":"d05926e79aa6dfc235193b9d8c6aa03118b0eade","modified":1602570653694},{"_id":"themes/hexo-theme-matery/layout/_partial/reward.ejs","hash":"ffc55bc7e73bc698bfc58d8e3780c336b83282cf","modified":1602570653694},{"_id":"themes/hexo-theme-matery/layout/_partial/share.ejs","hash":"c941730a2471d6aab367cbb6e09ed08b56c83143","modified":1602570653695},{"_id":"themes/hexo-theme-matery/layout/_partial/search.ejs","hash":"b09872f69c962cb6dd9d4050a322fdea94903f84","modified":1602570653694},{"_id":"themes/hexo-theme-matery/layout/_partial/social-link.ejs","hash":"6f871bd3a70f720e4e451f1f4f625cbc6d8994a4","modified":1602570653695},{"_id":"themes/hexo-theme-matery/layout/_widget/artitalk.ejs","hash":"b14e486f12b9ac42a273b80e4d785fcb94cf04b2","modified":1602570653695},{"_id":"themes/hexo-theme-matery/layout/_partial/valine.ejs","hash":"0e4c0a6154aa34007849928ca88f05b6185b256e","modified":1602570653695},{"_id":"themes/hexo-theme-matery/layout/_widget/category-radar.ejs","hash":"1d8747fda89a0b2ca3c7008867cbfeecad0578a6","modified":1602570653696},{"_id":"themes/hexo-theme-matery/layout/_widget/music.ejs","hash":"e9e3e327d5de9d7aeadbde32e1d558652d9e9195","modified":1602570653696},{"_id":"themes/hexo-theme-matery/layout/_widget/category-cloud.ejs","hash":"1b3df1009234c0112424b497b18b4ad8240b3bc7","modified":1602570653696},{"_id":"themes/hexo-theme-matery/layout/_widget/my-gallery.ejs","hash":"65a2d2f9722f84c7fd98f6bdf79087a14848ebd8","modified":1602570653696},{"_id":"themes/hexo-theme-matery/layout/_widget/dream.ejs","hash":"9a472ad5591100cdb65d0df9d01034163bd6dd9d","modified":1602570653696},{"_id":"themes/hexo-theme-matery/layout/_widget/my-projects.ejs","hash":"ef60b64021fa349b0048425d858dfcf6c906fede","modified":1602570653696},{"_id":"themes/hexo-theme-matery/layout/_widget/my-skills.ejs","hash":"89a0092df72d23093128f2fbbdc8ca7f83ebcfd9","modified":1602570653696},{"_id":"themes/hexo-theme-matery/layout/_widget/post-calendar.ejs","hash":"48821e644bc73553d7c5c56d2e8ee111a70cd776","modified":1602570653696},{"_id":"themes/hexo-theme-matery/layout/_widget/post-charts.ejs","hash":"ab5f986f428215941aeaa0c88aefd440c47d3bcf","modified":1602570653697},{"_id":"themes/hexo-theme-matery/layout/_widget/recommend.ejs","hash":"8551137e94ca4e2e3b8b63d5626255884cb60cb5","modified":1602570653697},{"_id":"themes/hexo-theme-matery/layout/_widget/tag-cloud.ejs","hash":"fc42b72cddc231f7485cdc1fd6852b66be6add26","modified":1602570653697},{"_id":"themes/hexo-theme-matery/layout/_widget/tag-wordcloud.ejs","hash":"487aacb2454d6bf0d21cdb07ddd1fd5ddbca9038","modified":1602570653697},{"_id":"themes/hexo-theme-matery/layout/_widget/video.ejs","hash":"a0e002377af2a7f7e4da6d9a644de97adb035925","modified":1602570653697},{"_id":"themes/hexo-theme-matery/source/css/gitment.css","hash":"2bd15cc17dca35ac3ecc0acf167a23a1dd362acd","modified":1602570653699},{"_id":"themes/hexo-theme-matery/source/css/my-gitalk.css","hash":"eeda46a83d0db1cc239a9cd27d544faf663f9883","modified":1602570653700},{"_id":"themes/hexo-theme-matery/source/css/matery.css","hash":"87bd1dacf48c9daab7ea43466368247f1e4107d1","modified":1602570653700},{"_id":"themes/hexo-theme-matery/source/css/my.css","hash":"10577fbc30f241b126d1b51b1f56136ecba86b19","modified":1602570653700},{"_id":"themes/hexo-theme-matery/source/js/matery.js","hash":"b86de5fe3e9766b7ff80df12ea41c3a9e30825f7","modified":1602570653700},{"_id":"themes/hexo-theme-matery/source/js/search.js","hash":"d559d402b4d4a0931821fe6e22a8831fc43a953d","modified":1602570653700},{"_id":"themes/hexo-theme-matery/source/medias/avatar.jpg","hash":"2a6287308628881ce27b9a7de53ba15c2be00d02","modified":1602570653748},{"_id":"themes/hexo-theme-matery/source/medias/icp.png","hash":"27a96f31f7d0413c6ade6f40e06f021f501151c7","modified":1602570653776},{"_id":"themes/hexo-theme-matery/source/medias/comment_bg.png","hash":"dfc93d24081884fbc58cab0f8fd19e77d31d6123","modified":1602570653758},{"_id":"themes/hexo-theme-matery/source/medias/logo.png","hash":"d9095f5ea8719374d9d1ff020279426f5b2a1396","modified":1602570653776},{"_id":"themes/hexo-theme-matery/source/libs/aos/aos.css","hash":"191a3705a8f63e589a50a0ff2f2c5559f1a1b6b2","modified":1602570653701},{"_id":"themes/hexo-theme-matery/source/libs/animate/animate.min.css","hash":"97afa151569f046b2e01f27c1871646e9cd87caf","modified":1602570653701},{"_id":"themes/hexo-theme-matery/source/libs/aos/aos.js","hash":"02bfb40b0c4b6e9b0b4081218357145cbb327d74","modified":1602570653701},{"_id":"themes/hexo-theme-matery/source/libs/aplayer/APlayer.min.css","hash":"07372a2ba507388d0fed166d761b1c2c2a659dce","modified":1602570653702},{"_id":"themes/hexo-theme-matery/source/libs/aplayer/APlayer.min.js","hash":"22caa28ff6b41a16ff40f15d38f1739e22359478","modified":1602570653702},{"_id":"themes/hexo-theme-matery/source/libs/codeBlock/codeBlockFuction.js","hash":"c7ab06d27a525b15b1eb69027135269e9b9132fb","modified":1602570653727},{"_id":"themes/hexo-theme-matery/source/libs/codeBlock/codeCopy.js","hash":"6d39a766af62e625f177c4d5cf3adc35eed71e61","modified":1602570653727},{"_id":"themes/hexo-theme-matery/source/libs/codeBlock/codeLang.js","hash":"bac88b4d4e3679732d29bd037c34f089cf27cf05","modified":1602570653727},{"_id":"themes/hexo-theme-matery/source/libs/codeBlock/codeShrink.js","hash":"201e8cd761b4be557247bdaf1ebc7c11c83194f6","modified":1602570653727},{"_id":"themes/hexo-theme-matery/source/libs/background/canvas-nest.js","hash":"65333d0dbb9c1173a1b13031b230161fc42c8b2f","modified":1602570653726},{"_id":"themes/hexo-theme-matery/source/libs/background/ribbon-dynamic.js","hash":"052b80c29e6bc585aa28d4504b743bdbac220a88","modified":1602570653726},{"_id":"themes/hexo-theme-matery/source/libs/background/ribbon-refresh.min.js","hash":"6d98692b2cad8c746a562db18b170b35c24402f4","modified":1602570653726},{"_id":"themes/hexo-theme-matery/source/libs/background/ribbon.min.js","hash":"6a99d494c030388f96f6086a7aaa0f03f3fe532e","modified":1602570653727},{"_id":"themes/hexo-theme-matery/source/libs/cryptojs/crypto-js.min.js","hash":"5989527a378b55011a59522f41eeb3981518325c","modified":1602570653728},{"_id":"themes/hexo-theme-matery/source/libs/jqcloud/jqcloud-1.0.4.min.js","hash":"257eaae3020599e4939f50d5008a743827f25b8c","modified":1602570653738},{"_id":"themes/hexo-theme-matery/source/libs/dplayer/DPlayer.min.css","hash":"f7d19655f873b813ffba5d1a17145c91f82631b8","modified":1602570653728},{"_id":"themes/hexo-theme-matery/source/libs/jqcloud/jqcloud.css","hash":"20d9f11a19d95c70e27cb922e0d6dccbec4eae89","modified":1602570653738},{"_id":"themes/hexo-theme-matery/source/libs/gitment/gitment-default.css","hash":"2903c59ee06b965bef32e937bd69f5b0b2190717","modified":1602570653737},{"_id":"themes/hexo-theme-matery/source/libs/gitalk/gitalk.css","hash":"940ded3ea12c2fe1ab0820d2831ec405f3f1fe9f","modified":1602570653735},{"_id":"themes/hexo-theme-matery/source/libs/instantpage/instantpage.js","hash":"83ce8919b1a69b2f1809ffaf99b52a8627e650e9","modified":1602570653738},{"_id":"themes/hexo-theme-matery/source/libs/masonry/masonry.pkgd.min.js","hash":"ff940b4ea68368ca0e4d5560cbb79fb147dfc3c5","modified":1602570653741},{"_id":"themes/hexo-theme-matery/source/libs/others/busuanzi.pure.mini.js","hash":"6e41f31100ae7eb3a6f23f2c168f6dd56e7f7a9a","modified":1602570653743},{"_id":"themes/hexo-theme-matery/source/libs/others/clicklove.js","hash":"6a39b8c683ba5dcd92f70c6ab45d1cfac3213e8e","modified":1602570653743},{"_id":"themes/hexo-theme-matery/source/libs/scrollprogress/scrollProgress.min.js","hash":"777ffe5d07e85a14fbe97d846f45ffc0087251cc","modified":1602570653743},{"_id":"themes/hexo-theme-matery/source/libs/tocbot/tocbot.css","hash":"9ab8ef576c9a57115194152e79cca79b0a41dd70","modified":1602570653746},{"_id":"themes/hexo-theme-matery/source/libs/tocbot/tocbot.min.js","hash":"5ec27317f0270b8cf6b884c6f12025700b9a565c","modified":1602570653746},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/10.jpg","hash":"98e7f6fd9c97d4de9044b6871ca08ebf14db11b9","modified":1602570653761},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/13.jpg","hash":"35a320174f8e316e3eadaec658024276b651c6e9","modified":1602570653765},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/15.jpg","hash":"da0fbee3b7bde1607eace377ddf834c0be99edfe","modified":1602570653766},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/16.jpg","hash":"97a829c4bc94f9d2929b20a1a9b798c57b9f7205","modified":1602570653766},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/2.jpg","hash":"4bba691cf71a517ecaeaf42afd3e8f8b31e346c1","modified":1602570653768},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/21.jpg","hash":"b26edb128bb0bf58b23fd2f014e9555e89a2ca3b","modified":1602570653770},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/22.jpg","hash":"754579747a3e99747d890fca3162f370b96a7941","modified":1602570653770},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/23.jpg","hash":"7d7f37da3fa7128343adac23866449eb2c6a549a","modified":1602570653771},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/3.jpg","hash":"6ec646c2a70f5f11edacf225c1477f2200a37a96","modified":1602570653771},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/5.jpg","hash":"41ca20129a37fedc573eec28dd7d7b9e5b09228a","modified":1602570653773},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/7.jpg","hash":"7975141cd64e875122c0ea33daaca1a06bf00b8e","modified":1602570653774},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/8.jpg","hash":"8e4b7186352085483ca1174c7c0800114c48df8b","modified":1602570653775},{"_id":"themes/hexo-theme-matery/source/medias/reward/alipay.jpg","hash":"1abc719b95d1b26f1f898e6b0a9b7609146e332f","modified":1602570653777},{"_id":"themes/hexo-theme-matery/source/medias/reward/wechat.png","hash":"f1c84ceb948d0876b5ec7d4d55b654ef3f5132e1","modified":1602577429437},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-regular-400.woff","hash":"59439d3ad31d856d78ec3e2bd9f1eafa2c7a581c","modified":1602570653716},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-regular-400.eot","hash":"439c8afd3373acb4a73135a34e220464a89cd5e2","modified":1602570653713},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-regular-400.ttf","hash":"0f4bd02942a54a6b3200d9078adff88c2812e751","modified":1602570653716},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-regular-400.woff2","hash":"f6f653b4ea8fc487bdb590d39d5a726258a55f40","modified":1602570653717},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/fonts/lg.eot","hash":"54caf05a81e33d7bf04f2e420736ce6f1de5f936","modified":1602570653739},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/css/lightgallery.min.css","hash":"1b7227237f9785c66062a4811508916518e4132c","modified":1602570653739},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/fonts/lg.ttf","hash":"f6421c0c397311ae09f9257aa58bcd5e9720f493","modified":1602570653740},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/fonts/lg.svg","hash":"9c6632aeec67d3e84a1434884aa801514ff8103b","modified":1602570653739},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/fonts/lg.woff","hash":"3048de344dd5cad4624e0127e58eaae4b576f574","modified":1602570653740},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/img/loading.gif","hash":"15a76af2739482d8de7354abc6d8dc4fca8d145e","modified":1602570653740},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/img/video-play.png","hash":"2962e03ddbe04d7e201a5acccac531a2bbccddfc","modified":1602570653740},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/img/youtube-play.png","hash":"f8d11384d33b7a79ee2ba8d522844f14d5067a80","modified":1602570653740},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/img/vimeo-play.png","hash":"9b72fc0f86a01467ed0b68c9cc4d604ec316d517","modified":1602570653740},{"_id":"themes/hexo-theme-matery/source/libs/share/css/share.min.css","hash":"8a778a86f3ce9a042df6be63a9f1039631e351a5","modified":1602570653744},{"_id":"themes/hexo-theme-matery/source/libs/share/fonts/iconfont.svg","hash":"1d56c9d5db0273f07c43cc1397e440f98ba7827a","modified":1602570653744},{"_id":"themes/hexo-theme-matery/source/libs/share/fonts/iconfont.eot","hash":"00ff749c8e202401190cc98d56087cdda716abe4","modified":1602570653744},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/js/lightgallery-all.min.js","hash":"9f5ef4bc8a0a3c746ca4f3c3e6d64493b1a977d8","modified":1602570653741},{"_id":"themes/hexo-theme-matery/source/libs/share/js/social-share.min.js","hash":"a3090a02786dcd4efc6355c1c1dc978add8d6827","modified":1602570653746},{"_id":"themes/hexo-theme-matery/source/libs/share/fonts/iconfont.woff","hash":"2e3fce1dcfbd6e2114e7bfbeaf72d3c62e15a1bd","modified":1602570653745},{"_id":"themes/hexo-theme-matery/source/libs/share/fonts/iconfont.ttf","hash":"afd898f59d363887418669520b24d175f966a083","modified":1602570653745},{"_id":"themes/hexo-theme-matery/source/libs/share/js/jquery.share.min.js","hash":"41367dcb857e02e3c417ebe68a554ce1d4430806","modified":1602570653745},{"_id":"themes/hexo-theme-matery/source/libs/gitment/gitment.js","hash":"28c02c45ce568e084cd1041dc493f83f9c6c88c6","modified":1602570653737},{"_id":"themes/hexo-theme-matery/source/libs/dplayer/DPlayer.min.js","hash":"c3bad7b265574fab0ae4d45867422ea1cb9d6599","modified":1602570653729},{"_id":"themes/hexo-theme-matery/source/libs/jquery/jquery.min.js","hash":"2115753ca5fb7032aec498db7bb5dca624dbe6be","modified":1602570653739},{"_id":"themes/hexo-theme-matery/source/libs/valine/Valine.min.js","hash":"6cbdbf91e1f046dd41267a5ff0691a1fccba99df","modified":1602570653747},{"_id":"themes/hexo-theme-matery/source/medias/banner/0.jpg","hash":"69ec96cd9b4bc3aa631adc9da61353f50c39f031","modified":1602570653749},{"_id":"themes/hexo-theme-matery/source/medias/banner/2.jpg","hash":"39fb2535460ce66cc0b34e07ffb9411db1405f09","modified":1602570653752},{"_id":"themes/hexo-theme-matery/source/medias/banner/3.jpg","hash":"4ac047e92d0363b1a61ab756aca6dac13fb77494","modified":1602570653754},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/1.jpg","hash":"684ae89de8cb7acefae19f5aee6c612037c46393","modified":1602570653761},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/0.jpg","hash":"1c3300f029fc85d6dda6fa4f1d699551034cdaf7","modified":1602570653760},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/11.jpg","hash":"f55972ce7175684f2b11c3c9fc2b5b14bccbfae8","modified":1602570653762},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/12.jpg","hash":"8a4b2e7d92ae95c3b0c921db23c35aa9a41a7d58","modified":1602570653764},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/17.jpg","hash":"42d47903551ee81885c1386022982cae165841c5","modified":1602570653767},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/14.jpg","hash":"38e11221406785bcd93aa9cd23e568e164630ef1","modified":1602570653765},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/20.jpg","hash":"3b11f9b461168d907073f793190865fe621a8573","modified":1602570653769},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/19.jpg","hash":"eb250906fdbc0c408f42ae9933725bc1a05d79fb","modified":1602570653768},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/6.jpg","hash":"c8f2aa4bbb041158b4e73733a341e6a77c8583f7","modified":1602570653774},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/18.jpg","hash":"64829272ec85bb819d55ff89e5b5fd6f64aa436b","modified":1602570653767},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/4.jpg","hash":"e06c47de27619984be9d5d02947f8370a432dfea","modified":1602570653772},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/9.jpg","hash":"b956a2291a04b2132366b53666cf34858b8bdb1f","modified":1602570653776},{"_id":"themes/hexo-theme-matery/source/libs/awesome/css/all.css","hash":"ecc41e32ad2696877a1656749841f3b5543bbe3d","modified":1602570653703},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-brands-400.eot","hash":"22f9e7d5226408eb2d0a11e118257a3ca22b8670","modified":1602570653705},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-brands-400.ttf","hash":"91cbeeaceb644a971241c08362898599d6d968ce","modified":1602570653711},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-brands-400.woff2","hash":"a46bd47ff0a90b812aafafda587d095cdb844271","modified":1602570653713},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-brands-400.woff","hash":"18838f5260317da3c5ed29bf844ac8a4f7ad0529","modified":1602570653712},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-solid-900.woff","hash":"92803b8753ceda573c6906774677c5a7081d2fbb","modified":1602570653725},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-solid-900.woff2","hash":"9c081b88b106c6c04ecb895ba7ba7d3dcb3b55ac","modified":1602570653726},{"_id":"themes/hexo-theme-matery/source/medias/cover.jpg","hash":"d4957ff7cc5e88555cd840f2956ab0561e6f1ccf","modified":1602570653759},{"_id":"themes/hexo-theme-matery/source/libs/gitalk/gitalk.min.js","hash":"8fefe38f28804f90116bdcb74a0875c9de9f3b7d","modified":1602570653736},{"_id":"themes/hexo-theme-matery/source/libs/materialize/materialize.min.css","hash":"a69d456e3345e7f59cd0d47d1b3e70fd4a496a05","modified":1602570653742},{"_id":"themes/hexo-theme-matery/source/libs/materialize/materialize.min.js","hash":"c8b4c65651921d888cf5f27430dfe2ad190d35bf","modified":1602570653743},{"_id":"themes/hexo-theme-matery/source/libs/valine/av-min.js","hash":"541efb9edc1ce425cbe3897cfc25803211fe6a05","modified":1602570653747},{"_id":"themes/hexo-theme-matery/source/medias/banner/1.jpg","hash":"ab122a36998a4f62a61e61a4fc5e00248113413b","modified":1602570653751},{"_id":"themes/hexo-theme-matery/source/medias/banner/5.jpg","hash":"852418f4f09e796e12bc3bab7a1488d3f37d6486","modified":1602570653757},{"_id":"themes/hexo-theme-matery/source/medias/banner/6.jpg","hash":"ed7282cc129c4ff9f322d2f2897fb4aac5c48589","modified":1602570653758},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-regular-400.svg","hash":"3d3a49445343d80f3b553e3e3425b9a7bd49acaf","modified":1602570653714},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-solid-900.ttf","hash":"9521ed12274c2cbc910cea77657116fcf6545da3","modified":1602570653724},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-solid-900.eot","hash":"cab8e84ae5682d1d556e234df9c790985888def8","modified":1602570653718},{"_id":"themes/hexo-theme-matery/source/medias/banner/4.jpg","hash":"e5ac5033678afa9d69edffe9a61004f836cb5734","modified":1602570653756},{"_id":"themes/hexo-theme-matery/source/libs/echarts/echarts.min.js","hash":"9496f386a0da4601cad22c479cc5543913a4d67f","modified":1602570653735},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-brands-400.svg","hash":"5e2d2a159294576bea69cc3360efb5ffe110ab2d","modified":1602570653708},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-solid-900.svg","hash":"7da88b19e1486f8c968d3cf5ab3f194f01ea17fd","modified":1602570653722},{"_id":"public/atom.xml","hash":"5c7a8fc43a4af0e821750d2dd7b9de1772090c24","modified":1607589283062},{"_id":"public/search.xml","hash":"51ee0c645fb4137d6fc560df04508503fead20fc","modified":1607589283062},{"_id":"public/categories/index.html","hash":"7ea9374d2dead8bfd8d00baffa1d55e90eb6afbe","modified":1607335732288},{"_id":"public/about/index.html","hash":"bd3a882d3a9c9adc36529c78e38b323678f396ab","modified":1607587146319},{"_id":"public/tags/index.html","hash":"f192af0eaa15353b1f1d5add1a5f57fb44a00653","modified":1607335732288},{"_id":"public/2020/10/13/hello-world/index.html","hash":"cd274fbb6171d22d206412e3c2ed506998a9444c","modified":1602576511910},{"_id":"public/archives/index.html","hash":"bd1f972596e5e0a475a76f18ba43bca024139eea","modified":1607587146319},{"_id":"public/archives/2020/index.html","hash":"7c63499a730ba1dd57fcdbd967feca0636e8efa4","modified":1607587146319},{"_id":"public/archives/2020/10/index.html","hash":"cb62de7c08cd1c63ec5df312b43cab2f6104cf49","modified":1607587146319},{"_id":"public/index.html","hash":"6f59c73e6307e4203cda0bda94adaa6c67439cac","modified":1607587332408},{"_id":"public/favicon.png","hash":"774fee8c6d0be9dbb010b20f36c06848d06e3da0","modified":1602571742461},{"_id":"public/medias/comment_bg.png","hash":"dfc93d24081884fbc58cab0f8fd19e77d31d6123","modified":1602571742461},{"_id":"public/medias/icp.png","hash":"27a96f31f7d0413c6ade6f40e06f021f501151c7","modified":1602571742461},{"_id":"public/medias/logo.png","hash":"d9095f5ea8719374d9d1ff020279426f5b2a1396","modified":1602571742461},{"_id":"public/medias/featureimages/2.jpg","hash":"4bba691cf71a517ecaeaf42afd3e8f8b31e346c1","modified":1602571742461},{"_id":"public/medias/featureimages/22.jpg","hash":"754579747a3e99747d890fca3162f370b96a7941","modified":1602571742461},{"_id":"public/medias/featureimages/23.jpg","hash":"7d7f37da3fa7128343adac23866449eb2c6a549a","modified":1602571742461},{"_id":"public/medias/featureimages/5.jpg","hash":"41ca20129a37fedc573eec28dd7d7b9e5b09228a","modified":1602571742461},{"_id":"public/medias/featureimages/3.jpg","hash":"6ec646c2a70f5f11edacf225c1477f2200a37a96","modified":1602571742461},{"_id":"public/medias/featureimages/7.jpg","hash":"7975141cd64e875122c0ea33daaca1a06bf00b8e","modified":1602571742461},{"_id":"public/medias/reward/alipay.jpg","hash":"1abc719b95d1b26f1f898e6b0a9b7609146e332f","modified":1602571742461},{"_id":"public/medias/reward/wechat.png","hash":"f1c84ceb948d0876b5ec7d4d55b654ef3f5132e1","modified":1602577498584},{"_id":"public/libs/awesome/webfonts/fa-regular-400.woff","hash":"59439d3ad31d856d78ec3e2bd9f1eafa2c7a581c","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-regular-400.woff2","hash":"f6f653b4ea8fc487bdb590d39d5a726258a55f40","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-regular-400.ttf","hash":"0f4bd02942a54a6b3200d9078adff88c2812e751","modified":1602571742461},{"_id":"public/libs/lightGallery/fonts/lg.eot","hash":"54caf05a81e33d7bf04f2e420736ce6f1de5f936","modified":1602571742461},{"_id":"public/libs/lightGallery/fonts/lg.woff","hash":"3048de344dd5cad4624e0127e58eaae4b576f574","modified":1602571742461},{"_id":"public/libs/lightGallery/fonts/lg.ttf","hash":"f6421c0c397311ae09f9257aa58bcd5e9720f493","modified":1602571742461},{"_id":"public/libs/lightGallery/fonts/lg.svg","hash":"9c6632aeec67d3e84a1434884aa801514ff8103b","modified":1602571742461},{"_id":"public/libs/lightGallery/img/video-play.png","hash":"2962e03ddbe04d7e201a5acccac531a2bbccddfc","modified":1602571742461},{"_id":"public/libs/lightGallery/img/vimeo-play.png","hash":"9b72fc0f86a01467ed0b68c9cc4d604ec316d517","modified":1602571742461},{"_id":"public/libs/lightGallery/img/loading.gif","hash":"15a76af2739482d8de7354abc6d8dc4fca8d145e","modified":1602571742461},{"_id":"public/libs/lightGallery/img/youtube-play.png","hash":"f8d11384d33b7a79ee2ba8d522844f14d5067a80","modified":1602571742461},{"_id":"public/libs/share/fonts/iconfont.woff","hash":"2e3fce1dcfbd6e2114e7bfbeaf72d3c62e15a1bd","modified":1602571742461},{"_id":"public/libs/share/fonts/iconfont.eot","hash":"00ff749c8e202401190cc98d56087cdda716abe4","modified":1602571742461},{"_id":"public/libs/share/fonts/iconfont.svg","hash":"1d56c9d5db0273f07c43cc1397e440f98ba7827a","modified":1602571742461},{"_id":"public/libs/share/fonts/iconfont.ttf","hash":"afd898f59d363887418669520b24d175f966a083","modified":1602571742461},{"_id":"public/medias/avatar.jpg","hash":"2a6287308628881ce27b9a7de53ba15c2be00d02","modified":1602571742461},{"_id":"public/medias/banner/0.jpg","hash":"69ec96cd9b4bc3aa631adc9da61353f50c39f031","modified":1602571742461},{"_id":"public/medias/featureimages/10.jpg","hash":"98e7f6fd9c97d4de9044b6871ca08ebf14db11b9","modified":1602571742461},{"_id":"public/medias/featureimages/13.jpg","hash":"35a320174f8e316e3eadaec658024276b651c6e9","modified":1602571742461},{"_id":"public/medias/featureimages/15.jpg","hash":"da0fbee3b7bde1607eace377ddf834c0be99edfe","modified":1602571742461},{"_id":"public/medias/featureimages/16.jpg","hash":"97a829c4bc94f9d2929b20a1a9b798c57b9f7205","modified":1602571742461},{"_id":"public/medias/featureimages/17.jpg","hash":"42d47903551ee81885c1386022982cae165841c5","modified":1602571742461},{"_id":"public/medias/featureimages/19.jpg","hash":"eb250906fdbc0c408f42ae9933725bc1a05d79fb","modified":1602571742461},{"_id":"public/medias/featureimages/18.jpg","hash":"64829272ec85bb819d55ff89e5b5fd6f64aa436b","modified":1602571742461},{"_id":"public/medias/featureimages/20.jpg","hash":"3b11f9b461168d907073f793190865fe621a8573","modified":1602571742461},{"_id":"public/medias/featureimages/21.jpg","hash":"b26edb128bb0bf58b23fd2f014e9555e89a2ca3b","modified":1602571742461},{"_id":"public/medias/featureimages/4.jpg","hash":"e06c47de27619984be9d5d02947f8370a432dfea","modified":1602571742461},{"_id":"public/medias/featureimages/9.jpg","hash":"b956a2291a04b2132366b53666cf34858b8bdb1f","modified":1602571742461},{"_id":"public/medias/featureimages/8.jpg","hash":"8e4b7186352085483ca1174c7c0800114c48df8b","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-brands-400.woff","hash":"18838f5260317da3c5ed29bf844ac8a4f7ad0529","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-brands-400.woff2","hash":"a46bd47ff0a90b812aafafda587d095cdb844271","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-regular-400.eot","hash":"439c8afd3373acb4a73135a34e220464a89cd5e2","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-solid-900.woff","hash":"92803b8753ceda573c6906774677c5a7081d2fbb","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-solid-900.woff2","hash":"9c081b88b106c6c04ecb895ba7ba7d3dcb3b55ac","modified":1602571742461},{"_id":"public/medias/cover.jpg","hash":"d4957ff7cc5e88555cd840f2956ab0561e6f1ccf","modified":1602571742461},{"_id":"public/medias/banner/2.jpg","hash":"39fb2535460ce66cc0b34e07ffb9411db1405f09","modified":1602571742461},{"_id":"public/medias/banner/5.jpg","hash":"852418f4f09e796e12bc3bab7a1488d3f37d6486","modified":1602571742461},{"_id":"public/medias/featureimages/0.jpg","hash":"1c3300f029fc85d6dda6fa4f1d699551034cdaf7","modified":1602571742461},{"_id":"public/medias/featureimages/1.jpg","hash":"684ae89de8cb7acefae19f5aee6c612037c46393","modified":1602571742461},{"_id":"public/medias/featureimages/12.jpg","hash":"8a4b2e7d92ae95c3b0c921db23c35aa9a41a7d58","modified":1602571742461},{"_id":"public/medias/featureimages/11.jpg","hash":"f55972ce7175684f2b11c3c9fc2b5b14bccbfae8","modified":1602571742461},{"_id":"public/medias/featureimages/14.jpg","hash":"38e11221406785bcd93aa9cd23e568e164630ef1","modified":1602571742461},{"_id":"public/medias/featureimages/6.jpg","hash":"c8f2aa4bbb041158b4e73733a341e6a77c8583f7","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-brands-400.eot","hash":"22f9e7d5226408eb2d0a11e118257a3ca22b8670","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-regular-400.svg","hash":"3d3a49445343d80f3b553e3e3425b9a7bd49acaf","modified":1602571742461},{"_id":"public/css/gitment.css","hash":"2bd15cc17dca35ac3ecc0acf167a23a1dd362acd","modified":1602571742461},{"_id":"public/css/my-gitalk.css","hash":"eeda46a83d0db1cc239a9cd27d544faf663f9883","modified":1602571742461},{"_id":"public/css/my.css","hash":"10577fbc30f241b126d1b51b1f56136ecba86b19","modified":1602571742461},{"_id":"public/js/matery.js","hash":"b86de5fe3e9766b7ff80df12ea41c3a9e30825f7","modified":1602571742461},{"_id":"public/js/search.js","hash":"d559d402b4d4a0931821fe6e22a8831fc43a953d","modified":1602571742461},{"_id":"public/libs/aos/aos.js","hash":"02bfb40b0c4b6e9b0b4081218357145cbb327d74","modified":1602571742461},{"_id":"public/libs/aplayer/APlayer.min.css","hash":"07372a2ba507388d0fed166d761b1c2c2a659dce","modified":1602571742461},{"_id":"public/libs/codeBlock/codeBlockFuction.js","hash":"c7ab06d27a525b15b1eb69027135269e9b9132fb","modified":1602571742461},{"_id":"public/libs/codeBlock/codeCopy.js","hash":"6d39a766af62e625f177c4d5cf3adc35eed71e61","modified":1602571742461},{"_id":"public/libs/codeBlock/codeLang.js","hash":"bac88b4d4e3679732d29bd037c34f089cf27cf05","modified":1602571742461},{"_id":"public/libs/codeBlock/codeShrink.js","hash":"201e8cd761b4be557247bdaf1ebc7c11c83194f6","modified":1602571742461},{"_id":"public/libs/background/canvas-nest.js","hash":"65333d0dbb9c1173a1b13031b230161fc42c8b2f","modified":1602571742461},{"_id":"public/libs/background/ribbon-dynamic.js","hash":"052b80c29e6bc585aa28d4504b743bdbac220a88","modified":1602571742461},{"_id":"public/libs/background/ribbon-refresh.min.js","hash":"6d98692b2cad8c746a562db18b170b35c24402f4","modified":1602571742461},{"_id":"public/libs/background/ribbon.min.js","hash":"6a99d494c030388f96f6086a7aaa0f03f3fe532e","modified":1602571742461},{"_id":"public/libs/jqcloud/jqcloud-1.0.4.min.js","hash":"257eaae3020599e4939f50d5008a743827f25b8c","modified":1602571742461},{"_id":"public/libs/jqcloud/jqcloud.css","hash":"20d9f11a19d95c70e27cb922e0d6dccbec4eae89","modified":1602571742461},{"_id":"public/libs/instantpage/instantpage.js","hash":"83ce8919b1a69b2f1809ffaf99b52a8627e650e9","modified":1602571742461},{"_id":"public/libs/others/clicklove.js","hash":"6a39b8c683ba5dcd92f70c6ab45d1cfac3213e8e","modified":1602571742461},{"_id":"public/libs/others/busuanzi.pure.mini.js","hash":"6e41f31100ae7eb3a6f23f2c168f6dd56e7f7a9a","modified":1602571742461},{"_id":"public/libs/scrollprogress/scrollProgress.min.js","hash":"777ffe5d07e85a14fbe97d846f45ffc0087251cc","modified":1602571742461},{"_id":"public/libs/tocbot/tocbot.css","hash":"9ab8ef576c9a57115194152e79cca79b0a41dd70","modified":1602571742461},{"_id":"public/libs/tocbot/tocbot.min.js","hash":"5ec27317f0270b8cf6b884c6f12025700b9a565c","modified":1602571742461},{"_id":"public/libs/share/css/share.min.css","hash":"8a778a86f3ce9a042df6be63a9f1039631e351a5","modified":1602571742461},{"_id":"public/medias/banner/3.jpg","hash":"4ac047e92d0363b1a61ab756aca6dac13fb77494","modified":1602571742461},{"_id":"public/medias/banner/6.jpg","hash":"ed7282cc129c4ff9f322d2f2897fb4aac5c48589","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-brands-400.ttf","hash":"91cbeeaceb644a971241c08362898599d6d968ce","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-solid-900.ttf","hash":"9521ed12274c2cbc910cea77657116fcf6545da3","modified":1602571742461},{"_id":"public/libs/aos/aos.css","hash":"191a3705a8f63e589a50a0ff2f2c5559f1a1b6b2","modified":1602571742461},{"_id":"public/libs/gitalk/gitalk.css","hash":"940ded3ea12c2fe1ab0820d2831ec405f3f1fe9f","modified":1602571742461},{"_id":"public/libs/gitment/gitment-default.css","hash":"2903c59ee06b965bef32e937bd69f5b0b2190717","modified":1602571742461},{"_id":"public/libs/masonry/masonry.pkgd.min.js","hash":"ff940b4ea68368ca0e4d5560cbb79fb147dfc3c5","modified":1602571742461},{"_id":"public/libs/lightGallery/css/lightgallery.min.css","hash":"1b7227237f9785c66062a4811508916518e4132c","modified":1602571742461},{"_id":"public/libs/share/js/social-share.min.js","hash":"a3090a02786dcd4efc6355c1c1dc978add8d6827","modified":1602571742461},{"_id":"public/libs/share/js/jquery.share.min.js","hash":"41367dcb857e02e3c417ebe68a554ce1d4430806","modified":1602571742461},{"_id":"public/medias/banner/1.jpg","hash":"ab122a36998a4f62a61e61a4fc5e00248113413b","modified":1602571742461},{"_id":"public/css/matery.css","hash":"87bd1dacf48c9daab7ea43466368247f1e4107d1","modified":1602571742461},{"_id":"public/libs/cryptojs/crypto-js.min.js","hash":"5989527a378b55011a59522f41eeb3981518325c","modified":1602571742461},{"_id":"public/libs/dplayer/DPlayer.min.css","hash":"f7d19655f873b813ffba5d1a17145c91f82631b8","modified":1602571742461},{"_id":"public/libs/animate/animate.min.css","hash":"97afa151569f046b2e01f27c1871646e9cd87caf","modified":1602571742461},{"_id":"public/libs/aplayer/APlayer.min.js","hash":"22caa28ff6b41a16ff40f15d38f1739e22359478","modified":1602571742461},{"_id":"public/libs/lightGallery/js/lightgallery-all.min.js","hash":"9f5ef4bc8a0a3c746ca4f3c3e6d64493b1a977d8","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-solid-900.eot","hash":"cab8e84ae5682d1d556e234df9c790985888def8","modified":1602571742461},{"_id":"public/libs/gitment/gitment.js","hash":"28c02c45ce568e084cd1041dc493f83f9c6c88c6","modified":1602571742461},{"_id":"public/libs/valine/Valine.min.js","hash":"6cbdbf91e1f046dd41267a5ff0691a1fccba99df","modified":1602571742461},{"_id":"public/libs/awesome/css/all.css","hash":"ecc41e32ad2696877a1656749841f3b5543bbe3d","modified":1602571742461},{"_id":"public/medias/banner/4.jpg","hash":"e5ac5033678afa9d69edffe9a61004f836cb5734","modified":1602571742461},{"_id":"public/libs/jquery/jquery.min.js","hash":"2115753ca5fb7032aec498db7bb5dca624dbe6be","modified":1602571742461},{"_id":"public/libs/dplayer/DPlayer.min.js","hash":"c3bad7b265574fab0ae4d45867422ea1cb9d6599","modified":1602571742461},{"_id":"public/libs/materialize/materialize.min.css","hash":"a69d456e3345e7f59cd0d47d1b3e70fd4a496a05","modified":1602571742461},{"_id":"public/libs/gitalk/gitalk.min.js","hash":"8fefe38f28804f90116bdcb74a0875c9de9f3b7d","modified":1602571742461},{"_id":"public/libs/valine/av-min.js","hash":"541efb9edc1ce425cbe3897cfc25803211fe6a05","modified":1602571742461},{"_id":"public/libs/materialize/materialize.min.js","hash":"c8b4c65651921d888cf5f27430dfe2ad190d35bf","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-brands-400.svg","hash":"5e2d2a159294576bea69cc3360efb5ffe110ab2d","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-solid-900.svg","hash":"7da88b19e1486f8c968d3cf5ab3f194f01ea17fd","modified":1602571742461},{"_id":"public/libs/echarts/echarts.min.js","hash":"9496f386a0da4601cad22c479cc5543913a4d67f","modified":1602571742461},{"_id":"themes/hexo-theme-matery/source/medias/wisesky.jpg","hash":"ca0e4de76b63d2782d8618aeea148467258ae14f","modified":1602572521751},{"_id":"public/medias/wisesky.jpg","hash":"ca0e4de76b63d2782d8618aeea148467258ae14f","modified":1602574164658},{"_id":"themes/hexo-theme-matery/source/medias/reward/.DS_Store","hash":"fe9ec4436feaf1a9fedf0f2a2938c80df09fa8fa","modified":1602577812069},{"_id":"themes/hexo-theme-matery/source/medias/.DS_Store","hash":"49a8c2d15a0f57fe40bb502e337a296afdb1ef83","modified":1602577383050},{"_id":"themes/hexo-theme-matery/source/medias/reward/alipay.png","hash":"7c1cd3be7825da29c2aca0d8c0a77f6f791b6d3a","modified":1602577351418},{"_id":"public/uncategorized/hello-world/index.html","hash":"38d91d91a28d189bc1f36f490d64f1cab54bb5dd","modified":1602577909635},{"_id":"public/medias/reward/alipay.png","hash":"7c1cd3be7825da29c2aca0d8c0a77f6f791b6d3a","modified":1602577498584},{"_id":"public/tags/test/index.html","hash":"b4d51cfc8a01d3550013a71db1270bb8a7097248","modified":1607335732288},{"_id":"public/test/hello-world/index.html","hash":"258096a2581e401331d1f13925ab96d8154cfa38","modified":1607587332408},{"_id":"public/categories/test/index.html","hash":"0be5012db47b75a77f4b79b363ace62b57fbeabc","modified":1607335732288},{"_id":"source/_posts/my-first-blog.md","hash":"c7e5400935afb8289e78a08d62bfe907fba89d36","modified":1606285615417},{"_id":"source/.DS_Store","hash":"b48c4f7d61a5928be717d4bd654481ff1eab36ee","modified":1602580239660},{"_id":"themes/hexo-theme-matery/source/medias/ACG/9.jpg","hash":"10b2ddfbe7f03c4513ace7a2855a47f907478d55","modified":1602578209287},{"_id":"themes/hexo-theme-matery/source/medias/ACG/4.jpg","hash":"f06a264751553ff6b2e3b158c21154ce0aacda16","modified":1602578203288},{"_id":"themes/hexo-theme-matery/source/medias/ACG/3.jpg","hash":"f779883e34f277950961f5f1ab62a433dcd91567","modified":1602578201822},{"_id":"themes/hexo-theme-matery/source/medias/ACG/8.jpg","hash":"63c5ff3afa220b73031891a08cf56146f7fd9fee","modified":1602578208159},{"_id":"themes/hexo-theme-matery/source/medias/ACG/2.jpg","hash":"28792c2f8c55005db8c0e6aeae70b394c7dd0fff","modified":1602578200558},{"_id":"themes/hexo-theme-matery/source/medias/ACG/7.jpg","hash":"f80b51d620ca4fd2878295b00c68659c7990cfa0","modified":1602578206996},{"_id":"themes/hexo-theme-matery/source/medias/ACG/6.jpg","hash":"f82a96d6e74947be00c049d57d4e49a102344774","modified":1602578205731},{"_id":"themes/hexo-theme-matery/source/medias/ACG/5.jpg","hash":"5dfa664bb0bcf5ed1480e44bbeb92291a59b3326","modified":1602578204467},{"_id":"themes/hexo-theme-matery/source/medias/ACG/1.jpg","hash":"33c9e2585ba65798847eb7d10009db4089593350","modified":1602578198957},{"_id":"public/sui-bi/my-first-blog/index.html","hash":"f083da341fc1d909ef9996332c20945f036443cb","modified":1607587146319},{"_id":"public/categories//index.html","hash":"4b8361d1765fd3ae98f39e678fe2df3b2c53d37b","modified":1607335732288},{"_id":"public/tags//index.html","hash":"14580b631c3dfb3a29b991f518c7e4f9100aeb1f","modified":1607335732288},{"_id":"public/css/prism-tomorrow.css","hash":"3b99487dfc9b4e51e9105a93743b92a761840e34","modified":1602665020148},{"_id":"public/medias/ACG/9.jpg","hash":"10b2ddfbe7f03c4513ace7a2855a47f907478d55","modified":1602665020148},{"_id":"public/medias/ACG/3.jpg","hash":"f779883e34f277950961f5f1ab62a433dcd91567","modified":1602665020148},{"_id":"public/medias/ACG/4.jpg","hash":"f06a264751553ff6b2e3b158c21154ce0aacda16","modified":1602665020148},{"_id":"public/medias/ACG/8.jpg","hash":"63c5ff3afa220b73031891a08cf56146f7fd9fee","modified":1602665020148},{"_id":"public/medias/ACG/2.jpg","hash":"28792c2f8c55005db8c0e6aeae70b394c7dd0fff","modified":1602665020148},{"_id":"public/medias/ACG/7.jpg","hash":"f80b51d620ca4fd2878295b00c68659c7990cfa0","modified":1602665020148},{"_id":"public/medias/ACG/6.jpg","hash":"f82a96d6e74947be00c049d57d4e49a102344774","modified":1602665020148},{"_id":"public/medias/ACG/5.jpg","hash":"5dfa664bb0bcf5ed1480e44bbeb92291a59b3326","modified":1602665020148},{"_id":"public/medias/ACG/1.jpg","hash":"33c9e2585ba65798847eb7d10009db4089593350","modified":1602665020148},{"_id":"source/_posts/.DS_Store","hash":"15e613ffb85d4c1c2580803f9a341ecb8eb340d1","modified":1607335687233},{"_id":"source/_posts/my-first-blog/Euphonium_Movie_2nd_KV.jpg","hash":"ca061b9e7f84622a9786b51979258304ed68f2ab","modified":1602665209583},{"_id":"source/_posts/my-first-blog/p2409965093.jpg","hash":"3e95adce661298a77553ffb74451bf8d8f94dbd3","modified":1602665506316},{"_id":"source/_posts/my-first-blog/p2382958621.png","hash":"8f905289010e20f1ec91485c06fefb9aa3969210","modified":1602665506289},{"_id":"source/_posts/my-first-blog/Euphonium_Movie_Finale_KV2.jpg","hash":"4bc062c352ba272482c27be24c373e3aa0dd99db","modified":1602665209643},{"_id":"public/sui-bi/my-first-blog/Euphonium_Movie_2nd_KV.jpg","hash":"ca061b9e7f84622a9786b51979258304ed68f2ab","modified":1602665561247},{"_id":"public/sui-bi/my-first-blog/p2409965093.jpg","hash":"3e95adce661298a77553ffb74451bf8d8f94dbd3","modified":1602665561247},{"_id":"public/sui-bi/my-first-blog/p2382958621.png","hash":"8f905289010e20f1ec91485c06fefb9aa3969210","modified":1602665561247},{"_id":"public/sui-bi/my-first-blog/Euphonium_Movie_Finale_KV2.jpg","hash":"4bc062c352ba272482c27be24c373e3aa0dd99db","modified":1602665561247},{"_id":"source/_posts/my-first-blog/relife-2.jpg","hash":"3e95adce661298a77553ffb74451bf8d8f94dbd3","modified":1602665691497},{"_id":"source/_posts/my-first-blog/relife-1.png","hash":"8f905289010e20f1ec91485c06fefb9aa3969210","modified":1602665687425},{"_id":"public/sui-bi/my-first-blog/relife-2.jpg","hash":"3e95adce661298a77553ffb74451bf8d8f94dbd3","modified":1602665761392},{"_id":"public/sui-bi/my-first-blog/relife-1.png","hash":"8f905289010e20f1ec91485c06fefb9aa3969210","modified":1602665761392},{"_id":"source/_posts/.md","hash":"a1309fc7ecd208af09188de98e92de9dd6fbe719","modified":1603879037910},{"_id":"source/_posts/sth-change-myself/2020-07-31.jpg","hash":"1e70ffa9161923f5a7b25c5f9da60f925ade49cf","modified":1602319160796},{"_id":"public/sui-bi/gai-bian-yu-lan-duo/index.html","hash":"21bb755d93994a7f7da145416bef2867cb72bd5c","modified":1607587146319},{"_id":"source/_posts//2020-07-31.jpg","hash":"1e70ffa9161923f5a7b25c5f9da60f925ade49cf","modified":1602319160796},{"_id":"public/sui-bi/gai-bian-yu-lan-duo/2020-07-31.jpg","hash":"1e70ffa9161923f5a7b25c5f9da60f925ade49cf","modified":1603877001247},{"_id":"source/_posts//IMG_9373.jpeg","hash":"3abf97bca9a9719e6c3797cb69fbaf3e3b82f183","modified":1603878562340},{"_id":"public/sui-bi/gai-bian-yu-lan-duo/IMG_9373.jpeg","hash":"3abf97bca9a9719e6c3797cb69fbaf3e3b82f183","modified":1603878590251},{"_id":"source/_posts/ubuntu.md","hash":"77ae20e01b2eda05026ae6c56d48638eb72e059c","modified":1607587287729},{"_id":"public/tags/ubuntu/index.html","hash":"41e73965a0560b2d8764e3aa03fb7c5049e337b4","modified":1607335732288},{"_id":"public/tags//index.html","hash":"bde3b098b6bc911083a64f70e381d07871e92086","modified":1607335732288},{"_id":"public/yun-wei/ji-ubuntu-chong-qi-yin-qi-de-gu-zhang-pai-cha/index.html","hash":"e1258cbdaf70fb557e6b32d11f8facebfa0689b4","modified":1607587146319},{"_id":"public/archives/2020/11/index.html","hash":"23401988a76c71b53990f4aae117ac93409c5405","modified":1607587146319},{"_id":"public/tags/grub/index.html","hash":"3383b87b3fd97130bee949b8878741dff1babd9d","modified":1607335732288},{"_id":"public/tags/linux/index.html","hash":"84209fc1e06b35056a85de545347c9b568ab387a","modified":1607335732288},{"_id":"public/categories//index.html","hash":"6219a7bb1fa9807853abe282af5f9e3070aae2f0","modified":1607335732288},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array.md","hash":"38c137779b3225cbe8640623089ecd5e7d8d2519","modified":1607587296687},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img2.jpg","hash":"8839af294b39128cc5ebac6602b4432c78f1f702","modified":1605604665263},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img3.jpg","hash":"91dc5aa98d0e31eea2c8a7e06aa02a93c2fed481","modified":1605604660564},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img6.jpg","hash":"8839af294b39128cc5ebac6602b4432c78f1f702","modified":1605605676330},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img7.jpg","hash":"99da6e3b4c7064128818cd54cf48f54464a4b379","modified":1605605679078},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img8.jpg","hash":"02e2ce27e3ec791a9f78b746b321e90def906ca3","modified":1605605684311},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img1.png","hash":"1561362bfc8b07eeca9e43b997431177a3b7601a","modified":1605601397721},{"_id":"public/algorithm-leetcode/leetcode-33-search-in-rotated-sorted-array/index.html","hash":"14e755a215967c9e99d9d29bd48df693b977acb3","modified":1605609271672},{"_id":"public/categories/Algorithm-LeetCode/index.html","hash":"9d46e00e60588a274764937e1e653a06083659f8","modified":1605609271672},{"_id":"public/tags/Algorithm/index.html","hash":"00d94af1ef33c47cf201149f3b3fb8c9afc5da07","modified":1607335732288},{"_id":"public/algorithm-leetcode/leetcode-33-search-in-rotated-sorted-array/img3.jpg","hash":"91dc5aa98d0e31eea2c8a7e06aa02a93c2fed481","modified":1605609207295},{"_id":"public/algorithm-leetcode/leetcode-33-search-in-rotated-sorted-array/img7.jpg","hash":"99da6e3b4c7064128818cd54cf48f54464a4b379","modified":1605609207295},{"_id":"public/algorithm-leetcode/leetcode-33-search-in-rotated-sorted-array/img6.jpg","hash":"8839af294b39128cc5ebac6602b4432c78f1f702","modified":1605609207295},{"_id":"public/algorithm-leetcode/leetcode-33-search-in-rotated-sorted-array/img8.jpg","hash":"02e2ce27e3ec791a9f78b746b321e90def906ca3","modified":1605609207295},{"_id":"public/algorithm-leetcode/leetcode-33-search-in-rotated-sorted-array/img2.jpg","hash":"8839af294b39128cc5ebac6602b4432c78f1f702","modified":1605609207295},{"_id":"public/algorithm-leetcode/leetcode-33-search-in-rotated-sorted-array/img1.png","hash":"1561362bfc8b07eeca9e43b997431177a3b7601a","modified":1605609207295},{"_id":"public/algorithm/leetcode/leetcode-33-search-in-rotated-sorted-array/index.html","hash":"d51f222777c9e812ba0f8b9aad05cf96023265f8","modified":1605609346756},{"_id":"public/categories/Algorithm/index.html","hash":"3de52a00d9d83f7e5f38c55423ffa0b4f692d23c","modified":1607335732288},{"_id":"public/categories/Algorithm/LeetCode/index.html","hash":"d46713a23432b4b6230b9a873316706befaf8bfd","modified":1605609346756},{"_id":"public/algorithm/leetcode/leetcode-33-search-in-rotated-sorted-array/img2.jpg","hash":"8839af294b39128cc5ebac6602b4432c78f1f702","modified":1605609346756},{"_id":"public/algorithm/leetcode/leetcode-33-search-in-rotated-sorted-array/img3.jpg","hash":"91dc5aa98d0e31eea2c8a7e06aa02a93c2fed481","modified":1605609346756},{"_id":"public/algorithm/leetcode/leetcode-33-search-in-rotated-sorted-array/img6.jpg","hash":"8839af294b39128cc5ebac6602b4432c78f1f702","modified":1605609346756},{"_id":"public/algorithm/leetcode/leetcode-33-search-in-rotated-sorted-array/img7.jpg","hash":"99da6e3b4c7064128818cd54cf48f54464a4b379","modified":1605609346756},{"_id":"public/algorithm/leetcode/leetcode-33-search-in-rotated-sorted-array/img8.jpg","hash":"02e2ce27e3ec791a9f78b746b321e90def906ca3","modified":1605609346756},{"_id":"public/algorithm/leetcode/leetcode-33-search-in-rotated-sorted-array/img1.png","hash":"1561362bfc8b07eeca9e43b997431177a3b7601a","modified":1605609346756},{"_id":"public/algorithm/leetcode-33-search-in-rotated-sorted-array/index.html","hash":"4e1c329df7da6db9e1cc8eaa15e2159986c681e7","modified":1607587146319},{"_id":"public/tags/LeetCode/index.html","hash":"10976378c71714f061f8daa7a2a85c5124330e0c","modified":1607335732288},{"_id":"public/algorithm/leetcode-33-search-in-rotated-sorted-array/img2.jpg","hash":"8839af294b39128cc5ebac6602b4432c78f1f702","modified":1605609419212},{"_id":"public/algorithm/leetcode-33-search-in-rotated-sorted-array/img6.jpg","hash":"8839af294b39128cc5ebac6602b4432c78f1f702","modified":1605609419212},{"_id":"public/algorithm/leetcode-33-search-in-rotated-sorted-array/img3.jpg","hash":"91dc5aa98d0e31eea2c8a7e06aa02a93c2fed481","modified":1605609419212},{"_id":"public/algorithm/leetcode-33-search-in-rotated-sorted-array/img7.jpg","hash":"99da6e3b4c7064128818cd54cf48f54464a4b379","modified":1605609419212},{"_id":"public/algorithm/leetcode-33-search-in-rotated-sorted-array/img8.jpg","hash":"02e2ce27e3ec791a9f78b746b321e90def906ca3","modified":1605609419212},{"_id":"public/algorithm/leetcode-33-search-in-rotated-sorted-array/img1.png","hash":"1561362bfc8b07eeca9e43b997431177a3b7601a","modified":1605609419212},{"_id":"source/_posts/-.md","hash":"3ff38ea1dbc739dfd8b6d693adbe44dd57da2bb7","modified":1607587302373},{"_id":"source/_posts/-/1.jpeg","hash":"7ab7e4f532a7ceca75c6b9dbeffe4512883483f6","modified":1606125898681},{"_id":"source/_posts/-/2.jpeg","hash":"e8afe7e8b17d1ee02551c2c6956f24fe802ad147","modified":1606125894245},{"_id":"public/algorithms/tong-bu-he-yi-bu-bian-cheng-ru-he-bing-xing-xie-tong-yi-ge-ri-zhi-wen-jian-shi-jian/index.html","hash":"310794bc97548bfc974065999f98901e6f097cf0","modified":1607587146319},{"_id":"public/categories/Algorithms/index.html","hash":"27ffcd5442f17f429c80b38ab5f8ba42eacc775b","modified":1607335732288},{"_id":"public/tags/Algorithms/index.html","hash":"3a76565f29a4e10b07cb92bed478cfc0047931ac","modified":1607335732288},{"_id":"public/tags/OS/index.html","hash":"18f61f057f5e6c1e2f3b90bbce06cbbf0f017611","modified":1607335732288},{"_id":"public/algorithms/tong-bu-he-yi-bu-bian-cheng-ru-he-bing-xing-xie-tong-yi-ge-ri-zhi-wen-jian-shi-jian/1.jpeg","hash":"7ab7e4f532a7ceca75c6b9dbeffe4512883483f6","modified":1606208305818},{"_id":"public/algorithms/tong-bu-he-yi-bu-bian-cheng-ru-he-bing-xing-xie-tong-yi-ge-ri-zhi-wen-jian-shi-jian/2.jpeg","hash":"e8afe7e8b17d1ee02551c2c6956f24fe802ad147","modified":1606208305818},{"_id":"source/_posts/GPU-in-Pytorch-.md","hash":"90673fcde2e78a233ad63307906499c67590290c","modified":1607589280141},{"_id":"source/_posts/-in-Pytorch.md","hash":"359fac6de0a159566dfeb31a9200758eb18eb270","modified":1606287881380},{"_id":"public/pytorch/gpu-in-pytorch-bing-xing-he-fen-bu-shi-shi-jian/index.html","hash":"f6a916e2b796f89f724b018caa9c1b144caf6e0b","modified":1607589283062},{"_id":"public/pytorch/bing-xing-he-fen-bu-shi-xun-lian-in-pytorch/index.html","hash":"737e7305cf12eab004519847a2160137caa2ee0c","modified":1606386112132},{"_id":"public/categories/Pytorch/index.html","hash":"e8356f0f3dcdbda1cd47bb8e731a4e83f53a3749","modified":1607335732288},{"_id":"public/tags/Pytorch/index.html","hash":"634e6af55883df0c6f5117edc6caacb30a37d977","modified":1607335732288},{"_id":"public/tags/CUDA/index.html","hash":"e4005be7375703b8047422f592a4971bc6483994","modified":1607335732288},{"_id":"public/tags/GPU/index.html","hash":"ac16a0136078dfe6fda2f803c6c658685c7de085","modified":1607335732288},{"_id":"public/tags/NLP/index.html","hash":"9602a400454f51e6ff0f75f0cd168ed803d5200b","modified":1607334899261},{"_id":"source/_posts/LeetCode-1-100-.md","hash":"77eea51910993543a022d21d671abebb38afe513","modified":1607587307037},{"_id":"public/sui-bi/leetcode-1-100-shua-ti-you-gan/index.html","hash":"190d338f8202a7a1f4cd33a5d5cc5e2a8ce4d38e","modified":1607587146319},{"_id":"public/archives/2020/12/index.html","hash":"dcd793120e98608ce8d77ca6c21f99613868c245","modified":1607587146319},{"_id":"source/_posts/LeetCode-115-Distinct-Subsequences.md","hash":"47d434aa4f3c7def280330dbf7ebbe9ffc6f468e","modified":1607509072722},{"_id":"source/_posts/GPU-in-Pytorch-/1.png","hash":"8a8b4c079a90aaa0773d0e98f8fdbfcc422ff4c8","modified":1607581522220},{"_id":"public/uncategorized/leetcode-115-distinct-subsequences/index.html","hash":"2cc7b7801bcfef9f6307d71660124ec392c02b72","modified":1607587146319},{"_id":"public/pytorch/gpu-in-pytorch-bing-xing-he-fen-bu-shi-shi-jian/1.png","hash":"8a8b4c079a90aaa0773d0e98f8fdbfcc422ff4c8","modified":1607587146319}],"Category":[{"name":"test","_id":"ckg7piwa500006a2813an1zrb"},{"name":"","_id":"ckg7qrrd000047j280hei7e4u"},{"name":"","_id":"ckh35zvby0001u72804px1qo7"},{"name":"-[Algorithm] -[LeetCode]","_id":"ckhlu9lvi0001w3289jga9zva"},{"name":"-Algorithm -LeetCode","_id":"ckhluawp60000x728ehjadxzq"},{"name":"Algorithm","_id":"ckhlubjrc0000y9282so58qf8"},{"name":"LeetCode","parent":"ckhlubjrc0000y9282so58qf8","_id":"ckhlubjrd0003y928345fh68r"},{"name":"Algorithms","_id":"ckhvqydbp00016z281mnxdhf3"},{"name":"Pytorch","_id":"ckhyotdje0002l928gyqa3gxs"}],"Data":[],"Page":[{"title":"about","date":"2020-10-13T06:35:21.000Z","type":"about","layout":"about","_content":"","source":"about/index.md","raw":"---\ntitle: about\ndate: 2020-10-13 14:35:21\ntype: 'about'\nlayout: 'about'\n---\n","updated":"2020-10-13T06:35:37.056Z","path":"about/index.html","comments":1,"_id":"ckg7lu78t0000bz288lkhb9ig","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"categories","date":"2020-10-13T06:33:52.000Z","type":"categories","layout":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2020-10-13 14:33:52\ntype: 'categories'\nlayout: 'categories'\n---","updated":"2020-10-13T08:12:35.198Z","path":"categories/index.html","_id":"ckg7lu78z0002bz281zevhen5","comments":1,"content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"tags","date":"2018-09-30T10:23:38.000Z","type":"tags","layout":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2018-09-30 18:23:38\ntype: \"tags\"\nlayout: \"tags\"\n---","updated":"2020-10-13T08:13:18.016Z","path":"tags/index.html","_id":"ckg7lu7900003bz282emkhkys","comments":1,"content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"Hello World","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\ntags: test\ncategories: test\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","slug":"hello-world","published":1,"date":"2020-10-13T08:32:14.454Z","updated":"2020-10-13T08:32:14.454Z","_id":"ckg7lu78w0001bz2813ba5wh9","comments":1,"layout":"post","photos":[],"link":"","content":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><pre class=\" language-bash\"><code class=\"language-bash\">$ hexo new <span class=\"token string\">\"My New Post\"</span></code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><pre class=\" language-bash\"><code class=\"language-bash\">$ hexo server</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><pre class=\" language-bash\"><code class=\"language-bash\">$ hexo generate</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><pre class=\" language-bash\"><code class=\"language-bash\">$ hexo deploy</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><pre><code class=\"bash\">$ hexo new &quot;My New Post&quot;</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><pre><code class=\"bash\">$ hexo server</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><pre><code class=\"bash\">$ hexo generate</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><pre><code class=\"bash\">$ hexo deploy</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n"},{"title":"","date":"2020-10-13T09:00:43.000Z","summary":"","toc":false,"mathjax":true,"top":true,"cover":true,"_content":"\nblogiOS\n\n<img src=\"Euphonium_Movie_2nd_KV.jpg\" width=\"50%\" height=\"50%\">\n\nblog[pluskid/freemind](http://freemind.pluskid.org) [](http://freemind.pluskid.org/misc/knowledge-accumulate/) \n\n7-8wordpressHexoBlogbloghexo\n\n![](relife-1.png)\n\nblogRelife  \n\n![](relife-2.jpg)","source":"_posts/my-first-blog.md","raw":"---\ntitle: \ndate: 2020-10-13 17:00:43\ncategories: \ntags: \nsummary: \ntoc: false\nmathjax: true\n#password: \n\ntop: true\ncover: true\n\n#img: #feature image\n#coverImg: # cover roll image\n---\n\nblogiOS\n\n<img src=\"Euphonium_Movie_2nd_KV.jpg\" width=\"50%\" height=\"50%\">\n\nblog[pluskid/freemind](http://freemind.pluskid.org) [](http://freemind.pluskid.org/misc/knowledge-accumulate/) \n\n7-8wordpressHexoBlogbloghexo\n\n![](relife-1.png)\n\nblogRelife  \n\n![](relife-2.jpg)","slug":"my-first-blog","published":1,"updated":"2020-11-25T06:26:55.417Z","_id":"ckg7qjivj00037j28eqt5h8o6","comments":1,"layout":"post","photos":[],"link":"","content":"<p>blogiOS</p>\n<img src=\"Euphonium_Movie_2nd_KV.jpg\" width=\"50%\" height=\"50%\">\n\n<p>blog<a href=\"http://freemind.pluskid.org/\">pluskid/freemind</a> <a href=\"http://freemind.pluskid.org/misc/knowledge-accumulate/\"></a> </p>\n<p>7-8wordpressHexoBlogbloghexo</p>\n<p><img src=\"relife-1.png\"></p>\n<p>blogRelife  </p>\n<p><img src=\"relife-2.jpg\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p>blogiOS</p>\n<img src=\"Euphonium_Movie_2nd_KV.jpg\" width=\"50%\" height=\"50%\">\n\n<p>blog<a href=\"http://freemind.pluskid.org/\">pluskid/freemind</a> <a href=\"http://freemind.pluskid.org/misc/knowledge-accumulate/\"></a> </p>\n<p>7-8wordpressHexoBlogbloghexo</p>\n<p><img src=\"relife-1.png\"></p>\n<p>blogRelife  </p>\n<p><img src=\"relife-2.jpg\"></p>\n"},{"title":"","toc":true,"mathjax":true,"top":true,"cover":true,"date":"2020-10-27T06:52:29.000Z","_content":"\n\n\n![2020-07-31](IMG_9373.jpeg)\n\n731   **** \n\n  \n\n\t\n\n\t*** ***\n\n\n\n 10offerOJ   **** \n\n\n\n\n\n\n\n![](2020-07-31.jpg)","source":"_posts/.md","raw":"---\ntitle: \ntoc: true\nmathjax: true\ntop: true\ncover: true\ndate: 2020-10-27 14:52:29\ncategories: \ntags: \n---\n\n\n\n![2020-07-31](IMG_9373.jpeg)\n\n731   **** \n\n  \n\n\t\n\n\t*** ***\n\n\n\n 10offerOJ   **** \n\n\n\n\n\n\n\n![](2020-07-31.jpg)","slug":"","published":1,"updated":"2020-10-28T09:57:17.910Z","_id":"ckgt6w3by0000wx280fap2hbl","comments":1,"layout":"post","photos":[],"link":"","content":"<p></p>\n<p><img src=\"IMG_9373.jpeg\" alt=\"2020-07-31\"></p>\n<p>731   <strong></strong> </p>\n<p>  </p>\n<p>    </p>\n<p>    <strong><em> </em></strong></p>\n<p></p>\n<p> 10offerOJ   <strong></strong> </p>\n<p></p>\n<p></p>\n<p></p>\n<p><img src=\"2020-07-31.jpg\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p></p>\n<p><img src=\"IMG_9373.jpeg\" alt=\"2020-07-31\"></p>\n<p>731   <strong></strong> </p>\n<p>  </p>\n<p>    </p>\n<p>    <strong><em> </em></strong></p>\n<p></p>\n<p> 10offerOJ   <strong></strong> </p>\n<p></p>\n<p></p>\n<p></p>\n<p><img src=\"2020-07-31.jpg\"></p>\n"},{"title":"ubuntu","toc":true,"mathjax":true,"top":false,"cover":true,"date":"2020-11-04T07:25:29.000Z","_content":"\nubuntu 18.04 LTSB \n\n### \n\nipv4ipv6\n\n```bash\nifconfig -a # ipifconfig \n# or\nip a\n```\n\n\n\nipv6ipB 3mins ipUI\n\n```bash\nvi /etc/network/interfaces\n# change\nauto enp1s0 # enp5s0 ifconfig \niface enp1s0 inet static\n\t\taddress x.x.x.x\n\t\tnetmask 255.255.255.0\n\t\tgateway x.x.x.x\n\t\tdns-nameservers 114.114.114.114 8.8.8.8\n\t\t\nsudo ip a flush enp1s0\nsudo systemctl restart networking.service\n```\n\n networking.service  Ubuntu 18.04 LTS  netplan netplanip\n\n```bash\nvi /etc/netplan/50-cloud-init.yaml # yaml   \\t\n# change\n# This file describes the network interfaces available on your system\n# For more information, see netplan(5).\nnetwork:\n  version: 2\n  renderer: networkd\n  ethernets:\n    enp1s0:\n     dhcp4: no\n     addresses: [192.168.1.222/24]\n     gateway4: 192.168.1.1\n     nameservers:\n       addresses: [8.8.8.8,8.8.4.4]\n       \nsudo netplan apply\n# or\nsudo netplan --dubug apply\n```\n\n netplan /etc/netplan/50-cloud-init.yaml StackOverFlow  StackExchange  Ubuntu 16.04  Upgrade Ubuntu 18.04  netplan  /etc/network/interfaces ip\n\n```bash\n# netplan  \nsudo apt install netplan.io # \n```\n\n\n\nipUbuntu 18.04 Upgrade ip24h\n\n```bash\n#  ip  dns \n#  \nifconfig enp1s0 x.x.x.x netmask 255.255.255.0\nroute add default gw x.x.x.x\nvi /etc/resolf.conf\n\n```\n\n\n\n### nvidia-smi  docker \n\nnvidia-smidocker\n\n```bash\nnvidia-smi\nNVIDIA-SMI has failed because it couldnt communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n```\n\n### \n\n Ubuntu ubuntunvidia Ubuntu 18.04 linux\n\n\n\n### \n\n grub ,Advanced options for Ubuntu -> Ubuntu , with linux x.x.x-x-generic  4.15.0-60-generic 4.15.0-122-generic\n\n \n\n```bash\n# \ndpkg --get-selections |grep linux-image\nlinux-image-4.15.0-122-generic\t\t\tdeinstall\nlinux-image-4.15.0-60-generic\t\t\tinstall\nlinux-image-4.15.0-62-generic\t\t\tdeinstall\n\n#  grub  \ngrep menuentry /boot/grub/grub.cfg\nmenuentry 'Ubuntu, with Linux 4.15.0-60-generic' \n...\n```\n\n#### Solution 1: grub\n\n```bash\nvi /etc/default/grub\n# change\nGRUB_DEFAULT=Advanced options for Ubuntu > Ubuntu, with Linux 4.15.0-60-generic\n#   0\nGRUB_DEFAULT = \"1> 4\" #\n\nGRUB_TIMEOUT_STYLE=menu # default: hidden\nGRUB_TIMEOUT=3 # default: 0\n\nsudo update-grub\nsudo reboot\n```\n\n#### Solution 2: \n\n```bash\n#  \nsudo apt remove linux-image-xxx-xx-generic\n# or\nsudo dpkg --purge linux-image-x.x.x-xx-generic\n# \nsudo apt install linux-headers-x.x.x-x-generic linux-image-x.x.x-x-generic\n# \nsudo apt-mark hold linux-image-generic linux-headers-generic\n# \nsudo apt-mark unhold linux-image-generic linux-headers-generic\n```\n\n### Conclusion\n\nnvidiadocker netplan \n\n\n\nReferences:\n\n[How to configure static IP address on Ubuntu 18.04 ](https://linuxconfig.org/how-to-configure-static-ip-address-on-ubuntu-18-04-bionic-beaver-linux)\n\n[How to enable netplan on ubuntu server upgraded from 16.04 to 18.04](https://askubuntu.com/questions/1034711/how-to-enable-netplan-on-ubuntu-server-upgraded-from-16-04-to-18-04)\n\n[ubuntu18.04 ](https://blog.csdn.net/qq_43222384/article/details/90314297)","source":"_posts/ubuntu.md","raw":"---\ntitle: ubuntu\ntoc: true\nmathjax: true\ntop: false\ncover: true\ndate: 2020-11-04 15:25:29\ncategories: \ntags:\n- ubuntu\n- \n- linux\n- grub\n---\n\nubuntu 18.04 LTSB \n\n### \n\nipv4ipv6\n\n```bash\nifconfig -a # ipifconfig \n# or\nip a\n```\n\n\n\nipv6ipB 3mins ipUI\n\n```bash\nvi /etc/network/interfaces\n# change\nauto enp1s0 # enp5s0 ifconfig \niface enp1s0 inet static\n\t\taddress x.x.x.x\n\t\tnetmask 255.255.255.0\n\t\tgateway x.x.x.x\n\t\tdns-nameservers 114.114.114.114 8.8.8.8\n\t\t\nsudo ip a flush enp1s0\nsudo systemctl restart networking.service\n```\n\n networking.service  Ubuntu 18.04 LTS  netplan netplanip\n\n```bash\nvi /etc/netplan/50-cloud-init.yaml # yaml   \\t\n# change\n# This file describes the network interfaces available on your system\n# For more information, see netplan(5).\nnetwork:\n  version: 2\n  renderer: networkd\n  ethernets:\n    enp1s0:\n     dhcp4: no\n     addresses: [192.168.1.222/24]\n     gateway4: 192.168.1.1\n     nameservers:\n       addresses: [8.8.8.8,8.8.4.4]\n       \nsudo netplan apply\n# or\nsudo netplan --dubug apply\n```\n\n netplan /etc/netplan/50-cloud-init.yaml StackOverFlow  StackExchange  Ubuntu 16.04  Upgrade Ubuntu 18.04  netplan  /etc/network/interfaces ip\n\n```bash\n# netplan  \nsudo apt install netplan.io # \n```\n\n\n\nipUbuntu 18.04 Upgrade ip24h\n\n```bash\n#  ip  dns \n#  \nifconfig enp1s0 x.x.x.x netmask 255.255.255.0\nroute add default gw x.x.x.x\nvi /etc/resolf.conf\n\n```\n\n\n\n### nvidia-smi  docker \n\nnvidia-smidocker\n\n```bash\nnvidia-smi\nNVIDIA-SMI has failed because it couldnt communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n```\n\n### \n\n Ubuntu ubuntunvidia Ubuntu 18.04 linux\n\n\n\n### \n\n grub ,Advanced options for Ubuntu -> Ubuntu , with linux x.x.x-x-generic  4.15.0-60-generic 4.15.0-122-generic\n\n \n\n```bash\n# \ndpkg --get-selections |grep linux-image\nlinux-image-4.15.0-122-generic\t\t\tdeinstall\nlinux-image-4.15.0-60-generic\t\t\tinstall\nlinux-image-4.15.0-62-generic\t\t\tdeinstall\n\n#  grub  \ngrep menuentry /boot/grub/grub.cfg\nmenuentry 'Ubuntu, with Linux 4.15.0-60-generic' \n...\n```\n\n#### Solution 1: grub\n\n```bash\nvi /etc/default/grub\n# change\nGRUB_DEFAULT=Advanced options for Ubuntu > Ubuntu, with Linux 4.15.0-60-generic\n#   0\nGRUB_DEFAULT = \"1> 4\" #\n\nGRUB_TIMEOUT_STYLE=menu # default: hidden\nGRUB_TIMEOUT=3 # default: 0\n\nsudo update-grub\nsudo reboot\n```\n\n#### Solution 2: \n\n```bash\n#  \nsudo apt remove linux-image-xxx-xx-generic\n# or\nsudo dpkg --purge linux-image-x.x.x-xx-generic\n# \nsudo apt install linux-headers-x.x.x-x-generic linux-image-x.x.x-x-generic\n# \nsudo apt-mark hold linux-image-generic linux-headers-generic\n# \nsudo apt-mark unhold linux-image-generic linux-headers-generic\n```\n\n### Conclusion\n\nnvidiadocker netplan \n\n\n\nReferences:\n\n[How to configure static IP address on Ubuntu 18.04 ](https://linuxconfig.org/how-to-configure-static-ip-address-on-ubuntu-18-04-bionic-beaver-linux)\n\n[How to enable netplan on ubuntu server upgraded from 16.04 to 18.04](https://askubuntu.com/questions/1034711/how-to-enable-netplan-on-ubuntu-server-upgraded-from-16-04-to-18-04)\n\n[ubuntu18.04 ](https://blog.csdn.net/qq_43222384/article/details/90314297)","slug":"ubuntu","published":1,"updated":"2020-12-10T08:01:27.729Z","_id":"ckh35zvbu0000u7282nma27nl","comments":1,"layout":"post","photos":[],"link":"","content":"<p>ubuntu 18.04 LTSB </p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>ipv4ipv6</p>\n<pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token function\">ifconfig</span> -a <span class=\"token comment\" spellcheck=\"true\"># ipifconfig </span>\n<span class=\"token comment\" spellcheck=\"true\"># or</span>\nip a</code></pre>\n<p>ipv6ipB 3mins ipUI</p>\n<pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token function\">vi</span> /etc/network/interfaces\n<span class=\"token comment\" spellcheck=\"true\"># change</span>\nauto enp1s0 <span class=\"token comment\" spellcheck=\"true\"># enp5s0 ifconfig </span>\niface enp1s0 inet static\n        address x.x.x.x\n        netmask 255.255.255.0\n        gateway x.x.x.x\n        dns-nameservers 114.114.114.114 8.8.8.8\n\n<span class=\"token function\">sudo</span> ip a flush enp1s0\n<span class=\"token function\">sudo</span> systemctl restart networking.service</code></pre>\n<p> networking.service  Ubuntu 18.04 LTS  netplan netplanip</p>\n<pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token function\">vi</span> /etc/netplan/50-cloud-init.yaml <span class=\"token comment\" spellcheck=\"true\"># yaml   \\t</span>\n<span class=\"token comment\" spellcheck=\"true\"># change</span>\n<span class=\"token comment\" spellcheck=\"true\"># This file describes the network interfaces available on your system</span>\n<span class=\"token comment\" spellcheck=\"true\"># For more information, see netplan(5).</span>\nnetwork:\n  version: 2\n  renderer: networkd\n  ethernets:\n    enp1s0:\n     dhcp4: no\n     addresses: <span class=\"token punctuation\">[</span>192.168.1.222/24<span class=\"token punctuation\">]</span>\n     gateway4: 192.168.1.1\n     nameservers:\n       addresses: <span class=\"token punctuation\">[</span>8.8.8.8,8.8.4.4<span class=\"token punctuation\">]</span>\n\n<span class=\"token function\">sudo</span> netplan apply\n<span class=\"token comment\" spellcheck=\"true\"># or</span>\n<span class=\"token function\">sudo</span> netplan --dubug apply</code></pre>\n<p> netplan /etc/netplan/50-cloud-init.yaml StackOverFlow  StackExchange  Ubuntu 16.04  Upgrade Ubuntu 18.04  netplan  /etc/network/interfaces ip</p>\n<pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token comment\" spellcheck=\"true\"># netplan  </span>\n<span class=\"token function\">sudo</span> apt <span class=\"token function\">install</span> netplan.io <span class=\"token comment\" spellcheck=\"true\"># </span></code></pre>\n<p>ipUbuntu 18.04 Upgrade ip24h</p>\n<pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token comment\" spellcheck=\"true\">#  ip  dns </span>\n<span class=\"token comment\" spellcheck=\"true\">#  </span>\n<span class=\"token function\">ifconfig</span> enp1s0 x.x.x.x netmask 255.255.255.0\nroute add default gw x.x.x.x\n<span class=\"token function\">vi</span> /etc/resolf.conf\n</code></pre>\n<h3 id=\"nvidia-smi--docker-\"><a href=\"#nvidia-smi--docker-\" class=\"headerlink\" title=\"nvidia-smi  docker \"></a>nvidia-smi  docker </h3><p>nvidia-smidocker</p>\n<pre class=\" language-bash\"><code class=\"language-bash\">nvidia-smi\nNVIDIA-SMI has failed because it couldnt communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.</code></pre>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> Ubuntu ubuntunvidia Ubuntu 18.04 linux</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> grub ,Advanced options for Ubuntu -&gt; Ubuntu , with linux x.x.x-x-generic  4.15.0-60-generic 4.15.0-122-generic</p>\n<p> </p>\n<pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token comment\" spellcheck=\"true\"># </span>\ndpkg --get-selections <span class=\"token operator\">|</span><span class=\"token function\">grep</span> linux-image\nlinux-image-4.15.0-122-generic            deinstall\nlinux-image-4.15.0-60-generic            <span class=\"token function\">install</span>\nlinux-image-4.15.0-62-generic            deinstall\n\n<span class=\"token comment\" spellcheck=\"true\">#  grub  </span>\n<span class=\"token function\">grep</span> menuentry /boot/grub/grub.cfg\nmenuentry <span class=\"token string\">'Ubuntu, with Linux 4.15.0-60-generic'</span> \n<span class=\"token punctuation\">..</span>.</code></pre>\n<h4 id=\"Solution-1-grub\"><a href=\"#Solution-1-grub\" class=\"headerlink\" title=\"Solution 1: grub\"></a>Solution 1: grub</h4><pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token function\">vi</span> /etc/default/grub\n<span class=\"token comment\" spellcheck=\"true\"># change</span>\nGRUB_DEFAULT<span class=\"token operator\">=</span>Advanced options <span class=\"token keyword\">for</span> Ubuntu <span class=\"token operator\">></span> Ubuntu, with Linux 4.15.0-60-generic\n<span class=\"token comment\" spellcheck=\"true\">#   0</span>\nGRUB_DEFAULT <span class=\"token operator\">=</span> <span class=\"token string\">\"1> 4\"</span> <span class=\"token comment\" spellcheck=\"true\">#</span>\n\nGRUB_TIMEOUT_STYLE<span class=\"token operator\">=</span>menu <span class=\"token comment\" spellcheck=\"true\"># default: hidden</span>\nGRUB_TIMEOUT<span class=\"token operator\">=</span>3 <span class=\"token comment\" spellcheck=\"true\"># default: 0</span>\n\n<span class=\"token function\">sudo</span> update-grub\n<span class=\"token function\">sudo</span> <span class=\"token function\">reboot</span></code></pre>\n<h4 id=\"Solution-2-\"><a href=\"#Solution-2-\" class=\"headerlink\" title=\"Solution 2: \"></a>Solution 2: </h4><pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token comment\" spellcheck=\"true\">#  </span>\n<span class=\"token function\">sudo</span> apt remove linux-image-xxx-xx-generic\n<span class=\"token comment\" spellcheck=\"true\"># or</span>\n<span class=\"token function\">sudo</span> dpkg --purge linux-image-x.x.x-xx-generic\n<span class=\"token comment\" spellcheck=\"true\"># </span>\n<span class=\"token function\">sudo</span> apt <span class=\"token function\">install</span> linux-headers-x.x.x-x-generic linux-image-x.x.x-x-generic\n<span class=\"token comment\" spellcheck=\"true\"># </span>\n<span class=\"token function\">sudo</span> apt-mark hold linux-image-generic linux-headers-generic\n<span class=\"token comment\" spellcheck=\"true\"># </span>\n<span class=\"token function\">sudo</span> apt-mark unhold linux-image-generic linux-headers-generic</code></pre>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>nvidiadocker netplan </p>\n<p>References:</p>\n<p><a href=\"https://linuxconfig.org/how-to-configure-static-ip-address-on-ubuntu-18-04-bionic-beaver-linux\">How to configure static IP address on Ubuntu 18.04 </a></p>\n<p><a href=\"https://askubuntu.com/questions/1034711/how-to-enable-netplan-on-ubuntu-server-upgraded-from-16-04-to-18-04\">How to enable netplan on ubuntu server upgraded from 16.04 to 18.04</a></p>\n<p><a href=\"https://blog.csdn.net/qq_43222384/article/details/90314297\">ubuntu18.04 </a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>ubuntu 18.04 LTSB </p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>ipv4ipv6</p>\n<pre><code class=\"bash\">ifconfig -a # ipifconfig \n# or\nip a</code></pre>\n<p>ipv6ipB 3mins ipUI</p>\n<pre><code class=\"bash\">vi /etc/network/interfaces\n# change\nauto enp1s0 # enp5s0 ifconfig \niface enp1s0 inet static\n        address x.x.x.x\n        netmask 255.255.255.0\n        gateway x.x.x.x\n        dns-nameservers 114.114.114.114 8.8.8.8\n\nsudo ip a flush enp1s0\nsudo systemctl restart networking.service</code></pre>\n<p> networking.service  Ubuntu 18.04 LTS  netplan netplanip</p>\n<pre><code class=\"bash\">vi /etc/netplan/50-cloud-init.yaml # yaml   \\t\n# change\n# This file describes the network interfaces available on your system\n# For more information, see netplan(5).\nnetwork:\n  version: 2\n  renderer: networkd\n  ethernets:\n    enp1s0:\n     dhcp4: no\n     addresses: [192.168.1.222/24]\n     gateway4: 192.168.1.1\n     nameservers:\n       addresses: [8.8.8.8,8.8.4.4]\n\nsudo netplan apply\n# or\nsudo netplan --dubug apply</code></pre>\n<p> netplan /etc/netplan/50-cloud-init.yaml StackOverFlow  StackExchange  Ubuntu 16.04  Upgrade Ubuntu 18.04  netplan  /etc/network/interfaces ip</p>\n<pre><code class=\"bash\"># netplan  \nsudo apt install netplan.io # </code></pre>\n<p>ipUbuntu 18.04 Upgrade ip24h</p>\n<pre><code class=\"bash\">#  ip  dns \n#  \nifconfig enp1s0 x.x.x.x netmask 255.255.255.0\nroute add default gw x.x.x.x\nvi /etc/resolf.conf\n</code></pre>\n<h3 id=\"nvidia-smi--docker-\"><a href=\"#nvidia-smi--docker-\" class=\"headerlink\" title=\"nvidia-smi  docker \"></a>nvidia-smi  docker </h3><p>nvidia-smidocker</p>\n<pre><code class=\"bash\">nvidia-smi\nNVIDIA-SMI has failed because it couldnt communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.</code></pre>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> Ubuntu ubuntunvidia Ubuntu 18.04 linux</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> grub ,Advanced options for Ubuntu -&gt; Ubuntu , with linux x.x.x-x-generic  4.15.0-60-generic 4.15.0-122-generic</p>\n<p> </p>\n<pre><code class=\"bash\"># \ndpkg --get-selections |grep linux-image\nlinux-image-4.15.0-122-generic            deinstall\nlinux-image-4.15.0-60-generic            install\nlinux-image-4.15.0-62-generic            deinstall\n\n#  grub  \ngrep menuentry /boot/grub/grub.cfg\nmenuentry &#39;Ubuntu, with Linux 4.15.0-60-generic&#39; \n...</code></pre>\n<h4 id=\"Solution-1-grub\"><a href=\"#Solution-1-grub\" class=\"headerlink\" title=\"Solution 1: grub\"></a>Solution 1: grub</h4><pre><code class=\"bash\">vi /etc/default/grub\n# change\nGRUB_DEFAULT=Advanced options for Ubuntu &gt; Ubuntu, with Linux 4.15.0-60-generic\n#   0\nGRUB_DEFAULT = &quot;1&gt; 4&quot; #\n\nGRUB_TIMEOUT_STYLE=menu # default: hidden\nGRUB_TIMEOUT=3 # default: 0\n\nsudo update-grub\nsudo reboot</code></pre>\n<h4 id=\"Solution-2-\"><a href=\"#Solution-2-\" class=\"headerlink\" title=\"Solution 2: \"></a>Solution 2: </h4><pre><code class=\"bash\">#  \nsudo apt remove linux-image-xxx-xx-generic\n# or\nsudo dpkg --purge linux-image-x.x.x-xx-generic\n# \nsudo apt install linux-headers-x.x.x-x-generic linux-image-x.x.x-x-generic\n# \nsudo apt-mark hold linux-image-generic linux-headers-generic\n# \nsudo apt-mark unhold linux-image-generic linux-headers-generic</code></pre>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>nvidiadocker netplan </p>\n<p>References:</p>\n<p><a href=\"https://linuxconfig.org/how-to-configure-static-ip-address-on-ubuntu-18-04-bionic-beaver-linux\">How to configure static IP address on Ubuntu 18.04 </a></p>\n<p><a href=\"https://askubuntu.com/questions/1034711/how-to-enable-netplan-on-ubuntu-server-upgraded-from-16-04-to-18-04\">How to enable netplan on ubuntu server upgraded from 16.04 to 18.04</a></p>\n<p><a href=\"https://blog.csdn.net/qq_43222384/article/details/90314297\">ubuntu18.04 </a></p>\n"},{"title":"LeetCode 33. Search in Rotated Sorted Array","toc":true,"mathjax":true,"top":false,"cover":true,"date":"2020-11-17T08:19:58.000Z","updated":"2020-12-10T08:01:36.687Z","_content":"\n### LeetCode 33. Search in Rotated Sorted Array\n\n![](img1.png)\n\nif\n\n#### \n\ntarget $O(log(n))$$O(log(n))$start, mid, end \n\n- start  mid:\n\nmid > start :  in left\n\n![](img3.jpg)\n\nmid < start :  in left:\n\n![](img2.jpg)\n\n- mid  end\n\nmid < end:  in leftend = mid\n\n![](img6.jpg)\n\nmid > end :  in right, start = mid\n\n![](img7.jpg)\n\nmid  end\n\n![](img8.jpg)\n\nin right mid in right midvalue start = mid + 1 \n\n```python\nwhile start < end:\n\tmid = (start + end ) //2\n\tif nums[start] > nums[end]:\n\t\tstart = mid + 1\n\telse:\n\t\tend = mid\nbias = start\n```\n\nmid  midtarget\n\n```python\nstart = 0\nend = len(nums) - 1\nwhile start <= end:\n\tmid = (start+end) // 2\n\tmid_pos = (mid + bias) \t% len(nums)\n\tval = nums[mid_pos]\n\tif target == value:\n\t\treturn mid_pos\n\tif target < value:\n\t\tend = mid - 1\n\telse:\n\t\tstart = mid + 1\n    \nreturn -1\n```\n\n\n\nTime Complexity: $O(log(n))$\n\nSpace Complexity: $O(1)$\n\n#### \n\n22,[4 5 6 7 1 2 3 ] [4 5 6 7 ][1 2 3 ]target-inf or inf\n\n[ 4 5 6 7 1 2 3]  target = 5 [ 4 5 6 7 inf inf inf ]\n\n[ 4 5 6 7 1 2 3]  target = 2 [ -inf -inf - inf -inf 1 2 3]\n\nnums[mid]valvaltargetvalvaltarget-inf or inf\n\n- nums[mid]  target : \n\n```python\nnums[mid] > nums[0] and target > nums[0]\nor\nnums[mid] < nums[0] and target < nums[0]\n```\n\n\n\n- nums[mid]  target \n\n```python\nnums[mid] > nums[0] and target < nums[0]\nor\nnums[mid] < nums[0] and target > nums[0]\n```\n\nnums[mid]valnumstarget-inf or inf\n\n```python\nlo = 0\nhi = len(nums) - 1\n\nwhile lo <= hi:\n\tmid = (lo+hi) // 2\n\tval = nums[mid]\n\tif (val > nums[0] ) == (target > nums[0]):\n\t\tpass\n\telse:\n\t\tval =  float('-inf') if target < nums[0] else float('inf')\n   \n\tif val < target:\n\t\tlo = mid + 1\n\telif val > target:\n\t\thi = mid - 1\n\telse:\n\t\treturn mid\n      \n  return -1\n```\n\nTime Complexity: $O(log(n))$\n\nSpace Complexity: $O(1)$\n\n#### \n\n\n\n target\n\n```python\nlo = 0\nhi = len(nums) - 1\n\nwhile lo <= hi:\n\tmid = (lo+hi) // 2\n\tval = nums[mid]\n\tif target == val:\n\t\treturn mid\n\t\t# lo-mid \n\tif nums[lo] <= val:\n\t\tif target >= nums[lo] and target < nums[mid]:\n\t\t\thi = mid - 1\n\t\telse:\n\t\t\tlo = mid + 1\n\telse: # mid-hi \n\t\tif target > nums[mid] and target <= nums[hi]:\n\t\t\tlo = mid + 1\n\t\telse:\n\t\t\thi = mid - 1\n```\n\nTime Complexity: $O(log(n))$\n\nSpace Complexity: $O(1)$\n\n\n\n### Conclusion\n\n$log(n)$target  nums[mid]nums[mid]iftargettarget\n\n\n\nReference:\n\n[LeetCode 33. Search in Rotated Sorted Array](https://leetcode.wang/leetCode-33-Search-in-Rotated-Sorted-Array.html)\n\n","source":"_posts/LeetCode-33-Search-in-Rotated-Sorted-Array.md","raw":"---\ntitle: LeetCode 33. Search in Rotated Sorted Array\ntoc: true\nmathjax: true\ntop: false\ncover: true\ndate: 2020-11-17 16:19:58\nupdated:\ncategories: Algorithm\ntags:\n\t- Algorithm\n\t- LeetCode\n---\n\n### LeetCode 33. Search in Rotated Sorted Array\n\n![](img1.png)\n\nif\n\n#### \n\ntarget $O(log(n))$$O(log(n))$start, mid, end \n\n- start  mid:\n\nmid > start :  in left\n\n![](img3.jpg)\n\nmid < start :  in left:\n\n![](img2.jpg)\n\n- mid  end\n\nmid < end:  in leftend = mid\n\n![](img6.jpg)\n\nmid > end :  in right, start = mid\n\n![](img7.jpg)\n\nmid  end\n\n![](img8.jpg)\n\nin right mid in right midvalue start = mid + 1 \n\n```python\nwhile start < end:\n\tmid = (start + end ) //2\n\tif nums[start] > nums[end]:\n\t\tstart = mid + 1\n\telse:\n\t\tend = mid\nbias = start\n```\n\nmid  midtarget\n\n```python\nstart = 0\nend = len(nums) - 1\nwhile start <= end:\n\tmid = (start+end) // 2\n\tmid_pos = (mid + bias) \t% len(nums)\n\tval = nums[mid_pos]\n\tif target == value:\n\t\treturn mid_pos\n\tif target < value:\n\t\tend = mid - 1\n\telse:\n\t\tstart = mid + 1\n    \nreturn -1\n```\n\n\n\nTime Complexity: $O(log(n))$\n\nSpace Complexity: $O(1)$\n\n#### \n\n22,[4 5 6 7 1 2 3 ] [4 5 6 7 ][1 2 3 ]target-inf or inf\n\n[ 4 5 6 7 1 2 3]  target = 5 [ 4 5 6 7 inf inf inf ]\n\n[ 4 5 6 7 1 2 3]  target = 2 [ -inf -inf - inf -inf 1 2 3]\n\nnums[mid]valvaltargetvalvaltarget-inf or inf\n\n- nums[mid]  target : \n\n```python\nnums[mid] > nums[0] and target > nums[0]\nor\nnums[mid] < nums[0] and target < nums[0]\n```\n\n\n\n- nums[mid]  target \n\n```python\nnums[mid] > nums[0] and target < nums[0]\nor\nnums[mid] < nums[0] and target > nums[0]\n```\n\nnums[mid]valnumstarget-inf or inf\n\n```python\nlo = 0\nhi = len(nums) - 1\n\nwhile lo <= hi:\n\tmid = (lo+hi) // 2\n\tval = nums[mid]\n\tif (val > nums[0] ) == (target > nums[0]):\n\t\tpass\n\telse:\n\t\tval =  float('-inf') if target < nums[0] else float('inf')\n   \n\tif val < target:\n\t\tlo = mid + 1\n\telif val > target:\n\t\thi = mid - 1\n\telse:\n\t\treturn mid\n      \n  return -1\n```\n\nTime Complexity: $O(log(n))$\n\nSpace Complexity: $O(1)$\n\n#### \n\n\n\n target\n\n```python\nlo = 0\nhi = len(nums) - 1\n\nwhile lo <= hi:\n\tmid = (lo+hi) // 2\n\tval = nums[mid]\n\tif target == val:\n\t\treturn mid\n\t\t# lo-mid \n\tif nums[lo] <= val:\n\t\tif target >= nums[lo] and target < nums[mid]:\n\t\t\thi = mid - 1\n\t\telse:\n\t\t\tlo = mid + 1\n\telse: # mid-hi \n\t\tif target > nums[mid] and target <= nums[hi]:\n\t\t\tlo = mid + 1\n\t\telse:\n\t\t\thi = mid - 1\n```\n\nTime Complexity: $O(log(n))$\n\nSpace Complexity: $O(1)$\n\n\n\n### Conclusion\n\n$log(n)$target  nums[mid]nums[mid]iftargettarget\n\n\n\nReference:\n\n[LeetCode 33. Search in Rotated Sorted Array](https://leetcode.wang/leetCode-33-Search-in-Rotated-Sorted-Array.html)\n\n","slug":"LeetCode-33-Search-in-Rotated-Sorted-Array","published":1,"_id":"ckhlu9lv60000w328appuchpc","comments":1,"layout":"post","photos":[],"link":"","content":"<h3 id=\"LeetCode-33-Search-in-Rotated-Sorted-Array\"><a href=\"#LeetCode-33-Search-in-Rotated-Sorted-Array\" class=\"headerlink\" title=\"LeetCode 33. Search in Rotated Sorted Array\"></a>LeetCode 33. Search in Rotated Sorted Array</h3><p><img src=\"img1.png\"></p>\n<p>if</p>\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><p>target $O(log(n))$$O(log(n))$start, mid, end </p>\n<ul>\n<li>start  mid:</li>\n</ul>\n<p>mid &gt; start :  in left</p>\n<p><img src=\"img3.jpg\"></p>\n<p>mid &lt; start :  in left:</p>\n<p><img src=\"img2.jpg\"></p>\n<ul>\n<li>mid  end</li>\n</ul>\n<p>mid &lt; end:  in leftend = mid</p>\n<p><img src=\"img6.jpg\"></p>\n<p>mid &gt; end :  in right, start = mid</p>\n<p><img src=\"img7.jpg\"></p>\n<p>mid  end</p>\n<p><img src=\"img8.jpg\"></p>\n<p>in right mid in right midvalue start = mid + 1 </p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">while</span> start <span class=\"token operator\">&lt;</span> end<span class=\"token punctuation\">:</span>\n    mid <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>start <span class=\"token operator\">+</span> end <span class=\"token punctuation\">)</span> <span class=\"token operator\">//</span><span class=\"token number\">2</span>\n    <span class=\"token keyword\">if</span> nums<span class=\"token punctuation\">[</span>start<span class=\"token punctuation\">]</span> <span class=\"token operator\">></span> nums<span class=\"token punctuation\">[</span>end<span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n        start <span class=\"token operator\">=</span> mid <span class=\"token operator\">+</span> <span class=\"token number\">1</span>\n    <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n        end <span class=\"token operator\">=</span> mid\nbias <span class=\"token operator\">=</span> start</code></pre>\n<p>mid  midtarget</p>\n<pre class=\" language-python\"><code class=\"language-python\">start <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\nend <span class=\"token operator\">=</span> len<span class=\"token punctuation\">(</span>nums<span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span> <span class=\"token number\">1</span>\n<span class=\"token keyword\">while</span> start <span class=\"token operator\">&lt;=</span> end<span class=\"token punctuation\">:</span>\n    mid <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>start<span class=\"token operator\">+</span>end<span class=\"token punctuation\">)</span> <span class=\"token operator\">//</span> <span class=\"token number\">2</span>\n    mid_pos <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>mid <span class=\"token operator\">+</span> bias<span class=\"token punctuation\">)</span>     <span class=\"token operator\">%</span> len<span class=\"token punctuation\">(</span>nums<span class=\"token punctuation\">)</span>\n    val <span class=\"token operator\">=</span> nums<span class=\"token punctuation\">[</span>mid_pos<span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">if</span> target <span class=\"token operator\">==</span> value<span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> mid_pos\n    <span class=\"token keyword\">if</span> target <span class=\"token operator\">&lt;</span> value<span class=\"token punctuation\">:</span>\n        end <span class=\"token operator\">=</span> mid <span class=\"token operator\">-</span> <span class=\"token number\">1</span>\n    <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n        start <span class=\"token operator\">=</span> mid <span class=\"token operator\">+</span> <span class=\"token number\">1</span>\n\n<span class=\"token keyword\">return</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span></code></pre>\n<p>Time Complexity: $O(log(n))$</p>\n<p>Space Complexity: $O(1)$</p>\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><p>22,[4 5 6 7 1 2 3 ] [4 5 6 7 ][1 2 3 ]target-inf or inf</p>\n<p>[ 4 5 6 7 1 2 3]  target = 5 [ 4 5 6 7 inf inf inf ]</p>\n<p>[ 4 5 6 7 1 2 3]  target = 2 [ -inf -inf - inf -inf 1 2 3]</p>\n<p>nums[mid]valvaltargetvalvaltarget-inf or inf</p>\n<ul>\n<li>nums[mid]  target : </li>\n</ul>\n<pre class=\" language-python\"><code class=\"language-python\">nums<span class=\"token punctuation\">[</span>mid<span class=\"token punctuation\">]</span> <span class=\"token operator\">></span> nums<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">and</span> target <span class=\"token operator\">></span> nums<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\n<span class=\"token operator\">or</span>\nnums<span class=\"token punctuation\">[</span>mid<span class=\"token punctuation\">]</span> <span class=\"token operator\">&lt;</span> nums<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">and</span> target <span class=\"token operator\">&lt;</span> nums<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span></code></pre>\n<ul>\n<li>nums[mid]  target </li>\n</ul>\n<pre class=\" language-python\"><code class=\"language-python\">nums<span class=\"token punctuation\">[</span>mid<span class=\"token punctuation\">]</span> <span class=\"token operator\">></span> nums<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">and</span> target <span class=\"token operator\">&lt;</span> nums<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\n<span class=\"token operator\">or</span>\nnums<span class=\"token punctuation\">[</span>mid<span class=\"token punctuation\">]</span> <span class=\"token operator\">&lt;</span> nums<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">and</span> target <span class=\"token operator\">></span> nums<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span></code></pre>\n<p>nums[mid]valnumstarget-inf or inf</p>\n<pre class=\" language-python\"><code class=\"language-python\">lo <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\nhi <span class=\"token operator\">=</span> len<span class=\"token punctuation\">(</span>nums<span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span> <span class=\"token number\">1</span>\n\n<span class=\"token keyword\">while</span> lo <span class=\"token operator\">&lt;=</span> hi<span class=\"token punctuation\">:</span>\n    mid <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>lo<span class=\"token operator\">+</span>hi<span class=\"token punctuation\">)</span> <span class=\"token operator\">//</span> <span class=\"token number\">2</span>\n    val <span class=\"token operator\">=</span> nums<span class=\"token punctuation\">[</span>mid<span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>val <span class=\"token operator\">></span> nums<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token punctuation\">)</span> <span class=\"token operator\">==</span> <span class=\"token punctuation\">(</span>target <span class=\"token operator\">></span> nums<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">pass</span>\n    <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n        val <span class=\"token operator\">=</span>  float<span class=\"token punctuation\">(</span><span class=\"token string\">'-inf'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">if</span> target <span class=\"token operator\">&lt;</span> nums<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token keyword\">else</span> float<span class=\"token punctuation\">(</span><span class=\"token string\">'inf'</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">if</span> val <span class=\"token operator\">&lt;</span> target<span class=\"token punctuation\">:</span>\n        lo <span class=\"token operator\">=</span> mid <span class=\"token operator\">+</span> <span class=\"token number\">1</span>\n    <span class=\"token keyword\">elif</span> val <span class=\"token operator\">></span> target<span class=\"token punctuation\">:</span>\n        hi <span class=\"token operator\">=</span> mid <span class=\"token operator\">-</span> <span class=\"token number\">1</span>\n    <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> mid\n\n  <span class=\"token keyword\">return</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span></code></pre>\n<p>Time Complexity: $O(log(n))$</p>\n<p>Space Complexity: $O(1)$</p>\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><p></p>\n<p> target</p>\n<pre class=\" language-python\"><code class=\"language-python\">lo <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\nhi <span class=\"token operator\">=</span> len<span class=\"token punctuation\">(</span>nums<span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span> <span class=\"token number\">1</span>\n\n<span class=\"token keyword\">while</span> lo <span class=\"token operator\">&lt;=</span> hi<span class=\"token punctuation\">:</span>\n    mid <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>lo<span class=\"token operator\">+</span>hi<span class=\"token punctuation\">)</span> <span class=\"token operator\">//</span> <span class=\"token number\">2</span>\n    val <span class=\"token operator\">=</span> nums<span class=\"token punctuation\">[</span>mid<span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">if</span> target <span class=\"token operator\">==</span> val<span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> mid\n        <span class=\"token comment\" spellcheck=\"true\"># lo-mid </span>\n    <span class=\"token keyword\">if</span> nums<span class=\"token punctuation\">[</span>lo<span class=\"token punctuation\">]</span> <span class=\"token operator\">&lt;=</span> val<span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">if</span> target <span class=\"token operator\">>=</span> nums<span class=\"token punctuation\">[</span>lo<span class=\"token punctuation\">]</span> <span class=\"token operator\">and</span> target <span class=\"token operator\">&lt;</span> nums<span class=\"token punctuation\">[</span>mid<span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n            hi <span class=\"token operator\">=</span> mid <span class=\"token operator\">-</span> <span class=\"token number\">1</span>\n        <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n            lo <span class=\"token operator\">=</span> mid <span class=\"token operator\">+</span> <span class=\"token number\">1</span>\n    <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span> <span class=\"token comment\" spellcheck=\"true\"># mid-hi </span>\n        <span class=\"token keyword\">if</span> target <span class=\"token operator\">></span> nums<span class=\"token punctuation\">[</span>mid<span class=\"token punctuation\">]</span> <span class=\"token operator\">and</span> target <span class=\"token operator\">&lt;=</span> nums<span class=\"token punctuation\">[</span>hi<span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n            lo <span class=\"token operator\">=</span> mid <span class=\"token operator\">+</span> <span class=\"token number\">1</span>\n        <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n            hi <span class=\"token operator\">=</span> mid <span class=\"token operator\">-</span> <span class=\"token number\">1</span></code></pre>\n<p>Time Complexity: $O(log(n))$</p>\n<p>Space Complexity: $O(1)$</p>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>$log(n)$target  nums[mid]nums[mid]iftargettarget</p>\n<p>Reference:</p>\n<p><a href=\"https://leetcode.wang/leetCode-33-Search-in-Rotated-Sorted-Array.html\">LeetCode 33. Search in Rotated Sorted Array</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"LeetCode-33-Search-in-Rotated-Sorted-Array\"><a href=\"#LeetCode-33-Search-in-Rotated-Sorted-Array\" class=\"headerlink\" title=\"LeetCode 33. Search in Rotated Sorted Array\"></a>LeetCode 33. Search in Rotated Sorted Array</h3><p><img src=\"img1.png\"></p>\n<p>if</p>\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><p>target $O(log(n))$$O(log(n))$start, mid, end </p>\n<ul>\n<li>start  mid:</li>\n</ul>\n<p>mid &gt; start :  in left</p>\n<p><img src=\"img3.jpg\"></p>\n<p>mid &lt; start :  in left:</p>\n<p><img src=\"img2.jpg\"></p>\n<ul>\n<li>mid  end</li>\n</ul>\n<p>mid &lt; end:  in leftend = mid</p>\n<p><img src=\"img6.jpg\"></p>\n<p>mid &gt; end :  in right, start = mid</p>\n<p><img src=\"img7.jpg\"></p>\n<p>mid  end</p>\n<p><img src=\"img8.jpg\"></p>\n<p>in right mid in right midvalue start = mid + 1 </p>\n<pre><code class=\"python\">while start &lt; end:\n    mid = (start + end ) //2\n    if nums[start] &gt; nums[end]:\n        start = mid + 1\n    else:\n        end = mid\nbias = start</code></pre>\n<p>mid  midtarget</p>\n<pre><code class=\"python\">start = 0\nend = len(nums) - 1\nwhile start &lt;= end:\n    mid = (start+end) // 2\n    mid_pos = (mid + bias)     % len(nums)\n    val = nums[mid_pos]\n    if target == value:\n        return mid_pos\n    if target &lt; value:\n        end = mid - 1\n    else:\n        start = mid + 1\n\nreturn -1</code></pre>\n<p>Time Complexity: $O(log(n))$</p>\n<p>Space Complexity: $O(1)$</p>\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><p>22,[4 5 6 7 1 2 3 ] [4 5 6 7 ][1 2 3 ]target-inf or inf</p>\n<p>[ 4 5 6 7 1 2 3]  target = 5 [ 4 5 6 7 inf inf inf ]</p>\n<p>[ 4 5 6 7 1 2 3]  target = 2 [ -inf -inf - inf -inf 1 2 3]</p>\n<p>nums[mid]valvaltargetvalvaltarget-inf or inf</p>\n<ul>\n<li>nums[mid]  target : </li>\n</ul>\n<pre><code class=\"python\">nums[mid] &gt; nums[0] and target &gt; nums[0]\nor\nnums[mid] &lt; nums[0] and target &lt; nums[0]</code></pre>\n<ul>\n<li>nums[mid]  target </li>\n</ul>\n<pre><code class=\"python\">nums[mid] &gt; nums[0] and target &lt; nums[0]\nor\nnums[mid] &lt; nums[0] and target &gt; nums[0]</code></pre>\n<p>nums[mid]valnumstarget-inf or inf</p>\n<pre><code class=\"python\">lo = 0\nhi = len(nums) - 1\n\nwhile lo &lt;= hi:\n    mid = (lo+hi) // 2\n    val = nums[mid]\n    if (val &gt; nums[0] ) == (target &gt; nums[0]):\n        pass\n    else:\n        val =  float(&#39;-inf&#39;) if target &lt; nums[0] else float(&#39;inf&#39;)\n\n    if val &lt; target:\n        lo = mid + 1\n    elif val &gt; target:\n        hi = mid - 1\n    else:\n        return mid\n\n  return -1</code></pre>\n<p>Time Complexity: $O(log(n))$</p>\n<p>Space Complexity: $O(1)$</p>\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><p></p>\n<p> target</p>\n<pre><code class=\"python\">lo = 0\nhi = len(nums) - 1\n\nwhile lo &lt;= hi:\n    mid = (lo+hi) // 2\n    val = nums[mid]\n    if target == val:\n        return mid\n        # lo-mid \n    if nums[lo] &lt;= val:\n        if target &gt;= nums[lo] and target &lt; nums[mid]:\n            hi = mid - 1\n        else:\n            lo = mid + 1\n    else: # mid-hi \n        if target &gt; nums[mid] and target &lt;= nums[hi]:\n            lo = mid + 1\n        else:\n            hi = mid - 1</code></pre>\n<p>Time Complexity: $O(log(n))$</p>\n<p>Space Complexity: $O(1)$</p>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>$log(n)$target  nums[mid]nums[mid]iftargettarget</p>\n<p>Reference:</p>\n<p><a href=\"https://leetcode.wang/leetCode-33-Search-in-Rotated-Sorted-Array.html\">LeetCode 33. Search in Rotated Sorted Array</a></p>\n"},{"title":"()","toc":true,"mathjax":true,"top":false,"cover":true,"date":"2020-11-23T08:27:20.000Z","updated":"2020-12-10T08:01:42.373Z","_content":"\nRabbitMQ HTTP  Rabbit-Consumer I/ORabbitMQ Producer-Consumerprint\n\nOS\n\n#### \n\nOS/ //CPUOSIOIO/OS//\n\n> \n>\n> \n\n socket  recv() recv()  recv()  recv() \n\nsocket  send()TCP send() \n\n#### \n\n\n\n> CPU\n>\n> CPU \n\n![](1.jpeg)\n\n![](2.jpeg)\n\n#### \n\n\n\n> \n>\n> \n\n 2\n\n/OS\n\nOS/OSOS\n\n> \n\n\n\n\n\n cuda GPUcudaCPUCPUcuda\n\nPythonGIL, Global Interpreter LockPython GILPython\n\n```python\nimport threading, multiprocessing\n\ndef loop():\n\t\tx = 0\n\t\twhile True:\n\t\t\t\tx = x^1\n\t\t\nfor i in range(multiprocessing.cpu_count()):\n\t\tt = threading.Thread(target = loop)\n\t\tt.start()\n```\n\nCPU100%C++/Java4400%\n\nGIL100cudapythonGIL\n\nPythonGILHTTPproducer-consumer \n\n queue.Queue Queueproducer-consumer\n\n```python\nfrom queue import Queue\nfrom threading import Thread\n\ndef producer(out_q):\n\twhile True:\n\t\tout_q.put(data)\n        \ndef consumer(in_q):\n\twhile True:\n\t\tdata = in_q.get()\n\t\t\t...\n    \t# q \n      in_q.task_done()\n        \nq = Queue()\nt1 = Thread(target=consumer, args=(q,))\nt2 = Thread(taget=producer, args=(q,))\nt1.start()\nt2.start()\n\n# wait for all produced items to be consumed\nq.join()\n```\n\n\n\nqQueue\n\n```python\nq = Queue(100)\n\ntry:\n\tdata = q.get(block=False)\nexcept queue.Empty:\n\tpass\n    \ntry:\n\tq.put(item, timeout=5.0)\nexcept queue.Full:\n\tlog.warning('queued item %r discarded!', item)\n```\n\n\n\n### \n\ntornadohttprequest_bodyq.put()tornado producerconsumer\n\n```python\nfrom eliot import log_call, start_action, to_file\nclass LogHandler(tornado.web.RequestHandler):\n\t# eliot input, output\n\t@log_call\n\tdef post(self, *args, **kwargs):\n\t\ttry:\n\t\t\tself.timestamp = time.ctime()\n\t\t\tself.recv_json = json.loads(self.request.body, strict=False)\n\t\t\tq.put((self.timestamp, self.recv_json ))\n\t\t\toutput_dic = {\n                'status': '1',\n                'result': 'success'\n      }\n\t\texcept Exception as e:\n\t\t\tprint('LogHandler Error: ', e)\n\t\t\treturn\n          \n\n# eliot.start_action  with \ndef consume_msg(queue):\n\twhile True:\n\t\twith start_action(action_type='consume_msg', lenghth = queue.qsize()):\n\t\t\ttry:\n\t\t\t\ttimestamp, msg= q.get()\n\t\t\t\tstart_time = time.time()\n\t\t\t\twith start_action(action_type='save_request_body', timestamp=timestamp,msg=msg):\n\t\t\t\t\tsave_body(msg)\n\t\t\texcept Exception as e:\n\t\t\t\twith start_action(action_type='consume_msg exceptiosn', timestamp=timestamp,e=e, msg=msg):\n\t\t\t\t\tprint('consume_mgs exception : '  , e)\n                    \nconsumer = Process(target=consume_msg, args=(q, ))\nconsumer.daemon = True # \nconsumer.start()\n```\n\n\n\neliot\n\n### RabbiMQ\n\nRabbiMQ \n\n```python\n# rabbitmq init\ncredentials = pika.PlainCredentials(RabbitMQ.USERNAME, RabbitMQ.USERPWD)\nconnection = pika.BlockingConnection(\n                pika.ConnectionParameters(RabbitMQ.HOST, RabbitMQ.PORT,\n                                          credentials=credentials)\n    )\n# channel init and declare\nchannel = connection.channel()\nchannel.exchange_declare(exchange=RabbitMQ.EXCHANGE,\n                             exchange_type=RabbitMQ.EXCHANGE_TYPE,\n                             durable=True)\nchannel.queue_declare(queue=RabbitMQ.QUEUE_NAME_ALG, durable=True)\nchannel.queue_bind(exchange=RabbitMQ.EXCHANGE,\n                       queue=RabbitMQ.QUEUE_NAME_ALG,\n                       routing_key=RabbitMQ.ROUTING_KEY)\nchannel.basic_qos(prefetch_count=1)    # \nchannel.basic_consume(on_message_callback=consumer,\n                          queue=RabbitMQ.QUEUE_NAME_ALG, auto_ack=False)\nprint(' [*] Waiting for messages...')\nchannel.start_consuming()\n\ndef consumer(ch, method, properties, body):\n\tbody = json.loads(body, strict=False)\n\tflag = body['flag']\n\t# process body\n  \n\t# Ack manually\n\tch.basic_ack(delivery_tag=method.delivery_tag)\n  \n\n  \n# producer.py\ntry:\n\tcredentials = pika.PlainCredentials(\n                RabbitMQ.USERNAME, RabbitMQ.USERPWD)\n\tconnection = pika.BlockingConnection(\n                pika.ConnectionParameters(RabbitMQ.HOST, RabbitMQ.PORT,\n                                          credentials=credentials)\n            )\n\tchannel = self.connection.channel()\n\tchannel.exchange_declare(exchange=RabbitMQ.EXCHANGE,                                      \t\t\t\t\t\t\t\t\t\t\t\t\t\texchange_type=RabbitMQ.EXCHANGE_TYPE,\n                                          durable=True)\nexcept Exception as e:\n\tpass\n        \nsent_msg = {...}\nbody = json.dumps(sent_msg)\nchannel.basic_publish(exchange=RabbitMQ.EXCHANGE, \t\t\t\t\n                      routing_key=RabbitMQ.ROUTING_KEY,\n                      body=body, \n                      properties=pika.BasicProperties(delivery_mode=2))\n\nchannel.close()\nconnection.close()\n```\n\n\n\n###   IO\n\nIO\n\n> CPUPythonC++/Java\n>\n> IOIOC++/JavaPython\n\nPythonC++/Java\n\n### \n\nPython\n\nPythonasyncioIOasyncioEventLoopEventLoopIO,producer-consumer\n\n```python\nimport asyncio\nimport random\n\nasync def producer(queue, n):\n\tfor x in range(1, n+1):\n\t\tprint('producing : ', x)    \n\t\t# simulate io job\n\t\tawait asyncio.sleep(random.random())\n\t\titem = str(x)\n\t\tawait queue.put(item)\n        \nasync def consume(queue):\n\twhile True:\n\t\titem = await queue.get()\n    # process item\n    print('consuming : ', item)\n    # simulate io \n    await asyncio.sleep(random.random())\n    # notify queue that the item has been processed\n    queue.task_done()\n        \nasync def run(n):\n\tqueue = asyncio.Queue()\n  # schedule consumer\n  consumer= asyncio.ensure_future(consume(queue))\n  await producer(queue, n)\n\t# wait until consumer processed all items\n  await queue.join()\n  # consumer is still awaiting item, cancel it\n  consumer.cancel()\n    \nloop = asyncio.get_event_loop()\nloop.run_until_complete(run(10))\nloop.close()\n```\n\n\n\n### Conclusion\n\n\n\n\n\nReferences:\n\n[Producer/consumer-examlpe_asyncio](https://asyncio.readthedocs.io/en/latest/producer_consumer.html)\n\n[-](https://www.liaoxuefeng.com/wiki/1016959663602400/1017968846697824)\n\n[--](https://www.liaoxuefeng.com/wiki/1016959663602400/1017631469467456)\n\n[-Python_CookBook](https://python3-cookbook.readthedocs.io/zh_CN/latest/c12/p03_communicating_between_threads.html)\n\n[Python](https://juejin.cn/post/6844904039210024967#heading-3)\n\n","source":"_posts/-.md","raw":"---\ntitle: ()\ntoc: true\nmathjax: true\ntop: false\ncover: true\ndate: 2020-11-23 16:27:20\nupdated:\ncategories: Algorithms\ntags: \n\t- Algorithms\n\t- OS\n---\n\nRabbitMQ HTTP  Rabbit-Consumer I/ORabbitMQ Producer-Consumerprint\n\nOS\n\n#### \n\nOS/ //CPUOSIOIO/OS//\n\n> \n>\n> \n\n socket  recv() recv()  recv()  recv() \n\nsocket  send()TCP send() \n\n#### \n\n\n\n> CPU\n>\n> CPU \n\n![](1.jpeg)\n\n![](2.jpeg)\n\n#### \n\n\n\n> \n>\n> \n\n 2\n\n/OS\n\nOS/OSOS\n\n> \n\n\n\n\n\n cuda GPUcudaCPUCPUcuda\n\nPythonGIL, Global Interpreter LockPython GILPython\n\n```python\nimport threading, multiprocessing\n\ndef loop():\n\t\tx = 0\n\t\twhile True:\n\t\t\t\tx = x^1\n\t\t\nfor i in range(multiprocessing.cpu_count()):\n\t\tt = threading.Thread(target = loop)\n\t\tt.start()\n```\n\nCPU100%C++/Java4400%\n\nGIL100cudapythonGIL\n\nPythonGILHTTPproducer-consumer \n\n queue.Queue Queueproducer-consumer\n\n```python\nfrom queue import Queue\nfrom threading import Thread\n\ndef producer(out_q):\n\twhile True:\n\t\tout_q.put(data)\n        \ndef consumer(in_q):\n\twhile True:\n\t\tdata = in_q.get()\n\t\t\t...\n    \t# q \n      in_q.task_done()\n        \nq = Queue()\nt1 = Thread(target=consumer, args=(q,))\nt2 = Thread(taget=producer, args=(q,))\nt1.start()\nt2.start()\n\n# wait for all produced items to be consumed\nq.join()\n```\n\n\n\nqQueue\n\n```python\nq = Queue(100)\n\ntry:\n\tdata = q.get(block=False)\nexcept queue.Empty:\n\tpass\n    \ntry:\n\tq.put(item, timeout=5.0)\nexcept queue.Full:\n\tlog.warning('queued item %r discarded!', item)\n```\n\n\n\n### \n\ntornadohttprequest_bodyq.put()tornado producerconsumer\n\n```python\nfrom eliot import log_call, start_action, to_file\nclass LogHandler(tornado.web.RequestHandler):\n\t# eliot input, output\n\t@log_call\n\tdef post(self, *args, **kwargs):\n\t\ttry:\n\t\t\tself.timestamp = time.ctime()\n\t\t\tself.recv_json = json.loads(self.request.body, strict=False)\n\t\t\tq.put((self.timestamp, self.recv_json ))\n\t\t\toutput_dic = {\n                'status': '1',\n                'result': 'success'\n      }\n\t\texcept Exception as e:\n\t\t\tprint('LogHandler Error: ', e)\n\t\t\treturn\n          \n\n# eliot.start_action  with \ndef consume_msg(queue):\n\twhile True:\n\t\twith start_action(action_type='consume_msg', lenghth = queue.qsize()):\n\t\t\ttry:\n\t\t\t\ttimestamp, msg= q.get()\n\t\t\t\tstart_time = time.time()\n\t\t\t\twith start_action(action_type='save_request_body', timestamp=timestamp,msg=msg):\n\t\t\t\t\tsave_body(msg)\n\t\t\texcept Exception as e:\n\t\t\t\twith start_action(action_type='consume_msg exceptiosn', timestamp=timestamp,e=e, msg=msg):\n\t\t\t\t\tprint('consume_mgs exception : '  , e)\n                    \nconsumer = Process(target=consume_msg, args=(q, ))\nconsumer.daemon = True # \nconsumer.start()\n```\n\n\n\neliot\n\n### RabbiMQ\n\nRabbiMQ \n\n```python\n# rabbitmq init\ncredentials = pika.PlainCredentials(RabbitMQ.USERNAME, RabbitMQ.USERPWD)\nconnection = pika.BlockingConnection(\n                pika.ConnectionParameters(RabbitMQ.HOST, RabbitMQ.PORT,\n                                          credentials=credentials)\n    )\n# channel init and declare\nchannel = connection.channel()\nchannel.exchange_declare(exchange=RabbitMQ.EXCHANGE,\n                             exchange_type=RabbitMQ.EXCHANGE_TYPE,\n                             durable=True)\nchannel.queue_declare(queue=RabbitMQ.QUEUE_NAME_ALG, durable=True)\nchannel.queue_bind(exchange=RabbitMQ.EXCHANGE,\n                       queue=RabbitMQ.QUEUE_NAME_ALG,\n                       routing_key=RabbitMQ.ROUTING_KEY)\nchannel.basic_qos(prefetch_count=1)    # \nchannel.basic_consume(on_message_callback=consumer,\n                          queue=RabbitMQ.QUEUE_NAME_ALG, auto_ack=False)\nprint(' [*] Waiting for messages...')\nchannel.start_consuming()\n\ndef consumer(ch, method, properties, body):\n\tbody = json.loads(body, strict=False)\n\tflag = body['flag']\n\t# process body\n  \n\t# Ack manually\n\tch.basic_ack(delivery_tag=method.delivery_tag)\n  \n\n  \n# producer.py\ntry:\n\tcredentials = pika.PlainCredentials(\n                RabbitMQ.USERNAME, RabbitMQ.USERPWD)\n\tconnection = pika.BlockingConnection(\n                pika.ConnectionParameters(RabbitMQ.HOST, RabbitMQ.PORT,\n                                          credentials=credentials)\n            )\n\tchannel = self.connection.channel()\n\tchannel.exchange_declare(exchange=RabbitMQ.EXCHANGE,                                      \t\t\t\t\t\t\t\t\t\t\t\t\t\texchange_type=RabbitMQ.EXCHANGE_TYPE,\n                                          durable=True)\nexcept Exception as e:\n\tpass\n        \nsent_msg = {...}\nbody = json.dumps(sent_msg)\nchannel.basic_publish(exchange=RabbitMQ.EXCHANGE, \t\t\t\t\n                      routing_key=RabbitMQ.ROUTING_KEY,\n                      body=body, \n                      properties=pika.BasicProperties(delivery_mode=2))\n\nchannel.close()\nconnection.close()\n```\n\n\n\n###   IO\n\nIO\n\n> CPUPythonC++/Java\n>\n> IOIOC++/JavaPython\n\nPythonC++/Java\n\n### \n\nPython\n\nPythonasyncioIOasyncioEventLoopEventLoopIO,producer-consumer\n\n```python\nimport asyncio\nimport random\n\nasync def producer(queue, n):\n\tfor x in range(1, n+1):\n\t\tprint('producing : ', x)    \n\t\t# simulate io job\n\t\tawait asyncio.sleep(random.random())\n\t\titem = str(x)\n\t\tawait queue.put(item)\n        \nasync def consume(queue):\n\twhile True:\n\t\titem = await queue.get()\n    # process item\n    print('consuming : ', item)\n    # simulate io \n    await asyncio.sleep(random.random())\n    # notify queue that the item has been processed\n    queue.task_done()\n        \nasync def run(n):\n\tqueue = asyncio.Queue()\n  # schedule consumer\n  consumer= asyncio.ensure_future(consume(queue))\n  await producer(queue, n)\n\t# wait until consumer processed all items\n  await queue.join()\n  # consumer is still awaiting item, cancel it\n  consumer.cancel()\n    \nloop = asyncio.get_event_loop()\nloop.run_until_complete(run(10))\nloop.close()\n```\n\n\n\n### Conclusion\n\n\n\n\n\nReferences:\n\n[Producer/consumer-examlpe_asyncio](https://asyncio.readthedocs.io/en/latest/producer_consumer.html)\n\n[-](https://www.liaoxuefeng.com/wiki/1016959663602400/1017968846697824)\n\n[--](https://www.liaoxuefeng.com/wiki/1016959663602400/1017631469467456)\n\n[-Python_CookBook](https://python3-cookbook.readthedocs.io/zh_CN/latest/c12/p03_communicating_between_threads.html)\n\n[Python](https://juejin.cn/post/6844904039210024967#heading-3)\n\n","slug":"-","published":1,"_id":"ckhvqydbn00006z28dot614mu","comments":1,"layout":"post","photos":[],"link":"","content":"<p>RabbitMQ HTTP  Rabbit-Consumer I/ORabbitMQ Producer-Consumerprint</p>\n<p>OS</p>\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><p>OS/ //CPUOSIOIO/OS//</p>\n<blockquote>\n<p></p>\n<p></p>\n</blockquote>\n<p> socket  recv() recv()  recv()  recv() </p>\n<p>socket  send()TCP send() </p>\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><p></p>\n<blockquote>\n<p>CPU</p>\n<p>CPU </p>\n</blockquote>\n<p><img src=\"1.jpeg\"></p>\n<p><img src=\"2.jpeg\"></p>\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><p></p>\n<blockquote>\n<p></p>\n<p></p>\n</blockquote>\n<p> 2</p>\n<p>/OS</p>\n<p>OS/OSOS</p>\n<blockquote>\n<p></p>\n</blockquote>\n<p></p>\n<p></p>\n<p> cuda GPUcudaCPUCPUcuda</p>\n<p>PythonGIL, Global Interpreter LockPython GILPython</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> threading<span class=\"token punctuation\">,</span> multiprocessing\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">loop</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        x <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n        <span class=\"token keyword\">while</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">:</span>\n                x <span class=\"token operator\">=</span> x<span class=\"token operator\">^</span><span class=\"token number\">1</span>\n\n<span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span>multiprocessing<span class=\"token punctuation\">.</span>cpu_count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        t <span class=\"token operator\">=</span> threading<span class=\"token punctuation\">.</span>Thread<span class=\"token punctuation\">(</span>target <span class=\"token operator\">=</span> loop<span class=\"token punctuation\">)</span>\n        t<span class=\"token punctuation\">.</span>start<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre>\n<p>CPU100%C++/Java4400%</p>\n<p>GIL100cudapythonGIL</p>\n<p>PythonGILHTTPproducer-consumer </p>\n<p> queue.Queue Queueproducer-consumer</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> queue <span class=\"token keyword\">import</span> Queue\n<span class=\"token keyword\">from</span> threading <span class=\"token keyword\">import</span> Thread\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">producer</span><span class=\"token punctuation\">(</span>out_q<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">while</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">:</span>\n        out_q<span class=\"token punctuation\">.</span>put<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">consumer</span><span class=\"token punctuation\">(</span>in_q<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">while</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">:</span>\n        data <span class=\"token operator\">=</span> in_q<span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n            <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>\n        <span class=\"token comment\" spellcheck=\"true\"># q </span>\n      in_q<span class=\"token punctuation\">.</span>task_done<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\nq <span class=\"token operator\">=</span> Queue<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nt1 <span class=\"token operator\">=</span> Thread<span class=\"token punctuation\">(</span>target<span class=\"token operator\">=</span>consumer<span class=\"token punctuation\">,</span> args<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>q<span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nt2 <span class=\"token operator\">=</span> Thread<span class=\"token punctuation\">(</span>taget<span class=\"token operator\">=</span>producer<span class=\"token punctuation\">,</span> args<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>q<span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nt1<span class=\"token punctuation\">.</span>start<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nt2<span class=\"token punctuation\">.</span>start<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># wait for all produced items to be consumed</span>\nq<span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre>\n<p>qQueue</p>\n<pre class=\" language-python\"><code class=\"language-python\">q <span class=\"token operator\">=</span> Queue<span class=\"token punctuation\">(</span><span class=\"token number\">100</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">try</span><span class=\"token punctuation\">:</span>\n    data <span class=\"token operator\">=</span> q<span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span>block<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">except</span> queue<span class=\"token punctuation\">.</span>Empty<span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">pass</span>\n\n<span class=\"token keyword\">try</span><span class=\"token punctuation\">:</span>\n    q<span class=\"token punctuation\">.</span>put<span class=\"token punctuation\">(</span>item<span class=\"token punctuation\">,</span> timeout<span class=\"token operator\">=</span><span class=\"token number\">5.0</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">except</span> queue<span class=\"token punctuation\">.</span>Full<span class=\"token punctuation\">:</span>\n    log<span class=\"token punctuation\">.</span>warning<span class=\"token punctuation\">(</span><span class=\"token string\">'queued item %r discarded!'</span><span class=\"token punctuation\">,</span> item<span class=\"token punctuation\">)</span></code></pre>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>tornadohttprequest_bodyq.put()tornado producerconsumer</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> eliot <span class=\"token keyword\">import</span> log_call<span class=\"token punctuation\">,</span> start_action<span class=\"token punctuation\">,</span> to_file\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">LogHandler</span><span class=\"token punctuation\">(</span>tornado<span class=\"token punctuation\">.</span>web<span class=\"token punctuation\">.</span>RequestHandler<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\" spellcheck=\"true\"># eliot input, output</span>\n    @log_call\n    <span class=\"token keyword\">def</span> <span class=\"token function\">post</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> <span class=\"token operator\">*</span>args<span class=\"token punctuation\">,</span> <span class=\"token operator\">**</span>kwargs<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">try</span><span class=\"token punctuation\">:</span>\n            self<span class=\"token punctuation\">.</span>timestamp <span class=\"token operator\">=</span> time<span class=\"token punctuation\">.</span>ctime<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n            self<span class=\"token punctuation\">.</span>recv_json <span class=\"token operator\">=</span> json<span class=\"token punctuation\">.</span>loads<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>request<span class=\"token punctuation\">.</span>body<span class=\"token punctuation\">,</span> strict<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span>\n            q<span class=\"token punctuation\">.</span>put<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>timestamp<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>recv_json <span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n            output_dic <span class=\"token operator\">=</span> <span class=\"token operator\">&amp;</span><span class=\"token comment\" spellcheck=\"true\">#123;</span>\n                <span class=\"token string\">'status'</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'1'</span><span class=\"token punctuation\">,</span>\n                <span class=\"token string\">'result'</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'success'</span>\n      <span class=\"token operator\">&amp;</span><span class=\"token comment\" spellcheck=\"true\">#125;</span>\n        <span class=\"token keyword\">except</span> Exception <span class=\"token keyword\">as</span> e<span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'LogHandler Error: '</span><span class=\"token punctuation\">,</span> e<span class=\"token punctuation\">)</span>\n            <span class=\"token keyword\">return</span>\n\n\n<span class=\"token comment\" spellcheck=\"true\"># eliot.start_action  with </span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">consume_msg</span><span class=\"token punctuation\">(</span>queue<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">while</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">with</span> start_action<span class=\"token punctuation\">(</span>action_type<span class=\"token operator\">=</span><span class=\"token string\">'consume_msg'</span><span class=\"token punctuation\">,</span> lenghth <span class=\"token operator\">=</span> queue<span class=\"token punctuation\">.</span>qsize<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">try</span><span class=\"token punctuation\">:</span>\n                timestamp<span class=\"token punctuation\">,</span> msg<span class=\"token operator\">=</span> q<span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n                start_time <span class=\"token operator\">=</span> time<span class=\"token punctuation\">.</span>time<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n                <span class=\"token keyword\">with</span> start_action<span class=\"token punctuation\">(</span>action_type<span class=\"token operator\">=</span><span class=\"token string\">'save_request_body'</span><span class=\"token punctuation\">,</span> timestamp<span class=\"token operator\">=</span>timestamp<span class=\"token punctuation\">,</span>msg<span class=\"token operator\">=</span>msg<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n                    save_body<span class=\"token punctuation\">(</span>msg<span class=\"token punctuation\">)</span>\n            <span class=\"token keyword\">except</span> Exception <span class=\"token keyword\">as</span> e<span class=\"token punctuation\">:</span>\n                <span class=\"token keyword\">with</span> start_action<span class=\"token punctuation\">(</span>action_type<span class=\"token operator\">=</span><span class=\"token string\">'consume_msg exceptiosn'</span><span class=\"token punctuation\">,</span> timestamp<span class=\"token operator\">=</span>timestamp<span class=\"token punctuation\">,</span>e<span class=\"token operator\">=</span>e<span class=\"token punctuation\">,</span> msg<span class=\"token operator\">=</span>msg<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n                    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'consume_mgs exception : '</span>  <span class=\"token punctuation\">,</span> e<span class=\"token punctuation\">)</span>\n\nconsumer <span class=\"token operator\">=</span> Process<span class=\"token punctuation\">(</span>target<span class=\"token operator\">=</span>consume_msg<span class=\"token punctuation\">,</span> args<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>q<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nconsumer<span class=\"token punctuation\">.</span>daemon <span class=\"token operator\">=</span> <span class=\"token boolean\">True</span> <span class=\"token comment\" spellcheck=\"true\"># </span>\nconsumer<span class=\"token punctuation\">.</span>start<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre>\n<p>eliot</p>\n<h3 id=\"RabbiMQ\"><a href=\"#RabbiMQ\" class=\"headerlink\" title=\"RabbiMQ\"></a>RabbiMQ</h3><p>RabbiMQ </p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token comment\" spellcheck=\"true\"># rabbitmq init</span>\ncredentials <span class=\"token operator\">=</span> pika<span class=\"token punctuation\">.</span>PlainCredentials<span class=\"token punctuation\">(</span>RabbitMQ<span class=\"token punctuation\">.</span>USERNAME<span class=\"token punctuation\">,</span> RabbitMQ<span class=\"token punctuation\">.</span>USERPWD<span class=\"token punctuation\">)</span>\nconnection <span class=\"token operator\">=</span> pika<span class=\"token punctuation\">.</span>BlockingConnection<span class=\"token punctuation\">(</span>\n                pika<span class=\"token punctuation\">.</span>ConnectionParameters<span class=\"token punctuation\">(</span>RabbitMQ<span class=\"token punctuation\">.</span>HOST<span class=\"token punctuation\">,</span> RabbitMQ<span class=\"token punctuation\">.</span>PORT<span class=\"token punctuation\">,</span>\n                                          credentials<span class=\"token operator\">=</span>credentials<span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">)</span>\n<span class=\"token comment\" spellcheck=\"true\"># channel init and declare</span>\nchannel <span class=\"token operator\">=</span> connection<span class=\"token punctuation\">.</span>channel<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nchannel<span class=\"token punctuation\">.</span>exchange_declare<span class=\"token punctuation\">(</span>exchange<span class=\"token operator\">=</span>RabbitMQ<span class=\"token punctuation\">.</span>EXCHANGE<span class=\"token punctuation\">,</span>\n                             exchange_type<span class=\"token operator\">=</span>RabbitMQ<span class=\"token punctuation\">.</span>EXCHANGE_TYPE<span class=\"token punctuation\">,</span>\n                             durable<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\nchannel<span class=\"token punctuation\">.</span>queue_declare<span class=\"token punctuation\">(</span>queue<span class=\"token operator\">=</span>RabbitMQ<span class=\"token punctuation\">.</span>QUEUE_NAME_ALG<span class=\"token punctuation\">,</span> durable<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\nchannel<span class=\"token punctuation\">.</span>queue_bind<span class=\"token punctuation\">(</span>exchange<span class=\"token operator\">=</span>RabbitMQ<span class=\"token punctuation\">.</span>EXCHANGE<span class=\"token punctuation\">,</span>\n                       queue<span class=\"token operator\">=</span>RabbitMQ<span class=\"token punctuation\">.</span>QUEUE_NAME_ALG<span class=\"token punctuation\">,</span>\n                       routing_key<span class=\"token operator\">=</span>RabbitMQ<span class=\"token punctuation\">.</span>ROUTING_KEY<span class=\"token punctuation\">)</span>\nchannel<span class=\"token punctuation\">.</span>basic_qos<span class=\"token punctuation\">(</span>prefetch_count<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>    <span class=\"token comment\" spellcheck=\"true\"># </span>\nchannel<span class=\"token punctuation\">.</span>basic_consume<span class=\"token punctuation\">(</span>on_message_callback<span class=\"token operator\">=</span>consumer<span class=\"token punctuation\">,</span>\n                          queue<span class=\"token operator\">=</span>RabbitMQ<span class=\"token punctuation\">.</span>QUEUE_NAME_ALG<span class=\"token punctuation\">,</span> auto_ack<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">' [*] Waiting for messages...'</span><span class=\"token punctuation\">)</span>\nchannel<span class=\"token punctuation\">.</span>start_consuming<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">consumer</span><span class=\"token punctuation\">(</span>ch<span class=\"token punctuation\">,</span> method<span class=\"token punctuation\">,</span> properties<span class=\"token punctuation\">,</span> body<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    body <span class=\"token operator\">=</span> json<span class=\"token punctuation\">.</span>loads<span class=\"token punctuation\">(</span>body<span class=\"token punctuation\">,</span> strict<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span>\n    flag <span class=\"token operator\">=</span> body<span class=\"token punctuation\">[</span><span class=\"token string\">'flag'</span><span class=\"token punctuation\">]</span>\n    <span class=\"token comment\" spellcheck=\"true\"># process body</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># Ack manually</span>\n    ch<span class=\"token punctuation\">.</span>basic_ack<span class=\"token punctuation\">(</span>delivery_tag<span class=\"token operator\">=</span>method<span class=\"token punctuation\">.</span>delivery_tag<span class=\"token punctuation\">)</span>\n\n\n\n<span class=\"token comment\" spellcheck=\"true\"># producer.py</span>\n<span class=\"token keyword\">try</span><span class=\"token punctuation\">:</span>\n    credentials <span class=\"token operator\">=</span> pika<span class=\"token punctuation\">.</span>PlainCredentials<span class=\"token punctuation\">(</span>\n                RabbitMQ<span class=\"token punctuation\">.</span>USERNAME<span class=\"token punctuation\">,</span> RabbitMQ<span class=\"token punctuation\">.</span>USERPWD<span class=\"token punctuation\">)</span>\n    connection <span class=\"token operator\">=</span> pika<span class=\"token punctuation\">.</span>BlockingConnection<span class=\"token punctuation\">(</span>\n                pika<span class=\"token punctuation\">.</span>ConnectionParameters<span class=\"token punctuation\">(</span>RabbitMQ<span class=\"token punctuation\">.</span>HOST<span class=\"token punctuation\">,</span> RabbitMQ<span class=\"token punctuation\">.</span>PORT<span class=\"token punctuation\">,</span>\n                                          credentials<span class=\"token operator\">=</span>credentials<span class=\"token punctuation\">)</span>\n            <span class=\"token punctuation\">)</span>\n    channel <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>connection<span class=\"token punctuation\">.</span>channel<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    channel<span class=\"token punctuation\">.</span>exchange_declare<span class=\"token punctuation\">(</span>exchange<span class=\"token operator\">=</span>RabbitMQ<span class=\"token punctuation\">.</span>EXCHANGE<span class=\"token punctuation\">,</span>                                                                                              exchange_type<span class=\"token operator\">=</span>RabbitMQ<span class=\"token punctuation\">.</span>EXCHANGE_TYPE<span class=\"token punctuation\">,</span>\n                                          durable<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">except</span> Exception <span class=\"token keyword\">as</span> e<span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">pass</span>\n\nsent_msg <span class=\"token operator\">=</span> <span class=\"token operator\">&amp;</span><span class=\"token comment\" spellcheck=\"true\">#123;...&amp;#125;</span>\nbody <span class=\"token operator\">=</span> json<span class=\"token punctuation\">.</span>dumps<span class=\"token punctuation\">(</span>sent_msg<span class=\"token punctuation\">)</span>\nchannel<span class=\"token punctuation\">.</span>basic_publish<span class=\"token punctuation\">(</span>exchange<span class=\"token operator\">=</span>RabbitMQ<span class=\"token punctuation\">.</span>EXCHANGE<span class=\"token punctuation\">,</span>                 \n                      routing_key<span class=\"token operator\">=</span>RabbitMQ<span class=\"token punctuation\">.</span>ROUTING_KEY<span class=\"token punctuation\">,</span>\n                      body<span class=\"token operator\">=</span>body<span class=\"token punctuation\">,</span> \n                      properties<span class=\"token operator\">=</span>pika<span class=\"token punctuation\">.</span>BasicProperties<span class=\"token punctuation\">(</span>delivery_mode<span class=\"token operator\">=</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\nchannel<span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nconnection<span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre>\n<h3 id=\"--IO\"><a href=\"#--IO\" class=\"headerlink\" title=\"  IO\"></a>  IO</h3><p>IO</p>\n<blockquote>\n<p>CPUPythonC++/Java</p>\n<p>IOIOC++/JavaPython</p>\n</blockquote>\n<p>PythonC++/Java</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>Python</p>\n<p>PythonasyncioIOasyncioEventLoopEventLoopIO,producer-consumer</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> asyncio\n<span class=\"token keyword\">import</span> random\n\n<span class=\"token keyword\">async</span> <span class=\"token keyword\">def</span> <span class=\"token function\">producer</span><span class=\"token punctuation\">(</span>queue<span class=\"token punctuation\">,</span> n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> n<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'producing : '</span><span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span>    \n        <span class=\"token comment\" spellcheck=\"true\"># simulate io job</span>\n        <span class=\"token keyword\">await</span> asyncio<span class=\"token punctuation\">.</span>sleep<span class=\"token punctuation\">(</span>random<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        item <span class=\"token operator\">=</span> str<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">await</span> queue<span class=\"token punctuation\">.</span>put<span class=\"token punctuation\">(</span>item<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">async</span> <span class=\"token keyword\">def</span> <span class=\"token function\">consume</span><span class=\"token punctuation\">(</span>queue<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">while</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">:</span>\n        item <span class=\"token operator\">=</span> <span class=\"token keyword\">await</span> queue<span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># process item</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'consuming : '</span><span class=\"token punctuation\">,</span> item<span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># simulate io </span>\n    <span class=\"token keyword\">await</span> asyncio<span class=\"token punctuation\">.</span>sleep<span class=\"token punctuation\">(</span>random<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># notify queue that the item has been processed</span>\n    queue<span class=\"token punctuation\">.</span>task_done<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">async</span> <span class=\"token keyword\">def</span> <span class=\"token function\">run</span><span class=\"token punctuation\">(</span>n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    queue <span class=\"token operator\">=</span> asyncio<span class=\"token punctuation\">.</span>Queue<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n  <span class=\"token comment\" spellcheck=\"true\"># schedule consumer</span>\n  consumer<span class=\"token operator\">=</span> asyncio<span class=\"token punctuation\">.</span>ensure_future<span class=\"token punctuation\">(</span>consume<span class=\"token punctuation\">(</span>queue<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n  <span class=\"token keyword\">await</span> producer<span class=\"token punctuation\">(</span>queue<span class=\"token punctuation\">,</span> n<span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># wait until consumer processed all items</span>\n  <span class=\"token keyword\">await</span> queue<span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n  <span class=\"token comment\" spellcheck=\"true\"># consumer is still awaiting item, cancel it</span>\n  consumer<span class=\"token punctuation\">.</span>cancel<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\nloop <span class=\"token operator\">=</span> asyncio<span class=\"token punctuation\">.</span>get_event_loop<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nloop<span class=\"token punctuation\">.</span>run_until_complete<span class=\"token punctuation\">(</span>run<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nloop<span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p></p>\n<p>References:</p>\n<p><a href=\"https://asyncio.readthedocs.io/en/latest/producer_consumer.html\">Producer/consumer-examlpe_asyncio</a></p>\n<p><a href=\"https://www.liaoxuefeng.com/wiki/1016959663602400/1017968846697824\">-</a></p>\n<p><a href=\"https://www.liaoxuefeng.com/wiki/1016959663602400/1017631469467456\">--</a></p>\n<p><a href=\"https://python3-cookbook.readthedocs.io/zh_CN/latest/c12/p03_communicating_between_threads.html\">-Python_CookBook</a></p>\n<p><a href=\"https://juejin.cn/post/6844904039210024967#heading-3\">Python</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>RabbitMQ HTTP  Rabbit-Consumer I/ORabbitMQ Producer-Consumerprint</p>\n<p>OS</p>\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><p>OS/ //CPUOSIOIO/OS//</p>\n<blockquote>\n<p></p>\n<p></p>\n</blockquote>\n<p> socket  recv() recv()  recv()  recv() </p>\n<p>socket  send()TCP send() </p>\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><p></p>\n<blockquote>\n<p>CPU</p>\n<p>CPU </p>\n</blockquote>\n<p><img src=\"1.jpeg\"></p>\n<p><img src=\"2.jpeg\"></p>\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><p></p>\n<blockquote>\n<p></p>\n<p></p>\n</blockquote>\n<p> 2</p>\n<p>/OS</p>\n<p>OS/OSOS</p>\n<blockquote>\n<p></p>\n</blockquote>\n<p></p>\n<p></p>\n<p> cuda GPUcudaCPUCPUcuda</p>\n<p>PythonGIL, Global Interpreter LockPython GILPython</p>\n<pre><code class=\"python\">import threading, multiprocessing\n\ndef loop():\n        x = 0\n        while True:\n                x = x^1\n\nfor i in range(multiprocessing.cpu_count()):\n        t = threading.Thread(target = loop)\n        t.start()</code></pre>\n<p>CPU100%C++/Java4400%</p>\n<p>GIL100cudapythonGIL</p>\n<p>PythonGILHTTPproducer-consumer </p>\n<p> queue.Queue Queueproducer-consumer</p>\n<pre><code class=\"python\">from queue import Queue\nfrom threading import Thread\n\ndef producer(out_q):\n    while True:\n        out_q.put(data)\n\ndef consumer(in_q):\n    while True:\n        data = in_q.get()\n            ...\n        # q \n      in_q.task_done()\n\nq = Queue()\nt1 = Thread(target=consumer, args=(q,))\nt2 = Thread(taget=producer, args=(q,))\nt1.start()\nt2.start()\n\n# wait for all produced items to be consumed\nq.join()</code></pre>\n<p>qQueue</p>\n<pre><code class=\"python\">q = Queue(100)\n\ntry:\n    data = q.get(block=False)\nexcept queue.Empty:\n    pass\n\ntry:\n    q.put(item, timeout=5.0)\nexcept queue.Full:\n    log.warning(&#39;queued item %r discarded!&#39;, item)</code></pre>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>tornadohttprequest_bodyq.put()tornado producerconsumer</p>\n<pre><code class=\"python\">from eliot import log_call, start_action, to_file\nclass LogHandler(tornado.web.RequestHandler):\n    # eliot input, output\n    @log_call\n    def post(self, *args, **kwargs):\n        try:\n            self.timestamp = time.ctime()\n            self.recv_json = json.loads(self.request.body, strict=False)\n            q.put((self.timestamp, self.recv_json ))\n            output_dic = &#123;\n                &#39;status&#39;: &#39;1&#39;,\n                &#39;result&#39;: &#39;success&#39;\n      &#125;\n        except Exception as e:\n            print(&#39;LogHandler Error: &#39;, e)\n            return\n\n\n# eliot.start_action  with \ndef consume_msg(queue):\n    while True:\n        with start_action(action_type=&#39;consume_msg&#39;, lenghth = queue.qsize()):\n            try:\n                timestamp, msg= q.get()\n                start_time = time.time()\n                with start_action(action_type=&#39;save_request_body&#39;, timestamp=timestamp,msg=msg):\n                    save_body(msg)\n            except Exception as e:\n                with start_action(action_type=&#39;consume_msg exceptiosn&#39;, timestamp=timestamp,e=e, msg=msg):\n                    print(&#39;consume_mgs exception : &#39;  , e)\n\nconsumer = Process(target=consume_msg, args=(q, ))\nconsumer.daemon = True # \nconsumer.start()</code></pre>\n<p>eliot</p>\n<h3 id=\"RabbiMQ\"><a href=\"#RabbiMQ\" class=\"headerlink\" title=\"RabbiMQ\"></a>RabbiMQ</h3><p>RabbiMQ </p>\n<pre><code class=\"python\"># rabbitmq init\ncredentials = pika.PlainCredentials(RabbitMQ.USERNAME, RabbitMQ.USERPWD)\nconnection = pika.BlockingConnection(\n                pika.ConnectionParameters(RabbitMQ.HOST, RabbitMQ.PORT,\n                                          credentials=credentials)\n    )\n# channel init and declare\nchannel = connection.channel()\nchannel.exchange_declare(exchange=RabbitMQ.EXCHANGE,\n                             exchange_type=RabbitMQ.EXCHANGE_TYPE,\n                             durable=True)\nchannel.queue_declare(queue=RabbitMQ.QUEUE_NAME_ALG, durable=True)\nchannel.queue_bind(exchange=RabbitMQ.EXCHANGE,\n                       queue=RabbitMQ.QUEUE_NAME_ALG,\n                       routing_key=RabbitMQ.ROUTING_KEY)\nchannel.basic_qos(prefetch_count=1)    # \nchannel.basic_consume(on_message_callback=consumer,\n                          queue=RabbitMQ.QUEUE_NAME_ALG, auto_ack=False)\nprint(&#39; [*] Waiting for messages...&#39;)\nchannel.start_consuming()\n\ndef consumer(ch, method, properties, body):\n    body = json.loads(body, strict=False)\n    flag = body[&#39;flag&#39;]\n    # process body\n\n    # Ack manually\n    ch.basic_ack(delivery_tag=method.delivery_tag)\n\n\n\n# producer.py\ntry:\n    credentials = pika.PlainCredentials(\n                RabbitMQ.USERNAME, RabbitMQ.USERPWD)\n    connection = pika.BlockingConnection(\n                pika.ConnectionParameters(RabbitMQ.HOST, RabbitMQ.PORT,\n                                          credentials=credentials)\n            )\n    channel = self.connection.channel()\n    channel.exchange_declare(exchange=RabbitMQ.EXCHANGE,                                                                                              exchange_type=RabbitMQ.EXCHANGE_TYPE,\n                                          durable=True)\nexcept Exception as e:\n    pass\n\nsent_msg = &#123;...&#125;\nbody = json.dumps(sent_msg)\nchannel.basic_publish(exchange=RabbitMQ.EXCHANGE,                 \n                      routing_key=RabbitMQ.ROUTING_KEY,\n                      body=body, \n                      properties=pika.BasicProperties(delivery_mode=2))\n\nchannel.close()\nconnection.close()</code></pre>\n<h3 id=\"--IO\"><a href=\"#--IO\" class=\"headerlink\" title=\"  IO\"></a>  IO</h3><p>IO</p>\n<blockquote>\n<p>CPUPythonC++/Java</p>\n<p>IOIOC++/JavaPython</p>\n</blockquote>\n<p>PythonC++/Java</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>Python</p>\n<p>PythonasyncioIOasyncioEventLoopEventLoopIO,producer-consumer</p>\n<pre><code class=\"python\">import asyncio\nimport random\n\nasync def producer(queue, n):\n    for x in range(1, n+1):\n        print(&#39;producing : &#39;, x)    \n        # simulate io job\n        await asyncio.sleep(random.random())\n        item = str(x)\n        await queue.put(item)\n\nasync def consume(queue):\n    while True:\n        item = await queue.get()\n    # process item\n    print(&#39;consuming : &#39;, item)\n    # simulate io \n    await asyncio.sleep(random.random())\n    # notify queue that the item has been processed\n    queue.task_done()\n\nasync def run(n):\n    queue = asyncio.Queue()\n  # schedule consumer\n  consumer= asyncio.ensure_future(consume(queue))\n  await producer(queue, n)\n    # wait until consumer processed all items\n  await queue.join()\n  # consumer is still awaiting item, cancel it\n  consumer.cancel()\n\nloop = asyncio.get_event_loop()\nloop.run_until_complete(run(10))\nloop.close()</code></pre>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p></p>\n<p>References:</p>\n<p><a href=\"https://asyncio.readthedocs.io/en/latest/producer_consumer.html\">Producer/consumer-examlpe_asyncio</a></p>\n<p><a href=\"https://www.liaoxuefeng.com/wiki/1016959663602400/1017968846697824\">-</a></p>\n<p><a href=\"https://www.liaoxuefeng.com/wiki/1016959663602400/1017631469467456\">--</a></p>\n<p><a href=\"https://python3-cookbook.readthedocs.io/zh_CN/latest/c12/p03_communicating_between_threads.html\">-Python_CookBook</a></p>\n<p><a href=\"https://juejin.cn/post/6844904039210024967#heading-3\">Python</a></p>\n"},{"title":"GPU in Pytorch  ","toc":true,"mathjax":true,"top":false,"cover":true,"date":"2020-11-25T07:07:07.000Z","updated":"2020-12-10T08:34:40.141Z","_content":"\nCUDAGPUPytorch\n\nPytorch 3torch.multiprocessing, nn.DataParallel, nn.parallel.DistributedDataParallelGPU nn.parallle.DistributedDataParallel, [CUDA SEMANTIC](https://pytorch.org/docs/stable/notes/cuda.html#cuda-nn-ddp-instead) \n\n> **Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel**\n>\n> Most use cases involving batched inputs and multiple GPUs should default to using [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel) to utilize more than one GPU.\n>\n> There are significant caveats to using CUDA models with [`multiprocessing`](https://pytorch.org/docs/stable/multiprocessing.html#module-torch.multiprocessing); unless care is taken to meet the data handling requirements exactly, it is likely that your program will have incorrect or undefined behavior.\n>\n> It is recommended to use [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel), instead of [`DataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel) to do multi-GPU training, even if there is only a single node.\n>\n> The difference between [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel) and [`DataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel) is: [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel) uses multiprocessing where a process is created for each GPU, while [`DataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel) uses multithreading. By using multiprocessing, each GPU has its dedicated process, this avoids the performance overhead caused by GIL of Python interpreter.\n>\n> If you use [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel), you could use torch.distributed.launch utility to launch your program, see [Third-party backends](https://pytorch.org/docs/stable/distributed.html#distributed-launch).\n\n\n\n### torch.multiprocessing\n\ntorch.multiprocessing  Python multiprocessing  python: multiprocessing.Queue Pytorch [MULTIPROCESSING BEST PRACTICES](https://pytorch.org/docs/stable/notes/multiprocessing.html), [](https://pytorch.apachecn.org/docs/1.4/64.html)\n\n```python\nimport torch.multiprocessing as mp\nfrom model import MyModel\n\ndef train(model):\n    # Construct data_loader, optimizer, etc.\n    for data, labels in data_loader:\n        optimizer.zero_grad()\n        loss_fn(model(data), labels).backward()\n        optimizer.step()  # This will update the shared parameters\n\nif __name__ == '__main__':\n    num_processes = 4\n    model = MyModel()\n    # NOTE: this is required for the ``fork`` method to work\n    model.share_memory()\n    processes = []\n    for rank in range(num_processes):\n        p = mp.Process(target=train, args=(model,))\n        p.start()\n        processes.append(p)\n    for p in processes:\n        p.join()\n```\n\n\n\n### DataParallel\n\nDataParallelmodel copyGPUGPUSIMDGPU\n\n#### DataParallel\n\n```python\nmodel = nn.DataParallel(model)\n```\n\n outside model    inside model \n\n```python\nclass Model(nn.Module):\n    # Our model\n\n    def __init__(self, input_size, output_size):\n        super(Model, self).__init__()\n        self.fc = nn.Linear(input_size, output_size)\n\n    def forward(self, input):\n        output = self.fc(input)\n        print(\"\\tIn Model: input size\", input.size(),\n              \"output size\", output.size())\n\n        return output\n        \n\nmodel = Model(input_size, output_size)\nif torch.cuda.device_count() > 1:\n  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n  model = nn.DataParallel(model)\n\nmodel.to(device)\n\nfor data in rand_loader:\n    input = data.to(device)\n    output = model(input)\n    print(\"Outside: input size\", input.size(),\n          \"output_size\", output.size())\n    \n# 2 GPUs\n# on 2 GPUs\nLet's use 2 GPUs!\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\nOutside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n```\n\nGPUTrickGPU GPU  GPU ( `m` 10 `DataParallel` GPU  10  GPU  GPU  5 \n\n#### \n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = torch.nn.Linear(10, 10).to('cuda:0')\n        self.relu = torch.nn.ReLU()\n        self.net2 = torch.nn.Linear(10, 5).to('cuda:1')\n\n    def forward(self, x):\n        x = self.relu(self.net1(x.to('cuda:0')))\n        return self.net2(x.to('cuda:1'))\n```\n\n GPU  GPU GPU   GPU  `layer2``layer3``cuda:0``cuda:1`\n\nTrick\n\n#### Pipeline \n\n```python\nclass PipelineParallelResNet50(ModelParallelResNet50):\n    def __init__(self, split_size=20, *args, **kwargs):\n        super(PipelineParallelResNet50, self).__init__(*args, **kwargs)\n        self.split_size = split_size\n\n    def forward(self, x):\n        splits = iter(x.split(self.split_size, dim=0))\n        s_next = next(splits)\n        s_prev = self.seq1(s_next).to('cuda:1')\n        ret = []\n\n        for s_next in splits:\n            # A. s_prev runs on cuda:1\n            s_prev = self.seq2(s_prev)\n            ret.append(self.fc(s_prev.view(s_prev.size(0), -1)))\n\n            # B. s_next runs on cuda:0, which can run concurrently with A\n            s_prev = self.seq1(s_next).to('cuda:1')\n\n        s_prev = self.seq2(s_prev)\n        ret.append(self.fc(s_prev.view(s_prev.size(0), -1)))\n\n        return torch.cat(ret)\n\nsetup = \"model = PipelineParallelResNet50()\"\npp_run_times = timeit.repeat(\n    stmt, setup, number=1, repeat=num_repeat, globals=globals())\npp_mean, pp_std = np.mean(pp_run_times), np.std(pp_run_times)\n\nplot([mp_mean, rn_mean, pp_mean],\n     [mp_std, rn_std, pp_std],\n     ['Model Parallel', 'Single GPU', 'Pipelining Model Parallel'],\n     'mp_vs_rn_vs_pp.png')\n```\n\n2 GPUs50%100%\n\n###![1](1.png) \n\n### \n\n 2-GPU 1-GPU 2-GPU Pipelining  \n\n\n\n### nn.parallel.DistributedDataParallel\n\ntorch.distributed DataParallel \n\n`DistributedDataParallel`\n\n#### GPU\n\n```python\ntorch.distributed.init_process_group(backend=\"nccl\")\n# device_ids will include all GPU devices by default\nmodel = DistributedDataParallel(model) \n\n```\n\n#### GPU\n\nGPUGILDDP(DistributedDataParallel)GPUtorch.nn.DataParallelPytorch\n\n\n\n1. NGPUNtorch.distributed.launch\n\n```bash\npython -m torch.distributed.launch --nproc_per_node=n distributed_data_parallel.py\n```\n\n2. GPU model\n\n```python\nparser = argparse.ArgumentParser()\n# FOR DISTRIBUTED:  Parse for the local_rank argument, which will be supplied\n# automatically by torch.distributed.launch.\nparser.add_argument(\"--local_rank\", default=0, type=int)\nargs = parser.parse_args()\n\n#  GPU rank id\ntorch.cuda.set_device(args.local_rank)\n# model\ntorch.distributed.init_process_group(backend='nccl', world_size=n, init_method='env://')\nmodel = torch.nn.parallel.DistributedDataParallel(\n  \t\t\t\t\t\t\t\t\tmodel,\n\t\t\t\t\t\t\t\t\t\tdevice_ids=[args.local_rank],\n\t\t\t\t\t\t\t\t\t\toutput_device=args.local_rank)\n```\n\nNote:\n\n1. nccl \n2. nccl\n3. no_sync DDPForward-Backward\n\n```python\nddp = torch.nn.DistributedDataParallel(model, pg)\nwith ddp.no_sync():\nfor input in inputs:\n\tddp(input).backward()  # no synchronization, accumulate grads\nddp(another_input).backward()  # synchronize grads\n```\n\n\n\n### apex.parallel.DistributedDataParallel\n\ntorch.nn.parallel.DistributedDataParallelwrapperNCCL\n\n``` bash\n#  n <= GPU  1GPU1\ntorch.distributed.launch --nproc_per_node=n distributed_data_parallel.py\n# :\n# args.local_rank\n# os.environ['WORLD_SIZE']\n\n```\n\n\n\n1. model = DDP(model)  devices_ids output_device\n\n2. init_process_group  init_method='env://'\n\n   ``` python\n   torch.distributed.init_process_group(backend='nccl',init_method='env://')\n   ```\n\n\n\n```python\n# distributed_data_parallel.py\nimport torch\nimport argparse\nimport os\nfrom apex import amp\n# FOR DISTRIBUTED: (can also use torch.nn.parallel.DistributedDataParallel instead)\nfrom apex.parallel import DistributedDataParallel\n\nparser = argparse.ArgumentParser()\n# FOR DISTRIBUTED:  Parse for the local_rank argument, which will be supplied\n# automatically by torch.distributed.launch.\nparser.add_argument(\"--local_rank\", default=0, type=int)\nargs = parser.parse_args()\n\n# FOR DISTRIBUTED:  If we are running under torch.distributed.launch,\n# the 'WORLD_SIZE' environment variable will also be set automatically.\nargs.distributed = False\nif 'WORLD_SIZE' in os.environ:\n    args.distributed = int(os.environ['WORLD_SIZE']) > 1\n\nif args.distributed:\n    # FOR DISTRIBUTED:  Set the device according to local_rank.\n    torch.cuda.set_device(args.local_rank)\n\n    # FOR DISTRIBUTED:  Initialize the backend.  torch.distributed.launch will provide\n    # environment variables, and requires that you use init_method=`env://`.\n    torch.distributed.init_process_group(backend='nccl',\n                                         init_method='env://')\n\ntorch.backends.cudnn.benchmark = True\n\nN, D_in, D_out = 64, 1024, 16\n\n# Each process receives its own batch of \"fake input data\" and \"fake target data.\"\n# The \"training loop\" in each process just uses this fake batch over and over.\n# https://github.com/NVIDIA/apex/tree/master/examples/imagenet provides a more realistic\n# example of distributed data sampling for both training and validation.\nx = torch.randn(N, D_in, device='cuda')\ny = torch.randn(N, D_out, device='cuda')\n\nmodel = torch.nn.Linear(D_in, D_out).cuda()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n\nmodel, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n\nif args.distributed:\n    # FOR DISTRIBUTED:  After amp.initialize, wrap the model with\n    # apex.parallel.DistributedDataParallel.\n    model = DistributedDataParallel(model)\n    # torch.nn.parallel.DistributedDataParallel is also fine, with some added args:\n    # model = torch.nn.parallel.DistributedDataParallel(model,\n    #                                                   device_ids=[args.local_rank],\n    #                                                   output_device=args.local_rank)\n\nloss_fn = torch.nn.MSELoss()\n\nfor t in range(500):\n    optimizer.zero_grad()\n    y_pred = model(x)\n    loss = loss_fn(y_pred, y)\n    with amp.scale_loss(loss, optimizer) as scaled_loss:\n        scaled_loss.backward()\n    optimizer.step()\n\nif args.local_rank == 0:\n    print(\"final loss = \", loss)\n```\n\n [mixed precision training with DDP](https://github.com/NVIDIA/apex/tree/master/examples/imagenet)\n\n\n\n### DDP \n\ntorch.save  torch.load \n\n``` python\nimport os\nimport tempfile\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\ndef demo_checkpoint(rank, world_size):\n    setup(rank, world_size)\n\n    # setup devices for this process, rank 1 uses GPUs [0, 1, 2, 3] and\n    # rank 2 uses GPUs [4, 5, 6, 7].\n    n = torch.cuda.device_count() // world_size\n    device_ids = list(range(rank * n, (rank + 1) * n))\n\n    model = ToyModel().to(device_ids[0])\n    # output_device defaults to device_ids[0]\n    ddp_model = DDP(model, device_ids=device_ids)\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    CHECKPOINT_PATH = tempfile.gettempdir() + \"/model.checkpoint\"\n    if rank == 0:\n        # All processes should see same parameters as they all start from same\n        # random parameters and gradients are synchronized in backward passes.\n        # Therefore, saving it in one process is sufficient.\n        torch.save(ddp_model.state_dict(), CHECKPOINT_PATH)\n\n    # Use a barrier() to make sure that process 1 loads the model after process\n    # 0 saves it.\n    dist.barrier()\n    # configure map_location properly\n    rank0_devices = [x - rank * len(device_ids) for x in device_ids]\n    device_pairs = zip(rank0_devices, device_ids)\n    map_location = {'cuda:%d' % x: 'cuda:%d' % y for x, y in device_pairs}\n    ddp_model.load_state_dict(\n        torch.load(CHECKPOINT_PATH, map_location=map_location))\n\n    optimizer.zero_grad()\n    outputs = ddp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(device_ids[0])\n    loss_fn = nn.MSELoss()\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    # Use a barrier() to make sure that all processes have finished reading the\n    # checkpoint\n    dist.barrier()\n\n    if rank == 0:\n        os.remove(CHECKPOINT_PATH)\n\n    cleanup()\n```\n\n\n\n### DDP \n\nDDP  GPU   DDP  GPU   GPU \n\n``` python\nclass ToyMpModel(nn.Module):\n    def __init__(self, dev0, dev1):\n        super(ToyMpModel, self).__init__()\n        self.dev0 = dev0\n        self.dev1 = dev1\n        self.net1 = torch.nn.Linear(10, 10).to(dev0)\n        self.relu = torch.nn.ReLU()\n        self.net2 = torch.nn.Linear(10, 5).to(dev1)\n\n    def forward(self, x):\n        x = x.to(self.dev0)\n        x = self.relu(self.net1(x))\n        x = x.to(self.dev1)\n        return self.net2(x)\n```\n\n GPU  DDP `device_ids``output_device` `forward()`\n\n``` python\ndef demo_model_parallel(rank, world_size):\n    setup(rank, world_size)\n\n    # setup mp_model and devices for this process\n    dev0 = rank * 2\n    dev1 = rank * 2 + 1\n    mp_model = ToyMpModel(dev0, dev1)\n    ddp_mp_model = DDP(mp_model)\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_mp_model.parameters(), lr=0.001)\n\n    optimizer.zero_grad()\n    # outputs will be on dev1\n    outputs = ddp_mp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(dev1)\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    cleanup()\n\nif __name__ == \"__main__\":\n    run_demo(demo_basic, 2)\n    run_demo(demo_checkpoint, 2)\n\n    if torch.cuda.device_count() >= 8:\n        run_demo(demo_model_parallel, 4)\n```\n\nsetup  torch.multiprocessing.spawn setup\n\n```python\nimport os\nimport tempfile\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n\n    # initialize the process group\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n\n    # Explicitly setting seed to make sure that models created in two processes\n    # start from same random weights and biases.\n    torch.manual_seed(42)\n\ndef cleanup():\n    dist.destroy_process_group()\n    \nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(10, 10)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(10, 5)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\ndef demo_basic(rank, world_size):\n    setup(rank, world_size)\n\n    # setup devices for this process, rank 1 uses GPUs [0, 1, 2, 3] and\n    # rank 2 uses GPUs [4, 5, 6, 7].\n    n = torch.cuda.device_count() // world_size\n    device_ids = list(range(rank * n, (rank + 1) * n))\n\n    # create model and move it to device_ids[0]\n    model = ToyModel().to(device_ids[0])\n    # output_device defaults to device_ids[0]\n    ddp_model = DDP(model, device_ids=device_ids)\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    optimizer.zero_grad()\n    outputs = ddp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(device_ids[0])\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    cleanup()\n\ndef run_demo(demo_fn, world_size):\n    mp.spawn(demo_fn,\n             args=(world_size,),\n             nprocs=world_size,\n             join=True)\n```\n\n\n\n### Conclusion\n\nPytorchGPUGILDDP model \n\n\n\nReferences:\n\n[torch.nn](https://pytorch.apachecn.org/docs/1.4/75.html)\n\n[DistributedDataParallel API](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html?highlight=distributeddataparallel#torch.nn.parallel.DistributedDataParallel)\n\n[CUDA Semantics](https://pytorch.org/docs/stable/notes/cuda.html#cuda-nn-ddp-instead)\n\n[](https://pytorch.apachecn.org/docs/1.4/34.html)\n\n[Pytorch](https://pytorch.apachecn.org/docs/1.4/35.html)\n\n[apex.parallel](https://nvidia.github.io/apex/parallel.html)\n\n","source":"_posts/GPU-in-Pytorch-.md","raw":"---\ntitle: GPU in Pytorch  \ntoc: true\nmathjax: true\ntop: false\ncover: true\ndate: 2020-11-25 15:07:07\nupdated:\ncategories: Pytorch\ntags:\n\t- Pytorch\n\t- CUDA\n\t- GPU\n---\n\nCUDAGPUPytorch\n\nPytorch 3torch.multiprocessing, nn.DataParallel, nn.parallel.DistributedDataParallelGPU nn.parallle.DistributedDataParallel, [CUDA SEMANTIC](https://pytorch.org/docs/stable/notes/cuda.html#cuda-nn-ddp-instead) \n\n> **Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel**\n>\n> Most use cases involving batched inputs and multiple GPUs should default to using [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel) to utilize more than one GPU.\n>\n> There are significant caveats to using CUDA models with [`multiprocessing`](https://pytorch.org/docs/stable/multiprocessing.html#module-torch.multiprocessing); unless care is taken to meet the data handling requirements exactly, it is likely that your program will have incorrect or undefined behavior.\n>\n> It is recommended to use [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel), instead of [`DataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel) to do multi-GPU training, even if there is only a single node.\n>\n> The difference between [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel) and [`DataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel) is: [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel) uses multiprocessing where a process is created for each GPU, while [`DataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel) uses multithreading. By using multiprocessing, each GPU has its dedicated process, this avoids the performance overhead caused by GIL of Python interpreter.\n>\n> If you use [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel), you could use torch.distributed.launch utility to launch your program, see [Third-party backends](https://pytorch.org/docs/stable/distributed.html#distributed-launch).\n\n\n\n### torch.multiprocessing\n\ntorch.multiprocessing  Python multiprocessing  python: multiprocessing.Queue Pytorch [MULTIPROCESSING BEST PRACTICES](https://pytorch.org/docs/stable/notes/multiprocessing.html), [](https://pytorch.apachecn.org/docs/1.4/64.html)\n\n```python\nimport torch.multiprocessing as mp\nfrom model import MyModel\n\ndef train(model):\n    # Construct data_loader, optimizer, etc.\n    for data, labels in data_loader:\n        optimizer.zero_grad()\n        loss_fn(model(data), labels).backward()\n        optimizer.step()  # This will update the shared parameters\n\nif __name__ == '__main__':\n    num_processes = 4\n    model = MyModel()\n    # NOTE: this is required for the ``fork`` method to work\n    model.share_memory()\n    processes = []\n    for rank in range(num_processes):\n        p = mp.Process(target=train, args=(model,))\n        p.start()\n        processes.append(p)\n    for p in processes:\n        p.join()\n```\n\n\n\n### DataParallel\n\nDataParallelmodel copyGPUGPUSIMDGPU\n\n#### DataParallel\n\n```python\nmodel = nn.DataParallel(model)\n```\n\n outside model    inside model \n\n```python\nclass Model(nn.Module):\n    # Our model\n\n    def __init__(self, input_size, output_size):\n        super(Model, self).__init__()\n        self.fc = nn.Linear(input_size, output_size)\n\n    def forward(self, input):\n        output = self.fc(input)\n        print(\"\\tIn Model: input size\", input.size(),\n              \"output size\", output.size())\n\n        return output\n        \n\nmodel = Model(input_size, output_size)\nif torch.cuda.device_count() > 1:\n  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n  model = nn.DataParallel(model)\n\nmodel.to(device)\n\nfor data in rand_loader:\n    input = data.to(device)\n    output = model(input)\n    print(\"Outside: input size\", input.size(),\n          \"output_size\", output.size())\n    \n# 2 GPUs\n# on 2 GPUs\nLet's use 2 GPUs!\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\nOutside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n```\n\nGPUTrickGPU GPU  GPU ( `m` 10 `DataParallel` GPU  10  GPU  GPU  5 \n\n#### \n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = torch.nn.Linear(10, 10).to('cuda:0')\n        self.relu = torch.nn.ReLU()\n        self.net2 = torch.nn.Linear(10, 5).to('cuda:1')\n\n    def forward(self, x):\n        x = self.relu(self.net1(x.to('cuda:0')))\n        return self.net2(x.to('cuda:1'))\n```\n\n GPU  GPU GPU   GPU  `layer2``layer3``cuda:0``cuda:1`\n\nTrick\n\n#### Pipeline \n\n```python\nclass PipelineParallelResNet50(ModelParallelResNet50):\n    def __init__(self, split_size=20, *args, **kwargs):\n        super(PipelineParallelResNet50, self).__init__(*args, **kwargs)\n        self.split_size = split_size\n\n    def forward(self, x):\n        splits = iter(x.split(self.split_size, dim=0))\n        s_next = next(splits)\n        s_prev = self.seq1(s_next).to('cuda:1')\n        ret = []\n\n        for s_next in splits:\n            # A. s_prev runs on cuda:1\n            s_prev = self.seq2(s_prev)\n            ret.append(self.fc(s_prev.view(s_prev.size(0), -1)))\n\n            # B. s_next runs on cuda:0, which can run concurrently with A\n            s_prev = self.seq1(s_next).to('cuda:1')\n\n        s_prev = self.seq2(s_prev)\n        ret.append(self.fc(s_prev.view(s_prev.size(0), -1)))\n\n        return torch.cat(ret)\n\nsetup = \"model = PipelineParallelResNet50()\"\npp_run_times = timeit.repeat(\n    stmt, setup, number=1, repeat=num_repeat, globals=globals())\npp_mean, pp_std = np.mean(pp_run_times), np.std(pp_run_times)\n\nplot([mp_mean, rn_mean, pp_mean],\n     [mp_std, rn_std, pp_std],\n     ['Model Parallel', 'Single GPU', 'Pipelining Model Parallel'],\n     'mp_vs_rn_vs_pp.png')\n```\n\n2 GPUs50%100%\n\n###![1](1.png) \n\n### \n\n 2-GPU 1-GPU 2-GPU Pipelining  \n\n\n\n### nn.parallel.DistributedDataParallel\n\ntorch.distributed DataParallel \n\n`DistributedDataParallel`\n\n#### GPU\n\n```python\ntorch.distributed.init_process_group(backend=\"nccl\")\n# device_ids will include all GPU devices by default\nmodel = DistributedDataParallel(model) \n\n```\n\n#### GPU\n\nGPUGILDDP(DistributedDataParallel)GPUtorch.nn.DataParallelPytorch\n\n\n\n1. NGPUNtorch.distributed.launch\n\n```bash\npython -m torch.distributed.launch --nproc_per_node=n distributed_data_parallel.py\n```\n\n2. GPU model\n\n```python\nparser = argparse.ArgumentParser()\n# FOR DISTRIBUTED:  Parse for the local_rank argument, which will be supplied\n# automatically by torch.distributed.launch.\nparser.add_argument(\"--local_rank\", default=0, type=int)\nargs = parser.parse_args()\n\n#  GPU rank id\ntorch.cuda.set_device(args.local_rank)\n# model\ntorch.distributed.init_process_group(backend='nccl', world_size=n, init_method='env://')\nmodel = torch.nn.parallel.DistributedDataParallel(\n  \t\t\t\t\t\t\t\t\tmodel,\n\t\t\t\t\t\t\t\t\t\tdevice_ids=[args.local_rank],\n\t\t\t\t\t\t\t\t\t\toutput_device=args.local_rank)\n```\n\nNote:\n\n1. nccl \n2. nccl\n3. no_sync DDPForward-Backward\n\n```python\nddp = torch.nn.DistributedDataParallel(model, pg)\nwith ddp.no_sync():\nfor input in inputs:\n\tddp(input).backward()  # no synchronization, accumulate grads\nddp(another_input).backward()  # synchronize grads\n```\n\n\n\n### apex.parallel.DistributedDataParallel\n\ntorch.nn.parallel.DistributedDataParallelwrapperNCCL\n\n``` bash\n#  n <= GPU  1GPU1\ntorch.distributed.launch --nproc_per_node=n distributed_data_parallel.py\n# :\n# args.local_rank\n# os.environ['WORLD_SIZE']\n\n```\n\n\n\n1. model = DDP(model)  devices_ids output_device\n\n2. init_process_group  init_method='env://'\n\n   ``` python\n   torch.distributed.init_process_group(backend='nccl',init_method='env://')\n   ```\n\n\n\n```python\n# distributed_data_parallel.py\nimport torch\nimport argparse\nimport os\nfrom apex import amp\n# FOR DISTRIBUTED: (can also use torch.nn.parallel.DistributedDataParallel instead)\nfrom apex.parallel import DistributedDataParallel\n\nparser = argparse.ArgumentParser()\n# FOR DISTRIBUTED:  Parse for the local_rank argument, which will be supplied\n# automatically by torch.distributed.launch.\nparser.add_argument(\"--local_rank\", default=0, type=int)\nargs = parser.parse_args()\n\n# FOR DISTRIBUTED:  If we are running under torch.distributed.launch,\n# the 'WORLD_SIZE' environment variable will also be set automatically.\nargs.distributed = False\nif 'WORLD_SIZE' in os.environ:\n    args.distributed = int(os.environ['WORLD_SIZE']) > 1\n\nif args.distributed:\n    # FOR DISTRIBUTED:  Set the device according to local_rank.\n    torch.cuda.set_device(args.local_rank)\n\n    # FOR DISTRIBUTED:  Initialize the backend.  torch.distributed.launch will provide\n    # environment variables, and requires that you use init_method=`env://`.\n    torch.distributed.init_process_group(backend='nccl',\n                                         init_method='env://')\n\ntorch.backends.cudnn.benchmark = True\n\nN, D_in, D_out = 64, 1024, 16\n\n# Each process receives its own batch of \"fake input data\" and \"fake target data.\"\n# The \"training loop\" in each process just uses this fake batch over and over.\n# https://github.com/NVIDIA/apex/tree/master/examples/imagenet provides a more realistic\n# example of distributed data sampling for both training and validation.\nx = torch.randn(N, D_in, device='cuda')\ny = torch.randn(N, D_out, device='cuda')\n\nmodel = torch.nn.Linear(D_in, D_out).cuda()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n\nmodel, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n\nif args.distributed:\n    # FOR DISTRIBUTED:  After amp.initialize, wrap the model with\n    # apex.parallel.DistributedDataParallel.\n    model = DistributedDataParallel(model)\n    # torch.nn.parallel.DistributedDataParallel is also fine, with some added args:\n    # model = torch.nn.parallel.DistributedDataParallel(model,\n    #                                                   device_ids=[args.local_rank],\n    #                                                   output_device=args.local_rank)\n\nloss_fn = torch.nn.MSELoss()\n\nfor t in range(500):\n    optimizer.zero_grad()\n    y_pred = model(x)\n    loss = loss_fn(y_pred, y)\n    with amp.scale_loss(loss, optimizer) as scaled_loss:\n        scaled_loss.backward()\n    optimizer.step()\n\nif args.local_rank == 0:\n    print(\"final loss = \", loss)\n```\n\n [mixed precision training with DDP](https://github.com/NVIDIA/apex/tree/master/examples/imagenet)\n\n\n\n### DDP \n\ntorch.save  torch.load \n\n``` python\nimport os\nimport tempfile\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\ndef demo_checkpoint(rank, world_size):\n    setup(rank, world_size)\n\n    # setup devices for this process, rank 1 uses GPUs [0, 1, 2, 3] and\n    # rank 2 uses GPUs [4, 5, 6, 7].\n    n = torch.cuda.device_count() // world_size\n    device_ids = list(range(rank * n, (rank + 1) * n))\n\n    model = ToyModel().to(device_ids[0])\n    # output_device defaults to device_ids[0]\n    ddp_model = DDP(model, device_ids=device_ids)\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    CHECKPOINT_PATH = tempfile.gettempdir() + \"/model.checkpoint\"\n    if rank == 0:\n        # All processes should see same parameters as they all start from same\n        # random parameters and gradients are synchronized in backward passes.\n        # Therefore, saving it in one process is sufficient.\n        torch.save(ddp_model.state_dict(), CHECKPOINT_PATH)\n\n    # Use a barrier() to make sure that process 1 loads the model after process\n    # 0 saves it.\n    dist.barrier()\n    # configure map_location properly\n    rank0_devices = [x - rank * len(device_ids) for x in device_ids]\n    device_pairs = zip(rank0_devices, device_ids)\n    map_location = {'cuda:%d' % x: 'cuda:%d' % y for x, y in device_pairs}\n    ddp_model.load_state_dict(\n        torch.load(CHECKPOINT_PATH, map_location=map_location))\n\n    optimizer.zero_grad()\n    outputs = ddp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(device_ids[0])\n    loss_fn = nn.MSELoss()\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    # Use a barrier() to make sure that all processes have finished reading the\n    # checkpoint\n    dist.barrier()\n\n    if rank == 0:\n        os.remove(CHECKPOINT_PATH)\n\n    cleanup()\n```\n\n\n\n### DDP \n\nDDP  GPU   DDP  GPU   GPU \n\n``` python\nclass ToyMpModel(nn.Module):\n    def __init__(self, dev0, dev1):\n        super(ToyMpModel, self).__init__()\n        self.dev0 = dev0\n        self.dev1 = dev1\n        self.net1 = torch.nn.Linear(10, 10).to(dev0)\n        self.relu = torch.nn.ReLU()\n        self.net2 = torch.nn.Linear(10, 5).to(dev1)\n\n    def forward(self, x):\n        x = x.to(self.dev0)\n        x = self.relu(self.net1(x))\n        x = x.to(self.dev1)\n        return self.net2(x)\n```\n\n GPU  DDP `device_ids``output_device` `forward()`\n\n``` python\ndef demo_model_parallel(rank, world_size):\n    setup(rank, world_size)\n\n    # setup mp_model and devices for this process\n    dev0 = rank * 2\n    dev1 = rank * 2 + 1\n    mp_model = ToyMpModel(dev0, dev1)\n    ddp_mp_model = DDP(mp_model)\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_mp_model.parameters(), lr=0.001)\n\n    optimizer.zero_grad()\n    # outputs will be on dev1\n    outputs = ddp_mp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(dev1)\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    cleanup()\n\nif __name__ == \"__main__\":\n    run_demo(demo_basic, 2)\n    run_demo(demo_checkpoint, 2)\n\n    if torch.cuda.device_count() >= 8:\n        run_demo(demo_model_parallel, 4)\n```\n\nsetup  torch.multiprocessing.spawn setup\n\n```python\nimport os\nimport tempfile\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n\n    # initialize the process group\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n\n    # Explicitly setting seed to make sure that models created in two processes\n    # start from same random weights and biases.\n    torch.manual_seed(42)\n\ndef cleanup():\n    dist.destroy_process_group()\n    \nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(10, 10)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(10, 5)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\ndef demo_basic(rank, world_size):\n    setup(rank, world_size)\n\n    # setup devices for this process, rank 1 uses GPUs [0, 1, 2, 3] and\n    # rank 2 uses GPUs [4, 5, 6, 7].\n    n = torch.cuda.device_count() // world_size\n    device_ids = list(range(rank * n, (rank + 1) * n))\n\n    # create model and move it to device_ids[0]\n    model = ToyModel().to(device_ids[0])\n    # output_device defaults to device_ids[0]\n    ddp_model = DDP(model, device_ids=device_ids)\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    optimizer.zero_grad()\n    outputs = ddp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(device_ids[0])\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    cleanup()\n\ndef run_demo(demo_fn, world_size):\n    mp.spawn(demo_fn,\n             args=(world_size,),\n             nprocs=world_size,\n             join=True)\n```\n\n\n\n### Conclusion\n\nPytorchGPUGILDDP model \n\n\n\nReferences:\n\n[torch.nn](https://pytorch.apachecn.org/docs/1.4/75.html)\n\n[DistributedDataParallel API](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html?highlight=distributeddataparallel#torch.nn.parallel.DistributedDataParallel)\n\n[CUDA Semantics](https://pytorch.org/docs/stable/notes/cuda.html#cuda-nn-ddp-instead)\n\n[](https://pytorch.apachecn.org/docs/1.4/34.html)\n\n[Pytorch](https://pytorch.apachecn.org/docs/1.4/35.html)\n\n[apex.parallel](https://nvidia.github.io/apex/parallel.html)\n\n","slug":"GPU-in-Pytorch-","published":1,"_id":"ckhyotdja0000l9289pxq20m8","comments":1,"layout":"post","photos":[],"link":"","content":"<p>CUDAGPUPytorch</p>\n<p>Pytorch 3torch.multiprocessing, nn.DataParallel, nn.parallel.DistributedDataParallelGPU nn.parallle.DistributedDataParallel, <a href=\"https://pytorch.org/docs/stable/notes/cuda.html#cuda-nn-ddp-instead\">CUDA SEMANTIC</a> </p>\n<blockquote>\n<p><strong>Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel</strong></p>\n<p>Most use cases involving batched inputs and multiple GPUs should default to using <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\"><code>DistributedDataParallel</code></a> to utilize more than one GPU.</p>\n<p>There are significant caveats to using CUDA models with <a href=\"https://pytorch.org/docs/stable/multiprocessing.html#module-torch.multiprocessing\"><code>multiprocessing</code></a>; unless care is taken to meet the data handling requirements exactly, it is likely that your program will have incorrect or undefined behavior.</p>\n<p>It is recommended to use <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\"><code>DistributedDataParallel</code></a>, instead of <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel\"><code>DataParallel</code></a> to do multi-GPU training, even if there is only a single node.</p>\n<p>The difference between <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\"><code>DistributedDataParallel</code></a> and <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel\"><code>DataParallel</code></a> is: <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\"><code>DistributedDataParallel</code></a> uses multiprocessing where a process is created for each GPU, while <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel\"><code>DataParallel</code></a> uses multithreading. By using multiprocessing, each GPU has its dedicated process, this avoids the performance overhead caused by GIL of Python interpreter.</p>\n<p>If you use <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\"><code>DistributedDataParallel</code></a>, you could use torch.distributed.launch utility to launch your program, see <a href=\"https://pytorch.org/docs/stable/distributed.html#distributed-launch\">Third-party backends</a>.</p>\n</blockquote>\n<h3 id=\"torch-multiprocessing\"><a href=\"#torch-multiprocessing\" class=\"headerlink\" title=\"torch.multiprocessing\"></a>torch.multiprocessing</h3><p>torch.multiprocessing  Python multiprocessing  python: multiprocessing.Queue Pytorch <a href=\"https://pytorch.org/docs/stable/notes/multiprocessing.html\">MULTIPROCESSING BEST PRACTICES</a>, <a href=\"https://pytorch.apachecn.org/docs/1.4/64.html\"></a></p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>multiprocessing <span class=\"token keyword\">as</span> mp\n<span class=\"token keyword\">from</span> model <span class=\"token keyword\">import</span> MyModel\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">train</span><span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\" spellcheck=\"true\"># Construct data_loader, optimizer, etc.</span>\n    <span class=\"token keyword\">for</span> data<span class=\"token punctuation\">,</span> labels <span class=\"token keyword\">in</span> data_loader<span class=\"token punctuation\">:</span>\n        optimizer<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        loss_fn<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> labels<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        optimizer<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\" spellcheck=\"true\"># This will update the shared parameters</span>\n\n<span class=\"token keyword\">if</span> __name__ <span class=\"token operator\">==</span> <span class=\"token string\">'__main__'</span><span class=\"token punctuation\">:</span>\n    num_processes <span class=\"token operator\">=</span> <span class=\"token number\">4</span>\n    model <span class=\"token operator\">=</span> MyModel<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># NOTE: this is required for the ``fork`` method to work</span>\n    model<span class=\"token punctuation\">.</span>share_memory<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    processes <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">for</span> rank <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span>num_processes<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        p <span class=\"token operator\">=</span> mp<span class=\"token punctuation\">.</span>Process<span class=\"token punctuation\">(</span>target<span class=\"token operator\">=</span>train<span class=\"token punctuation\">,</span> args<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        p<span class=\"token punctuation\">.</span>start<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        processes<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>p<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">for</span> p <span class=\"token keyword\">in</span> processes<span class=\"token punctuation\">:</span>\n        p<span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre>\n<h3 id=\"DataParallel\"><a href=\"#DataParallel\" class=\"headerlink\" title=\"DataParallel\"></a>DataParallel</h3><p>DataParallelmodel copyGPUGPUSIMDGPU</p>\n<h4 id=\"DataParallel\"><a href=\"#DataParallel\" class=\"headerlink\" title=\"DataParallel\"></a>DataParallel</h4><pre class=\" language-python\"><code class=\"language-python\">model <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>DataParallel<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">)</span></code></pre>\n<p> outside model    inside model </p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">Model</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\" spellcheck=\"true\"># Our model</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> input_size<span class=\"token punctuation\">,</span> output_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        super<span class=\"token punctuation\">(</span>Model<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>fc <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>input_size<span class=\"token punctuation\">,</span> output_size<span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> input<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        output <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>fc<span class=\"token punctuation\">(</span>input<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\tIn Model: input size\"</span><span class=\"token punctuation\">,</span> input<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n              <span class=\"token string\">\"output size\"</span><span class=\"token punctuation\">,</span> output<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n        <span class=\"token keyword\">return</span> output\n\n\nmodel <span class=\"token operator\">=</span> Model<span class=\"token punctuation\">(</span>input_size<span class=\"token punctuation\">,</span> output_size<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">if</span> torch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>device_count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">></span> <span class=\"token number\">1</span><span class=\"token punctuation\">:</span>\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Let's use\"</span><span class=\"token punctuation\">,</span> torch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>device_count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"GPUs!\"</span><span class=\"token punctuation\">)</span>\n  <span class=\"token comment\" spellcheck=\"true\"># dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs</span>\n  model <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>DataParallel<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">)</span>\n\nmodel<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">for</span> data <span class=\"token keyword\">in</span> rand_loader<span class=\"token punctuation\">:</span>\n    input <span class=\"token operator\">=</span> data<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device<span class=\"token punctuation\">)</span>\n    output <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>input<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Outside: input size\"</span><span class=\"token punctuation\">,</span> input<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n          <span class=\"token string\">\"output_size\"</span><span class=\"token punctuation\">,</span> output<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># 2 GPUs</span>\n<span class=\"token comment\" spellcheck=\"true\"># on 2 GPUs</span>\nLet's use <span class=\"token number\">2</span> GPUs!\n    In Model<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    In Model<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nOutside<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">30</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output_size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">30</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    In Model<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    In Model<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nOutside<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">30</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output_size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">30</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    In Model<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    In Model<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nOutside<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">30</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output_size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">30</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    In Model<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    In Model<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nOutside<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output_size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre>\n<p>GPUTrickGPU GPU  GPU ( <code>m</code> 10 <code>DataParallel</code> GPU  10  GPU  GPU  5 </p>\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>nn <span class=\"token keyword\">as</span> nn\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>optim <span class=\"token keyword\">as</span> optim\n\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">ToyModel</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        super<span class=\"token punctuation\">(</span>ToyModel<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>net1 <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span><span class=\"token string\">'cuda:0'</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>relu <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>ReLU<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>net2 <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span><span class=\"token string\">'cuda:1'</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        x <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>relu<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>net1<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span><span class=\"token string\">'cuda:0'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> self<span class=\"token punctuation\">.</span>net2<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span><span class=\"token string\">'cuda:1'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre>\n<p> GPU  GPU GPU   GPU  <code>layer2</code><code>layer3</code><code>cuda:0</code><code>cuda:1</code></p>\n<p>Trick</p>\n<h4 id=\"Pipeline-\"><a href=\"#Pipeline-\" class=\"headerlink\" title=\"Pipeline \"></a>Pipeline </h4><pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">PipelineParallelResNet50</span><span class=\"token punctuation\">(</span>ModelParallelResNet50<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> split_size<span class=\"token operator\">=</span><span class=\"token number\">20</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">*</span>args<span class=\"token punctuation\">,</span> <span class=\"token operator\">**</span>kwargs<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        super<span class=\"token punctuation\">(</span>PipelineParallelResNet50<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token operator\">*</span>args<span class=\"token punctuation\">,</span> <span class=\"token operator\">**</span>kwargs<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>split_size <span class=\"token operator\">=</span> split_size\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        splits <span class=\"token operator\">=</span> iter<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>split_size<span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        s_next <span class=\"token operator\">=</span> next<span class=\"token punctuation\">(</span>splits<span class=\"token punctuation\">)</span>\n        s_prev <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>seq1<span class=\"token punctuation\">(</span>s_next<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span><span class=\"token string\">'cuda:1'</span><span class=\"token punctuation\">)</span>\n        ret <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n\n        <span class=\"token keyword\">for</span> s_next <span class=\"token keyword\">in</span> splits<span class=\"token punctuation\">:</span>\n            <span class=\"token comment\" spellcheck=\"true\"># A. s_prev runs on cuda:1</span>\n            s_prev <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>seq2<span class=\"token punctuation\">(</span>s_prev<span class=\"token punctuation\">)</span>\n            ret<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>fc<span class=\"token punctuation\">(</span>s_prev<span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span>s_prev<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n            <span class=\"token comment\" spellcheck=\"true\"># B. s_next runs on cuda:0, which can run concurrently with A</span>\n            s_prev <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>seq1<span class=\"token punctuation\">(</span>s_next<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span><span class=\"token string\">'cuda:1'</span><span class=\"token punctuation\">)</span>\n\n        s_prev <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>seq2<span class=\"token punctuation\">(</span>s_prev<span class=\"token punctuation\">)</span>\n        ret<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>fc<span class=\"token punctuation\">(</span>s_prev<span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span>s_prev<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n        <span class=\"token keyword\">return</span> torch<span class=\"token punctuation\">.</span>cat<span class=\"token punctuation\">(</span>ret<span class=\"token punctuation\">)</span>\n\nsetup <span class=\"token operator\">=</span> <span class=\"token string\">\"model = PipelineParallelResNet50()\"</span>\npp_run_times <span class=\"token operator\">=</span> timeit<span class=\"token punctuation\">.</span>repeat<span class=\"token punctuation\">(</span>\n    stmt<span class=\"token punctuation\">,</span> setup<span class=\"token punctuation\">,</span> number<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> repeat<span class=\"token operator\">=</span>num_repeat<span class=\"token punctuation\">,</span> globals<span class=\"token operator\">=</span>globals<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\npp_mean<span class=\"token punctuation\">,</span> pp_std <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>mean<span class=\"token punctuation\">(</span>pp_run_times<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> np<span class=\"token punctuation\">.</span>std<span class=\"token punctuation\">(</span>pp_run_times<span class=\"token punctuation\">)</span>\n\nplot<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>mp_mean<span class=\"token punctuation\">,</span> rn_mean<span class=\"token punctuation\">,</span> pp_mean<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n     <span class=\"token punctuation\">[</span>mp_std<span class=\"token punctuation\">,</span> rn_std<span class=\"token punctuation\">,</span> pp_std<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n     <span class=\"token punctuation\">[</span><span class=\"token string\">'Model Parallel'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Single GPU'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Pipelining Model Parallel'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n     <span class=\"token string\">'mp_vs_rn_vs_pp.png'</span><span class=\"token punctuation\">)</span></code></pre>\n<p>2 GPUs50%100%</p>\n<p>###<img src=\"1.png\" alt=\"1\"> </p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> 2-GPU 1-GPU 2-GPU Pipelining  </p>\n<h3 id=\"nn-parallel-DistributedDataParallel\"><a href=\"#nn-parallel-DistributedDataParallel\" class=\"headerlink\" title=\"nn.parallel.DistributedDataParallel\"></a>nn.parallel.DistributedDataParallel</h3><p>torch.distributed DataParallel </p>\n<p><code>DistributedDataParallel</code></p>\n<h4 id=\"GPU\"><a href=\"#GPU\" class=\"headerlink\" title=\"GPU\"></a>GPU</h4><pre class=\" language-python\"><code class=\"language-python\">torch<span class=\"token punctuation\">.</span>distributed<span class=\"token punctuation\">.</span>init_process_group<span class=\"token punctuation\">(</span>backend<span class=\"token operator\">=</span><span class=\"token string\">\"nccl\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\" spellcheck=\"true\"># device_ids will include all GPU devices by default</span>\nmodel <span class=\"token operator\">=</span> DistributedDataParallel<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">)</span> \n</code></pre>\n<h4 id=\"GPU\"><a href=\"#GPU\" class=\"headerlink\" title=\"GPU\"></a>GPU</h4><p>GPUGILDDP(DistributedDataParallel)GPUtorch.nn.DataParallelPytorch</p>\n<p></p>\n<ol>\n<li>NGPUNtorch.distributed.launch</li>\n</ol>\n<pre class=\" language-bash\"><code class=\"language-bash\">python -m torch.distributed.launch --nproc_per_node<span class=\"token operator\">=</span>n distributed_data_parallel.py</code></pre>\n<ol start=\"2\">\n<li>GPU model</li>\n</ol>\n<pre class=\" language-python\"><code class=\"language-python\">parser <span class=\"token operator\">=</span> argparse<span class=\"token punctuation\">.</span>ArgumentParser<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\" spellcheck=\"true\"># FOR DISTRIBUTED:  Parse for the local_rank argument, which will be supplied</span>\n<span class=\"token comment\" spellcheck=\"true\"># automatically by torch.distributed.launch.</span>\nparser<span class=\"token punctuation\">.</span>add_argument<span class=\"token punctuation\">(</span><span class=\"token string\">\"--local_rank\"</span><span class=\"token punctuation\">,</span> default<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> type<span class=\"token operator\">=</span>int<span class=\"token punctuation\">)</span>\nargs <span class=\"token operator\">=</span> parser<span class=\"token punctuation\">.</span>parse_args<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\">#  GPU rank id</span>\ntorch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>set_device<span class=\"token punctuation\">(</span>args<span class=\"token punctuation\">.</span>local_rank<span class=\"token punctuation\">)</span>\n<span class=\"token comment\" spellcheck=\"true\"># model</span>\ntorch<span class=\"token punctuation\">.</span>distributed<span class=\"token punctuation\">.</span>init_process_group<span class=\"token punctuation\">(</span>backend<span class=\"token operator\">=</span><span class=\"token string\">'nccl'</span><span class=\"token punctuation\">,</span> world_size<span class=\"token operator\">=</span>n<span class=\"token punctuation\">,</span> init_method<span class=\"token operator\">=</span><span class=\"token string\">'env://'</span><span class=\"token punctuation\">)</span>\nmodel <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>parallel<span class=\"token punctuation\">.</span>DistributedDataParallel<span class=\"token punctuation\">(</span>\n                                      model<span class=\"token punctuation\">,</span>\n                                        device_ids<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span>args<span class=\"token punctuation\">.</span>local_rank<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n                                        output_device<span class=\"token operator\">=</span>args<span class=\"token punctuation\">.</span>local_rank<span class=\"token punctuation\">)</span></code></pre>\n<p>Note:</p>\n<ol>\n<li>nccl </li>\n<li>nccl</li>\n<li>no_sync DDPForward-Backward</li>\n</ol>\n<pre class=\" language-python\"><code class=\"language-python\">ddp <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>DistributedDataParallel<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span> pg<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">with</span> ddp<span class=\"token punctuation\">.</span>no_sync<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n<span class=\"token keyword\">for</span> input <span class=\"token keyword\">in</span> inputs<span class=\"token punctuation\">:</span>\n    ddp<span class=\"token punctuation\">(</span>input<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\" spellcheck=\"true\"># no synchronization, accumulate grads</span>\nddp<span class=\"token punctuation\">(</span>another_input<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\" spellcheck=\"true\"># synchronize grads</span></code></pre>\n<h3 id=\"apex-parallel-DistributedDataParallel\"><a href=\"#apex-parallel-DistributedDataParallel\" class=\"headerlink\" title=\"apex.parallel.DistributedDataParallel\"></a>apex.parallel.DistributedDataParallel</h3><p>torch.nn.parallel.DistributedDataParallelwrapperNCCL</p>\n<pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token comment\" spellcheck=\"true\">#  n &lt;= GPU  1GPU1</span>\ntorch.distributed.launch --nproc_per_node<span class=\"token operator\">=</span>n distributed_data_parallel.py\n<span class=\"token comment\" spellcheck=\"true\"># :</span>\n<span class=\"token comment\" spellcheck=\"true\"># args.local_rank</span>\n<span class=\"token comment\" spellcheck=\"true\"># os.environ['WORLD_SIZE']</span>\n</code></pre>\n<p></p>\n<ol>\n<li><p>model = DDP(model)  devices_ids output_device</p>\n</li>\n<li><p>init_process_group  init_method=env://</p>\n<pre class=\" language-python\"><code class=\"language-python\">torch<span class=\"token punctuation\">.</span>distributed<span class=\"token punctuation\">.</span>init_process_group<span class=\"token punctuation\">(</span>backend<span class=\"token operator\">=</span><span class=\"token string\">'nccl'</span><span class=\"token punctuation\">,</span>init_method<span class=\"token operator\">=</span><span class=\"token string\">'env://'</span><span class=\"token punctuation\">)</span></code></pre>\n</li>\n</ol>\n<p></p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token comment\" spellcheck=\"true\"># distributed_data_parallel.py</span>\n<span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">import</span> argparse\n<span class=\"token keyword\">import</span> os\n<span class=\"token keyword\">from</span> apex <span class=\"token keyword\">import</span> amp\n<span class=\"token comment\" spellcheck=\"true\"># FOR DISTRIBUTED: (can also use torch.nn.parallel.DistributedDataParallel instead)</span>\n<span class=\"token keyword\">from</span> apex<span class=\"token punctuation\">.</span>parallel <span class=\"token keyword\">import</span> DistributedDataParallel\n\nparser <span class=\"token operator\">=</span> argparse<span class=\"token punctuation\">.</span>ArgumentParser<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\" spellcheck=\"true\"># FOR DISTRIBUTED:  Parse for the local_rank argument, which will be supplied</span>\n<span class=\"token comment\" spellcheck=\"true\"># automatically by torch.distributed.launch.</span>\nparser<span class=\"token punctuation\">.</span>add_argument<span class=\"token punctuation\">(</span><span class=\"token string\">\"--local_rank\"</span><span class=\"token punctuation\">,</span> default<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> type<span class=\"token operator\">=</span>int<span class=\"token punctuation\">)</span>\nargs <span class=\"token operator\">=</span> parser<span class=\"token punctuation\">.</span>parse_args<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># FOR DISTRIBUTED:  If we are running under torch.distributed.launch,</span>\n<span class=\"token comment\" spellcheck=\"true\"># the 'WORLD_SIZE' environment variable will also be set automatically.</span>\nargs<span class=\"token punctuation\">.</span>distributed <span class=\"token operator\">=</span> <span class=\"token boolean\">False</span>\n<span class=\"token keyword\">if</span> <span class=\"token string\">'WORLD_SIZE'</span> <span class=\"token keyword\">in</span> os<span class=\"token punctuation\">.</span>environ<span class=\"token punctuation\">:</span>\n    args<span class=\"token punctuation\">.</span>distributed <span class=\"token operator\">=</span> int<span class=\"token punctuation\">(</span>os<span class=\"token punctuation\">.</span>environ<span class=\"token punctuation\">[</span><span class=\"token string\">'WORLD_SIZE'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">></span> <span class=\"token number\">1</span>\n\n<span class=\"token keyword\">if</span> args<span class=\"token punctuation\">.</span>distributed<span class=\"token punctuation\">:</span>\n    <span class=\"token comment\" spellcheck=\"true\"># FOR DISTRIBUTED:  Set the device according to local_rank.</span>\n    torch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>set_device<span class=\"token punctuation\">(</span>args<span class=\"token punctuation\">.</span>local_rank<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># FOR DISTRIBUTED:  Initialize the backend.  torch.distributed.launch will provide</span>\n    <span class=\"token comment\" spellcheck=\"true\"># environment variables, and requires that you use init_method=`env://`.</span>\n    torch<span class=\"token punctuation\">.</span>distributed<span class=\"token punctuation\">.</span>init_process_group<span class=\"token punctuation\">(</span>backend<span class=\"token operator\">=</span><span class=\"token string\">'nccl'</span><span class=\"token punctuation\">,</span>\n                                         init_method<span class=\"token operator\">=</span><span class=\"token string\">'env://'</span><span class=\"token punctuation\">)</span>\n\ntorch<span class=\"token punctuation\">.</span>backends<span class=\"token punctuation\">.</span>cudnn<span class=\"token punctuation\">.</span>benchmark <span class=\"token operator\">=</span> <span class=\"token boolean\">True</span>\n\nN<span class=\"token punctuation\">,</span> D_in<span class=\"token punctuation\">,</span> D_out <span class=\"token operator\">=</span> <span class=\"token number\">64</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1024</span><span class=\"token punctuation\">,</span> <span class=\"token number\">16</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># Each process receives its own batch of \"fake input data\" and \"fake target data.\"</span>\n<span class=\"token comment\" spellcheck=\"true\"># The \"training loop\" in each process just uses this fake batch over and over.</span>\n<span class=\"token comment\" spellcheck=\"true\"># https://github.com/NVIDIA/apex/tree/master/examples/imagenet provides a more realistic</span>\n<span class=\"token comment\" spellcheck=\"true\"># example of distributed data sampling for both training and validation.</span>\nx <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span>N<span class=\"token punctuation\">,</span> D_in<span class=\"token punctuation\">,</span> device<span class=\"token operator\">=</span><span class=\"token string\">'cuda'</span><span class=\"token punctuation\">)</span>\ny <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span>N<span class=\"token punctuation\">,</span> D_out<span class=\"token punctuation\">,</span> device<span class=\"token operator\">=</span><span class=\"token string\">'cuda'</span><span class=\"token punctuation\">)</span>\n\nmodel <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>D_in<span class=\"token punctuation\">,</span> D_out<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\noptimizer <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>optim<span class=\"token punctuation\">.</span>SGD<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> lr<span class=\"token operator\">=</span><span class=\"token number\">1e</span><span class=\"token operator\">-</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span>\n\nmodel<span class=\"token punctuation\">,</span> optimizer <span class=\"token operator\">=</span> amp<span class=\"token punctuation\">.</span>initialize<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span> optimizer<span class=\"token punctuation\">,</span> opt_level<span class=\"token operator\">=</span><span class=\"token string\">\"O1\"</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">if</span> args<span class=\"token punctuation\">.</span>distributed<span class=\"token punctuation\">:</span>\n    <span class=\"token comment\" spellcheck=\"true\"># FOR DISTRIBUTED:  After amp.initialize, wrap the model with</span>\n    <span class=\"token comment\" spellcheck=\"true\"># apex.parallel.DistributedDataParallel.</span>\n    model <span class=\"token operator\">=</span> DistributedDataParallel<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># torch.nn.parallel.DistributedDataParallel is also fine, with some added args:</span>\n    <span class=\"token comment\" spellcheck=\"true\"># model = torch.nn.parallel.DistributedDataParallel(model,</span>\n    <span class=\"token comment\" spellcheck=\"true\">#                                                   device_ids=[args.local_rank],</span>\n    <span class=\"token comment\" spellcheck=\"true\">#                                                   output_device=args.local_rank)</span>\n\nloss_fn <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>MSELoss<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">for</span> t <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span><span class=\"token number\">500</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    optimizer<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    y_pred <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n    loss <span class=\"token operator\">=</span> loss_fn<span class=\"token punctuation\">(</span>y_pred<span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">with</span> amp<span class=\"token punctuation\">.</span>scale_loss<span class=\"token punctuation\">(</span>loss<span class=\"token punctuation\">,</span> optimizer<span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> scaled_loss<span class=\"token punctuation\">:</span>\n        scaled_loss<span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    optimizer<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">if</span> args<span class=\"token punctuation\">.</span>local_rank <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"final loss = \"</span><span class=\"token punctuation\">,</span> loss<span class=\"token punctuation\">)</span></code></pre>\n<p> <a href=\"https://github.com/NVIDIA/apex/tree/master/examples/imagenet\">mixed precision training with DDP</a></p>\n<h3 id=\"DDP-\"><a href=\"#DDP-\" class=\"headerlink\" title=\"DDP \"></a>DDP </h3><p>torch.save  torch.load </p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> os\n<span class=\"token keyword\">import</span> tempfile\n<span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>distributed <span class=\"token keyword\">as</span> dist\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>nn <span class=\"token keyword\">as</span> nn\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>optim <span class=\"token keyword\">as</span> optim\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>multiprocessing <span class=\"token keyword\">as</span> mp\n\n<span class=\"token keyword\">from</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>parallel <span class=\"token keyword\">import</span> DistributedDataParallel <span class=\"token keyword\">as</span> DDP\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">demo_checkpoint</span><span class=\"token punctuation\">(</span>rank<span class=\"token punctuation\">,</span> world_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    setup<span class=\"token punctuation\">(</span>rank<span class=\"token punctuation\">,</span> world_size<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># setup devices for this process, rank 1 uses GPUs [0, 1, 2, 3] and</span>\n    <span class=\"token comment\" spellcheck=\"true\"># rank 2 uses GPUs [4, 5, 6, 7].</span>\n    n <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>device_count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">//</span> world_size\n    device_ids <span class=\"token operator\">=</span> list<span class=\"token punctuation\">(</span>range<span class=\"token punctuation\">(</span>rank <span class=\"token operator\">*</span> n<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>rank <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    model <span class=\"token operator\">=</span> ToyModel<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device_ids<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># output_device defaults to device_ids[0]</span>\n    ddp_model <span class=\"token operator\">=</span> DDP<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span> device_ids<span class=\"token operator\">=</span>device_ids<span class=\"token punctuation\">)</span>\n\n    loss_fn <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>MSELoss<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    optimizer <span class=\"token operator\">=</span> optim<span class=\"token punctuation\">.</span>SGD<span class=\"token punctuation\">(</span>ddp_model<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> lr<span class=\"token operator\">=</span><span class=\"token number\">0.001</span><span class=\"token punctuation\">)</span>\n\n    CHECKPOINT_PATH <span class=\"token operator\">=</span> tempfile<span class=\"token punctuation\">.</span>gettempdir<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> <span class=\"token string\">\"/model.checkpoint\"</span>\n    <span class=\"token keyword\">if</span> rank <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n        <span class=\"token comment\" spellcheck=\"true\"># All processes should see same parameters as they all start from same</span>\n        <span class=\"token comment\" spellcheck=\"true\"># random parameters and gradients are synchronized in backward passes.</span>\n        <span class=\"token comment\" spellcheck=\"true\"># Therefore, saving it in one process is sufficient.</span>\n        torch<span class=\"token punctuation\">.</span>save<span class=\"token punctuation\">(</span>ddp_model<span class=\"token punctuation\">.</span>state_dict<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> CHECKPOINT_PATH<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># Use a barrier() to make sure that process 1 loads the model after process</span>\n    <span class=\"token comment\" spellcheck=\"true\"># 0 saves it.</span>\n    dist<span class=\"token punctuation\">.</span>barrier<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># configure map_location properly</span>\n    rank0_devices <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>x <span class=\"token operator\">-</span> rank <span class=\"token operator\">*</span> len<span class=\"token punctuation\">(</span>device_ids<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> device_ids<span class=\"token punctuation\">]</span>\n    device_pairs <span class=\"token operator\">=</span> zip<span class=\"token punctuation\">(</span>rank0_devices<span class=\"token punctuation\">,</span> device_ids<span class=\"token punctuation\">)</span>\n    map_location <span class=\"token operator\">=</span> <span class=\"token operator\">&amp;</span><span class=\"token comment\" spellcheck=\"true\">#123;'cuda:%d' % x: 'cuda:%d' % y for x, y in device_pairs&amp;#125;</span>\n    ddp_model<span class=\"token punctuation\">.</span>load_state_dict<span class=\"token punctuation\">(</span>\n        torch<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span>CHECKPOINT_PATH<span class=\"token punctuation\">,</span> map_location<span class=\"token operator\">=</span>map_location<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    optimizer<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    outputs <span class=\"token operator\">=</span> ddp_model<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span><span class=\"token number\">20</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    labels <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span><span class=\"token number\">20</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device_ids<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    loss_fn <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>MSELoss<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    loss_fn<span class=\"token punctuation\">(</span>outputs<span class=\"token punctuation\">,</span> labels<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    optimizer<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># Use a barrier() to make sure that all processes have finished reading the</span>\n    <span class=\"token comment\" spellcheck=\"true\"># checkpoint</span>\n    dist<span class=\"token punctuation\">.</span>barrier<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">if</span> rank <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n        os<span class=\"token punctuation\">.</span>remove<span class=\"token punctuation\">(</span>CHECKPOINT_PATH<span class=\"token punctuation\">)</span>\n\n    cleanup<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre>\n<h3 id=\"DDP-\"><a href=\"#DDP-\" class=\"headerlink\" title=\"DDP \"></a>DDP </h3><p>DDP  GPU   DDP  GPU   GPU </p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">ToyMpModel</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> dev0<span class=\"token punctuation\">,</span> dev1<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        super<span class=\"token punctuation\">(</span>ToyMpModel<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>dev0 <span class=\"token operator\">=</span> dev0\n        self<span class=\"token punctuation\">.</span>dev1 <span class=\"token operator\">=</span> dev1\n        self<span class=\"token punctuation\">.</span>net1 <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>dev0<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>relu <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>ReLU<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>net2 <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>dev1<span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        x <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>dev0<span class=\"token punctuation\">)</span>\n        x <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>relu<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>net1<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        x <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>dev1<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> self<span class=\"token punctuation\">.</span>net2<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span></code></pre>\n<p> GPU  DDP <code>device_ids</code><code>output_device</code> <code>forward()</code></p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">demo_model_parallel</span><span class=\"token punctuation\">(</span>rank<span class=\"token punctuation\">,</span> world_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    setup<span class=\"token punctuation\">(</span>rank<span class=\"token punctuation\">,</span> world_size<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># setup mp_model and devices for this process</span>\n    dev0 <span class=\"token operator\">=</span> rank <span class=\"token operator\">*</span> <span class=\"token number\">2</span>\n    dev1 <span class=\"token operator\">=</span> rank <span class=\"token operator\">*</span> <span class=\"token number\">2</span> <span class=\"token operator\">+</span> <span class=\"token number\">1</span>\n    mp_model <span class=\"token operator\">=</span> ToyMpModel<span class=\"token punctuation\">(</span>dev0<span class=\"token punctuation\">,</span> dev1<span class=\"token punctuation\">)</span>\n    ddp_mp_model <span class=\"token operator\">=</span> DDP<span class=\"token punctuation\">(</span>mp_model<span class=\"token punctuation\">)</span>\n\n    loss_fn <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>MSELoss<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    optimizer <span class=\"token operator\">=</span> optim<span class=\"token punctuation\">.</span>SGD<span class=\"token punctuation\">(</span>ddp_mp_model<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> lr<span class=\"token operator\">=</span><span class=\"token number\">0.001</span><span class=\"token punctuation\">)</span>\n\n    optimizer<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># outputs will be on dev1</span>\n    outputs <span class=\"token operator\">=</span> ddp_mp_model<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span><span class=\"token number\">20</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    labels <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span><span class=\"token number\">20</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>dev1<span class=\"token punctuation\">)</span>\n    loss_fn<span class=\"token punctuation\">(</span>outputs<span class=\"token punctuation\">,</span> labels<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    optimizer<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    cleanup<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">if</span> __name__ <span class=\"token operator\">==</span> <span class=\"token string\">\"__main__\"</span><span class=\"token punctuation\">:</span>\n    run_demo<span class=\"token punctuation\">(</span>demo_basic<span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n    run_demo<span class=\"token punctuation\">(</span>demo_checkpoint<span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">if</span> torch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>device_count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">>=</span> <span class=\"token number\">8</span><span class=\"token punctuation\">:</span>\n        run_demo<span class=\"token punctuation\">(</span>demo_model_parallel<span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token punctuation\">)</span></code></pre>\n<p>setup  torch.multiprocessing.spawn setup</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> os\n<span class=\"token keyword\">import</span> tempfile\n<span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>distributed <span class=\"token keyword\">as</span> dist\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>nn <span class=\"token keyword\">as</span> nn\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>optim <span class=\"token keyword\">as</span> optim\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>multiprocessing <span class=\"token keyword\">as</span> mp\n\n<span class=\"token keyword\">from</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>parallel <span class=\"token keyword\">import</span> DistributedDataParallel <span class=\"token keyword\">as</span> DDP\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">setup</span><span class=\"token punctuation\">(</span>rank<span class=\"token punctuation\">,</span> world_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    os<span class=\"token punctuation\">.</span>environ<span class=\"token punctuation\">[</span><span class=\"token string\">'MASTER_ADDR'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token string\">'localhost'</span>\n    os<span class=\"token punctuation\">.</span>environ<span class=\"token punctuation\">[</span><span class=\"token string\">'MASTER_PORT'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token string\">'12355'</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># initialize the process group</span>\n    dist<span class=\"token punctuation\">.</span>init_process_group<span class=\"token punctuation\">(</span><span class=\"token string\">\"gloo\"</span><span class=\"token punctuation\">,</span> rank<span class=\"token operator\">=</span>rank<span class=\"token punctuation\">,</span> world_size<span class=\"token operator\">=</span>world_size<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># Explicitly setting seed to make sure that models created in two processes</span>\n    <span class=\"token comment\" spellcheck=\"true\"># start from same random weights and biases.</span>\n    torch<span class=\"token punctuation\">.</span>manual_seed<span class=\"token punctuation\">(</span><span class=\"token number\">42</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">cleanup</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    dist<span class=\"token punctuation\">.</span>destroy_process_group<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">ToyModel</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        super<span class=\"token punctuation\">(</span>ToyModel<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>net1 <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>relu <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>ReLU<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>net2 <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> self<span class=\"token punctuation\">.</span>net2<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>relu<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>net1<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">demo_basic</span><span class=\"token punctuation\">(</span>rank<span class=\"token punctuation\">,</span> world_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    setup<span class=\"token punctuation\">(</span>rank<span class=\"token punctuation\">,</span> world_size<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># setup devices for this process, rank 1 uses GPUs [0, 1, 2, 3] and</span>\n    <span class=\"token comment\" spellcheck=\"true\"># rank 2 uses GPUs [4, 5, 6, 7].</span>\n    n <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>device_count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">//</span> world_size\n    device_ids <span class=\"token operator\">=</span> list<span class=\"token punctuation\">(</span>range<span class=\"token punctuation\">(</span>rank <span class=\"token operator\">*</span> n<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>rank <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># create model and move it to device_ids[0]</span>\n    model <span class=\"token operator\">=</span> ToyModel<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device_ids<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># output_device defaults to device_ids[0]</span>\n    ddp_model <span class=\"token operator\">=</span> DDP<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span> device_ids<span class=\"token operator\">=</span>device_ids<span class=\"token punctuation\">)</span>\n\n    loss_fn <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>MSELoss<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    optimizer <span class=\"token operator\">=</span> optim<span class=\"token punctuation\">.</span>SGD<span class=\"token punctuation\">(</span>ddp_model<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> lr<span class=\"token operator\">=</span><span class=\"token number\">0.001</span><span class=\"token punctuation\">)</span>\n\n    optimizer<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    outputs <span class=\"token operator\">=</span> ddp_model<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span><span class=\"token number\">20</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    labels <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span><span class=\"token number\">20</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device_ids<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    loss_fn<span class=\"token punctuation\">(</span>outputs<span class=\"token punctuation\">,</span> labels<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    optimizer<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    cleanup<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">run_demo</span><span class=\"token punctuation\">(</span>demo_fn<span class=\"token punctuation\">,</span> world_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    mp<span class=\"token punctuation\">.</span>spawn<span class=\"token punctuation\">(</span>demo_fn<span class=\"token punctuation\">,</span>\n             args<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>world_size<span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n             nprocs<span class=\"token operator\">=</span>world_size<span class=\"token punctuation\">,</span>\n             join<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span></code></pre>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>PytorchGPUGILDDP model </p>\n<p>References:</p>\n<p><a href=\"https://pytorch.apachecn.org/docs/1.4/75.html\">torch.nn</a></p>\n<p><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html?highlight=distributeddataparallel#torch.nn.parallel.DistributedDataParallel\">DistributedDataParallel API</a></p>\n<p><a href=\"https://pytorch.org/docs/stable/notes/cuda.html#cuda-nn-ddp-instead\">CUDA Semantics</a></p>\n<p><a href=\"https://pytorch.apachecn.org/docs/1.4/34.html\"></a></p>\n<p><a href=\"https://pytorch.apachecn.org/docs/1.4/35.html\">Pytorch</a></p>\n<p><a href=\"https://nvidia.github.io/apex/parallel.html\">apex.parallel</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>CUDAGPUPytorch</p>\n<p>Pytorch 3torch.multiprocessing, nn.DataParallel, nn.parallel.DistributedDataParallelGPU nn.parallle.DistributedDataParallel, <a href=\"https://pytorch.org/docs/stable/notes/cuda.html#cuda-nn-ddp-instead\">CUDA SEMANTIC</a> </p>\n<blockquote>\n<p><strong>Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel</strong></p>\n<p>Most use cases involving batched inputs and multiple GPUs should default to using <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\"><code>DistributedDataParallel</code></a> to utilize more than one GPU.</p>\n<p>There are significant caveats to using CUDA models with <a href=\"https://pytorch.org/docs/stable/multiprocessing.html#module-torch.multiprocessing\"><code>multiprocessing</code></a>; unless care is taken to meet the data handling requirements exactly, it is likely that your program will have incorrect or undefined behavior.</p>\n<p>It is recommended to use <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\"><code>DistributedDataParallel</code></a>, instead of <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel\"><code>DataParallel</code></a> to do multi-GPU training, even if there is only a single node.</p>\n<p>The difference between <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\"><code>DistributedDataParallel</code></a> and <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel\"><code>DataParallel</code></a> is: <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\"><code>DistributedDataParallel</code></a> uses multiprocessing where a process is created for each GPU, while <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel\"><code>DataParallel</code></a> uses multithreading. By using multiprocessing, each GPU has its dedicated process, this avoids the performance overhead caused by GIL of Python interpreter.</p>\n<p>If you use <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\"><code>DistributedDataParallel</code></a>, you could use torch.distributed.launch utility to launch your program, see <a href=\"https://pytorch.org/docs/stable/distributed.html#distributed-launch\">Third-party backends</a>.</p>\n</blockquote>\n<h3 id=\"torch-multiprocessing\"><a href=\"#torch-multiprocessing\" class=\"headerlink\" title=\"torch.multiprocessing\"></a>torch.multiprocessing</h3><p>torch.multiprocessing  Python multiprocessing  python: multiprocessing.Queue Pytorch <a href=\"https://pytorch.org/docs/stable/notes/multiprocessing.html\">MULTIPROCESSING BEST PRACTICES</a>, <a href=\"https://pytorch.apachecn.org/docs/1.4/64.html\"></a></p>\n<pre><code class=\"python\">import torch.multiprocessing as mp\nfrom model import MyModel\n\ndef train(model):\n    # Construct data_loader, optimizer, etc.\n    for data, labels in data_loader:\n        optimizer.zero_grad()\n        loss_fn(model(data), labels).backward()\n        optimizer.step()  # This will update the shared parameters\n\nif __name__ == &#39;__main__&#39;:\n    num_processes = 4\n    model = MyModel()\n    # NOTE: this is required for the ``fork`` method to work\n    model.share_memory()\n    processes = []\n    for rank in range(num_processes):\n        p = mp.Process(target=train, args=(model,))\n        p.start()\n        processes.append(p)\n    for p in processes:\n        p.join()</code></pre>\n<h3 id=\"DataParallel\"><a href=\"#DataParallel\" class=\"headerlink\" title=\"DataParallel\"></a>DataParallel</h3><p>DataParallelmodel copyGPUGPUSIMDGPU</p>\n<h4 id=\"DataParallel\"><a href=\"#DataParallel\" class=\"headerlink\" title=\"DataParallel\"></a>DataParallel</h4><pre><code class=\"python\">model = nn.DataParallel(model)</code></pre>\n<p> outside model    inside model </p>\n<pre><code class=\"python\">class Model(nn.Module):\n    # Our model\n\n    def __init__(self, input_size, output_size):\n        super(Model, self).__init__()\n        self.fc = nn.Linear(input_size, output_size)\n\n    def forward(self, input):\n        output = self.fc(input)\n        print(&quot;\\tIn Model: input size&quot;, input.size(),\n              &quot;output size&quot;, output.size())\n\n        return output\n\n\nmodel = Model(input_size, output_size)\nif torch.cuda.device_count() &gt; 1:\n  print(&quot;Let&#39;s use&quot;, torch.cuda.device_count(), &quot;GPUs!&quot;)\n  # dim = 0 [30, xxx] -&gt; [10, ...], [10, ...], [10, ...] on 3 GPUs\n  model = nn.DataParallel(model)\n\nmodel.to(device)\n\nfor data in rand_loader:\n    input = data.to(device)\n    output = model(input)\n    print(&quot;Outside: input size&quot;, input.size(),\n          &quot;output_size&quot;, output.size())\n\n# 2 GPUs\n# on 2 GPUs\nLet&#39;s use 2 GPUs!\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\nOutside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</code></pre>\n<p>GPUTrickGPU GPU  GPU ( <code>m</code> 10 <code>DataParallel</code> GPU  10  GPU  GPU  5 </p>\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4><pre><code class=\"python\">import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = torch.nn.Linear(10, 10).to(&#39;cuda:0&#39;)\n        self.relu = torch.nn.ReLU()\n        self.net2 = torch.nn.Linear(10, 5).to(&#39;cuda:1&#39;)\n\n    def forward(self, x):\n        x = self.relu(self.net1(x.to(&#39;cuda:0&#39;)))\n        return self.net2(x.to(&#39;cuda:1&#39;))</code></pre>\n<p> GPU  GPU GPU   GPU  <code>layer2</code><code>layer3</code><code>cuda:0</code><code>cuda:1</code></p>\n<p>Trick</p>\n<h4 id=\"Pipeline-\"><a href=\"#Pipeline-\" class=\"headerlink\" title=\"Pipeline \"></a>Pipeline </h4><pre><code class=\"python\">class PipelineParallelResNet50(ModelParallelResNet50):\n    def __init__(self, split_size=20, *args, **kwargs):\n        super(PipelineParallelResNet50, self).__init__(*args, **kwargs)\n        self.split_size = split_size\n\n    def forward(self, x):\n        splits = iter(x.split(self.split_size, dim=0))\n        s_next = next(splits)\n        s_prev = self.seq1(s_next).to(&#39;cuda:1&#39;)\n        ret = []\n\n        for s_next in splits:\n            # A. s_prev runs on cuda:1\n            s_prev = self.seq2(s_prev)\n            ret.append(self.fc(s_prev.view(s_prev.size(0), -1)))\n\n            # B. s_next runs on cuda:0, which can run concurrently with A\n            s_prev = self.seq1(s_next).to(&#39;cuda:1&#39;)\n\n        s_prev = self.seq2(s_prev)\n        ret.append(self.fc(s_prev.view(s_prev.size(0), -1)))\n\n        return torch.cat(ret)\n\nsetup = &quot;model = PipelineParallelResNet50()&quot;\npp_run_times = timeit.repeat(\n    stmt, setup, number=1, repeat=num_repeat, globals=globals())\npp_mean, pp_std = np.mean(pp_run_times), np.std(pp_run_times)\n\nplot([mp_mean, rn_mean, pp_mean],\n     [mp_std, rn_std, pp_std],\n     [&#39;Model Parallel&#39;, &#39;Single GPU&#39;, &#39;Pipelining Model Parallel&#39;],\n     &#39;mp_vs_rn_vs_pp.png&#39;)</code></pre>\n<p>2 GPUs50%100%</p>\n<p>###<img src=\"1.png\" alt=\"1\"> </p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p> 2-GPU 1-GPU 2-GPU Pipelining  </p>\n<h3 id=\"nn-parallel-DistributedDataParallel\"><a href=\"#nn-parallel-DistributedDataParallel\" class=\"headerlink\" title=\"nn.parallel.DistributedDataParallel\"></a>nn.parallel.DistributedDataParallel</h3><p>torch.distributed DataParallel </p>\n<p><code>DistributedDataParallel</code></p>\n<h4 id=\"GPU\"><a href=\"#GPU\" class=\"headerlink\" title=\"GPU\"></a>GPU</h4><pre><code class=\"python\">torch.distributed.init_process_group(backend=&quot;nccl&quot;)\n# device_ids will include all GPU devices by default\nmodel = DistributedDataParallel(model) \n</code></pre>\n<h4 id=\"GPU\"><a href=\"#GPU\" class=\"headerlink\" title=\"GPU\"></a>GPU</h4><p>GPUGILDDP(DistributedDataParallel)GPUtorch.nn.DataParallelPytorch</p>\n<p></p>\n<ol>\n<li>NGPUNtorch.distributed.launch</li>\n</ol>\n<pre><code class=\"bash\">python -m torch.distributed.launch --nproc_per_node=n distributed_data_parallel.py</code></pre>\n<ol start=\"2\">\n<li>GPU model</li>\n</ol>\n<pre><code class=\"python\">parser = argparse.ArgumentParser()\n# FOR DISTRIBUTED:  Parse for the local_rank argument, which will be supplied\n# automatically by torch.distributed.launch.\nparser.add_argument(&quot;--local_rank&quot;, default=0, type=int)\nargs = parser.parse_args()\n\n#  GPU rank id\ntorch.cuda.set_device(args.local_rank)\n# model\ntorch.distributed.init_process_group(backend=&#39;nccl&#39;, world_size=n, init_method=&#39;env://&#39;)\nmodel = torch.nn.parallel.DistributedDataParallel(\n                                      model,\n                                        device_ids=[args.local_rank],\n                                        output_device=args.local_rank)</code></pre>\n<p>Note:</p>\n<ol>\n<li>nccl </li>\n<li>nccl</li>\n<li>no_sync DDPForward-Backward</li>\n</ol>\n<pre><code class=\"python\">ddp = torch.nn.DistributedDataParallel(model, pg)\nwith ddp.no_sync():\nfor input in inputs:\n    ddp(input).backward()  # no synchronization, accumulate grads\nddp(another_input).backward()  # synchronize grads</code></pre>\n<h3 id=\"apex-parallel-DistributedDataParallel\"><a href=\"#apex-parallel-DistributedDataParallel\" class=\"headerlink\" title=\"apex.parallel.DistributedDataParallel\"></a>apex.parallel.DistributedDataParallel</h3><p>torch.nn.parallel.DistributedDataParallelwrapperNCCL</p>\n<pre><code class=\"bash\">#  n &lt;= GPU  1GPU1\ntorch.distributed.launch --nproc_per_node=n distributed_data_parallel.py\n# :\n# args.local_rank\n# os.environ[&#39;WORLD_SIZE&#39;]\n</code></pre>\n<p></p>\n<ol>\n<li><p>model = DDP(model)  devices_ids output_device</p>\n</li>\n<li><p>init_process_group  init_method=env://</p>\n<pre><code class=\"python\">torch.distributed.init_process_group(backend=&#39;nccl&#39;,init_method=&#39;env://&#39;)</code></pre>\n</li>\n</ol>\n<p></p>\n<pre><code class=\"python\"># distributed_data_parallel.py\nimport torch\nimport argparse\nimport os\nfrom apex import amp\n# FOR DISTRIBUTED: (can also use torch.nn.parallel.DistributedDataParallel instead)\nfrom apex.parallel import DistributedDataParallel\n\nparser = argparse.ArgumentParser()\n# FOR DISTRIBUTED:  Parse for the local_rank argument, which will be supplied\n# automatically by torch.distributed.launch.\nparser.add_argument(&quot;--local_rank&quot;, default=0, type=int)\nargs = parser.parse_args()\n\n# FOR DISTRIBUTED:  If we are running under torch.distributed.launch,\n# the &#39;WORLD_SIZE&#39; environment variable will also be set automatically.\nargs.distributed = False\nif &#39;WORLD_SIZE&#39; in os.environ:\n    args.distributed = int(os.environ[&#39;WORLD_SIZE&#39;]) &gt; 1\n\nif args.distributed:\n    # FOR DISTRIBUTED:  Set the device according to local_rank.\n    torch.cuda.set_device(args.local_rank)\n\n    # FOR DISTRIBUTED:  Initialize the backend.  torch.distributed.launch will provide\n    # environment variables, and requires that you use init_method=`env://`.\n    torch.distributed.init_process_group(backend=&#39;nccl&#39;,\n                                         init_method=&#39;env://&#39;)\n\ntorch.backends.cudnn.benchmark = True\n\nN, D_in, D_out = 64, 1024, 16\n\n# Each process receives its own batch of &quot;fake input data&quot; and &quot;fake target data.&quot;\n# The &quot;training loop&quot; in each process just uses this fake batch over and over.\n# https://github.com/NVIDIA/apex/tree/master/examples/imagenet provides a more realistic\n# example of distributed data sampling for both training and validation.\nx = torch.randn(N, D_in, device=&#39;cuda&#39;)\ny = torch.randn(N, D_out, device=&#39;cuda&#39;)\n\nmodel = torch.nn.Linear(D_in, D_out).cuda()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n\nmodel, optimizer = amp.initialize(model, optimizer, opt_level=&quot;O1&quot;)\n\nif args.distributed:\n    # FOR DISTRIBUTED:  After amp.initialize, wrap the model with\n    # apex.parallel.DistributedDataParallel.\n    model = DistributedDataParallel(model)\n    # torch.nn.parallel.DistributedDataParallel is also fine, with some added args:\n    # model = torch.nn.parallel.DistributedDataParallel(model,\n    #                                                   device_ids=[args.local_rank],\n    #                                                   output_device=args.local_rank)\n\nloss_fn = torch.nn.MSELoss()\n\nfor t in range(500):\n    optimizer.zero_grad()\n    y_pred = model(x)\n    loss = loss_fn(y_pred, y)\n    with amp.scale_loss(loss, optimizer) as scaled_loss:\n        scaled_loss.backward()\n    optimizer.step()\n\nif args.local_rank == 0:\n    print(&quot;final loss = &quot;, loss)</code></pre>\n<p> <a href=\"https://github.com/NVIDIA/apex/tree/master/examples/imagenet\">mixed precision training with DDP</a></p>\n<h3 id=\"DDP-\"><a href=\"#DDP-\" class=\"headerlink\" title=\"DDP \"></a>DDP </h3><p>torch.save  torch.load </p>\n<pre><code class=\"python\">import os\nimport tempfile\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\ndef demo_checkpoint(rank, world_size):\n    setup(rank, world_size)\n\n    # setup devices for this process, rank 1 uses GPUs [0, 1, 2, 3] and\n    # rank 2 uses GPUs [4, 5, 6, 7].\n    n = torch.cuda.device_count() // world_size\n    device_ids = list(range(rank * n, (rank + 1) * n))\n\n    model = ToyModel().to(device_ids[0])\n    # output_device defaults to device_ids[0]\n    ddp_model = DDP(model, device_ids=device_ids)\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    CHECKPOINT_PATH = tempfile.gettempdir() + &quot;/model.checkpoint&quot;\n    if rank == 0:\n        # All processes should see same parameters as they all start from same\n        # random parameters and gradients are synchronized in backward passes.\n        # Therefore, saving it in one process is sufficient.\n        torch.save(ddp_model.state_dict(), CHECKPOINT_PATH)\n\n    # Use a barrier() to make sure that process 1 loads the model after process\n    # 0 saves it.\n    dist.barrier()\n    # configure map_location properly\n    rank0_devices = [x - rank * len(device_ids) for x in device_ids]\n    device_pairs = zip(rank0_devices, device_ids)\n    map_location = &#123;&#39;cuda:%d&#39; % x: &#39;cuda:%d&#39; % y for x, y in device_pairs&#125;\n    ddp_model.load_state_dict(\n        torch.load(CHECKPOINT_PATH, map_location=map_location))\n\n    optimizer.zero_grad()\n    outputs = ddp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(device_ids[0])\n    loss_fn = nn.MSELoss()\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    # Use a barrier() to make sure that all processes have finished reading the\n    # checkpoint\n    dist.barrier()\n\n    if rank == 0:\n        os.remove(CHECKPOINT_PATH)\n\n    cleanup()</code></pre>\n<h3 id=\"DDP-\"><a href=\"#DDP-\" class=\"headerlink\" title=\"DDP \"></a>DDP </h3><p>DDP  GPU   DDP  GPU   GPU </p>\n<pre><code class=\"python\">class ToyMpModel(nn.Module):\n    def __init__(self, dev0, dev1):\n        super(ToyMpModel, self).__init__()\n        self.dev0 = dev0\n        self.dev1 = dev1\n        self.net1 = torch.nn.Linear(10, 10).to(dev0)\n        self.relu = torch.nn.ReLU()\n        self.net2 = torch.nn.Linear(10, 5).to(dev1)\n\n    def forward(self, x):\n        x = x.to(self.dev0)\n        x = self.relu(self.net1(x))\n        x = x.to(self.dev1)\n        return self.net2(x)</code></pre>\n<p> GPU  DDP <code>device_ids</code><code>output_device</code> <code>forward()</code></p>\n<pre><code class=\"python\">def demo_model_parallel(rank, world_size):\n    setup(rank, world_size)\n\n    # setup mp_model and devices for this process\n    dev0 = rank * 2\n    dev1 = rank * 2 + 1\n    mp_model = ToyMpModel(dev0, dev1)\n    ddp_mp_model = DDP(mp_model)\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_mp_model.parameters(), lr=0.001)\n\n    optimizer.zero_grad()\n    # outputs will be on dev1\n    outputs = ddp_mp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(dev1)\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    cleanup()\n\nif __name__ == &quot;__main__&quot;:\n    run_demo(demo_basic, 2)\n    run_demo(demo_checkpoint, 2)\n\n    if torch.cuda.device_count() &gt;= 8:\n        run_demo(demo_model_parallel, 4)</code></pre>\n<p>setup  torch.multiprocessing.spawn setup</p>\n<pre><code class=\"python\">import os\nimport tempfile\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\ndef setup(rank, world_size):\n    os.environ[&#39;MASTER_ADDR&#39;] = &#39;localhost&#39;\n    os.environ[&#39;MASTER_PORT&#39;] = &#39;12355&#39;\n\n    # initialize the process group\n    dist.init_process_group(&quot;gloo&quot;, rank=rank, world_size=world_size)\n\n    # Explicitly setting seed to make sure that models created in two processes\n    # start from same random weights and biases.\n    torch.manual_seed(42)\n\ndef cleanup():\n    dist.destroy_process_group()\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(10, 10)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(10, 5)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\ndef demo_basic(rank, world_size):\n    setup(rank, world_size)\n\n    # setup devices for this process, rank 1 uses GPUs [0, 1, 2, 3] and\n    # rank 2 uses GPUs [4, 5, 6, 7].\n    n = torch.cuda.device_count() // world_size\n    device_ids = list(range(rank * n, (rank + 1) * n))\n\n    # create model and move it to device_ids[0]\n    model = ToyModel().to(device_ids[0])\n    # output_device defaults to device_ids[0]\n    ddp_model = DDP(model, device_ids=device_ids)\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    optimizer.zero_grad()\n    outputs = ddp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(device_ids[0])\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    cleanup()\n\ndef run_demo(demo_fn, world_size):\n    mp.spawn(demo_fn,\n             args=(world_size,),\n             nprocs=world_size,\n             join=True)</code></pre>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>PytorchGPUGILDDP model </p>\n<p>References:</p>\n<p><a href=\"https://pytorch.apachecn.org/docs/1.4/75.html\">torch.nn</a></p>\n<p><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html?highlight=distributeddataparallel#torch.nn.parallel.DistributedDataParallel\">DistributedDataParallel API</a></p>\n<p><a href=\"https://pytorch.org/docs/stable/notes/cuda.html#cuda-nn-ddp-instead\">CUDA Semantics</a></p>\n<p><a href=\"https://pytorch.apachecn.org/docs/1.4/34.html\"></a></p>\n<p><a href=\"https://pytorch.apachecn.org/docs/1.4/35.html\">Pytorch</a></p>\n<p><a href=\"https://nvidia.github.io/apex/parallel.html\">apex.parallel</a></p>\n"},{"title":"LeetCode 1-100 ","toc":true,"mathjax":true,"top":false,"cover":true,"date":"2020-12-07T03:06:05.000Z","updated":"2020-12-10T08:01:47.037Z","_content":"\nLeetCode 100 2012LeetCode 100LeetCodePOJACM2\n\nLeetCode2012OJOJLeetCode10EasyJavaLeetCodeJava mediumeasyHardDiscussLeetCode OJDebugDiscussLeetCode 100HardLeetCode\n\n LeetCode ****OJAC****-JavaJavaJavaI/OJavaJavaJarLeetCode OJJavaJavaLeetCode  \n\nLeetCode 100%BlogPython\n\nPythonPython 202Hard 100\n\n AC100100Blog100-200Blog100Blog\n\nLeetCode 100","source":"_posts/LeetCode-1-100-.md","raw":"---\ntitle: LeetCode 1-100 \ntoc: true\nmathjax: true\ntop: false\ncover: true\ndate: 2020-12-07 11:06:05\nupdated:\ncategories: \ntags:\n\t- Algorithms\n\t- LeetCode\n\t- \n---\n\nLeetCode 100 2012LeetCode 100LeetCodePOJACM2\n\nLeetCode2012OJOJLeetCode10EasyJavaLeetCodeJava mediumeasyHardDiscussLeetCode OJDebugDiscussLeetCode 100HardLeetCode\n\n LeetCode ****OJAC****-JavaJavaJavaI/OJavaJavaJarLeetCode OJJavaJavaLeetCode  \n\nLeetCode 100%BlogPython\n\nPythonPython 202Hard 100\n\n AC100100Blog100-200Blog100Blog\n\nLeetCode 100","slug":"LeetCode-1-100-","published":1,"_id":"ckiedp6df0000ue287z2u4kkg","comments":1,"layout":"post","photos":[],"link":"","content":"<p>LeetCode 100 2012LeetCode 100LeetCodePOJACM2</p>\n<p>LeetCode2012OJOJLeetCode10EasyJavaLeetCodeJava mediumeasyHardDiscussLeetCode OJDebugDiscussLeetCode 100HardLeetCode</p>\n<p> LeetCode <strong></strong>OJAC<strong></strong>-JavaJavaJavaI/OJavaJavaJarLeetCode OJJavaJavaLeetCode  </p>\n<p>LeetCode 100%BlogPython</p>\n<p>PythonPython 202Hard 100</p>\n<p> AC100100Blog100-200Blog100Blog</p>\n<p>LeetCode 100</p>\n","site":{"data":{}},"excerpt":"","more":"<p>LeetCode 100 2012LeetCode 100LeetCodePOJACM2</p>\n<p>LeetCode2012OJOJLeetCode10EasyJavaLeetCodeJava mediumeasyHardDiscussLeetCode OJDebugDiscussLeetCode 100HardLeetCode</p>\n<p> LeetCode <strong></strong>OJAC<strong></strong>-JavaJavaJavaI/OJavaJavaJarLeetCode OJJavaJavaLeetCode  </p>\n<p>LeetCode 100%BlogPython</p>\n<p>PythonPython 202Hard 100</p>\n<p> AC100100Blog100-200Blog100Blog</p>\n<p>LeetCode 100</p>\n"},{"title":"LeetCode 115. Distinct Subsequences","toc":true,"mathjax":true,"top":false,"cover":true,"date":"2020-12-09T09:52:16.000Z","updated":"2020-12-09T10:17:52.722Z","_content":"\n> Given two strings `s` and `t`, return *the number of distinct subsequences of `s` which equals `t`*.\n>\n> A string's **subsequence** is a new string formed from the original string by deleting some (can be none) of the characters without disturbing the relative positions of the remaining characters. (i.e., `\"ACE\"` is a subsequence of `\"ABCDE\"` while `\"AEC\"` is not).\n>\n> It's guaranteed the answer fits on a 32-bit signed integer.\n>\n>  \n>\n> **Example 1:**\n>\n> ```\n> Input: s = \"rabbbit\", t = \"rabbit\"\n> Output: 3\n> Explanation:\n> As shown below, there are 3 ways you can generate \"rabbit\" from S.\n> rabbbit\n> rabbbit\n> rabbbit\n> ```\n>\n> **Example 2:**\n>\n> ```\n> Input: s = \"babgbag\", t = \"bag\"\n> Output: 5\n> Explanation:\n> As shown below, there are 5 ways you can generate \"bag\" from S.\n> babgbag\n> babgbag\n> babgbag\n> babgbag\n> babgbag\n> ```\n>\n>  \n>\n> **Constraints:**\n>\n> - `0 <= s.length, t.length <= 1000`\n> - `s` and `t` consist of English letters.\n\n\n\nHard100DFSDFSDFSDFSAC  ACdpACACHard\n\n### DFS\n\n","source":"_posts/LeetCode-115-Distinct-Subsequences.md","raw":"---\ntitle: LeetCode 115. Distinct Subsequences\ntoc: true\nmathjax: true\ntop: false\ncover: true\ndate: 2020-12-09 17:52:16\nupdated:\ncategories:\ntags:\n---\n\n> Given two strings `s` and `t`, return *the number of distinct subsequences of `s` which equals `t`*.\n>\n> A string's **subsequence** is a new string formed from the original string by deleting some (can be none) of the characters without disturbing the relative positions of the remaining characters. (i.e., `\"ACE\"` is a subsequence of `\"ABCDE\"` while `\"AEC\"` is not).\n>\n> It's guaranteed the answer fits on a 32-bit signed integer.\n>\n>  \n>\n> **Example 1:**\n>\n> ```\n> Input: s = \"rabbbit\", t = \"rabbit\"\n> Output: 3\n> Explanation:\n> As shown below, there are 3 ways you can generate \"rabbit\" from S.\n> rabbbit\n> rabbbit\n> rabbbit\n> ```\n>\n> **Example 2:**\n>\n> ```\n> Input: s = \"babgbag\", t = \"bag\"\n> Output: 5\n> Explanation:\n> As shown below, there are 5 ways you can generate \"bag\" from S.\n> babgbag\n> babgbag\n> babgbag\n> babgbag\n> babgbag\n> ```\n>\n>  \n>\n> **Constraints:**\n>\n> - `0 <= s.length, t.length <= 1000`\n> - `s` and `t` consist of English letters.\n\n\n\nHard100DFSDFSDFSDFSAC  ACdpACACHard\n\n### DFS\n\n","slug":"LeetCode-115-Distinct-Subsequences","published":1,"_id":"ckiijvpg20000ce28c4jz8x92","comments":1,"layout":"post","photos":[],"link":"","content":"<blockquote>\n<p>Given two strings <code>s</code> and <code>t</code>, return <em>the number of distinct subsequences of <code>s</code> which equals <code>t</code></em>.</p>\n<p>A strings <strong>subsequence</strong> is a new string formed from the original string by deleting some (can be none) of the characters without disturbing the relative positions of the remaining characters. (i.e., <code>&quot;ACE&quot;</code> is a subsequence of <code>&quot;ABCDE&quot;</code> while <code>&quot;AEC&quot;</code> is not).</p>\n<p>Its guaranteed the answer fits on a 32-bit signed integer.</p>\n<p><strong>Example 1:</strong></p>\n<pre><code>Input: s = &quot;rabbbit&quot;, t = &quot;rabbit&quot;\nOutput: 3\nExplanation:\nAs shown below, there are 3 ways you can generate &quot;rabbit&quot; from S.\nrabbbit\nrabbbit\nrabbbit</code></pre>\n<p><strong>Example 2:</strong></p>\n<pre><code>Input: s = &quot;babgbag&quot;, t = &quot;bag&quot;\nOutput: 5\nExplanation:\nAs shown below, there are 5 ways you can generate &quot;bag&quot; from S.\nbabgbag\nbabgbag\nbabgbag\nbabgbag\nbabgbag</code></pre>\n<p><strong>Constraints:</strong></p>\n<ul>\n<li><code>0 &lt;= s.length, t.length &lt;= 1000</code></li>\n<li><code>s</code> and <code>t</code> consist of English letters.</li>\n</ul>\n</blockquote>\n<p>Hard100DFSDFSDFSDFSAC  ACdpACACHard</p>\n<h3 id=\"DFS\"><a href=\"#DFS\" class=\"headerlink\" title=\"DFS\"></a>DFS</h3><p></p>\n","site":{"data":{}},"excerpt":"","more":"<blockquote>\n<p>Given two strings <code>s</code> and <code>t</code>, return <em>the number of distinct subsequences of <code>s</code> which equals <code>t</code></em>.</p>\n<p>A strings <strong>subsequence</strong> is a new string formed from the original string by deleting some (can be none) of the characters without disturbing the relative positions of the remaining characters. (i.e., <code>&quot;ACE&quot;</code> is a subsequence of <code>&quot;ABCDE&quot;</code> while <code>&quot;AEC&quot;</code> is not).</p>\n<p>Its guaranteed the answer fits on a 32-bit signed integer.</p>\n<p><strong>Example 1:</strong></p>\n<pre><code>Input: s = &quot;rabbbit&quot;, t = &quot;rabbit&quot;\nOutput: 3\nExplanation:\nAs shown below, there are 3 ways you can generate &quot;rabbit&quot; from S.\nrabbbit\nrabbbit\nrabbbit</code></pre>\n<p><strong>Example 2:</strong></p>\n<pre><code>Input: s = &quot;babgbag&quot;, t = &quot;bag&quot;\nOutput: 5\nExplanation:\nAs shown below, there are 5 ways you can generate &quot;bag&quot; from S.\nbabgbag\nbabgbag\nbabgbag\nbabgbag\nbabgbag</code></pre>\n<p><strong>Constraints:</strong></p>\n<ul>\n<li><code>0 &lt;= s.length, t.length &lt;= 1000</code></li>\n<li><code>s</code> and <code>t</code> consist of English letters.</li>\n</ul>\n</blockquote>\n<p>Hard100DFSDFSDFSDFSAC  ACdpACACHard</p>\n<h3 id=\"DFS\"><a href=\"#DFS\" class=\"headerlink\" title=\"DFS\"></a>DFS</h3><p></p>\n"}],"PostAsset":[{"_id":"source/_posts/my-first-blog/Euphonium_Movie_2nd_KV.jpg","slug":"Euphonium_Movie_2nd_KV.jpg","post":"ckg7qjivj00037j28eqt5h8o6","modified":0,"renderable":0},{"_id":"source/_posts/my-first-blog/Euphonium_Movie_Finale_KV2.jpg","slug":"Euphonium_Movie_Finale_KV2.jpg","post":"ckg7qjivj00037j28eqt5h8o6","modified":0,"renderable":0},{"_id":"source/_posts/my-first-blog/relife-1.png","slug":"relife-1.png","post":"ckg7qjivj00037j28eqt5h8o6","modified":0,"renderable":0},{"_id":"source/_posts/my-first-blog/relife-2.jpg","slug":"relife-2.jpg","post":"ckg7qjivj00037j28eqt5h8o6","modified":0,"renderable":0},{"_id":"source/_posts//2020-07-31.jpg","slug":"2020-07-31.jpg","post":"ckgt6w3by0000wx280fap2hbl","modified":0,"renderable":0},{"_id":"source/_posts//IMG_9373.jpeg","slug":"IMG_9373.jpeg","post":"ckgt6w3by0000wx280fap2hbl","modified":0,"renderable":0},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img1.png","slug":"img1.png","post":"ckhlu9lv60000w328appuchpc","modified":0,"renderable":0},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img2.jpg","slug":"img2.jpg","post":"ckhlu9lv60000w328appuchpc","modified":0,"renderable":0},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img3.jpg","slug":"img3.jpg","post":"ckhlu9lv60000w328appuchpc","modified":0,"renderable":0},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img6.jpg","slug":"img6.jpg","post":"ckhlu9lv60000w328appuchpc","modified":0,"renderable":0},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img7.jpg","slug":"img7.jpg","post":"ckhlu9lv60000w328appuchpc","modified":0,"renderable":0},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img8.jpg","slug":"img8.jpg","post":"ckhlu9lv60000w328appuchpc","modified":0,"renderable":0},{"_id":"source/_posts/-/1.jpeg","slug":"1.jpeg","post":"ckhvqydbn00006z28dot614mu","modified":0,"renderable":0},{"_id":"source/_posts/-/2.jpeg","slug":"2.jpeg","post":"ckhvqydbn00006z28dot614mu","modified":0,"renderable":0},{"_id":"source/_posts/GPU-in-Pytorch-/1.png","slug":"1.png","post":"ckhyotdja0000l9289pxq20m8","modified":0,"renderable":0}],"PostCategory":[{"post_id":"ckg7lu78w0001bz2813ba5wh9","category_id":"ckg7piwa500006a2813an1zrb","_id":"ckg7piwa700016a285sew2411"},{"post_id":"ckg7qjivj00037j28eqt5h8o6","category_id":"ckg7qrrd000047j280hei7e4u","_id":"ckg7qrrd200077j2815sh1w8k"},{"post_id":"ckgt6w3by0000wx280fap2hbl","category_id":"ckg7qrrd000047j280hei7e4u","_id":"ckgt6w3c20002wx28byyididl"},{"post_id":"ckh35zvbu0000u7282nma27nl","category_id":"ckh35zvby0001u72804px1qo7","_id":"ckh35zvc10004u7286ara9hbw"},{"post_id":"ckhlu9lv60000w328appuchpc","category_id":"ckhlubjrc0000y9282so58qf8","_id":"ckhludw3200011e288weserfg"},{"post_id":"ckhvqydbn00006z28dot614mu","category_id":"ckhvqydbp00016z281mnxdhf3","_id":"ckhvqydbs00046z28hojpcuoy"},{"post_id":"ckhyotdja0000l9289pxq20m8","category_id":"ckhyotdje0002l928gyqa3gxs","_id":"ckhyotdji0006l928chmj50lm"},{"post_id":"ckiedp6df0000ue287z2u4kkg","category_id":"ckg7qrrd000047j280hei7e4u","_id":"ckiedp6dh0003ue28bxwgcd1k"}],"PostTag":[{"post_id":"ckg7lu78w0001bz2813ba5wh9","tag_id":"ckg7piagr00003v286tc5gsox","_id":"ckg7piagv00013v284nc01o56"},{"post_id":"ckg7qjivj00037j28eqt5h8o6","tag_id":"ckg7qrrd100057j289azi5q0k","_id":"ckg7qrrd100067j285lt81jqx"},{"post_id":"ckgt6w3by0000wx280fap2hbl","tag_id":"ckg7qrrd100057j289azi5q0k","_id":"ckgt6w3c10001wx28eo595uf9"},{"post_id":"ckh35zvbu0000u7282nma27nl","tag_id":"ckh35zvc00002u728a8q656mh","_id":"ckh35zvc20007u728fve34h0a"},{"post_id":"ckh35zvbu0000u7282nma27nl","tag_id":"ckh35zvc10003u7282616cu2s","_id":"ckh35zvc20008u7288k0q907w"},{"post_id":"ckh35zvbu0000u7282nma27nl","tag_id":"ckh35zvc10005u7281p3i2ffq","_id":"ckh35zvc20009u728720xdpaf"},{"post_id":"ckh35zvbu0000u7282nma27nl","tag_id":"ckh35zvc20006u7284k31d8oe","_id":"ckh35zvc2000au728gaocav7r"},{"post_id":"ckhlu9lv60000w328appuchpc","tag_id":"ckhlubjrc0001y928d4gv9u9l","_id":"ckhlubjrd0002y928dmdedxmp"},{"post_id":"ckhlu9lv60000w328appuchpc","tag_id":"ckhlue30200021e28486k1p79","_id":"ckhlue30300031e285vhycy46"},{"post_id":"ckhvqydbn00006z28dot614mu","tag_id":"ckhvqydbr00026z288h0ue571","_id":"ckhvqydbs00056z28fvjve3ff"},{"post_id":"ckhvqydbn00006z28dot614mu","tag_id":"ckhvqydbr00036z28g66r7hsq","_id":"ckhvqydbs00066z287q3d8g0m"},{"post_id":"ckhyotdja0000l9289pxq20m8","tag_id":"ckhyotdjg0003l928djuaby6g","_id":"ckhyotdjj000al928fghx5blk"},{"post_id":"ckhyotdja0000l9289pxq20m8","tag_id":"ckhyotdjh0005l928hv1rcwfl","_id":"ckhyotdjj000bl9280y7t97t2"},{"post_id":"ckhyotdja0000l9289pxq20m8","tag_id":"ckhyotdjj0007l928cxr4e0vg","_id":"ckhyotdjk000dl9284vfag7vy"},{"post_id":"ckiedp6df0000ue287z2u4kkg","tag_id":"ckhvqydbr00026z288h0ue571","_id":"ckiedp6dh0001ue28glzp1ror"},{"post_id":"ckiedp6df0000ue287z2u4kkg","tag_id":"ckhlue30200021e28486k1p79","_id":"ckiedp6dh0002ue28cbft5tkf"},{"post_id":"ckiedp6df0000ue287z2u4kkg","tag_id":"ckg7qrrd100057j289azi5q0k","_id":"ckiedp6dh0004ue2829v986c9"}],"Tag":[{"name":"test","_id":"ckg7piagr00003v286tc5gsox"},{"name":"","_id":"ckg7qrrd100057j289azi5q0k"},{"name":"ubuntu","_id":"ckh35zvc00002u728a8q656mh"},{"name":"","_id":"ckh35zvc10003u7282616cu2s"},{"name":"linux","_id":"ckh35zvc10005u7281p3i2ffq"},{"name":"grub","_id":"ckh35zvc20006u7284k31d8oe"},{"name":"-Algorithm","_id":"ckhlu9lvk0002w328hz4x9glf"},{"name":"Algorithm","_id":"ckhlubjrc0001y928d4gv9u9l"},{"name":"LeetCode","_id":"ckhlue30200021e28486k1p79"},{"name":"Algorithms","_id":"ckhvqydbr00026z288h0ue571"},{"name":"OS","_id":"ckhvqydbr00036z28g66r7hsq"},{"name":"Pytorch","_id":"ckhyotdjg0003l928djuaby6g"},{"name":"CUDA","_id":"ckhyotdjh0005l928hv1rcwfl"},{"name":"GPU","_id":"ckhyotdjj0007l928cxr4e0vg"},{"name":"NLP","_id":"ckhyotdjj000cl928cahc99hr"}]}}