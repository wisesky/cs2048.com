{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"themes/hexo-theme-matery/source/favicon.png","path":"favicon.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/css/gitment.css","path":"css/gitment.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/css/my-gitalk.css","path":"css/my-gitalk.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/css/matery.css","path":"css/matery.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/css/my.css","path":"css/my.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/js/matery.js","path":"js/matery.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/js/search.js","path":"js/search.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/avatar.jpg","path":"medias/avatar.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/comment_bg.png","path":"medias/comment_bg.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/cover.jpg","path":"medias/cover.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/icp.png","path":"medias/icp.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/logo.png","path":"medias/logo.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/animate/animate.min.css","path":"libs/animate/animate.min.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/aos/aos.css","path":"libs/aos/aos.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/aos/aos.js","path":"libs/aos/aos.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/aplayer/APlayer.min.css","path":"libs/aplayer/APlayer.min.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/aplayer/APlayer.min.js","path":"libs/aplayer/APlayer.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/codeBlock/codeBlockFuction.js","path":"libs/codeBlock/codeBlockFuction.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/codeBlock/codeCopy.js","path":"libs/codeBlock/codeCopy.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/codeBlock/codeLang.js","path":"libs/codeBlock/codeLang.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/codeBlock/codeShrink.js","path":"libs/codeBlock/codeShrink.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/background/canvas-nest.js","path":"libs/background/canvas-nest.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/background/ribbon-dynamic.js","path":"libs/background/ribbon-dynamic.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/background/ribbon-refresh.min.js","path":"libs/background/ribbon-refresh.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/background/ribbon.min.js","path":"libs/background/ribbon.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/cryptojs/crypto-js.min.js","path":"libs/cryptojs/crypto-js.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/dplayer/DPlayer.min.css","path":"libs/dplayer/DPlayer.min.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/dplayer/DPlayer.min.js","path":"libs/dplayer/DPlayer.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/echarts/echarts.min.js","path":"libs/echarts/echarts.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/jqcloud/jqcloud-1.0.4.min.js","path":"libs/jqcloud/jqcloud-1.0.4.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/jqcloud/jqcloud.css","path":"libs/jqcloud/jqcloud.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/gitalk/gitalk.css","path":"libs/gitalk/gitalk.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/gitalk/gitalk.min.js","path":"libs/gitalk/gitalk.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/gitment/gitment-default.css","path":"libs/gitment/gitment-default.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/gitment/gitment.js","path":"libs/gitment/gitment.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/instantpage/instantpage.js","path":"libs/instantpage/instantpage.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/jquery/jquery.min.js","path":"libs/jquery/jquery.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/masonry/masonry.pkgd.min.js","path":"libs/masonry/masonry.pkgd.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/materialize/materialize.min.css","path":"libs/materialize/materialize.min.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/materialize/materialize.min.js","path":"libs/materialize/materialize.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/others/busuanzi.pure.mini.js","path":"libs/others/busuanzi.pure.mini.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/others/clicklove.js","path":"libs/others/clicklove.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/scrollprogress/scrollProgress.min.js","path":"libs/scrollprogress/scrollProgress.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/tocbot/tocbot.css","path":"libs/tocbot/tocbot.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/tocbot/tocbot.min.js","path":"libs/tocbot/tocbot.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/valine/Valine.min.js","path":"libs/valine/Valine.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/valine/av-min.js","path":"libs/valine/av-min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/banner/0.jpg","path":"medias/banner/0.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/banner/1.jpg","path":"medias/banner/1.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/banner/2.jpg","path":"medias/banner/2.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/banner/3.jpg","path":"medias/banner/3.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/banner/4.jpg","path":"medias/banner/4.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/banner/5.jpg","path":"medias/banner/5.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/banner/6.jpg","path":"medias/banner/6.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/0.jpg","path":"medias/featureimages/0.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/1.jpg","path":"medias/featureimages/1.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/10.jpg","path":"medias/featureimages/10.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/11.jpg","path":"medias/featureimages/11.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/12.jpg","path":"medias/featureimages/12.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/13.jpg","path":"medias/featureimages/13.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/14.jpg","path":"medias/featureimages/14.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/15.jpg","path":"medias/featureimages/15.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/16.jpg","path":"medias/featureimages/16.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/17.jpg","path":"medias/featureimages/17.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/18.jpg","path":"medias/featureimages/18.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/19.jpg","path":"medias/featureimages/19.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/2.jpg","path":"medias/featureimages/2.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/20.jpg","path":"medias/featureimages/20.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/21.jpg","path":"medias/featureimages/21.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/22.jpg","path":"medias/featureimages/22.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/23.jpg","path":"medias/featureimages/23.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/3.jpg","path":"medias/featureimages/3.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/4.jpg","path":"medias/featureimages/4.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/5.jpg","path":"medias/featureimages/5.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/6.jpg","path":"medias/featureimages/6.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/7.jpg","path":"medias/featureimages/7.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/8.jpg","path":"medias/featureimages/8.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/9.jpg","path":"medias/featureimages/9.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/reward/wechat.png","path":"medias/reward/wechat.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/css/all.css","path":"libs/awesome/css/all.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-brands-400.eot","path":"libs/awesome/webfonts/fa-brands-400.eot","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-brands-400.svg","path":"libs/awesome/webfonts/fa-brands-400.svg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-brands-400.ttf","path":"libs/awesome/webfonts/fa-brands-400.ttf","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-brands-400.woff","path":"libs/awesome/webfonts/fa-brands-400.woff","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-brands-400.woff2","path":"libs/awesome/webfonts/fa-brands-400.woff2","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-regular-400.svg","path":"libs/awesome/webfonts/fa-regular-400.svg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-regular-400.eot","path":"libs/awesome/webfonts/fa-regular-400.eot","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-regular-400.woff","path":"libs/awesome/webfonts/fa-regular-400.woff","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-regular-400.ttf","path":"libs/awesome/webfonts/fa-regular-400.ttf","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-regular-400.woff2","path":"libs/awesome/webfonts/fa-regular-400.woff2","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-solid-900.svg","path":"libs/awesome/webfonts/fa-solid-900.svg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-solid-900.ttf","path":"libs/awesome/webfonts/fa-solid-900.ttf","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-solid-900.eot","path":"libs/awesome/webfonts/fa-solid-900.eot","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-solid-900.woff","path":"libs/awesome/webfonts/fa-solid-900.woff","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-solid-900.woff2","path":"libs/awesome/webfonts/fa-solid-900.woff2","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/css/lightgallery.min.css","path":"libs/lightGallery/css/lightgallery.min.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/fonts/lg.eot","path":"libs/lightGallery/fonts/lg.eot","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/fonts/lg.svg","path":"libs/lightGallery/fonts/lg.svg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/fonts/lg.ttf","path":"libs/lightGallery/fonts/lg.ttf","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/fonts/lg.woff","path":"libs/lightGallery/fonts/lg.woff","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/img/loading.gif","path":"libs/lightGallery/img/loading.gif","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/img/video-play.png","path":"libs/lightGallery/img/video-play.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/img/vimeo-play.png","path":"libs/lightGallery/img/vimeo-play.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/img/youtube-play.png","path":"libs/lightGallery/img/youtube-play.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/js/lightgallery-all.min.js","path":"libs/lightGallery/js/lightgallery-all.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/share/css/share.min.css","path":"libs/share/css/share.min.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/share/fonts/iconfont.svg","path":"libs/share/fonts/iconfont.svg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/share/fonts/iconfont.eot","path":"libs/share/fonts/iconfont.eot","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/share/fonts/iconfont.woff","path":"libs/share/fonts/iconfont.woff","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/share/fonts/iconfont.ttf","path":"libs/share/fonts/iconfont.ttf","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/share/js/social-share.min.js","path":"libs/share/js/social-share.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/libs/share/js/jquery.share.min.js","path":"libs/share/js/jquery.share.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/wisesky.jpg","path":"medias/wisesky.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/reward/alipay.png","path":"medias/reward/alipay.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/ACG/1.jpg","path":"medias/ACG/1.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/ACG/2.jpg","path":"medias/ACG/2.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/ACG/3.jpg","path":"medias/ACG/3.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/ACG/4.jpg","path":"medias/ACG/4.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/ACG/5.jpg","path":"medias/ACG/5.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/ACG/6.jpg","path":"medias/ACG/6.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/ACG/7.jpg","path":"medias/ACG/7.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/ACG/8.jpg","path":"medias/ACG/8.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-matery/source/medias/ACG/9.jpg","path":"medias/ACG/9.jpg","modified":0,"renderable":1}],"Cache":[{"_id":"source/_posts/hello-world.md","hash":"8b7386961fcf8a46dc6cc34fa9d7422c423b08aa","modified":1602577934454},{"_id":"source/tags/index.md","hash":"6c223043ba32b7c76c33eafceeff9ed879520bfb","modified":1602576798016},{"_id":"source/about/index.md","hash":"48622c880dbd4a56f3346b15c7dc5c36d5e4239b","modified":1602570937056},{"_id":"source/categories/index.md","hash":"03ecf6ae4723c42f6eef84663eb50841513e4b62","modified":1602576755198},{"_id":"themes/hexo-theme-matery/.gitignore","hash":"727607929a51db7ea10968f547c26041eee9cfff","modified":1602570653689},{"_id":"themes/hexo-theme-matery/LICENSE","hash":"7df059597099bb7dcf25d2a9aedfaf4465f72d8d","modified":1602570653689},{"_id":"themes/hexo-theme-matery/README.md","hash":"56299cf1fe60a11fef61b3948fe148f995df747e","modified":1602570653690},{"_id":"themes/hexo-theme-matery/layout/404.ejs","hash":"9c8ca67377211e5d60fdde272a975faa9a91a22a","modified":1602570653691},{"_id":"themes/hexo-theme-matery/README_CN.md","hash":"0fdf818476a444663cc8ffa2f194199d9fd93508","modified":1602570653690},{"_id":"themes/hexo-theme-matery/_config.yml","hash":"9440d60860d7823cb6193934528caedb8ace2939","modified":1606210030449},{"_id":"themes/hexo-theme-matery/layout/archive.ejs","hash":"cdac701de8370f9f3794a0eed4165983993a1ca7","modified":1602570653697},{"_id":"themes/hexo-theme-matery/layout/about.ejs","hash":"41849f9300b8dc47048333fcf4a897dd8a2a13ca","modified":1602570653697},{"_id":"themes/hexo-theme-matery/layout/contact.ejs","hash":"72fb5af3fc2f8955e2eb10926bbe4532a04ccd1b","modified":1602570653698},{"_id":"themes/hexo-theme-matery/layout/category.ejs","hash":"00019bca11fb46477f22017cb1f5ad8444da0580","modified":1602570653698},{"_id":"themes/hexo-theme-matery/layout/categories.ejs","hash":"8e54665cc25d7c333da7d9f312987190be6215da","modified":1602570653697},{"_id":"themes/hexo-theme-matery/layout/friends.ejs","hash":"f5d6459bed0f4ecb214f2dbff5b2207a80c44f66","modified":1602570653698},{"_id":"themes/hexo-theme-matery/layout/index.ejs","hash":"4dc6f08e7709cc04e886be72dbf0d06469f0effc","modified":1602570653698},{"_id":"themes/hexo-theme-matery/layout/tag.ejs","hash":"85a4b05bd8a6ad0f17ff2e97dae56949b379c204","modified":1602570653699},{"_id":"themes/hexo-theme-matery/layout/post.ejs","hash":"90b5a4c1f70e4756db569c15a7c6cad0c77c4500","modified":1602570653699},{"_id":"themes/hexo-theme-matery/layout/tags.ejs","hash":"cf9517aa6a0111355121f44615d6923e312283c7","modified":1602570653699},{"_id":"themes/hexo-theme-matery/languages/default.yml","hash":"54ccc01b097c5bf6820f0edfcece1a87b78ab32d","modified":1602570653690},{"_id":"themes/hexo-theme-matery/layout/layout.ejs","hash":"746abd6bec5ed42bfeb54575fa613d38fb19fe96","modified":1602570653698},{"_id":"themes/hexo-theme-matery/languages/zh-CN.yml","hash":"ec0c18fb0e3ab3ee44268dc2b44fc832cffe3c1b","modified":1602580129662},{"_id":"themes/hexo-theme-matery/languages/zh-HK.yml","hash":"ae34ac0e175c3037675722e436637efbceea32f0","modified":1602570653691},{"_id":"themes/hexo-theme-matery/layout/_partial/back-top.ejs","hash":"47ee36a042bb6d52bbe1d0f329637e8ffcf1d0aa","modified":1602570653691},{"_id":"themes/hexo-theme-matery/source/favicon.png","hash":"774fee8c6d0be9dbb010b20f36c06848d06e3da0","modified":1602570653700},{"_id":"themes/hexo-theme-matery/layout/_partial/background.ejs","hash":"aef6edeeb11209831a11d8c7f5d59992e2573335","modified":1602570653691},{"_id":"themes/hexo-theme-matery/layout/_partial/baidu-push.ejs","hash":"2cebcc5ea3614d7f76ec36670e68050cbe611202","modified":1602570653691},{"_id":"themes/hexo-theme-matery/layout/_partial/bg-cover.ejs","hash":"02191109712f61c0e487b8f0b8466597181a9004","modified":1602570653692},{"_id":"themes/hexo-theme-matery/layout/_partial/bg-cover-content.ejs","hash":"28617bf2a35a4269eba6df466acd174e416d2d1e","modified":1602570653691},{"_id":"themes/hexo-theme-matery/layout/_partial/baidu-analytics.ejs","hash":"3bbcdb474ca1dcad514bdc4b7763e17c55df04fd","modified":1602570653691},{"_id":"themes/hexo-theme-matery/layout/_partial/disqus.ejs","hash":"b2dc2c8b5ed56815e55cc2ea54a6dc4eeba2375d","modified":1602570653692},{"_id":"themes/hexo-theme-matery/layout/_partial/gitalk.ejs","hash":"2aa8fbb04b046fa7679092a48372d7e036835dff","modified":1602570653692},{"_id":"themes/hexo-theme-matery/layout/_partial/github-link.ejs","hash":"3aeb581bd78ab8e15b858e4c44c03bcf92f20b9e","modified":1602570653692},{"_id":"themes/hexo-theme-matery/layout/_partial/gitment.ejs","hash":"90f6218512ef2eab63ada7ad2fc766ae635a2297","modified":1602570653692},{"_id":"themes/hexo-theme-matery/layout/_partial/footer.ejs","hash":"4b5476478ba12183b7c97a33d5545fc53be362a8","modified":1602570653692},{"_id":"themes/hexo-theme-matery/layout/_partial/google-analytics.ejs","hash":"5f4992205617da5f8cc5863c62b5ec46e414e2fb","modified":1602570653692},{"_id":"themes/hexo-theme-matery/layout/_partial/head.ejs","hash":"8d263ebccccd0f9e69539f402955296de6f24a62","modified":1602570653692},{"_id":"themes/hexo-theme-matery/layout/_partial/header.ejs","hash":"59e38c70f3d8e7165e686e5e84a627835f4321b0","modified":1602570653693},{"_id":"themes/hexo-theme-matery/layout/_partial/index-cover.ejs","hash":"76b4a37e0364380b143fdf94bf1a5e6941564414","modified":1602570653693},{"_id":"themes/hexo-theme-matery/layout/_partial/livere.ejs","hash":"9c3401b42ea7f26410a5593bae93ada7e57b43be","modified":1602570653693},{"_id":"themes/hexo-theme-matery/layout/_partial/minivaline.ejs","hash":"5f09386aece8f9cf31f6059bbde79cd6c5171493","modified":1602570653693},{"_id":"themes/hexo-theme-matery/layout/_partial/navigation.ejs","hash":"78b70ff24b3039c871331ebec114b936c1756cc8","modified":1602570653693},{"_id":"themes/hexo-theme-matery/layout/_partial/paging.ejs","hash":"e2df12cf92a82b1a7a7add2eac1db1d954bc5511","modified":1602570653693},{"_id":"themes/hexo-theme-matery/layout/_partial/post-cover.ejs","hash":"d1c873c5de54498c722e155aadb8c0ec39485dfa","modified":1602570653693},{"_id":"themes/hexo-theme-matery/layout/_partial/post-detail-toc.ejs","hash":"a8c9abd8cf806235cadb087a5acca3f9182b76ea","modified":1602570653694},{"_id":"themes/hexo-theme-matery/layout/_partial/post-statis.ejs","hash":"04889f9031743c6b081d02fa4027b0dbfcc45ecf","modified":1602570653694},{"_id":"themes/hexo-theme-matery/layout/_partial/mobile-nav.ejs","hash":"cb0cb452be1cd1857ba600f04025b506f3b6fc79","modified":1602570653693},{"_id":"themes/hexo-theme-matery/layout/_partial/prev-next.ejs","hash":"c76b78782ea82340104fccc089417572e0adece4","modified":1602570653694},{"_id":"themes/hexo-theme-matery/layout/_partial/reprint-statement.ejs","hash":"0ce3f9361f558b99cc2f059c5e50b0e2a152ae38","modified":1602570653694},{"_id":"themes/hexo-theme-matery/layout/_partial/post-detail.ejs","hash":"d05926e79aa6dfc235193b9d8c6aa03118b0eade","modified":1602570653694},{"_id":"themes/hexo-theme-matery/layout/_partial/reward.ejs","hash":"ffc55bc7e73bc698bfc58d8e3780c336b83282cf","modified":1602570653694},{"_id":"themes/hexo-theme-matery/layout/_partial/share.ejs","hash":"c941730a2471d6aab367cbb6e09ed08b56c83143","modified":1602570653695},{"_id":"themes/hexo-theme-matery/layout/_partial/search.ejs","hash":"b09872f69c962cb6dd9d4050a322fdea94903f84","modified":1602570653694},{"_id":"themes/hexo-theme-matery/layout/_partial/social-link.ejs","hash":"6f871bd3a70f720e4e451f1f4f625cbc6d8994a4","modified":1602570653695},{"_id":"themes/hexo-theme-matery/layout/_widget/artitalk.ejs","hash":"b14e486f12b9ac42a273b80e4d785fcb94cf04b2","modified":1602570653695},{"_id":"themes/hexo-theme-matery/layout/_partial/valine.ejs","hash":"0e4c0a6154aa34007849928ca88f05b6185b256e","modified":1602570653695},{"_id":"themes/hexo-theme-matery/layout/_widget/category-radar.ejs","hash":"1d8747fda89a0b2ca3c7008867cbfeecad0578a6","modified":1602570653696},{"_id":"themes/hexo-theme-matery/layout/_widget/music.ejs","hash":"e9e3e327d5de9d7aeadbde32e1d558652d9e9195","modified":1602570653696},{"_id":"themes/hexo-theme-matery/layout/_widget/category-cloud.ejs","hash":"1b3df1009234c0112424b497b18b4ad8240b3bc7","modified":1602570653696},{"_id":"themes/hexo-theme-matery/layout/_widget/my-gallery.ejs","hash":"65a2d2f9722f84c7fd98f6bdf79087a14848ebd8","modified":1602570653696},{"_id":"themes/hexo-theme-matery/layout/_widget/dream.ejs","hash":"9a472ad5591100cdb65d0df9d01034163bd6dd9d","modified":1602570653696},{"_id":"themes/hexo-theme-matery/layout/_widget/my-projects.ejs","hash":"ef60b64021fa349b0048425d858dfcf6c906fede","modified":1602570653696},{"_id":"themes/hexo-theme-matery/layout/_widget/my-skills.ejs","hash":"89a0092df72d23093128f2fbbdc8ca7f83ebcfd9","modified":1602570653696},{"_id":"themes/hexo-theme-matery/layout/_widget/post-calendar.ejs","hash":"48821e644bc73553d7c5c56d2e8ee111a70cd776","modified":1602570653696},{"_id":"themes/hexo-theme-matery/layout/_widget/post-charts.ejs","hash":"ab5f986f428215941aeaa0c88aefd440c47d3bcf","modified":1602570653697},{"_id":"themes/hexo-theme-matery/layout/_widget/recommend.ejs","hash":"8551137e94ca4e2e3b8b63d5626255884cb60cb5","modified":1602570653697},{"_id":"themes/hexo-theme-matery/layout/_widget/tag-cloud.ejs","hash":"fc42b72cddc231f7485cdc1fd6852b66be6add26","modified":1602570653697},{"_id":"themes/hexo-theme-matery/layout/_widget/tag-wordcloud.ejs","hash":"487aacb2454d6bf0d21cdb07ddd1fd5ddbca9038","modified":1602570653697},{"_id":"themes/hexo-theme-matery/layout/_widget/video.ejs","hash":"a0e002377af2a7f7e4da6d9a644de97adb035925","modified":1602570653697},{"_id":"themes/hexo-theme-matery/source/css/gitment.css","hash":"2bd15cc17dca35ac3ecc0acf167a23a1dd362acd","modified":1602570653699},{"_id":"themes/hexo-theme-matery/source/css/my-gitalk.css","hash":"eeda46a83d0db1cc239a9cd27d544faf663f9883","modified":1602570653700},{"_id":"themes/hexo-theme-matery/source/css/matery.css","hash":"87bd1dacf48c9daab7ea43466368247f1e4107d1","modified":1602570653700},{"_id":"themes/hexo-theme-matery/source/css/my.css","hash":"10577fbc30f241b126d1b51b1f56136ecba86b19","modified":1602570653700},{"_id":"themes/hexo-theme-matery/source/js/matery.js","hash":"b86de5fe3e9766b7ff80df12ea41c3a9e30825f7","modified":1602570653700},{"_id":"themes/hexo-theme-matery/source/js/search.js","hash":"d559d402b4d4a0931821fe6e22a8831fc43a953d","modified":1602570653700},{"_id":"themes/hexo-theme-matery/source/medias/avatar.jpg","hash":"2a6287308628881ce27b9a7de53ba15c2be00d02","modified":1602570653748},{"_id":"themes/hexo-theme-matery/source/medias/icp.png","hash":"27a96f31f7d0413c6ade6f40e06f021f501151c7","modified":1602570653776},{"_id":"themes/hexo-theme-matery/source/medias/comment_bg.png","hash":"dfc93d24081884fbc58cab0f8fd19e77d31d6123","modified":1602570653758},{"_id":"themes/hexo-theme-matery/source/medias/logo.png","hash":"d9095f5ea8719374d9d1ff020279426f5b2a1396","modified":1602570653776},{"_id":"themes/hexo-theme-matery/source/libs/aos/aos.css","hash":"191a3705a8f63e589a50a0ff2f2c5559f1a1b6b2","modified":1602570653701},{"_id":"themes/hexo-theme-matery/source/libs/animate/animate.min.css","hash":"97afa151569f046b2e01f27c1871646e9cd87caf","modified":1602570653701},{"_id":"themes/hexo-theme-matery/source/libs/aos/aos.js","hash":"02bfb40b0c4b6e9b0b4081218357145cbb327d74","modified":1602570653701},{"_id":"themes/hexo-theme-matery/source/libs/aplayer/APlayer.min.css","hash":"07372a2ba507388d0fed166d761b1c2c2a659dce","modified":1602570653702},{"_id":"themes/hexo-theme-matery/source/libs/aplayer/APlayer.min.js","hash":"22caa28ff6b41a16ff40f15d38f1739e22359478","modified":1602570653702},{"_id":"themes/hexo-theme-matery/source/libs/codeBlock/codeBlockFuction.js","hash":"c7ab06d27a525b15b1eb69027135269e9b9132fb","modified":1602570653727},{"_id":"themes/hexo-theme-matery/source/libs/codeBlock/codeCopy.js","hash":"6d39a766af62e625f177c4d5cf3adc35eed71e61","modified":1602570653727},{"_id":"themes/hexo-theme-matery/source/libs/codeBlock/codeLang.js","hash":"bac88b4d4e3679732d29bd037c34f089cf27cf05","modified":1602570653727},{"_id":"themes/hexo-theme-matery/source/libs/codeBlock/codeShrink.js","hash":"201e8cd761b4be557247bdaf1ebc7c11c83194f6","modified":1602570653727},{"_id":"themes/hexo-theme-matery/source/libs/background/canvas-nest.js","hash":"65333d0dbb9c1173a1b13031b230161fc42c8b2f","modified":1602570653726},{"_id":"themes/hexo-theme-matery/source/libs/background/ribbon-dynamic.js","hash":"052b80c29e6bc585aa28d4504b743bdbac220a88","modified":1602570653726},{"_id":"themes/hexo-theme-matery/source/libs/background/ribbon-refresh.min.js","hash":"6d98692b2cad8c746a562db18b170b35c24402f4","modified":1602570653726},{"_id":"themes/hexo-theme-matery/source/libs/background/ribbon.min.js","hash":"6a99d494c030388f96f6086a7aaa0f03f3fe532e","modified":1602570653727},{"_id":"themes/hexo-theme-matery/source/libs/cryptojs/crypto-js.min.js","hash":"5989527a378b55011a59522f41eeb3981518325c","modified":1602570653728},{"_id":"themes/hexo-theme-matery/source/libs/jqcloud/jqcloud-1.0.4.min.js","hash":"257eaae3020599e4939f50d5008a743827f25b8c","modified":1602570653738},{"_id":"themes/hexo-theme-matery/source/libs/dplayer/DPlayer.min.css","hash":"f7d19655f873b813ffba5d1a17145c91f82631b8","modified":1602570653728},{"_id":"themes/hexo-theme-matery/source/libs/jqcloud/jqcloud.css","hash":"20d9f11a19d95c70e27cb922e0d6dccbec4eae89","modified":1602570653738},{"_id":"themes/hexo-theme-matery/source/libs/gitment/gitment-default.css","hash":"2903c59ee06b965bef32e937bd69f5b0b2190717","modified":1602570653737},{"_id":"themes/hexo-theme-matery/source/libs/gitalk/gitalk.css","hash":"940ded3ea12c2fe1ab0820d2831ec405f3f1fe9f","modified":1602570653735},{"_id":"themes/hexo-theme-matery/source/libs/instantpage/instantpage.js","hash":"83ce8919b1a69b2f1809ffaf99b52a8627e650e9","modified":1602570653738},{"_id":"themes/hexo-theme-matery/source/libs/masonry/masonry.pkgd.min.js","hash":"ff940b4ea68368ca0e4d5560cbb79fb147dfc3c5","modified":1602570653741},{"_id":"themes/hexo-theme-matery/source/libs/others/busuanzi.pure.mini.js","hash":"6e41f31100ae7eb3a6f23f2c168f6dd56e7f7a9a","modified":1602570653743},{"_id":"themes/hexo-theme-matery/source/libs/others/clicklove.js","hash":"6a39b8c683ba5dcd92f70c6ab45d1cfac3213e8e","modified":1602570653743},{"_id":"themes/hexo-theme-matery/source/libs/scrollprogress/scrollProgress.min.js","hash":"777ffe5d07e85a14fbe97d846f45ffc0087251cc","modified":1602570653743},{"_id":"themes/hexo-theme-matery/source/libs/tocbot/tocbot.css","hash":"9ab8ef576c9a57115194152e79cca79b0a41dd70","modified":1602570653746},{"_id":"themes/hexo-theme-matery/source/libs/tocbot/tocbot.min.js","hash":"5ec27317f0270b8cf6b884c6f12025700b9a565c","modified":1602570653746},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/10.jpg","hash":"98e7f6fd9c97d4de9044b6871ca08ebf14db11b9","modified":1602570653761},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/13.jpg","hash":"35a320174f8e316e3eadaec658024276b651c6e9","modified":1602570653765},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/15.jpg","hash":"da0fbee3b7bde1607eace377ddf834c0be99edfe","modified":1602570653766},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/16.jpg","hash":"97a829c4bc94f9d2929b20a1a9b798c57b9f7205","modified":1602570653766},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/2.jpg","hash":"4bba691cf71a517ecaeaf42afd3e8f8b31e346c1","modified":1602570653768},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/21.jpg","hash":"b26edb128bb0bf58b23fd2f014e9555e89a2ca3b","modified":1602570653770},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/22.jpg","hash":"754579747a3e99747d890fca3162f370b96a7941","modified":1602570653770},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/23.jpg","hash":"7d7f37da3fa7128343adac23866449eb2c6a549a","modified":1602570653771},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/3.jpg","hash":"6ec646c2a70f5f11edacf225c1477f2200a37a96","modified":1602570653771},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/5.jpg","hash":"41ca20129a37fedc573eec28dd7d7b9e5b09228a","modified":1602570653773},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/7.jpg","hash":"7975141cd64e875122c0ea33daaca1a06bf00b8e","modified":1602570653774},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/8.jpg","hash":"8e4b7186352085483ca1174c7c0800114c48df8b","modified":1602570653775},{"_id":"themes/hexo-theme-matery/source/medias/reward/alipay.jpg","hash":"1abc719b95d1b26f1f898e6b0a9b7609146e332f","modified":1602570653777},{"_id":"themes/hexo-theme-matery/source/medias/reward/wechat.png","hash":"f1c84ceb948d0876b5ec7d4d55b654ef3f5132e1","modified":1602577429437},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-regular-400.woff","hash":"59439d3ad31d856d78ec3e2bd9f1eafa2c7a581c","modified":1602570653716},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-regular-400.eot","hash":"439c8afd3373acb4a73135a34e220464a89cd5e2","modified":1602570653713},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-regular-400.ttf","hash":"0f4bd02942a54a6b3200d9078adff88c2812e751","modified":1602570653716},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-regular-400.woff2","hash":"f6f653b4ea8fc487bdb590d39d5a726258a55f40","modified":1602570653717},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/fonts/lg.eot","hash":"54caf05a81e33d7bf04f2e420736ce6f1de5f936","modified":1602570653739},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/css/lightgallery.min.css","hash":"1b7227237f9785c66062a4811508916518e4132c","modified":1602570653739},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/fonts/lg.ttf","hash":"f6421c0c397311ae09f9257aa58bcd5e9720f493","modified":1602570653740},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/fonts/lg.svg","hash":"9c6632aeec67d3e84a1434884aa801514ff8103b","modified":1602570653739},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/fonts/lg.woff","hash":"3048de344dd5cad4624e0127e58eaae4b576f574","modified":1602570653740},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/img/loading.gif","hash":"15a76af2739482d8de7354abc6d8dc4fca8d145e","modified":1602570653740},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/img/video-play.png","hash":"2962e03ddbe04d7e201a5acccac531a2bbccddfc","modified":1602570653740},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/img/youtube-play.png","hash":"f8d11384d33b7a79ee2ba8d522844f14d5067a80","modified":1602570653740},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/img/vimeo-play.png","hash":"9b72fc0f86a01467ed0b68c9cc4d604ec316d517","modified":1602570653740},{"_id":"themes/hexo-theme-matery/source/libs/share/css/share.min.css","hash":"8a778a86f3ce9a042df6be63a9f1039631e351a5","modified":1602570653744},{"_id":"themes/hexo-theme-matery/source/libs/share/fonts/iconfont.svg","hash":"1d56c9d5db0273f07c43cc1397e440f98ba7827a","modified":1602570653744},{"_id":"themes/hexo-theme-matery/source/libs/share/fonts/iconfont.eot","hash":"00ff749c8e202401190cc98d56087cdda716abe4","modified":1602570653744},{"_id":"themes/hexo-theme-matery/source/libs/lightGallery/js/lightgallery-all.min.js","hash":"9f5ef4bc8a0a3c746ca4f3c3e6d64493b1a977d8","modified":1602570653741},{"_id":"themes/hexo-theme-matery/source/libs/share/js/social-share.min.js","hash":"a3090a02786dcd4efc6355c1c1dc978add8d6827","modified":1602570653746},{"_id":"themes/hexo-theme-matery/source/libs/share/fonts/iconfont.woff","hash":"2e3fce1dcfbd6e2114e7bfbeaf72d3c62e15a1bd","modified":1602570653745},{"_id":"themes/hexo-theme-matery/source/libs/share/fonts/iconfont.ttf","hash":"afd898f59d363887418669520b24d175f966a083","modified":1602570653745},{"_id":"themes/hexo-theme-matery/source/libs/share/js/jquery.share.min.js","hash":"41367dcb857e02e3c417ebe68a554ce1d4430806","modified":1602570653745},{"_id":"themes/hexo-theme-matery/source/libs/gitment/gitment.js","hash":"28c02c45ce568e084cd1041dc493f83f9c6c88c6","modified":1602570653737},{"_id":"themes/hexo-theme-matery/source/libs/dplayer/DPlayer.min.js","hash":"c3bad7b265574fab0ae4d45867422ea1cb9d6599","modified":1602570653729},{"_id":"themes/hexo-theme-matery/source/libs/jquery/jquery.min.js","hash":"2115753ca5fb7032aec498db7bb5dca624dbe6be","modified":1602570653739},{"_id":"themes/hexo-theme-matery/source/libs/valine/Valine.min.js","hash":"6cbdbf91e1f046dd41267a5ff0691a1fccba99df","modified":1602570653747},{"_id":"themes/hexo-theme-matery/source/medias/banner/0.jpg","hash":"69ec96cd9b4bc3aa631adc9da61353f50c39f031","modified":1602570653749},{"_id":"themes/hexo-theme-matery/source/medias/banner/2.jpg","hash":"39fb2535460ce66cc0b34e07ffb9411db1405f09","modified":1602570653752},{"_id":"themes/hexo-theme-matery/source/medias/banner/3.jpg","hash":"4ac047e92d0363b1a61ab756aca6dac13fb77494","modified":1602570653754},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/1.jpg","hash":"684ae89de8cb7acefae19f5aee6c612037c46393","modified":1602570653761},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/0.jpg","hash":"1c3300f029fc85d6dda6fa4f1d699551034cdaf7","modified":1602570653760},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/11.jpg","hash":"f55972ce7175684f2b11c3c9fc2b5b14bccbfae8","modified":1602570653762},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/12.jpg","hash":"8a4b2e7d92ae95c3b0c921db23c35aa9a41a7d58","modified":1602570653764},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/17.jpg","hash":"42d47903551ee81885c1386022982cae165841c5","modified":1602570653767},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/14.jpg","hash":"38e11221406785bcd93aa9cd23e568e164630ef1","modified":1602570653765},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/20.jpg","hash":"3b11f9b461168d907073f793190865fe621a8573","modified":1602570653769},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/19.jpg","hash":"eb250906fdbc0c408f42ae9933725bc1a05d79fb","modified":1602570653768},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/6.jpg","hash":"c8f2aa4bbb041158b4e73733a341e6a77c8583f7","modified":1602570653774},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/18.jpg","hash":"64829272ec85bb819d55ff89e5b5fd6f64aa436b","modified":1602570653767},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/4.jpg","hash":"e06c47de27619984be9d5d02947f8370a432dfea","modified":1602570653772},{"_id":"themes/hexo-theme-matery/source/medias/featureimages/9.jpg","hash":"b956a2291a04b2132366b53666cf34858b8bdb1f","modified":1602570653776},{"_id":"themes/hexo-theme-matery/source/libs/awesome/css/all.css","hash":"ecc41e32ad2696877a1656749841f3b5543bbe3d","modified":1602570653703},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-brands-400.eot","hash":"22f9e7d5226408eb2d0a11e118257a3ca22b8670","modified":1602570653705},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-brands-400.ttf","hash":"91cbeeaceb644a971241c08362898599d6d968ce","modified":1602570653711},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-brands-400.woff2","hash":"a46bd47ff0a90b812aafafda587d095cdb844271","modified":1602570653713},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-brands-400.woff","hash":"18838f5260317da3c5ed29bf844ac8a4f7ad0529","modified":1602570653712},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-solid-900.woff","hash":"92803b8753ceda573c6906774677c5a7081d2fbb","modified":1602570653725},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-solid-900.woff2","hash":"9c081b88b106c6c04ecb895ba7ba7d3dcb3b55ac","modified":1602570653726},{"_id":"themes/hexo-theme-matery/source/medias/cover.jpg","hash":"d4957ff7cc5e88555cd840f2956ab0561e6f1ccf","modified":1602570653759},{"_id":"themes/hexo-theme-matery/source/libs/gitalk/gitalk.min.js","hash":"8fefe38f28804f90116bdcb74a0875c9de9f3b7d","modified":1602570653736},{"_id":"themes/hexo-theme-matery/source/libs/materialize/materialize.min.css","hash":"a69d456e3345e7f59cd0d47d1b3e70fd4a496a05","modified":1602570653742},{"_id":"themes/hexo-theme-matery/source/libs/materialize/materialize.min.js","hash":"c8b4c65651921d888cf5f27430dfe2ad190d35bf","modified":1602570653743},{"_id":"themes/hexo-theme-matery/source/libs/valine/av-min.js","hash":"541efb9edc1ce425cbe3897cfc25803211fe6a05","modified":1602570653747},{"_id":"themes/hexo-theme-matery/source/medias/banner/1.jpg","hash":"ab122a36998a4f62a61e61a4fc5e00248113413b","modified":1602570653751},{"_id":"themes/hexo-theme-matery/source/medias/banner/5.jpg","hash":"852418f4f09e796e12bc3bab7a1488d3f37d6486","modified":1602570653757},{"_id":"themes/hexo-theme-matery/source/medias/banner/6.jpg","hash":"ed7282cc129c4ff9f322d2f2897fb4aac5c48589","modified":1602570653758},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-regular-400.svg","hash":"3d3a49445343d80f3b553e3e3425b9a7bd49acaf","modified":1602570653714},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-solid-900.ttf","hash":"9521ed12274c2cbc910cea77657116fcf6545da3","modified":1602570653724},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-solid-900.eot","hash":"cab8e84ae5682d1d556e234df9c790985888def8","modified":1602570653718},{"_id":"themes/hexo-theme-matery/source/medias/banner/4.jpg","hash":"e5ac5033678afa9d69edffe9a61004f836cb5734","modified":1602570653756},{"_id":"themes/hexo-theme-matery/source/libs/echarts/echarts.min.js","hash":"9496f386a0da4601cad22c479cc5543913a4d67f","modified":1602570653735},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-brands-400.svg","hash":"5e2d2a159294576bea69cc3360efb5ffe110ab2d","modified":1602570653708},{"_id":"themes/hexo-theme-matery/source/libs/awesome/webfonts/fa-solid-900.svg","hash":"7da88b19e1486f8c968d3cf5ab3f194f01ea17fd","modified":1602570653722},{"_id":"public/atom.xml","hash":"5c7a8fc43a4af0e821750d2dd7b9de1772090c24","modified":1607589283062},{"_id":"public/search.xml","hash":"51ee0c645fb4137d6fc560df04508503fead20fc","modified":1607589283062},{"_id":"public/categories/index.html","hash":"7ea9374d2dead8bfd8d00baffa1d55e90eb6afbe","modified":1607335732288},{"_id":"public/about/index.html","hash":"bd3a882d3a9c9adc36529c78e38b323678f396ab","modified":1607587146319},{"_id":"public/tags/index.html","hash":"f192af0eaa15353b1f1d5add1a5f57fb44a00653","modified":1607335732288},{"_id":"public/2020/10/13/hello-world/index.html","hash":"cd274fbb6171d22d206412e3c2ed506998a9444c","modified":1602576511910},{"_id":"public/archives/index.html","hash":"bd1f972596e5e0a475a76f18ba43bca024139eea","modified":1607587146319},{"_id":"public/archives/2020/index.html","hash":"7c63499a730ba1dd57fcdbd967feca0636e8efa4","modified":1607587146319},{"_id":"public/archives/2020/10/index.html","hash":"cb62de7c08cd1c63ec5df312b43cab2f6104cf49","modified":1607587146319},{"_id":"public/index.html","hash":"6f59c73e6307e4203cda0bda94adaa6c67439cac","modified":1607587332408},{"_id":"public/favicon.png","hash":"774fee8c6d0be9dbb010b20f36c06848d06e3da0","modified":1602571742461},{"_id":"public/medias/comment_bg.png","hash":"dfc93d24081884fbc58cab0f8fd19e77d31d6123","modified":1602571742461},{"_id":"public/medias/icp.png","hash":"27a96f31f7d0413c6ade6f40e06f021f501151c7","modified":1602571742461},{"_id":"public/medias/logo.png","hash":"d9095f5ea8719374d9d1ff020279426f5b2a1396","modified":1602571742461},{"_id":"public/medias/featureimages/2.jpg","hash":"4bba691cf71a517ecaeaf42afd3e8f8b31e346c1","modified":1602571742461},{"_id":"public/medias/featureimages/22.jpg","hash":"754579747a3e99747d890fca3162f370b96a7941","modified":1602571742461},{"_id":"public/medias/featureimages/23.jpg","hash":"7d7f37da3fa7128343adac23866449eb2c6a549a","modified":1602571742461},{"_id":"public/medias/featureimages/5.jpg","hash":"41ca20129a37fedc573eec28dd7d7b9e5b09228a","modified":1602571742461},{"_id":"public/medias/featureimages/3.jpg","hash":"6ec646c2a70f5f11edacf225c1477f2200a37a96","modified":1602571742461},{"_id":"public/medias/featureimages/7.jpg","hash":"7975141cd64e875122c0ea33daaca1a06bf00b8e","modified":1602571742461},{"_id":"public/medias/reward/alipay.jpg","hash":"1abc719b95d1b26f1f898e6b0a9b7609146e332f","modified":1602571742461},{"_id":"public/medias/reward/wechat.png","hash":"f1c84ceb948d0876b5ec7d4d55b654ef3f5132e1","modified":1602577498584},{"_id":"public/libs/awesome/webfonts/fa-regular-400.woff","hash":"59439d3ad31d856d78ec3e2bd9f1eafa2c7a581c","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-regular-400.woff2","hash":"f6f653b4ea8fc487bdb590d39d5a726258a55f40","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-regular-400.ttf","hash":"0f4bd02942a54a6b3200d9078adff88c2812e751","modified":1602571742461},{"_id":"public/libs/lightGallery/fonts/lg.eot","hash":"54caf05a81e33d7bf04f2e420736ce6f1de5f936","modified":1602571742461},{"_id":"public/libs/lightGallery/fonts/lg.woff","hash":"3048de344dd5cad4624e0127e58eaae4b576f574","modified":1602571742461},{"_id":"public/libs/lightGallery/fonts/lg.ttf","hash":"f6421c0c397311ae09f9257aa58bcd5e9720f493","modified":1602571742461},{"_id":"public/libs/lightGallery/fonts/lg.svg","hash":"9c6632aeec67d3e84a1434884aa801514ff8103b","modified":1602571742461},{"_id":"public/libs/lightGallery/img/video-play.png","hash":"2962e03ddbe04d7e201a5acccac531a2bbccddfc","modified":1602571742461},{"_id":"public/libs/lightGallery/img/vimeo-play.png","hash":"9b72fc0f86a01467ed0b68c9cc4d604ec316d517","modified":1602571742461},{"_id":"public/libs/lightGallery/img/loading.gif","hash":"15a76af2739482d8de7354abc6d8dc4fca8d145e","modified":1602571742461},{"_id":"public/libs/lightGallery/img/youtube-play.png","hash":"f8d11384d33b7a79ee2ba8d522844f14d5067a80","modified":1602571742461},{"_id":"public/libs/share/fonts/iconfont.woff","hash":"2e3fce1dcfbd6e2114e7bfbeaf72d3c62e15a1bd","modified":1602571742461},{"_id":"public/libs/share/fonts/iconfont.eot","hash":"00ff749c8e202401190cc98d56087cdda716abe4","modified":1602571742461},{"_id":"public/libs/share/fonts/iconfont.svg","hash":"1d56c9d5db0273f07c43cc1397e440f98ba7827a","modified":1602571742461},{"_id":"public/libs/share/fonts/iconfont.ttf","hash":"afd898f59d363887418669520b24d175f966a083","modified":1602571742461},{"_id":"public/medias/avatar.jpg","hash":"2a6287308628881ce27b9a7de53ba15c2be00d02","modified":1602571742461},{"_id":"public/medias/banner/0.jpg","hash":"69ec96cd9b4bc3aa631adc9da61353f50c39f031","modified":1602571742461},{"_id":"public/medias/featureimages/10.jpg","hash":"98e7f6fd9c97d4de9044b6871ca08ebf14db11b9","modified":1602571742461},{"_id":"public/medias/featureimages/13.jpg","hash":"35a320174f8e316e3eadaec658024276b651c6e9","modified":1602571742461},{"_id":"public/medias/featureimages/15.jpg","hash":"da0fbee3b7bde1607eace377ddf834c0be99edfe","modified":1602571742461},{"_id":"public/medias/featureimages/16.jpg","hash":"97a829c4bc94f9d2929b20a1a9b798c57b9f7205","modified":1602571742461},{"_id":"public/medias/featureimages/17.jpg","hash":"42d47903551ee81885c1386022982cae165841c5","modified":1602571742461},{"_id":"public/medias/featureimages/19.jpg","hash":"eb250906fdbc0c408f42ae9933725bc1a05d79fb","modified":1602571742461},{"_id":"public/medias/featureimages/18.jpg","hash":"64829272ec85bb819d55ff89e5b5fd6f64aa436b","modified":1602571742461},{"_id":"public/medias/featureimages/20.jpg","hash":"3b11f9b461168d907073f793190865fe621a8573","modified":1602571742461},{"_id":"public/medias/featureimages/21.jpg","hash":"b26edb128bb0bf58b23fd2f014e9555e89a2ca3b","modified":1602571742461},{"_id":"public/medias/featureimages/4.jpg","hash":"e06c47de27619984be9d5d02947f8370a432dfea","modified":1602571742461},{"_id":"public/medias/featureimages/9.jpg","hash":"b956a2291a04b2132366b53666cf34858b8bdb1f","modified":1602571742461},{"_id":"public/medias/featureimages/8.jpg","hash":"8e4b7186352085483ca1174c7c0800114c48df8b","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-brands-400.woff","hash":"18838f5260317da3c5ed29bf844ac8a4f7ad0529","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-brands-400.woff2","hash":"a46bd47ff0a90b812aafafda587d095cdb844271","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-regular-400.eot","hash":"439c8afd3373acb4a73135a34e220464a89cd5e2","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-solid-900.woff","hash":"92803b8753ceda573c6906774677c5a7081d2fbb","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-solid-900.woff2","hash":"9c081b88b106c6c04ecb895ba7ba7d3dcb3b55ac","modified":1602571742461},{"_id":"public/medias/cover.jpg","hash":"d4957ff7cc5e88555cd840f2956ab0561e6f1ccf","modified":1602571742461},{"_id":"public/medias/banner/2.jpg","hash":"39fb2535460ce66cc0b34e07ffb9411db1405f09","modified":1602571742461},{"_id":"public/medias/banner/5.jpg","hash":"852418f4f09e796e12bc3bab7a1488d3f37d6486","modified":1602571742461},{"_id":"public/medias/featureimages/0.jpg","hash":"1c3300f029fc85d6dda6fa4f1d699551034cdaf7","modified":1602571742461},{"_id":"public/medias/featureimages/1.jpg","hash":"684ae89de8cb7acefae19f5aee6c612037c46393","modified":1602571742461},{"_id":"public/medias/featureimages/12.jpg","hash":"8a4b2e7d92ae95c3b0c921db23c35aa9a41a7d58","modified":1602571742461},{"_id":"public/medias/featureimages/11.jpg","hash":"f55972ce7175684f2b11c3c9fc2b5b14bccbfae8","modified":1602571742461},{"_id":"public/medias/featureimages/14.jpg","hash":"38e11221406785bcd93aa9cd23e568e164630ef1","modified":1602571742461},{"_id":"public/medias/featureimages/6.jpg","hash":"c8f2aa4bbb041158b4e73733a341e6a77c8583f7","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-brands-400.eot","hash":"22f9e7d5226408eb2d0a11e118257a3ca22b8670","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-regular-400.svg","hash":"3d3a49445343d80f3b553e3e3425b9a7bd49acaf","modified":1602571742461},{"_id":"public/css/gitment.css","hash":"2bd15cc17dca35ac3ecc0acf167a23a1dd362acd","modified":1602571742461},{"_id":"public/css/my-gitalk.css","hash":"eeda46a83d0db1cc239a9cd27d544faf663f9883","modified":1602571742461},{"_id":"public/css/my.css","hash":"10577fbc30f241b126d1b51b1f56136ecba86b19","modified":1602571742461},{"_id":"public/js/matery.js","hash":"b86de5fe3e9766b7ff80df12ea41c3a9e30825f7","modified":1602571742461},{"_id":"public/js/search.js","hash":"d559d402b4d4a0931821fe6e22a8831fc43a953d","modified":1602571742461},{"_id":"public/libs/aos/aos.js","hash":"02bfb40b0c4b6e9b0b4081218357145cbb327d74","modified":1602571742461},{"_id":"public/libs/aplayer/APlayer.min.css","hash":"07372a2ba507388d0fed166d761b1c2c2a659dce","modified":1602571742461},{"_id":"public/libs/codeBlock/codeBlockFuction.js","hash":"c7ab06d27a525b15b1eb69027135269e9b9132fb","modified":1602571742461},{"_id":"public/libs/codeBlock/codeCopy.js","hash":"6d39a766af62e625f177c4d5cf3adc35eed71e61","modified":1602571742461},{"_id":"public/libs/codeBlock/codeLang.js","hash":"bac88b4d4e3679732d29bd037c34f089cf27cf05","modified":1602571742461},{"_id":"public/libs/codeBlock/codeShrink.js","hash":"201e8cd761b4be557247bdaf1ebc7c11c83194f6","modified":1602571742461},{"_id":"public/libs/background/canvas-nest.js","hash":"65333d0dbb9c1173a1b13031b230161fc42c8b2f","modified":1602571742461},{"_id":"public/libs/background/ribbon-dynamic.js","hash":"052b80c29e6bc585aa28d4504b743bdbac220a88","modified":1602571742461},{"_id":"public/libs/background/ribbon-refresh.min.js","hash":"6d98692b2cad8c746a562db18b170b35c24402f4","modified":1602571742461},{"_id":"public/libs/background/ribbon.min.js","hash":"6a99d494c030388f96f6086a7aaa0f03f3fe532e","modified":1602571742461},{"_id":"public/libs/jqcloud/jqcloud-1.0.4.min.js","hash":"257eaae3020599e4939f50d5008a743827f25b8c","modified":1602571742461},{"_id":"public/libs/jqcloud/jqcloud.css","hash":"20d9f11a19d95c70e27cb922e0d6dccbec4eae89","modified":1602571742461},{"_id":"public/libs/instantpage/instantpage.js","hash":"83ce8919b1a69b2f1809ffaf99b52a8627e650e9","modified":1602571742461},{"_id":"public/libs/others/clicklove.js","hash":"6a39b8c683ba5dcd92f70c6ab45d1cfac3213e8e","modified":1602571742461},{"_id":"public/libs/others/busuanzi.pure.mini.js","hash":"6e41f31100ae7eb3a6f23f2c168f6dd56e7f7a9a","modified":1602571742461},{"_id":"public/libs/scrollprogress/scrollProgress.min.js","hash":"777ffe5d07e85a14fbe97d846f45ffc0087251cc","modified":1602571742461},{"_id":"public/libs/tocbot/tocbot.css","hash":"9ab8ef576c9a57115194152e79cca79b0a41dd70","modified":1602571742461},{"_id":"public/libs/tocbot/tocbot.min.js","hash":"5ec27317f0270b8cf6b884c6f12025700b9a565c","modified":1602571742461},{"_id":"public/libs/share/css/share.min.css","hash":"8a778a86f3ce9a042df6be63a9f1039631e351a5","modified":1602571742461},{"_id":"public/medias/banner/3.jpg","hash":"4ac047e92d0363b1a61ab756aca6dac13fb77494","modified":1602571742461},{"_id":"public/medias/banner/6.jpg","hash":"ed7282cc129c4ff9f322d2f2897fb4aac5c48589","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-brands-400.ttf","hash":"91cbeeaceb644a971241c08362898599d6d968ce","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-solid-900.ttf","hash":"9521ed12274c2cbc910cea77657116fcf6545da3","modified":1602571742461},{"_id":"public/libs/aos/aos.css","hash":"191a3705a8f63e589a50a0ff2f2c5559f1a1b6b2","modified":1602571742461},{"_id":"public/libs/gitalk/gitalk.css","hash":"940ded3ea12c2fe1ab0820d2831ec405f3f1fe9f","modified":1602571742461},{"_id":"public/libs/gitment/gitment-default.css","hash":"2903c59ee06b965bef32e937bd69f5b0b2190717","modified":1602571742461},{"_id":"public/libs/masonry/masonry.pkgd.min.js","hash":"ff940b4ea68368ca0e4d5560cbb79fb147dfc3c5","modified":1602571742461},{"_id":"public/libs/lightGallery/css/lightgallery.min.css","hash":"1b7227237f9785c66062a4811508916518e4132c","modified":1602571742461},{"_id":"public/libs/share/js/social-share.min.js","hash":"a3090a02786dcd4efc6355c1c1dc978add8d6827","modified":1602571742461},{"_id":"public/libs/share/js/jquery.share.min.js","hash":"41367dcb857e02e3c417ebe68a554ce1d4430806","modified":1602571742461},{"_id":"public/medias/banner/1.jpg","hash":"ab122a36998a4f62a61e61a4fc5e00248113413b","modified":1602571742461},{"_id":"public/css/matery.css","hash":"87bd1dacf48c9daab7ea43466368247f1e4107d1","modified":1602571742461},{"_id":"public/libs/cryptojs/crypto-js.min.js","hash":"5989527a378b55011a59522f41eeb3981518325c","modified":1602571742461},{"_id":"public/libs/dplayer/DPlayer.min.css","hash":"f7d19655f873b813ffba5d1a17145c91f82631b8","modified":1602571742461},{"_id":"public/libs/animate/animate.min.css","hash":"97afa151569f046b2e01f27c1871646e9cd87caf","modified":1602571742461},{"_id":"public/libs/aplayer/APlayer.min.js","hash":"22caa28ff6b41a16ff40f15d38f1739e22359478","modified":1602571742461},{"_id":"public/libs/lightGallery/js/lightgallery-all.min.js","hash":"9f5ef4bc8a0a3c746ca4f3c3e6d64493b1a977d8","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-solid-900.eot","hash":"cab8e84ae5682d1d556e234df9c790985888def8","modified":1602571742461},{"_id":"public/libs/gitment/gitment.js","hash":"28c02c45ce568e084cd1041dc493f83f9c6c88c6","modified":1602571742461},{"_id":"public/libs/valine/Valine.min.js","hash":"6cbdbf91e1f046dd41267a5ff0691a1fccba99df","modified":1602571742461},{"_id":"public/libs/awesome/css/all.css","hash":"ecc41e32ad2696877a1656749841f3b5543bbe3d","modified":1602571742461},{"_id":"public/medias/banner/4.jpg","hash":"e5ac5033678afa9d69edffe9a61004f836cb5734","modified":1602571742461},{"_id":"public/libs/jquery/jquery.min.js","hash":"2115753ca5fb7032aec498db7bb5dca624dbe6be","modified":1602571742461},{"_id":"public/libs/dplayer/DPlayer.min.js","hash":"c3bad7b265574fab0ae4d45867422ea1cb9d6599","modified":1602571742461},{"_id":"public/libs/materialize/materialize.min.css","hash":"a69d456e3345e7f59cd0d47d1b3e70fd4a496a05","modified":1602571742461},{"_id":"public/libs/gitalk/gitalk.min.js","hash":"8fefe38f28804f90116bdcb74a0875c9de9f3b7d","modified":1602571742461},{"_id":"public/libs/valine/av-min.js","hash":"541efb9edc1ce425cbe3897cfc25803211fe6a05","modified":1602571742461},{"_id":"public/libs/materialize/materialize.min.js","hash":"c8b4c65651921d888cf5f27430dfe2ad190d35bf","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-brands-400.svg","hash":"5e2d2a159294576bea69cc3360efb5ffe110ab2d","modified":1602571742461},{"_id":"public/libs/awesome/webfonts/fa-solid-900.svg","hash":"7da88b19e1486f8c968d3cf5ab3f194f01ea17fd","modified":1602571742461},{"_id":"public/libs/echarts/echarts.min.js","hash":"9496f386a0da4601cad22c479cc5543913a4d67f","modified":1602571742461},{"_id":"themes/hexo-theme-matery/source/medias/wisesky.jpg","hash":"ca0e4de76b63d2782d8618aeea148467258ae14f","modified":1602572521751},{"_id":"public/medias/wisesky.jpg","hash":"ca0e4de76b63d2782d8618aeea148467258ae14f","modified":1602574164658},{"_id":"themes/hexo-theme-matery/source/medias/reward/.DS_Store","hash":"fe9ec4436feaf1a9fedf0f2a2938c80df09fa8fa","modified":1602577812069},{"_id":"themes/hexo-theme-matery/source/medias/.DS_Store","hash":"49a8c2d15a0f57fe40bb502e337a296afdb1ef83","modified":1602577383050},{"_id":"themes/hexo-theme-matery/source/medias/reward/alipay.png","hash":"7c1cd3be7825da29c2aca0d8c0a77f6f791b6d3a","modified":1602577351418},{"_id":"public/uncategorized/hello-world/index.html","hash":"38d91d91a28d189bc1f36f490d64f1cab54bb5dd","modified":1602577909635},{"_id":"public/medias/reward/alipay.png","hash":"7c1cd3be7825da29c2aca0d8c0a77f6f791b6d3a","modified":1602577498584},{"_id":"public/tags/test/index.html","hash":"b4d51cfc8a01d3550013a71db1270bb8a7097248","modified":1607335732288},{"_id":"public/test/hello-world/index.html","hash":"258096a2581e401331d1f13925ab96d8154cfa38","modified":1607587332408},{"_id":"public/categories/test/index.html","hash":"0be5012db47b75a77f4b79b363ace62b57fbeabc","modified":1607335732288},{"_id":"source/_posts/my-first-blog.md","hash":"c7e5400935afb8289e78a08d62bfe907fba89d36","modified":1606285615417},{"_id":"source/.DS_Store","hash":"b48c4f7d61a5928be717d4bd654481ff1eab36ee","modified":1602580239660},{"_id":"themes/hexo-theme-matery/source/medias/ACG/9.jpg","hash":"10b2ddfbe7f03c4513ace7a2855a47f907478d55","modified":1602578209287},{"_id":"themes/hexo-theme-matery/source/medias/ACG/4.jpg","hash":"f06a264751553ff6b2e3b158c21154ce0aacda16","modified":1602578203288},{"_id":"themes/hexo-theme-matery/source/medias/ACG/3.jpg","hash":"f779883e34f277950961f5f1ab62a433dcd91567","modified":1602578201822},{"_id":"themes/hexo-theme-matery/source/medias/ACG/8.jpg","hash":"63c5ff3afa220b73031891a08cf56146f7fd9fee","modified":1602578208159},{"_id":"themes/hexo-theme-matery/source/medias/ACG/2.jpg","hash":"28792c2f8c55005db8c0e6aeae70b394c7dd0fff","modified":1602578200558},{"_id":"themes/hexo-theme-matery/source/medias/ACG/7.jpg","hash":"f80b51d620ca4fd2878295b00c68659c7990cfa0","modified":1602578206996},{"_id":"themes/hexo-theme-matery/source/medias/ACG/6.jpg","hash":"f82a96d6e74947be00c049d57d4e49a102344774","modified":1602578205731},{"_id":"themes/hexo-theme-matery/source/medias/ACG/5.jpg","hash":"5dfa664bb0bcf5ed1480e44bbeb92291a59b3326","modified":1602578204467},{"_id":"themes/hexo-theme-matery/source/medias/ACG/1.jpg","hash":"33c9e2585ba65798847eb7d10009db4089593350","modified":1602578198957},{"_id":"public/sui-bi/my-first-blog/index.html","hash":"f083da341fc1d909ef9996332c20945f036443cb","modified":1607587146319},{"_id":"public/categories/随笔/index.html","hash":"4b8361d1765fd3ae98f39e678fe2df3b2c53d37b","modified":1607335732288},{"_id":"public/tags/随笔/index.html","hash":"14580b631c3dfb3a29b991f518c7e4f9100aeb1f","modified":1607335732288},{"_id":"public/css/prism-tomorrow.css","hash":"3b99487dfc9b4e51e9105a93743b92a761840e34","modified":1602665020148},{"_id":"public/medias/ACG/9.jpg","hash":"10b2ddfbe7f03c4513ace7a2855a47f907478d55","modified":1602665020148},{"_id":"public/medias/ACG/3.jpg","hash":"f779883e34f277950961f5f1ab62a433dcd91567","modified":1602665020148},{"_id":"public/medias/ACG/4.jpg","hash":"f06a264751553ff6b2e3b158c21154ce0aacda16","modified":1602665020148},{"_id":"public/medias/ACG/8.jpg","hash":"63c5ff3afa220b73031891a08cf56146f7fd9fee","modified":1602665020148},{"_id":"public/medias/ACG/2.jpg","hash":"28792c2f8c55005db8c0e6aeae70b394c7dd0fff","modified":1602665020148},{"_id":"public/medias/ACG/7.jpg","hash":"f80b51d620ca4fd2878295b00c68659c7990cfa0","modified":1602665020148},{"_id":"public/medias/ACG/6.jpg","hash":"f82a96d6e74947be00c049d57d4e49a102344774","modified":1602665020148},{"_id":"public/medias/ACG/5.jpg","hash":"5dfa664bb0bcf5ed1480e44bbeb92291a59b3326","modified":1602665020148},{"_id":"public/medias/ACG/1.jpg","hash":"33c9e2585ba65798847eb7d10009db4089593350","modified":1602665020148},{"_id":"source/_posts/.DS_Store","hash":"15e613ffb85d4c1c2580803f9a341ecb8eb340d1","modified":1607335687233},{"_id":"source/_posts/my-first-blog/Euphonium_Movie_2nd_KV.jpg","hash":"ca061b9e7f84622a9786b51979258304ed68f2ab","modified":1602665209583},{"_id":"source/_posts/my-first-blog/p2409965093.jpg","hash":"3e95adce661298a77553ffb74451bf8d8f94dbd3","modified":1602665506316},{"_id":"source/_posts/my-first-blog/p2382958621.png","hash":"8f905289010e20f1ec91485c06fefb9aa3969210","modified":1602665506289},{"_id":"source/_posts/my-first-blog/Euphonium_Movie_Finale_KV2.jpg","hash":"4bc062c352ba272482c27be24c373e3aa0dd99db","modified":1602665209643},{"_id":"public/sui-bi/my-first-blog/Euphonium_Movie_2nd_KV.jpg","hash":"ca061b9e7f84622a9786b51979258304ed68f2ab","modified":1602665561247},{"_id":"public/sui-bi/my-first-blog/p2409965093.jpg","hash":"3e95adce661298a77553ffb74451bf8d8f94dbd3","modified":1602665561247},{"_id":"public/sui-bi/my-first-blog/p2382958621.png","hash":"8f905289010e20f1ec91485c06fefb9aa3969210","modified":1602665561247},{"_id":"public/sui-bi/my-first-blog/Euphonium_Movie_Finale_KV2.jpg","hash":"4bc062c352ba272482c27be24c373e3aa0dd99db","modified":1602665561247},{"_id":"source/_posts/my-first-blog/relife-2.jpg","hash":"3e95adce661298a77553ffb74451bf8d8f94dbd3","modified":1602665691497},{"_id":"source/_posts/my-first-blog/relife-1.png","hash":"8f905289010e20f1ec91485c06fefb9aa3969210","modified":1602665687425},{"_id":"public/sui-bi/my-first-blog/relife-2.jpg","hash":"3e95adce661298a77553ffb74451bf8d8f94dbd3","modified":1602665761392},{"_id":"public/sui-bi/my-first-blog/relife-1.png","hash":"8f905289010e20f1ec91485c06fefb9aa3969210","modified":1602665761392},{"_id":"source/_posts/改变与懒惰.md","hash":"a1309fc7ecd208af09188de98e92de9dd6fbe719","modified":1603879037910},{"_id":"source/_posts/sth-change-myself/2020-07-31.jpg","hash":"1e70ffa9161923f5a7b25c5f9da60f925ade49cf","modified":1602319160796},{"_id":"public/sui-bi/gai-bian-yu-lan-duo/index.html","hash":"21bb755d93994a7f7da145416bef2867cb72bd5c","modified":1607587146319},{"_id":"source/_posts/改变与懒惰/2020-07-31.jpg","hash":"1e70ffa9161923f5a7b25c5f9da60f925ade49cf","modified":1602319160796},{"_id":"public/sui-bi/gai-bian-yu-lan-duo/2020-07-31.jpg","hash":"1e70ffa9161923f5a7b25c5f9da60f925ade49cf","modified":1603877001247},{"_id":"source/_posts/改变与懒惰/IMG_9373.jpeg","hash":"3abf97bca9a9719e6c3797cb69fbaf3e3b82f183","modified":1603878562340},{"_id":"public/sui-bi/gai-bian-yu-lan-duo/IMG_9373.jpeg","hash":"3abf97bca9a9719e6c3797cb69fbaf3e3b82f183","modified":1603878590251},{"_id":"source/_posts/记ubuntu重启引起的故障排查.md","hash":"77ae20e01b2eda05026ae6c56d48638eb72e059c","modified":1607587287729},{"_id":"public/tags/ubuntu/index.html","hash":"41e73965a0560b2d8764e3aa03fb7c5049e337b4","modified":1607335732288},{"_id":"public/tags/故障排查/index.html","hash":"bde3b098b6bc911083a64f70e381d07871e92086","modified":1607335732288},{"_id":"public/yun-wei/ji-ubuntu-chong-qi-yin-qi-de-gu-zhang-pai-cha/index.html","hash":"e1258cbdaf70fb557e6b32d11f8facebfa0689b4","modified":1607587146319},{"_id":"public/archives/2020/11/index.html","hash":"23401988a76c71b53990f4aae117ac93409c5405","modified":1607587146319},{"_id":"public/tags/grub/index.html","hash":"3383b87b3fd97130bee949b8878741dff1babd9d","modified":1607335732288},{"_id":"public/tags/linux内核/index.html","hash":"84209fc1e06b35056a85de545347c9b568ab387a","modified":1607335732288},{"_id":"public/categories/运维/index.html","hash":"6219a7bb1fa9807853abe282af5f9e3070aae2f0","modified":1607335732288},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array.md","hash":"38c137779b3225cbe8640623089ecd5e7d8d2519","modified":1607587296687},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img2.jpg","hash":"8839af294b39128cc5ebac6602b4432c78f1f702","modified":1605604665263},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img3.jpg","hash":"91dc5aa98d0e31eea2c8a7e06aa02a93c2fed481","modified":1605604660564},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img6.jpg","hash":"8839af294b39128cc5ebac6602b4432c78f1f702","modified":1605605676330},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img7.jpg","hash":"99da6e3b4c7064128818cd54cf48f54464a4b379","modified":1605605679078},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img8.jpg","hash":"02e2ce27e3ec791a9f78b746b321e90def906ca3","modified":1605605684311},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img1.png","hash":"1561362bfc8b07eeca9e43b997431177a3b7601a","modified":1605601397721},{"_id":"public/algorithm-leetcode/leetcode-33-search-in-rotated-sorted-array/index.html","hash":"14e755a215967c9e99d9d29bd48df693b977acb3","modified":1605609271672},{"_id":"public/categories/Algorithm-LeetCode/index.html","hash":"9d46e00e60588a274764937e1e653a06083659f8","modified":1605609271672},{"_id":"public/tags/Algorithm/index.html","hash":"00d94af1ef33c47cf201149f3b3fb8c9afc5da07","modified":1607335732288},{"_id":"public/algorithm-leetcode/leetcode-33-search-in-rotated-sorted-array/img3.jpg","hash":"91dc5aa98d0e31eea2c8a7e06aa02a93c2fed481","modified":1605609207295},{"_id":"public/algorithm-leetcode/leetcode-33-search-in-rotated-sorted-array/img7.jpg","hash":"99da6e3b4c7064128818cd54cf48f54464a4b379","modified":1605609207295},{"_id":"public/algorithm-leetcode/leetcode-33-search-in-rotated-sorted-array/img6.jpg","hash":"8839af294b39128cc5ebac6602b4432c78f1f702","modified":1605609207295},{"_id":"public/algorithm-leetcode/leetcode-33-search-in-rotated-sorted-array/img8.jpg","hash":"02e2ce27e3ec791a9f78b746b321e90def906ca3","modified":1605609207295},{"_id":"public/algorithm-leetcode/leetcode-33-search-in-rotated-sorted-array/img2.jpg","hash":"8839af294b39128cc5ebac6602b4432c78f1f702","modified":1605609207295},{"_id":"public/algorithm-leetcode/leetcode-33-search-in-rotated-sorted-array/img1.png","hash":"1561362bfc8b07eeca9e43b997431177a3b7601a","modified":1605609207295},{"_id":"public/algorithm/leetcode/leetcode-33-search-in-rotated-sorted-array/index.html","hash":"d51f222777c9e812ba0f8b9aad05cf96023265f8","modified":1605609346756},{"_id":"public/categories/Algorithm/index.html","hash":"3de52a00d9d83f7e5f38c55423ffa0b4f692d23c","modified":1607335732288},{"_id":"public/categories/Algorithm/LeetCode/index.html","hash":"d46713a23432b4b6230b9a873316706befaf8bfd","modified":1605609346756},{"_id":"public/algorithm/leetcode/leetcode-33-search-in-rotated-sorted-array/img2.jpg","hash":"8839af294b39128cc5ebac6602b4432c78f1f702","modified":1605609346756},{"_id":"public/algorithm/leetcode/leetcode-33-search-in-rotated-sorted-array/img3.jpg","hash":"91dc5aa98d0e31eea2c8a7e06aa02a93c2fed481","modified":1605609346756},{"_id":"public/algorithm/leetcode/leetcode-33-search-in-rotated-sorted-array/img6.jpg","hash":"8839af294b39128cc5ebac6602b4432c78f1f702","modified":1605609346756},{"_id":"public/algorithm/leetcode/leetcode-33-search-in-rotated-sorted-array/img7.jpg","hash":"99da6e3b4c7064128818cd54cf48f54464a4b379","modified":1605609346756},{"_id":"public/algorithm/leetcode/leetcode-33-search-in-rotated-sorted-array/img8.jpg","hash":"02e2ce27e3ec791a9f78b746b321e90def906ca3","modified":1605609346756},{"_id":"public/algorithm/leetcode/leetcode-33-search-in-rotated-sorted-array/img1.png","hash":"1561362bfc8b07eeca9e43b997431177a3b7601a","modified":1605609346756},{"_id":"public/algorithm/leetcode-33-search-in-rotated-sorted-array/index.html","hash":"4e1c329df7da6db9e1cc8eaa15e2159986c681e7","modified":1607587146319},{"_id":"public/tags/LeetCode/index.html","hash":"10976378c71714f061f8daa7a2a85c5124330e0c","modified":1607335732288},{"_id":"public/algorithm/leetcode-33-search-in-rotated-sorted-array/img2.jpg","hash":"8839af294b39128cc5ebac6602b4432c78f1f702","modified":1605609419212},{"_id":"public/algorithm/leetcode-33-search-in-rotated-sorted-array/img6.jpg","hash":"8839af294b39128cc5ebac6602b4432c78f1f702","modified":1605609419212},{"_id":"public/algorithm/leetcode-33-search-in-rotated-sorted-array/img3.jpg","hash":"91dc5aa98d0e31eea2c8a7e06aa02a93c2fed481","modified":1605609419212},{"_id":"public/algorithm/leetcode-33-search-in-rotated-sorted-array/img7.jpg","hash":"99da6e3b4c7064128818cd54cf48f54464a4b379","modified":1605609419212},{"_id":"public/algorithm/leetcode-33-search-in-rotated-sorted-array/img8.jpg","hash":"02e2ce27e3ec791a9f78b746b321e90def906ca3","modified":1605609419212},{"_id":"public/algorithm/leetcode-33-search-in-rotated-sorted-array/img1.png","hash":"1561362bfc8b07eeca9e43b997431177a3b7601a","modified":1605609419212},{"_id":"source/_posts/同步和异步编程-如何并行写同一个日志文件实践.md","hash":"3ff38ea1dbc739dfd8b6d693adbe44dd57da2bb7","modified":1607587302373},{"_id":"source/_posts/同步和异步编程-如何并行写同一个日志文件实践/1.jpeg","hash":"7ab7e4f532a7ceca75c6b9dbeffe4512883483f6","modified":1606125898681},{"_id":"source/_posts/同步和异步编程-如何并行写同一个日志文件实践/2.jpeg","hash":"e8afe7e8b17d1ee02551c2c6956f24fe802ad147","modified":1606125894245},{"_id":"public/algorithms/tong-bu-he-yi-bu-bian-cheng-ru-he-bing-xing-xie-tong-yi-ge-ri-zhi-wen-jian-shi-jian/index.html","hash":"310794bc97548bfc974065999f98901e6f097cf0","modified":1607587146319},{"_id":"public/categories/Algorithms/index.html","hash":"27ffcd5442f17f429c80b38ab5f8ba42eacc775b","modified":1607335732288},{"_id":"public/tags/Algorithms/index.html","hash":"3a76565f29a4e10b07cb92bed478cfc0047931ac","modified":1607335732288},{"_id":"public/tags/OS/index.html","hash":"18f61f057f5e6c1e2f3b90bbce06cbbf0f017611","modified":1607335732288},{"_id":"public/algorithms/tong-bu-he-yi-bu-bian-cheng-ru-he-bing-xing-xie-tong-yi-ge-ri-zhi-wen-jian-shi-jian/1.jpeg","hash":"7ab7e4f532a7ceca75c6b9dbeffe4512883483f6","modified":1606208305818},{"_id":"public/algorithms/tong-bu-he-yi-bu-bian-cheng-ru-he-bing-xing-xie-tong-yi-ge-ri-zhi-wen-jian-shi-jian/2.jpeg","hash":"e8afe7e8b17d1ee02551c2c6956f24fe802ad147","modified":1606208305818},{"_id":"source/_posts/GPU-in-Pytorch-并行和分布式实践.md","hash":"90673fcde2e78a233ad63307906499c67590290c","modified":1607589280141},{"_id":"source/_posts/并行和分布式训练-in-Pytorch.md","hash":"359fac6de0a159566dfeb31a9200758eb18eb270","modified":1606287881380},{"_id":"public/pytorch/gpu-in-pytorch-bing-xing-he-fen-bu-shi-shi-jian/index.html","hash":"f6a916e2b796f89f724b018caa9c1b144caf6e0b","modified":1607589283062},{"_id":"public/pytorch/bing-xing-he-fen-bu-shi-xun-lian-in-pytorch/index.html","hash":"737e7305cf12eab004519847a2160137caa2ee0c","modified":1606386112132},{"_id":"public/categories/Pytorch/index.html","hash":"e8356f0f3dcdbda1cd47bb8e731a4e83f53a3749","modified":1607335732288},{"_id":"public/tags/Pytorch/index.html","hash":"634e6af55883df0c6f5117edc6caacb30a37d977","modified":1607335732288},{"_id":"public/tags/CUDA/index.html","hash":"e4005be7375703b8047422f592a4971bc6483994","modified":1607335732288},{"_id":"public/tags/GPU/index.html","hash":"ac16a0136078dfe6fda2f803c6c658685c7de085","modified":1607335732288},{"_id":"public/tags/NLP/index.html","hash":"9602a400454f51e6ff0f75f0cd168ed803d5200b","modified":1607334899261},{"_id":"source/_posts/LeetCode-1-100-刷题有感.md","hash":"77eea51910993543a022d21d671abebb38afe513","modified":1607587307037},{"_id":"public/sui-bi/leetcode-1-100-shua-ti-you-gan/index.html","hash":"190d338f8202a7a1f4cd33a5d5cc5e2a8ce4d38e","modified":1607587146319},{"_id":"public/archives/2020/12/index.html","hash":"dcd793120e98608ce8d77ca6c21f99613868c245","modified":1607587146319},{"_id":"source/_posts/LeetCode-115-Distinct-Subsequences.md","hash":"47d434aa4f3c7def280330dbf7ebbe9ffc6f468e","modified":1607509072722},{"_id":"source/_posts/GPU-in-Pytorch-并行和分布式实践/1.png","hash":"8a8b4c079a90aaa0773d0e98f8fdbfcc422ff4c8","modified":1607581522220},{"_id":"public/uncategorized/leetcode-115-distinct-subsequences/index.html","hash":"2cc7b7801bcfef9f6307d71660124ec392c02b72","modified":1607587146319},{"_id":"public/pytorch/gpu-in-pytorch-bing-xing-he-fen-bu-shi-shi-jian/1.png","hash":"8a8b4c079a90aaa0773d0e98f8fdbfcc422ff4c8","modified":1607587146319}],"Category":[{"name":"test","_id":"ckg7piwa500006a2813an1zrb"},{"name":"随笔","_id":"ckg7qrrd000047j280hei7e4u"},{"name":"运维","_id":"ckh35zvby0001u72804px1qo7"},{"name":"-[Algorithm] -[LeetCode]","_id":"ckhlu9lvi0001w3289jga9zva"},{"name":"-Algorithm -LeetCode","_id":"ckhluawp60000x728ehjadxzq"},{"name":"Algorithm","_id":"ckhlubjrc0000y9282so58qf8"},{"name":"LeetCode","parent":"ckhlubjrc0000y9282so58qf8","_id":"ckhlubjrd0003y928345fh68r"},{"name":"Algorithms","_id":"ckhvqydbp00016z281mnxdhf3"},{"name":"Pytorch","_id":"ckhyotdje0002l928gyqa3gxs"}],"Data":[],"Page":[{"title":"about","date":"2020-10-13T06:35:21.000Z","type":"about","layout":"about","_content":"","source":"about/index.md","raw":"---\ntitle: about\ndate: 2020-10-13 14:35:21\ntype: 'about'\nlayout: 'about'\n---\n","updated":"2020-10-13T06:35:37.056Z","path":"about/index.html","comments":1,"_id":"ckg7lu78t0000bz288lkhb9ig","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"categories","date":"2020-10-13T06:33:52.000Z","type":"categories","layout":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2020-10-13 14:33:52\ntype: 'categories'\nlayout: 'categories'\n---","updated":"2020-10-13T08:12:35.198Z","path":"categories/index.html","_id":"ckg7lu78z0002bz281zevhen5","comments":1,"content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"tags","date":"2018-09-30T10:23:38.000Z","type":"tags","layout":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2018-09-30 18:23:38\ntype: \"tags\"\nlayout: \"tags\"\n---","updated":"2020-10-13T08:13:18.016Z","path":"tags/index.html","_id":"ckg7lu7900003bz282emkhkys","comments":1,"content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"Hello World","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\ntags: test\ncategories: test\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","slug":"hello-world","published":1,"date":"2020-10-13T08:32:14.454Z","updated":"2020-10-13T08:32:14.454Z","_id":"ckg7lu78w0001bz2813ba5wh9","comments":1,"layout":"post","photos":[],"link":"","content":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><pre class=\" language-bash\"><code class=\"language-bash\">$ hexo new <span class=\"token string\">\"My New Post\"</span></code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><pre class=\" language-bash\"><code class=\"language-bash\">$ hexo server</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><pre class=\" language-bash\"><code class=\"language-bash\">$ hexo generate</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><pre class=\" language-bash\"><code class=\"language-bash\">$ hexo deploy</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><pre><code class=\"bash\">$ hexo new &quot;My New Post&quot;</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><pre><code class=\"bash\">$ hexo server</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><pre><code class=\"bash\">$ hexo generate</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><pre><code class=\"bash\">$ hexo deploy</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n"},{"title":"为什么要写博客？","date":"2020-10-13T09:00:43.000Z","summary":"姗姗来迟的博客","toc":false,"mathjax":true,"top":true,"cover":true,"_content":"\n　　我有一个很奇特的习性，就是无论做任何事情，喜欢起个大早，却赶了个晚集；究其原因，想来应该是一直以来困扰我的慵懒毛病，做什么事情都是三分钟热度，热情消退之后，很多设想都被抛诸脑后，结果留下一地鸡毛。曾经几度自我挣扎过，甚至几度接近成功，但是每次达到一定高度之后，很快就如弹簧般反弹到最初的起点。从这个角度分析，我想应该是每次跟自我的斗争时候，都没有树立一个明显的目标，结果在对付完心中的对自己的愧疚和不甘之后，很快的便沉溺于自我满足的情绪中，直到意识到自己是不是该做点什么来拯救一下自己。写博客的设想也是如此，曾经看到过很多博主在序言中论述自己为何写博客，很多文字和想法都对自己很有感触，觉得自己也完全可以学习这种做法，努力提升自己，改变自己。这就有了第一次想写blog的想法，来自于模仿，然而遗憾的是，行动力太差，这个设想也仅仅停留在设想而已；随后无论是写日记，记录此时自己的所思所想，学习笔记，工作笔记，算法刷题的小结，看书感想，电影鉴赏，动漫观后感，旅游记录等，其实都零零碎碎都产生了很多碎片化的记录，只是没有按照合理的结构组织成一份文章而已。真正意识到这种记录的整理是非常非常有必要的是在看自己的相册的时候，恍惚间自己居然已经做过这么多事情，留下了这么多图片记录，自己居然完全不记得了，而且看自己相册的过程是一个跟理想中的美好回忆完全不同的艰难历程，其中充斥着废图，毫不相干的图片，甚至毫无意义，不明所以的照片，严重干扰了自己的回忆活动，尽管iOS很贴心的用算法整理出了一些有主题的照片合辑，但是仍然杯水车薪，难以从根本上解决这个问题。每次想到，曾经经历过如此重要的经历，尽管有文字，图片甚至视频记录下来了，但是却散落在这些毫不相干的地方，自己只是一味的寻求记录，仿佛有着疯狂的收集癖的人一样，疯狂的记录着自己周遭的一切，却完全忘记了要去用心去感受和回忆经历的那些重要时刻。等到自己忘记的差不多的时候，才偶然发现有这么一个记录存在，然而这个时候再回忆的时候，发现自己心里毫无涟漪，似乎在看一个跟自己无关的人的过去那样欣赏着这些破碎的记录，每次想到这里，感觉异常可惜。\n\n<img src=\"Euphonium_Movie_2nd_KV.jpg\" width=\"50%\" height=\"50%\">\n\n　　所以便有了想要通过写blog的方式来记录下自己的经历的想法，遗憾的是，这个虽然不算很复杂的事情，自己也一直没有提上日程，直到近一年来，由于疫情的原因，开始有了足够的时间来思考这个事情，这期间也培养了自己做规划，做总结并不断完善的习惯，这多亏了非常厉害的博主[pluskid/freemind](http://freemind.pluskid.org) 的一篇[关于知识整理，积累与记忆](http://freemind.pluskid.org/misc/knowledge-accumulate/) 的优秀文章，恰逢其时的解决了我关于知识和记忆的一些困扰，所以很顺利的便开始学习和实践起来，收到了很不错的效果，如果不是由于自己间歇性的懒惰，应该还能做的更好。（关于这位厉害的博主，之前也写过不少文章，活成自己希望的样子，每次都能恰到好处的激励自己，以后一定要把这些文章都挪到这里来）\n\n　　在囤积了足够多的文章之后，也到了要准备把这些记录整理输出到页面上的时候了，没想到这个进程被我从7-8月份生生拖到了十月份，才开始着手技术选型和部署相关的事宜，虽然从定制角度，wordpress是最初的首选，但是起数据库的管理方式对于文章为主的组织方式还是觉得有点不习惯，再则很早以前小试过Hexo，上手起来还是很舒适，奇怪的是，熟悉的概念和框架，也还是花了整个下午的时间去测试，磕磕碰碰的，虽然勉强完成了，但是潜意识里对自己的高要求和鄙夷，多少还是让这个成果来的不是那么爽快。我觉得是背负了之前曾经学过这个负担，所以测试途中的任何波折都会成为一个自我否定的理由。之前也讨论过，自己总是潜意识的用否定和打压来给自己前进的动力，但是为什么一直是负反馈的逼迫，而不是正反馈的鼓励，我不得而知，仿佛给压力是前进的唯一动力，而现在我意识到这种方法并不总是有效，反而很影响心情和状态的时候，决心想要改变这个势态。从部署Blog的过程中观察到的这个小心翼翼和不自信的事实，自己这方面还有很大的进步空间。此处就不多展开了，总之，大体的框架都布置好了，只剩下文章了。在选择技术框架的时候，有个回答很好，无论是什么blog框架，坚持写作才是最重要的。提醒了我，不断的优化和改进才是完美进化的捷径，一步到位只是完美主义者的美妙幻想而已。尽管只完成了hexo和主题的最基础的修改，但是起码最重要的文章已经准备就绪，其他的细节以后一步一步的改进和优化。\n\n![](relife-1.png)\n\n　　絮絮叨叨的说了很多，好像都没有触及到自己的blog想要记录的关键，想起不久前看的一部番Relife 中的一个问题，既然认定了青春会结束，恋爱会终结，毕业之后，这一切都烟消云散，为什么此刻要执着于对方呢？这一切的意义是什么？ 我和主人公想的是一样的，就是这些对黄金时代美好事物的追求，会成为生命中最珍贵的回忆，就算最后可能一无所获，但是曾经那份雄心壮志，觉得自己无所不能的勇气，以后和最珍惜的伙伴的度过那些美好岁月，都是未来路上自己所能依仗的一切了。自己早已不复黄金岁月，每每想起，总是心酸无奈，五味杂陈。很多事情都渐渐遗忘，我无法忍受自己面对这种消逝，却选择无动于衷，总是想方设法的要留住。最终，发现还是用文字记录下来，才是对自己最好的一种交代，因为用心在写，用心在看。幸运的是，自己并没有忘记自己的梦想和追求，有很多事很多计划都亟待自己去实现，戒骄戒躁，更要戒怠惰，享受前进途中的种种风景，毕竟人生路上还有很多美好等着自己去探索，怎能因为这些微不足道的原因妨碍自己去追求和享受呢？\n\n![](relife-2.jpg)","source":"_posts/my-first-blog.md","raw":"---\ntitle: 为什么要写博客？\ndate: 2020-10-13 17:00:43\ncategories: 随笔\ntags: 随笔\nsummary: 姗姗来迟的博客\ntoc: false\nmathjax: true\n#password: \n\ntop: true\ncover: true\n\n#img: #feature image\n#coverImg: # cover roll image\n---\n\n　　我有一个很奇特的习性，就是无论做任何事情，喜欢起个大早，却赶了个晚集；究其原因，想来应该是一直以来困扰我的慵懒毛病，做什么事情都是三分钟热度，热情消退之后，很多设想都被抛诸脑后，结果留下一地鸡毛。曾经几度自我挣扎过，甚至几度接近成功，但是每次达到一定高度之后，很快就如弹簧般反弹到最初的起点。从这个角度分析，我想应该是每次跟自我的斗争时候，都没有树立一个明显的目标，结果在对付完心中的对自己的愧疚和不甘之后，很快的便沉溺于自我满足的情绪中，直到意识到自己是不是该做点什么来拯救一下自己。写博客的设想也是如此，曾经看到过很多博主在序言中论述自己为何写博客，很多文字和想法都对自己很有感触，觉得自己也完全可以学习这种做法，努力提升自己，改变自己。这就有了第一次想写blog的想法，来自于模仿，然而遗憾的是，行动力太差，这个设想也仅仅停留在设想而已；随后无论是写日记，记录此时自己的所思所想，学习笔记，工作笔记，算法刷题的小结，看书感想，电影鉴赏，动漫观后感，旅游记录等，其实都零零碎碎都产生了很多碎片化的记录，只是没有按照合理的结构组织成一份文章而已。真正意识到这种记录的整理是非常非常有必要的是在看自己的相册的时候，恍惚间自己居然已经做过这么多事情，留下了这么多图片记录，自己居然完全不记得了，而且看自己相册的过程是一个跟理想中的美好回忆完全不同的艰难历程，其中充斥着废图，毫不相干的图片，甚至毫无意义，不明所以的照片，严重干扰了自己的回忆活动，尽管iOS很贴心的用算法整理出了一些有主题的照片合辑，但是仍然杯水车薪，难以从根本上解决这个问题。每次想到，曾经经历过如此重要的经历，尽管有文字，图片甚至视频记录下来了，但是却散落在这些毫不相干的地方，自己只是一味的寻求记录，仿佛有着疯狂的收集癖的人一样，疯狂的记录着自己周遭的一切，却完全忘记了要去用心去感受和回忆经历的那些重要时刻。等到自己忘记的差不多的时候，才偶然发现有这么一个记录存在，然而这个时候再回忆的时候，发现自己心里毫无涟漪，似乎在看一个跟自己无关的人的过去那样欣赏着这些破碎的记录，每次想到这里，感觉异常可惜。\n\n<img src=\"Euphonium_Movie_2nd_KV.jpg\" width=\"50%\" height=\"50%\">\n\n　　所以便有了想要通过写blog的方式来记录下自己的经历的想法，遗憾的是，这个虽然不算很复杂的事情，自己也一直没有提上日程，直到近一年来，由于疫情的原因，开始有了足够的时间来思考这个事情，这期间也培养了自己做规划，做总结并不断完善的习惯，这多亏了非常厉害的博主[pluskid/freemind](http://freemind.pluskid.org) 的一篇[关于知识整理，积累与记忆](http://freemind.pluskid.org/misc/knowledge-accumulate/) 的优秀文章，恰逢其时的解决了我关于知识和记忆的一些困扰，所以很顺利的便开始学习和实践起来，收到了很不错的效果，如果不是由于自己间歇性的懒惰，应该还能做的更好。（关于这位厉害的博主，之前也写过不少文章，活成自己希望的样子，每次都能恰到好处的激励自己，以后一定要把这些文章都挪到这里来）\n\n　　在囤积了足够多的文章之后，也到了要准备把这些记录整理输出到页面上的时候了，没想到这个进程被我从7-8月份生生拖到了十月份，才开始着手技术选型和部署相关的事宜，虽然从定制角度，wordpress是最初的首选，但是起数据库的管理方式对于文章为主的组织方式还是觉得有点不习惯，再则很早以前小试过Hexo，上手起来还是很舒适，奇怪的是，熟悉的概念和框架，也还是花了整个下午的时间去测试，磕磕碰碰的，虽然勉强完成了，但是潜意识里对自己的高要求和鄙夷，多少还是让这个成果来的不是那么爽快。我觉得是背负了之前曾经学过这个负担，所以测试途中的任何波折都会成为一个自我否定的理由。之前也讨论过，自己总是潜意识的用否定和打压来给自己前进的动力，但是为什么一直是负反馈的逼迫，而不是正反馈的鼓励，我不得而知，仿佛给压力是前进的唯一动力，而现在我意识到这种方法并不总是有效，反而很影响心情和状态的时候，决心想要改变这个势态。从部署Blog的过程中观察到的这个小心翼翼和不自信的事实，自己这方面还有很大的进步空间。此处就不多展开了，总之，大体的框架都布置好了，只剩下文章了。在选择技术框架的时候，有个回答很好，无论是什么blog框架，坚持写作才是最重要的。提醒了我，不断的优化和改进才是完美进化的捷径，一步到位只是完美主义者的美妙幻想而已。尽管只完成了hexo和主题的最基础的修改，但是起码最重要的文章已经准备就绪，其他的细节以后一步一步的改进和优化。\n\n![](relife-1.png)\n\n　　絮絮叨叨的说了很多，好像都没有触及到自己的blog想要记录的关键，想起不久前看的一部番Relife 中的一个问题，既然认定了青春会结束，恋爱会终结，毕业之后，这一切都烟消云散，为什么此刻要执着于对方呢？这一切的意义是什么？ 我和主人公想的是一样的，就是这些对黄金时代美好事物的追求，会成为生命中最珍贵的回忆，就算最后可能一无所获，但是曾经那份雄心壮志，觉得自己无所不能的勇气，以后和最珍惜的伙伴的度过那些美好岁月，都是未来路上自己所能依仗的一切了。自己早已不复黄金岁月，每每想起，总是心酸无奈，五味杂陈。很多事情都渐渐遗忘，我无法忍受自己面对这种消逝，却选择无动于衷，总是想方设法的要留住。最终，发现还是用文字记录下来，才是对自己最好的一种交代，因为用心在写，用心在看。幸运的是，自己并没有忘记自己的梦想和追求，有很多事很多计划都亟待自己去实现，戒骄戒躁，更要戒怠惰，享受前进途中的种种风景，毕竟人生路上还有很多美好等着自己去探索，怎能因为这些微不足道的原因妨碍自己去追求和享受呢？\n\n![](relife-2.jpg)","slug":"my-first-blog","published":1,"updated":"2020-11-25T06:26:55.417Z","_id":"ckg7qjivj00037j28eqt5h8o6","comments":1,"layout":"post","photos":[],"link":"","content":"<p>　　我有一个很奇特的习性，就是无论做任何事情，喜欢起个大早，却赶了个晚集；究其原因，想来应该是一直以来困扰我的慵懒毛病，做什么事情都是三分钟热度，热情消退之后，很多设想都被抛诸脑后，结果留下一地鸡毛。曾经几度自我挣扎过，甚至几度接近成功，但是每次达到一定高度之后，很快就如弹簧般反弹到最初的起点。从这个角度分析，我想应该是每次跟自我的斗争时候，都没有树立一个明显的目标，结果在对付完心中的对自己的愧疚和不甘之后，很快的便沉溺于自我满足的情绪中，直到意识到自己是不是该做点什么来拯救一下自己。写博客的设想也是如此，曾经看到过很多博主在序言中论述自己为何写博客，很多文字和想法都对自己很有感触，觉得自己也完全可以学习这种做法，努力提升自己，改变自己。这就有了第一次想写blog的想法，来自于模仿，然而遗憾的是，行动力太差，这个设想也仅仅停留在设想而已；随后无论是写日记，记录此时自己的所思所想，学习笔记，工作笔记，算法刷题的小结，看书感想，电影鉴赏，动漫观后感，旅游记录等，其实都零零碎碎都产生了很多碎片化的记录，只是没有按照合理的结构组织成一份文章而已。真正意识到这种记录的整理是非常非常有必要的是在看自己的相册的时候，恍惚间自己居然已经做过这么多事情，留下了这么多图片记录，自己居然完全不记得了，而且看自己相册的过程是一个跟理想中的美好回忆完全不同的艰难历程，其中充斥着废图，毫不相干的图片，甚至毫无意义，不明所以的照片，严重干扰了自己的回忆活动，尽管iOS很贴心的用算法整理出了一些有主题的照片合辑，但是仍然杯水车薪，难以从根本上解决这个问题。每次想到，曾经经历过如此重要的经历，尽管有文字，图片甚至视频记录下来了，但是却散落在这些毫不相干的地方，自己只是一味的寻求记录，仿佛有着疯狂的收集癖的人一样，疯狂的记录着自己周遭的一切，却完全忘记了要去用心去感受和回忆经历的那些重要时刻。等到自己忘记的差不多的时候，才偶然发现有这么一个记录存在，然而这个时候再回忆的时候，发现自己心里毫无涟漪，似乎在看一个跟自己无关的人的过去那样欣赏着这些破碎的记录，每次想到这里，感觉异常可惜。</p>\n<img src=\"Euphonium_Movie_2nd_KV.jpg\" width=\"50%\" height=\"50%\">\n\n<p>　　所以便有了想要通过写blog的方式来记录下自己的经历的想法，遗憾的是，这个虽然不算很复杂的事情，自己也一直没有提上日程，直到近一年来，由于疫情的原因，开始有了足够的时间来思考这个事情，这期间也培养了自己做规划，做总结并不断完善的习惯，这多亏了非常厉害的博主<a href=\"http://freemind.pluskid.org/\">pluskid/freemind</a> 的一篇<a href=\"http://freemind.pluskid.org/misc/knowledge-accumulate/\">关于知识整理，积累与记忆</a> 的优秀文章，恰逢其时的解决了我关于知识和记忆的一些困扰，所以很顺利的便开始学习和实践起来，收到了很不错的效果，如果不是由于自己间歇性的懒惰，应该还能做的更好。（关于这位厉害的博主，之前也写过不少文章，活成自己希望的样子，每次都能恰到好处的激励自己，以后一定要把这些文章都挪到这里来）</p>\n<p>　　在囤积了足够多的文章之后，也到了要准备把这些记录整理输出到页面上的时候了，没想到这个进程被我从7-8月份生生拖到了十月份，才开始着手技术选型和部署相关的事宜，虽然从定制角度，wordpress是最初的首选，但是起数据库的管理方式对于文章为主的组织方式还是觉得有点不习惯，再则很早以前小试过Hexo，上手起来还是很舒适，奇怪的是，熟悉的概念和框架，也还是花了整个下午的时间去测试，磕磕碰碰的，虽然勉强完成了，但是潜意识里对自己的高要求和鄙夷，多少还是让这个成果来的不是那么爽快。我觉得是背负了之前曾经学过这个负担，所以测试途中的任何波折都会成为一个自我否定的理由。之前也讨论过，自己总是潜意识的用否定和打压来给自己前进的动力，但是为什么一直是负反馈的逼迫，而不是正反馈的鼓励，我不得而知，仿佛给压力是前进的唯一动力，而现在我意识到这种方法并不总是有效，反而很影响心情和状态的时候，决心想要改变这个势态。从部署Blog的过程中观察到的这个小心翼翼和不自信的事实，自己这方面还有很大的进步空间。此处就不多展开了，总之，大体的框架都布置好了，只剩下文章了。在选择技术框架的时候，有个回答很好，无论是什么blog框架，坚持写作才是最重要的。提醒了我，不断的优化和改进才是完美进化的捷径，一步到位只是完美主义者的美妙幻想而已。尽管只完成了hexo和主题的最基础的修改，但是起码最重要的文章已经准备就绪，其他的细节以后一步一步的改进和优化。</p>\n<p><img src=\"relife-1.png\"></p>\n<p>　　絮絮叨叨的说了很多，好像都没有触及到自己的blog想要记录的关键，想起不久前看的一部番Relife 中的一个问题，既然认定了青春会结束，恋爱会终结，毕业之后，这一切都烟消云散，为什么此刻要执着于对方呢？这一切的意义是什么？ 我和主人公想的是一样的，就是这些对黄金时代美好事物的追求，会成为生命中最珍贵的回忆，就算最后可能一无所获，但是曾经那份雄心壮志，觉得自己无所不能的勇气，以后和最珍惜的伙伴的度过那些美好岁月，都是未来路上自己所能依仗的一切了。自己早已不复黄金岁月，每每想起，总是心酸无奈，五味杂陈。很多事情都渐渐遗忘，我无法忍受自己面对这种消逝，却选择无动于衷，总是想方设法的要留住。最终，发现还是用文字记录下来，才是对自己最好的一种交代，因为用心在写，用心在看。幸运的是，自己并没有忘记自己的梦想和追求，有很多事很多计划都亟待自己去实现，戒骄戒躁，更要戒怠惰，享受前进途中的种种风景，毕竟人生路上还有很多美好等着自己去探索，怎能因为这些微不足道的原因妨碍自己去追求和享受呢？</p>\n<p><img src=\"relife-2.jpg\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p>　　我有一个很奇特的习性，就是无论做任何事情，喜欢起个大早，却赶了个晚集；究其原因，想来应该是一直以来困扰我的慵懒毛病，做什么事情都是三分钟热度，热情消退之后，很多设想都被抛诸脑后，结果留下一地鸡毛。曾经几度自我挣扎过，甚至几度接近成功，但是每次达到一定高度之后，很快就如弹簧般反弹到最初的起点。从这个角度分析，我想应该是每次跟自我的斗争时候，都没有树立一个明显的目标，结果在对付完心中的对自己的愧疚和不甘之后，很快的便沉溺于自我满足的情绪中，直到意识到自己是不是该做点什么来拯救一下自己。写博客的设想也是如此，曾经看到过很多博主在序言中论述自己为何写博客，很多文字和想法都对自己很有感触，觉得自己也完全可以学习这种做法，努力提升自己，改变自己。这就有了第一次想写blog的想法，来自于模仿，然而遗憾的是，行动力太差，这个设想也仅仅停留在设想而已；随后无论是写日记，记录此时自己的所思所想，学习笔记，工作笔记，算法刷题的小结，看书感想，电影鉴赏，动漫观后感，旅游记录等，其实都零零碎碎都产生了很多碎片化的记录，只是没有按照合理的结构组织成一份文章而已。真正意识到这种记录的整理是非常非常有必要的是在看自己的相册的时候，恍惚间自己居然已经做过这么多事情，留下了这么多图片记录，自己居然完全不记得了，而且看自己相册的过程是一个跟理想中的美好回忆完全不同的艰难历程，其中充斥着废图，毫不相干的图片，甚至毫无意义，不明所以的照片，严重干扰了自己的回忆活动，尽管iOS很贴心的用算法整理出了一些有主题的照片合辑，但是仍然杯水车薪，难以从根本上解决这个问题。每次想到，曾经经历过如此重要的经历，尽管有文字，图片甚至视频记录下来了，但是却散落在这些毫不相干的地方，自己只是一味的寻求记录，仿佛有着疯狂的收集癖的人一样，疯狂的记录着自己周遭的一切，却完全忘记了要去用心去感受和回忆经历的那些重要时刻。等到自己忘记的差不多的时候，才偶然发现有这么一个记录存在，然而这个时候再回忆的时候，发现自己心里毫无涟漪，似乎在看一个跟自己无关的人的过去那样欣赏着这些破碎的记录，每次想到这里，感觉异常可惜。</p>\n<img src=\"Euphonium_Movie_2nd_KV.jpg\" width=\"50%\" height=\"50%\">\n\n<p>　　所以便有了想要通过写blog的方式来记录下自己的经历的想法，遗憾的是，这个虽然不算很复杂的事情，自己也一直没有提上日程，直到近一年来，由于疫情的原因，开始有了足够的时间来思考这个事情，这期间也培养了自己做规划，做总结并不断完善的习惯，这多亏了非常厉害的博主<a href=\"http://freemind.pluskid.org/\">pluskid/freemind</a> 的一篇<a href=\"http://freemind.pluskid.org/misc/knowledge-accumulate/\">关于知识整理，积累与记忆</a> 的优秀文章，恰逢其时的解决了我关于知识和记忆的一些困扰，所以很顺利的便开始学习和实践起来，收到了很不错的效果，如果不是由于自己间歇性的懒惰，应该还能做的更好。（关于这位厉害的博主，之前也写过不少文章，活成自己希望的样子，每次都能恰到好处的激励自己，以后一定要把这些文章都挪到这里来）</p>\n<p>　　在囤积了足够多的文章之后，也到了要准备把这些记录整理输出到页面上的时候了，没想到这个进程被我从7-8月份生生拖到了十月份，才开始着手技术选型和部署相关的事宜，虽然从定制角度，wordpress是最初的首选，但是起数据库的管理方式对于文章为主的组织方式还是觉得有点不习惯，再则很早以前小试过Hexo，上手起来还是很舒适，奇怪的是，熟悉的概念和框架，也还是花了整个下午的时间去测试，磕磕碰碰的，虽然勉强完成了，但是潜意识里对自己的高要求和鄙夷，多少还是让这个成果来的不是那么爽快。我觉得是背负了之前曾经学过这个负担，所以测试途中的任何波折都会成为一个自我否定的理由。之前也讨论过，自己总是潜意识的用否定和打压来给自己前进的动力，但是为什么一直是负反馈的逼迫，而不是正反馈的鼓励，我不得而知，仿佛给压力是前进的唯一动力，而现在我意识到这种方法并不总是有效，反而很影响心情和状态的时候，决心想要改变这个势态。从部署Blog的过程中观察到的这个小心翼翼和不自信的事实，自己这方面还有很大的进步空间。此处就不多展开了，总之，大体的框架都布置好了，只剩下文章了。在选择技术框架的时候，有个回答很好，无论是什么blog框架，坚持写作才是最重要的。提醒了我，不断的优化和改进才是完美进化的捷径，一步到位只是完美主义者的美妙幻想而已。尽管只完成了hexo和主题的最基础的修改，但是起码最重要的文章已经准备就绪，其他的细节以后一步一步的改进和优化。</p>\n<p><img src=\"relife-1.png\"></p>\n<p>　　絮絮叨叨的说了很多，好像都没有触及到自己的blog想要记录的关键，想起不久前看的一部番Relife 中的一个问题，既然认定了青春会结束，恋爱会终结，毕业之后，这一切都烟消云散，为什么此刻要执着于对方呢？这一切的意义是什么？ 我和主人公想的是一样的，就是这些对黄金时代美好事物的追求，会成为生命中最珍贵的回忆，就算最后可能一无所获，但是曾经那份雄心壮志，觉得自己无所不能的勇气，以后和最珍惜的伙伴的度过那些美好岁月，都是未来路上自己所能依仗的一切了。自己早已不复黄金岁月，每每想起，总是心酸无奈，五味杂陈。很多事情都渐渐遗忘，我无法忍受自己面对这种消逝，却选择无动于衷，总是想方设法的要留住。最终，发现还是用文字记录下来，才是对自己最好的一种交代，因为用心在写，用心在看。幸运的是，自己并没有忘记自己的梦想和追求，有很多事很多计划都亟待自己去实现，戒骄戒躁，更要戒怠惰，享受前进途中的种种风景，毕竟人生路上还有很多美好等着自己去探索，怎能因为这些微不足道的原因妨碍自己去追求和享受呢？</p>\n<p><img src=\"relife-2.jpg\"></p>\n"},{"title":"改变与懒惰：主观能动性讨论","toc":true,"mathjax":true,"top":true,"cover":true,"date":"2020-10-27T06:52:29.000Z","_content":"\n　　前段时间，大概是在暑假期间吧，似乎因为看了很多动漫，仍然沉迷在学生时代的火热的夏天气息中。是呀！美好的夏天，如梦似幻。然而最难过的是，尽管现在的我无论想要如何去追寻这种夏日的味道，哪怕只是夏天的尾气而已，我却仍然无比清晰的认识到，自己的暑假早已经一去不复返了。这让自己陷入到一个两难的境地，一面渴望夏日的美好，一面却又警醒自己去接受事实，既无法肆意去做自我，也没办法彻底的放弃，去追寻所谓的成熟，就特别的拧巴。至于为什么总是要自己接受自己不断长大的事实，我也不甚明了，仿佛跟自己说的多了，就真的成长了一样。可能唯一让自己神伤的就只是不断变老的事实吧，或许告诉自己去年的自己跟现在的自己其实没什么太多变化和进步，那么多少有点对不起自己不断老去的时间，所以滋生出这样心理暗示，说是聊以慰己也好，说是对自己的失望继而产生的愧疚也好，总之，都是一份对自己自甘堕落，原地踏步的一种惩罚吧！所以，带着这份不甘，不明就里的开始了这份夏日的无聊幻想。\n\n![2020-07-31](IMG_9373.jpeg)\n\n　　清楚记得是在7月31日，下班从公司回家， 开车走了一条平时从来没有走过的路，天色湛蓝，久违的看到了雾状的云絮，随着落日余晖，呈现出一幅不断变幻的黄昏美景，在这个陌生的却干净的路上走着，有一种异域他乡的漂泊感油然而生。不得不感叹，人有时候就是这么的任性，假如面对突然变化的外部环境，心情不免充满了惊慌和不安；但是一旦在舒适区呆的足够久，却又开始渴望起陌生的神秘起来。所以，在那一刻，我有点爱丽丝梦游仙境的不真实感，才从二次元的幻想出来，又来到这里，心中突然诞生起一股强烈的想要去亲眼见证 命运石之门，青春猪头少年，灌篮高手，你的名字 里面二次元名场景的三次元场景的冲动。这其中当然有自己近段时间一直懒于出游，疫情原因，甚至直接放弃远行的原因，已经快有一年没有认真旅行了，去年这个时候，还沉浸在五月天鸟巢演唱会的期待中。没想到一年后，都快要忘记旅行是什么感觉了，心中只有漠然和麻木。我熟悉这种感觉，当我十年前无法面对自己的时候，我就是这样一步一步麻痹自我，无视自己的感觉，任由时间和偶然的未来来试图冲淡这份难过，虽然几次试图反抗，但是最后还是来到了自己预想中的，失去了爱和共情能力的终点。等到意识到，这样堕落下去，并不能真正解决这份难过的时候，又过去了好久好久。想要改变的时候，心中的那份激情早已经烟消云散了，当我想要去喜欢的时候，内心却做到真正意义上的毫无波澜，直到再次遇见的二次元，这种状况才好起来。所以此时，我又重新遭遇了这份想要改变，却无法调动自己的内心的尴尬，似乎在这十年间，我无意识间培养出一股冷眼旁观的特质，无论是跟自己是否相关的人和事，习惯性的把自己置身事外，试图冷静分析利弊得失，却完全丧失了**意识主观能动性**，这股自己窃以为最核心和宝贵的动力，我明白，我所尊崇的理性，作为绝对意义上的中立，统治了自己的是非观，但是一旦要去自己去亲自上阵的时候，光有理性，总是感觉差点什么东西，实践起来也并不是那么得心应手，甚至还有点举步维艰。总之，无论做什么，我缺乏一种，“当我在做这件事，我在做什么的” 的自觉性。记得不久前，自己还有强烈的这种动力想要实现梦想，却在错失良机之后，陷入长久的沉寂。当工作从长远来看不受自己控制的变得愈加明朗起来的时候，自己陷入了对未来的强烈不安之中。造成如今的局面，都是有点自己自暴自弃的成分在里面，屏蔽自己感情，活在当下，试图忘掉自己的梦想，成为了如今自己生活的主题。自我屏蔽，前面已经聊过，被证实只是一种自我麻醉而已，跟逃避没什么两样，对于解决问题，没什么卵用；活在当下，虽然并没有什么不好，但是作为浑浑噩噩度日的借口，时间久了，自己也有点不好意思起来；试图忘记梦想，真的是逃避之集大成者。兜兜转转，最后还是来到了需要认真面对自己的这个路口，时间只不过让自己忘却了些许忧伤，积攒了一点可怜的勇气而已。如果自己没有根本性的转变和思想觉悟，我并不认为自己能有所突破。\n\n　　所以，回到，发现问题，认识问题，解决问题 的模式 ！\n\n发现问题：\t\n\n​\t***生活缺乏 目标，激情和动力！***\n\n认识问题：\n\n　　按照 身边人面对这个问题的惯常思路，没有动力，给压力就好了。我并不否认这种方法的有效性，但是我一方面不认可这种方法，另一方面我本身也不是很习惯这种被迫做事的急促感，我认为扼杀了创意和生活的乐趣。曾经我也讨论过这个问题，得出的结论是，我并不适合这种压迫产生的动力；但另一方面，自己却由于过分懒散，反而落入了完全忘记自己的目标是什么的另一个极端。这样的后果就是，摸鱼到一定程度，自己甚至都开始厌恶这样无所事事的日子，想要找点事情来分散下多余的脑力，这个时候甚至反而觉得有点儿事做还挺有意思的。很遗憾的是，目前自己正处于这种状态，由于工作内容对自己来说比较容易，自己也没有适时的给自己足够的挑战，10月份以来，自己一直处于这种慵懒的状态，所以整个月的都处于基本停止的状态。仔细想来，应该是上次的面试结束之后，由于非技术原因没拿到offer，尽管情绪上来说没什么影响，但是准备算法OJ的积极性有点下降，努力想要认真刷算法题的这个弦突然就松下来了，这才导致后续很多事情都开始丧失积极性直到彻底懒得去做了。从这个角度来说，自己在战略上，还是需要足够的压力来推进自己去实践，否则很容易落入偷懒的陷阱之中。譬如，每天的总结，每周的反思，都在那之后渐渐都没有了；细想之下，我都是用，不做也没什么，晚点做也一样这种轻描淡写的借口一带而过。想来，自己也足够鸡贼，在缺乏意识驱动的事情上，是没有感情的理性机器，在这种需要理性监督的日常惯例上，却贪图一时的便利，自欺欺人起来。说到这里，猛然发现自己有一个很坑爹的问题就是，每当某些事做的很好的时候，心里洋溢着开心和满足，发自内心的对自己说，原来认真一把，也很有意思很好玩的，信誓旦旦的对自己说，记住这种感觉，明天也要这样加油努力。 转头就把这些警醒忘得一干二净，反而在满足感的驱使下开始完全放松的日子。从根源上来讲，自己这个问题应该是，目标太小，易于满足；然而另一方面，为了从长远角度保证计划的实施同时保持生活的乐趣，避免陷入某天计划没完成，直接导致后续计划的崩盘，本来就没有给每天安排很多任务，所以这样就陷入了一种矛盾的状态。 面对这个事情，我想起来之前 看到的**延迟满足**这个词，当时看的就觉得虽然意思明白，但是实践起来是什么样子，没有什么概念。现在或许能有点明白，应该是，满足感留在完成那一天，但是再接再厉的警醒，还是需要时刻提醒自己，在整个规划中，自己走到哪一步了，有了今天 的成果，明天该怎么更进一步，这种不断进步的渴求，才是敦促自己前进的最原始的动力。\n\n解决问题：\n\n　　用认真的态度过好每一天，而不是得过且过的自我敷衍。这其实是自己一直没太关注的，总是试图用全局的计划和美好的理想来敦促自己，发现仍然过不好每一天的生活。渴望进步的决心，是绝对不会变的，但是认真的态度，特别是每天都坚持保持这种精神，实际是不容易办到的，而且偶尔的放松，实际上对自己有很大影响，因为自己特别容易自暴自弃，接受那些不受控制或者并非属于自身原因的变化，享受当下认真带来的满足和乐趣，放眼明日需要改变和努力的方向，过好自己的每一天，我想也不会有那种缺乏目标，激情和动力的乏味感觉了吧！\n\n　　说到这里，这篇关于懒惰和改变的讨论应该已经得到答案了。但是彼时对夏日的怀恋和异域的沉迷并没有得到解决，似乎一场旅行是非常有必要的，虽然真的很想去日本旅游，遗憾的是年末疫情复发可能对出国旅行产生很大的影响，不过究竟是什么程度的影响，自己也并没有做过多考察，剩下几个月的时间，自己还是应该鼓起干劲，多做一些信息上的搜集和准备，努力规划一场美好的动漫朝圣之旅。\n\n![](2020-07-31.jpg)","source":"_posts/改变与懒惰.md","raw":"---\ntitle: 改变与懒惰：主观能动性讨论\ntoc: true\nmathjax: true\ntop: true\ncover: true\ndate: 2020-10-27 14:52:29\ncategories: 随笔\ntags: 随笔\n---\n\n　　前段时间，大概是在暑假期间吧，似乎因为看了很多动漫，仍然沉迷在学生时代的火热的夏天气息中。是呀！美好的夏天，如梦似幻。然而最难过的是，尽管现在的我无论想要如何去追寻这种夏日的味道，哪怕只是夏天的尾气而已，我却仍然无比清晰的认识到，自己的暑假早已经一去不复返了。这让自己陷入到一个两难的境地，一面渴望夏日的美好，一面却又警醒自己去接受事实，既无法肆意去做自我，也没办法彻底的放弃，去追寻所谓的成熟，就特别的拧巴。至于为什么总是要自己接受自己不断长大的事实，我也不甚明了，仿佛跟自己说的多了，就真的成长了一样。可能唯一让自己神伤的就只是不断变老的事实吧，或许告诉自己去年的自己跟现在的自己其实没什么太多变化和进步，那么多少有点对不起自己不断老去的时间，所以滋生出这样心理暗示，说是聊以慰己也好，说是对自己的失望继而产生的愧疚也好，总之，都是一份对自己自甘堕落，原地踏步的一种惩罚吧！所以，带着这份不甘，不明就里的开始了这份夏日的无聊幻想。\n\n![2020-07-31](IMG_9373.jpeg)\n\n　　清楚记得是在7月31日，下班从公司回家， 开车走了一条平时从来没有走过的路，天色湛蓝，久违的看到了雾状的云絮，随着落日余晖，呈现出一幅不断变幻的黄昏美景，在这个陌生的却干净的路上走着，有一种异域他乡的漂泊感油然而生。不得不感叹，人有时候就是这么的任性，假如面对突然变化的外部环境，心情不免充满了惊慌和不安；但是一旦在舒适区呆的足够久，却又开始渴望起陌生的神秘起来。所以，在那一刻，我有点爱丽丝梦游仙境的不真实感，才从二次元的幻想出来，又来到这里，心中突然诞生起一股强烈的想要去亲眼见证 命运石之门，青春猪头少年，灌篮高手，你的名字 里面二次元名场景的三次元场景的冲动。这其中当然有自己近段时间一直懒于出游，疫情原因，甚至直接放弃远行的原因，已经快有一年没有认真旅行了，去年这个时候，还沉浸在五月天鸟巢演唱会的期待中。没想到一年后，都快要忘记旅行是什么感觉了，心中只有漠然和麻木。我熟悉这种感觉，当我十年前无法面对自己的时候，我就是这样一步一步麻痹自我，无视自己的感觉，任由时间和偶然的未来来试图冲淡这份难过，虽然几次试图反抗，但是最后还是来到了自己预想中的，失去了爱和共情能力的终点。等到意识到，这样堕落下去，并不能真正解决这份难过的时候，又过去了好久好久。想要改变的时候，心中的那份激情早已经烟消云散了，当我想要去喜欢的时候，内心却做到真正意义上的毫无波澜，直到再次遇见的二次元，这种状况才好起来。所以此时，我又重新遭遇了这份想要改变，却无法调动自己的内心的尴尬，似乎在这十年间，我无意识间培养出一股冷眼旁观的特质，无论是跟自己是否相关的人和事，习惯性的把自己置身事外，试图冷静分析利弊得失，却完全丧失了**意识主观能动性**，这股自己窃以为最核心和宝贵的动力，我明白，我所尊崇的理性，作为绝对意义上的中立，统治了自己的是非观，但是一旦要去自己去亲自上阵的时候，光有理性，总是感觉差点什么东西，实践起来也并不是那么得心应手，甚至还有点举步维艰。总之，无论做什么，我缺乏一种，“当我在做这件事，我在做什么的” 的自觉性。记得不久前，自己还有强烈的这种动力想要实现梦想，却在错失良机之后，陷入长久的沉寂。当工作从长远来看不受自己控制的变得愈加明朗起来的时候，自己陷入了对未来的强烈不安之中。造成如今的局面，都是有点自己自暴自弃的成分在里面，屏蔽自己感情，活在当下，试图忘掉自己的梦想，成为了如今自己生活的主题。自我屏蔽，前面已经聊过，被证实只是一种自我麻醉而已，跟逃避没什么两样，对于解决问题，没什么卵用；活在当下，虽然并没有什么不好，但是作为浑浑噩噩度日的借口，时间久了，自己也有点不好意思起来；试图忘记梦想，真的是逃避之集大成者。兜兜转转，最后还是来到了需要认真面对自己的这个路口，时间只不过让自己忘却了些许忧伤，积攒了一点可怜的勇气而已。如果自己没有根本性的转变和思想觉悟，我并不认为自己能有所突破。\n\n　　所以，回到，发现问题，认识问题，解决问题 的模式 ！\n\n发现问题：\t\n\n​\t***生活缺乏 目标，激情和动力！***\n\n认识问题：\n\n　　按照 身边人面对这个问题的惯常思路，没有动力，给压力就好了。我并不否认这种方法的有效性，但是我一方面不认可这种方法，另一方面我本身也不是很习惯这种被迫做事的急促感，我认为扼杀了创意和生活的乐趣。曾经我也讨论过这个问题，得出的结论是，我并不适合这种压迫产生的动力；但另一方面，自己却由于过分懒散，反而落入了完全忘记自己的目标是什么的另一个极端。这样的后果就是，摸鱼到一定程度，自己甚至都开始厌恶这样无所事事的日子，想要找点事情来分散下多余的脑力，这个时候甚至反而觉得有点儿事做还挺有意思的。很遗憾的是，目前自己正处于这种状态，由于工作内容对自己来说比较容易，自己也没有适时的给自己足够的挑战，10月份以来，自己一直处于这种慵懒的状态，所以整个月的都处于基本停止的状态。仔细想来，应该是上次的面试结束之后，由于非技术原因没拿到offer，尽管情绪上来说没什么影响，但是准备算法OJ的积极性有点下降，努力想要认真刷算法题的这个弦突然就松下来了，这才导致后续很多事情都开始丧失积极性直到彻底懒得去做了。从这个角度来说，自己在战略上，还是需要足够的压力来推进自己去实践，否则很容易落入偷懒的陷阱之中。譬如，每天的总结，每周的反思，都在那之后渐渐都没有了；细想之下，我都是用，不做也没什么，晚点做也一样这种轻描淡写的借口一带而过。想来，自己也足够鸡贼，在缺乏意识驱动的事情上，是没有感情的理性机器，在这种需要理性监督的日常惯例上，却贪图一时的便利，自欺欺人起来。说到这里，猛然发现自己有一个很坑爹的问题就是，每当某些事做的很好的时候，心里洋溢着开心和满足，发自内心的对自己说，原来认真一把，也很有意思很好玩的，信誓旦旦的对自己说，记住这种感觉，明天也要这样加油努力。 转头就把这些警醒忘得一干二净，反而在满足感的驱使下开始完全放松的日子。从根源上来讲，自己这个问题应该是，目标太小，易于满足；然而另一方面，为了从长远角度保证计划的实施同时保持生活的乐趣，避免陷入某天计划没完成，直接导致后续计划的崩盘，本来就没有给每天安排很多任务，所以这样就陷入了一种矛盾的状态。 面对这个事情，我想起来之前 看到的**延迟满足**这个词，当时看的就觉得虽然意思明白，但是实践起来是什么样子，没有什么概念。现在或许能有点明白，应该是，满足感留在完成那一天，但是再接再厉的警醒，还是需要时刻提醒自己，在整个规划中，自己走到哪一步了，有了今天 的成果，明天该怎么更进一步，这种不断进步的渴求，才是敦促自己前进的最原始的动力。\n\n解决问题：\n\n　　用认真的态度过好每一天，而不是得过且过的自我敷衍。这其实是自己一直没太关注的，总是试图用全局的计划和美好的理想来敦促自己，发现仍然过不好每一天的生活。渴望进步的决心，是绝对不会变的，但是认真的态度，特别是每天都坚持保持这种精神，实际是不容易办到的，而且偶尔的放松，实际上对自己有很大影响，因为自己特别容易自暴自弃，接受那些不受控制或者并非属于自身原因的变化，享受当下认真带来的满足和乐趣，放眼明日需要改变和努力的方向，过好自己的每一天，我想也不会有那种缺乏目标，激情和动力的乏味感觉了吧！\n\n　　说到这里，这篇关于懒惰和改变的讨论应该已经得到答案了。但是彼时对夏日的怀恋和异域的沉迷并没有得到解决，似乎一场旅行是非常有必要的，虽然真的很想去日本旅游，遗憾的是年末疫情复发可能对出国旅行产生很大的影响，不过究竟是什么程度的影响，自己也并没有做过多考察，剩下几个月的时间，自己还是应该鼓起干劲，多做一些信息上的搜集和准备，努力规划一场美好的动漫朝圣之旅。\n\n![](2020-07-31.jpg)","slug":"改变与懒惰","published":1,"updated":"2020-10-28T09:57:17.910Z","_id":"ckgt6w3by0000wx280fap2hbl","comments":1,"layout":"post","photos":[],"link":"","content":"<p>　　前段时间，大概是在暑假期间吧，似乎因为看了很多动漫，仍然沉迷在学生时代的火热的夏天气息中。是呀！美好的夏天，如梦似幻。然而最难过的是，尽管现在的我无论想要如何去追寻这种夏日的味道，哪怕只是夏天的尾气而已，我却仍然无比清晰的认识到，自己的暑假早已经一去不复返了。这让自己陷入到一个两难的境地，一面渴望夏日的美好，一面却又警醒自己去接受事实，既无法肆意去做自我，也没办法彻底的放弃，去追寻所谓的成熟，就特别的拧巴。至于为什么总是要自己接受自己不断长大的事实，我也不甚明了，仿佛跟自己说的多了，就真的成长了一样。可能唯一让自己神伤的就只是不断变老的事实吧，或许告诉自己去年的自己跟现在的自己其实没什么太多变化和进步，那么多少有点对不起自己不断老去的时间，所以滋生出这样心理暗示，说是聊以慰己也好，说是对自己的失望继而产生的愧疚也好，总之，都是一份对自己自甘堕落，原地踏步的一种惩罚吧！所以，带着这份不甘，不明就里的开始了这份夏日的无聊幻想。</p>\n<p><img src=\"IMG_9373.jpeg\" alt=\"2020-07-31\"></p>\n<p>　　清楚记得是在7月31日，下班从公司回家， 开车走了一条平时从来没有走过的路，天色湛蓝，久违的看到了雾状的云絮，随着落日余晖，呈现出一幅不断变幻的黄昏美景，在这个陌生的却干净的路上走着，有一种异域他乡的漂泊感油然而生。不得不感叹，人有时候就是这么的任性，假如面对突然变化的外部环境，心情不免充满了惊慌和不安；但是一旦在舒适区呆的足够久，却又开始渴望起陌生的神秘起来。所以，在那一刻，我有点爱丽丝梦游仙境的不真实感，才从二次元的幻想出来，又来到这里，心中突然诞生起一股强烈的想要去亲眼见证 命运石之门，青春猪头少年，灌篮高手，你的名字 里面二次元名场景的三次元场景的冲动。这其中当然有自己近段时间一直懒于出游，疫情原因，甚至直接放弃远行的原因，已经快有一年没有认真旅行了，去年这个时候，还沉浸在五月天鸟巢演唱会的期待中。没想到一年后，都快要忘记旅行是什么感觉了，心中只有漠然和麻木。我熟悉这种感觉，当我十年前无法面对自己的时候，我就是这样一步一步麻痹自我，无视自己的感觉，任由时间和偶然的未来来试图冲淡这份难过，虽然几次试图反抗，但是最后还是来到了自己预想中的，失去了爱和共情能力的终点。等到意识到，这样堕落下去，并不能真正解决这份难过的时候，又过去了好久好久。想要改变的时候，心中的那份激情早已经烟消云散了，当我想要去喜欢的时候，内心却做到真正意义上的毫无波澜，直到再次遇见的二次元，这种状况才好起来。所以此时，我又重新遭遇了这份想要改变，却无法调动自己的内心的尴尬，似乎在这十年间，我无意识间培养出一股冷眼旁观的特质，无论是跟自己是否相关的人和事，习惯性的把自己置身事外，试图冷静分析利弊得失，却完全丧失了<strong>意识主观能动性</strong>，这股自己窃以为最核心和宝贵的动力，我明白，我所尊崇的理性，作为绝对意义上的中立，统治了自己的是非观，但是一旦要去自己去亲自上阵的时候，光有理性，总是感觉差点什么东西，实践起来也并不是那么得心应手，甚至还有点举步维艰。总之，无论做什么，我缺乏一种，“当我在做这件事，我在做什么的” 的自觉性。记得不久前，自己还有强烈的这种动力想要实现梦想，却在错失良机之后，陷入长久的沉寂。当工作从长远来看不受自己控制的变得愈加明朗起来的时候，自己陷入了对未来的强烈不安之中。造成如今的局面，都是有点自己自暴自弃的成分在里面，屏蔽自己感情，活在当下，试图忘掉自己的梦想，成为了如今自己生活的主题。自我屏蔽，前面已经聊过，被证实只是一种自我麻醉而已，跟逃避没什么两样，对于解决问题，没什么卵用；活在当下，虽然并没有什么不好，但是作为浑浑噩噩度日的借口，时间久了，自己也有点不好意思起来；试图忘记梦想，真的是逃避之集大成者。兜兜转转，最后还是来到了需要认真面对自己的这个路口，时间只不过让自己忘却了些许忧伤，积攒了一点可怜的勇气而已。如果自己没有根本性的转变和思想觉悟，我并不认为自己能有所突破。</p>\n<p>　　所以，回到，发现问题，认识问题，解决问题 的模式 ！</p>\n<p>发现问题：    </p>\n<p>​    <strong><em>生活缺乏 目标，激情和动力！</em></strong></p>\n<p>认识问题：</p>\n<p>　　按照 身边人面对这个问题的惯常思路，没有动力，给压力就好了。我并不否认这种方法的有效性，但是我一方面不认可这种方法，另一方面我本身也不是很习惯这种被迫做事的急促感，我认为扼杀了创意和生活的乐趣。曾经我也讨论过这个问题，得出的结论是，我并不适合这种压迫产生的动力；但另一方面，自己却由于过分懒散，反而落入了完全忘记自己的目标是什么的另一个极端。这样的后果就是，摸鱼到一定程度，自己甚至都开始厌恶这样无所事事的日子，想要找点事情来分散下多余的脑力，这个时候甚至反而觉得有点儿事做还挺有意思的。很遗憾的是，目前自己正处于这种状态，由于工作内容对自己来说比较容易，自己也没有适时的给自己足够的挑战，10月份以来，自己一直处于这种慵懒的状态，所以整个月的都处于基本停止的状态。仔细想来，应该是上次的面试结束之后，由于非技术原因没拿到offer，尽管情绪上来说没什么影响，但是准备算法OJ的积极性有点下降，努力想要认真刷算法题的这个弦突然就松下来了，这才导致后续很多事情都开始丧失积极性直到彻底懒得去做了。从这个角度来说，自己在战略上，还是需要足够的压力来推进自己去实践，否则很容易落入偷懒的陷阱之中。譬如，每天的总结，每周的反思，都在那之后渐渐都没有了；细想之下，我都是用，不做也没什么，晚点做也一样这种轻描淡写的借口一带而过。想来，自己也足够鸡贼，在缺乏意识驱动的事情上，是没有感情的理性机器，在这种需要理性监督的日常惯例上，却贪图一时的便利，自欺欺人起来。说到这里，猛然发现自己有一个很坑爹的问题就是，每当某些事做的很好的时候，心里洋溢着开心和满足，发自内心的对自己说，原来认真一把，也很有意思很好玩的，信誓旦旦的对自己说，记住这种感觉，明天也要这样加油努力。 转头就把这些警醒忘得一干二净，反而在满足感的驱使下开始完全放松的日子。从根源上来讲，自己这个问题应该是，目标太小，易于满足；然而另一方面，为了从长远角度保证计划的实施同时保持生活的乐趣，避免陷入某天计划没完成，直接导致后续计划的崩盘，本来就没有给每天安排很多任务，所以这样就陷入了一种矛盾的状态。 面对这个事情，我想起来之前 看到的<strong>延迟满足</strong>这个词，当时看的就觉得虽然意思明白，但是实践起来是什么样子，没有什么概念。现在或许能有点明白，应该是，满足感留在完成那一天，但是再接再厉的警醒，还是需要时刻提醒自己，在整个规划中，自己走到哪一步了，有了今天 的成果，明天该怎么更进一步，这种不断进步的渴求，才是敦促自己前进的最原始的动力。</p>\n<p>解决问题：</p>\n<p>　　用认真的态度过好每一天，而不是得过且过的自我敷衍。这其实是自己一直没太关注的，总是试图用全局的计划和美好的理想来敦促自己，发现仍然过不好每一天的生活。渴望进步的决心，是绝对不会变的，但是认真的态度，特别是每天都坚持保持这种精神，实际是不容易办到的，而且偶尔的放松，实际上对自己有很大影响，因为自己特别容易自暴自弃，接受那些不受控制或者并非属于自身原因的变化，享受当下认真带来的满足和乐趣，放眼明日需要改变和努力的方向，过好自己的每一天，我想也不会有那种缺乏目标，激情和动力的乏味感觉了吧！</p>\n<p>　　说到这里，这篇关于懒惰和改变的讨论应该已经得到答案了。但是彼时对夏日的怀恋和异域的沉迷并没有得到解决，似乎一场旅行是非常有必要的，虽然真的很想去日本旅游，遗憾的是年末疫情复发可能对出国旅行产生很大的影响，不过究竟是什么程度的影响，自己也并没有做过多考察，剩下几个月的时间，自己还是应该鼓起干劲，多做一些信息上的搜集和准备，努力规划一场美好的动漫朝圣之旅。</p>\n<p><img src=\"2020-07-31.jpg\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p>　　前段时间，大概是在暑假期间吧，似乎因为看了很多动漫，仍然沉迷在学生时代的火热的夏天气息中。是呀！美好的夏天，如梦似幻。然而最难过的是，尽管现在的我无论想要如何去追寻这种夏日的味道，哪怕只是夏天的尾气而已，我却仍然无比清晰的认识到，自己的暑假早已经一去不复返了。这让自己陷入到一个两难的境地，一面渴望夏日的美好，一面却又警醒自己去接受事实，既无法肆意去做自我，也没办法彻底的放弃，去追寻所谓的成熟，就特别的拧巴。至于为什么总是要自己接受自己不断长大的事实，我也不甚明了，仿佛跟自己说的多了，就真的成长了一样。可能唯一让自己神伤的就只是不断变老的事实吧，或许告诉自己去年的自己跟现在的自己其实没什么太多变化和进步，那么多少有点对不起自己不断老去的时间，所以滋生出这样心理暗示，说是聊以慰己也好，说是对自己的失望继而产生的愧疚也好，总之，都是一份对自己自甘堕落，原地踏步的一种惩罚吧！所以，带着这份不甘，不明就里的开始了这份夏日的无聊幻想。</p>\n<p><img src=\"IMG_9373.jpeg\" alt=\"2020-07-31\"></p>\n<p>　　清楚记得是在7月31日，下班从公司回家， 开车走了一条平时从来没有走过的路，天色湛蓝，久违的看到了雾状的云絮，随着落日余晖，呈现出一幅不断变幻的黄昏美景，在这个陌生的却干净的路上走着，有一种异域他乡的漂泊感油然而生。不得不感叹，人有时候就是这么的任性，假如面对突然变化的外部环境，心情不免充满了惊慌和不安；但是一旦在舒适区呆的足够久，却又开始渴望起陌生的神秘起来。所以，在那一刻，我有点爱丽丝梦游仙境的不真实感，才从二次元的幻想出来，又来到这里，心中突然诞生起一股强烈的想要去亲眼见证 命运石之门，青春猪头少年，灌篮高手，你的名字 里面二次元名场景的三次元场景的冲动。这其中当然有自己近段时间一直懒于出游，疫情原因，甚至直接放弃远行的原因，已经快有一年没有认真旅行了，去年这个时候，还沉浸在五月天鸟巢演唱会的期待中。没想到一年后，都快要忘记旅行是什么感觉了，心中只有漠然和麻木。我熟悉这种感觉，当我十年前无法面对自己的时候，我就是这样一步一步麻痹自我，无视自己的感觉，任由时间和偶然的未来来试图冲淡这份难过，虽然几次试图反抗，但是最后还是来到了自己预想中的，失去了爱和共情能力的终点。等到意识到，这样堕落下去，并不能真正解决这份难过的时候，又过去了好久好久。想要改变的时候，心中的那份激情早已经烟消云散了，当我想要去喜欢的时候，内心却做到真正意义上的毫无波澜，直到再次遇见的二次元，这种状况才好起来。所以此时，我又重新遭遇了这份想要改变，却无法调动自己的内心的尴尬，似乎在这十年间，我无意识间培养出一股冷眼旁观的特质，无论是跟自己是否相关的人和事，习惯性的把自己置身事外，试图冷静分析利弊得失，却完全丧失了<strong>意识主观能动性</strong>，这股自己窃以为最核心和宝贵的动力，我明白，我所尊崇的理性，作为绝对意义上的中立，统治了自己的是非观，但是一旦要去自己去亲自上阵的时候，光有理性，总是感觉差点什么东西，实践起来也并不是那么得心应手，甚至还有点举步维艰。总之，无论做什么，我缺乏一种，“当我在做这件事，我在做什么的” 的自觉性。记得不久前，自己还有强烈的这种动力想要实现梦想，却在错失良机之后，陷入长久的沉寂。当工作从长远来看不受自己控制的变得愈加明朗起来的时候，自己陷入了对未来的强烈不安之中。造成如今的局面，都是有点自己自暴自弃的成分在里面，屏蔽自己感情，活在当下，试图忘掉自己的梦想，成为了如今自己生活的主题。自我屏蔽，前面已经聊过，被证实只是一种自我麻醉而已，跟逃避没什么两样，对于解决问题，没什么卵用；活在当下，虽然并没有什么不好，但是作为浑浑噩噩度日的借口，时间久了，自己也有点不好意思起来；试图忘记梦想，真的是逃避之集大成者。兜兜转转，最后还是来到了需要认真面对自己的这个路口，时间只不过让自己忘却了些许忧伤，积攒了一点可怜的勇气而已。如果自己没有根本性的转变和思想觉悟，我并不认为自己能有所突破。</p>\n<p>　　所以，回到，发现问题，认识问题，解决问题 的模式 ！</p>\n<p>发现问题：    </p>\n<p>​    <strong><em>生活缺乏 目标，激情和动力！</em></strong></p>\n<p>认识问题：</p>\n<p>　　按照 身边人面对这个问题的惯常思路，没有动力，给压力就好了。我并不否认这种方法的有效性，但是我一方面不认可这种方法，另一方面我本身也不是很习惯这种被迫做事的急促感，我认为扼杀了创意和生活的乐趣。曾经我也讨论过这个问题，得出的结论是，我并不适合这种压迫产生的动力；但另一方面，自己却由于过分懒散，反而落入了完全忘记自己的目标是什么的另一个极端。这样的后果就是，摸鱼到一定程度，自己甚至都开始厌恶这样无所事事的日子，想要找点事情来分散下多余的脑力，这个时候甚至反而觉得有点儿事做还挺有意思的。很遗憾的是，目前自己正处于这种状态，由于工作内容对自己来说比较容易，自己也没有适时的给自己足够的挑战，10月份以来，自己一直处于这种慵懒的状态，所以整个月的都处于基本停止的状态。仔细想来，应该是上次的面试结束之后，由于非技术原因没拿到offer，尽管情绪上来说没什么影响，但是准备算法OJ的积极性有点下降，努力想要认真刷算法题的这个弦突然就松下来了，这才导致后续很多事情都开始丧失积极性直到彻底懒得去做了。从这个角度来说，自己在战略上，还是需要足够的压力来推进自己去实践，否则很容易落入偷懒的陷阱之中。譬如，每天的总结，每周的反思，都在那之后渐渐都没有了；细想之下，我都是用，不做也没什么，晚点做也一样这种轻描淡写的借口一带而过。想来，自己也足够鸡贼，在缺乏意识驱动的事情上，是没有感情的理性机器，在这种需要理性监督的日常惯例上，却贪图一时的便利，自欺欺人起来。说到这里，猛然发现自己有一个很坑爹的问题就是，每当某些事做的很好的时候，心里洋溢着开心和满足，发自内心的对自己说，原来认真一把，也很有意思很好玩的，信誓旦旦的对自己说，记住这种感觉，明天也要这样加油努力。 转头就把这些警醒忘得一干二净，反而在满足感的驱使下开始完全放松的日子。从根源上来讲，自己这个问题应该是，目标太小，易于满足；然而另一方面，为了从长远角度保证计划的实施同时保持生活的乐趣，避免陷入某天计划没完成，直接导致后续计划的崩盘，本来就没有给每天安排很多任务，所以这样就陷入了一种矛盾的状态。 面对这个事情，我想起来之前 看到的<strong>延迟满足</strong>这个词，当时看的就觉得虽然意思明白，但是实践起来是什么样子，没有什么概念。现在或许能有点明白，应该是，满足感留在完成那一天，但是再接再厉的警醒，还是需要时刻提醒自己，在整个规划中，自己走到哪一步了，有了今天 的成果，明天该怎么更进一步，这种不断进步的渴求，才是敦促自己前进的最原始的动力。</p>\n<p>解决问题：</p>\n<p>　　用认真的态度过好每一天，而不是得过且过的自我敷衍。这其实是自己一直没太关注的，总是试图用全局的计划和美好的理想来敦促自己，发现仍然过不好每一天的生活。渴望进步的决心，是绝对不会变的，但是认真的态度，特别是每天都坚持保持这种精神，实际是不容易办到的，而且偶尔的放松，实际上对自己有很大影响，因为自己特别容易自暴自弃，接受那些不受控制或者并非属于自身原因的变化，享受当下认真带来的满足和乐趣，放眼明日需要改变和努力的方向，过好自己的每一天，我想也不会有那种缺乏目标，激情和动力的乏味感觉了吧！</p>\n<p>　　说到这里，这篇关于懒惰和改变的讨论应该已经得到答案了。但是彼时对夏日的怀恋和异域的沉迷并没有得到解决，似乎一场旅行是非常有必要的，虽然真的很想去日本旅游，遗憾的是年末疫情复发可能对出国旅行产生很大的影响，不过究竟是什么程度的影响，自己也并没有做过多考察，剩下几个月的时间，自己还是应该鼓起干劲，多做一些信息上的搜集和准备，努力规划一场美好的动漫朝圣之旅。</p>\n<p><img src=\"2020-07-31.jpg\"></p>\n"},{"title":"记ubuntu重启引起的故障排查","toc":true,"mathjax":true,"top":false,"cover":true,"date":"2020-11-04T07:25:29.000Z","_content":"\n　　前几天，由于自己服务所在的服务器需要停机维护，运行已有近半年的ubuntu 18.04 LTS终于迎来首次重启，尽管由于预料到长时间的服务器不停机，在关机或者重启阶段会有不可预知的事件发生。但是当服务器真的出现网络无法连接的状况的时候，长时间没有安装系统的我那一刻竟然还有点懵B了，此文主要关注此次故障问题排查和解决记录，作为以后服务器维护 参考之用。\n\n### 网络故障\n\n　　重启之后，首先出现的问题就是网络接口灯直接熄灭，几番周折接上显示器和键盘之后，发现是网卡没有ipv4的地址，反而是ipv6的地址是有的。\n\n```bash\nifconfig -a # 显示所有的网卡，如果网卡无ip，单纯的用ifconfig 是无法显示的所有网卡设备的\n# or\nip a\n```\n\n\n\n　　考虑现今的网络设备ipv6基本属于摆设，所以首先定位的问题是静态ip配置没有生效（此刻持续懵B 3mins，完全忘记如何手工配置 静态ip，某些最基础的操作，还是相当依赖UI），搜索一番得到结果是\n\n```bash\nvi /etc/network/interfaces\n# change\nauto enp1s0 # enp5s0 根据ifconfig 或许实际的网卡编号\niface enp1s0 inet static\n\t\taddress x.x.x.x\n\t\tnetmask 255.255.255.0\n\t\tgateway x.x.x.x\n\t\tdns-nameservers 114.114.114.114 8.8.8.8\n\t\t\nsudo ip a flush enp1s0\nsudo systemctl restart networking.service\n```\n\n　　悲剧的是，最后重启 networking.service 并没有发现这个服务，一度陷入僵局，最后发现 Ubuntu 18.04 LTS 开始启用 netplan 组建作为网络管理器，所以这里应该使用netplan来配置静态ip，又是搜索一番\n\n```bash\nvi /etc/netplan/50-cloud-init.yaml # 对于yaml文件可是深恶痛绝，奇怪冒号后必须接空格 和 不允许 缩进\\t的设定\n# change\n# This file describes the network interfaces available on your system\n# For more information, see netplan(5).\nnetwork:\n  version: 2\n  renderer: networkd\n  ethernets:\n    enp1s0:\n     dhcp4: no\n     addresses: [192.168.1.222/24]\n     gateway4: 192.168.1.1\n     nameservers:\n       addresses: [8.8.8.8,8.8.4.4]\n       \nsudo netplan apply\n# or\nsudo netplan --dubug apply\n```\n\n　　坑爹的是，居然没有 netplan 这个命令，然而却有/etc/netplan/50-cloud-init.yaml这个配置网络的文件，而且里面都是曾经配置完好的文件，应该是上次安装就已经确定下来生成的配置文件，这个就属于很小众的问题了，徜徉 StackOverFlow 和 StackExchange 数小时之后得到答案是：从 Ubuntu 16.04  Upgrade Ubuntu 18.04 会出现 netplan 配置已经安装，但是仍然使用 /etc/network/interfaces 配置ip生效的情况。\n\n```bash\n# netplan 安装 \nsudo apt install netplan.io # 坑爹的软件包命名\n```\n\n\n\n但是在这两个文件都存在，同时都配置的情况下，仍然无法使静态ip的配置生效，最主要的是，这个服务器就是原生安装的Ubuntu 18.04 ，并不存在Upgrade 导致这个问题存在的原因。最后无奈，只能通过命令临时生效的静态ip配置，来勉强达到可以上网的目的，想来毕竟服务器24h不关机，临时配置也算可用。唯一的缺点就是，万一重启，就又不得不去机房，现场维护，颇为不便。\n\n```bash\n# 临时生效的 ip 网关 dns 配置方法\n# 以下所有配置 重启失效\nifconfig enp1s0 x.x.x.x netmask 255.255.255.0\nroute add default gw x.x.x.x\nvi /etc/resolf.conf\n\n```\n\n\n\n### nvidia-smi 以及 docker 故障\n\n　　更坑爹是，总算以为完事了的时候，发现nvidia-smi挂了，docker也挂了\n\n```bash\nnvidia-smi\nNVIDIA-SMI has failed because it couldn’t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n```\n\n### 原因分析　　\n\n　　结合之前网络问题，推测应该是重启导致的系统层面的问题。考虑到此次维护硬件只有内存的变动，通过命令查看内存是正常运作的，判断应该是 Ubuntu 系统在重启的时候发生的变化，导致这个问题发生。以ubuntu重启，nvidia 失效，作为关键词，终于找到本次故障的最终原因：Ubuntu 18.04 会自动更新linux内核，并在重启的时候自动启动最新的内核\n\n\n\n### 故障解决，切换成旧版本内核\n\n　　首先需要确认旧版本内核是否可以解决上述所有问题，最简单的切换就内核的方法是，在 grub 启动菜单里面选择,Advanced options for Ubuntu -> Ubuntu , with linux x.x.x-x-generic ， 终于找到了4.15.0-60-generic 是旧版本内核，并且上述问题全部解决，而4.15.0-122-generic则是产生故障的最新内核版本。\n\n　　那么问题来了，怎么切换内核呢？ 搜索引擎救星又来了\n\n```bash\n# 查看目前系统已安装内核\ndpkg --get-selections |grep linux-image\nlinux-image-4.15.0-122-generic\t\t\tdeinstall\nlinux-image-4.15.0-60-generic\t\t\tinstall\nlinux-image-4.15.0-62-generic\t\t\tdeinstall\n\n# 查看 grub 已经生成 菜单入口名称\ngrep menuentry /boot/grub/grub.cfg\nmenuentry 'Ubuntu, with Linux 4.15.0-60-generic' \n...\n```\n\n#### Solution 1: 修改grub启动配置\n\n```bash\nvi /etc/default/grub\n# change\nGRUB_DEFAULT=“Advanced options for Ubuntu > Ubuntu, with Linux 4.15.0-60-generic”\n# 也可以 用数字标示 0作为第一个菜单\nGRUB_DEFAULT = \"1> 4\" #改成这样\n\nGRUB_TIMEOUT_STYLE=menu # default: hidden\nGRUB_TIMEOUT=3 # default: 0\n\nsudo update-grub\nsudo reboot\n```\n\n#### Solution 2: 删除新内核\n\n```bash\n# 注意 无法删除正在使用的内核\nsudo apt remove linux-image-xxx-xx-generic\n# or\nsudo dpkg --purge linux-image-x.x.x-xx-generic\n# 安装新内核\nsudo apt install linux-headers-x.x.x-x-generic linux-image-x.x.x-x-generic\n# 关闭内核自动更新\nsudo apt-mark hold linux-image-generic linux-headers-generic\n# 开启内核自动更新\nsudo apt-mark unhold linux-image-generic linux-headers-generic\n```\n\n### Conclusion\n\n　　至此，网络问题，nvidia驱动问题，docker问题都得到圆满解决。回顾过程，心态方面还是稍微不够沉着冷静，发现问题根本原因之前，过分关注表象，诸如 netplan 的配置花费的过多时间，反而问题原因没有深入思考，导致东弄西弄一下，试图用碰运气的方式来解决问题方式终究还是有瓶颈的，或许是碰运气的方式曾经取得的成果对现在行为抉择还是产生的一定的影响，其实在陷入僵局之后的思考基本已经锁定了问题，就算是没有解决问题的彼时，心里对故障的排除已经基本有底了，最后的解决也只是水到渠成而已。\n\n\n\nReferences:\n\n[How to configure static IP address on Ubuntu 18.04 ](https://linuxconfig.org/how-to-configure-static-ip-address-on-ubuntu-18-04-bionic-beaver-linux)\n\n[How to enable netplan on ubuntu server upgraded from 16.04 to 18.04](https://askubuntu.com/questions/1034711/how-to-enable-netplan-on-ubuntu-server-upgraded-from-16-04-to-18-04)\n\n[ubuntu18.04 内核自动更新导致驱动掉了](https://blog.csdn.net/qq_43222384/article/details/90314297)","source":"_posts/记ubuntu重启引起的故障排查.md","raw":"---\ntitle: 记ubuntu重启引起的故障排查\ntoc: true\nmathjax: true\ntop: false\ncover: true\ndate: 2020-11-04 15:25:29\ncategories: 运维\ntags:\n- ubuntu\n- 故障排查\n- linux内核\n- grub\n---\n\n　　前几天，由于自己服务所在的服务器需要停机维护，运行已有近半年的ubuntu 18.04 LTS终于迎来首次重启，尽管由于预料到长时间的服务器不停机，在关机或者重启阶段会有不可预知的事件发生。但是当服务器真的出现网络无法连接的状况的时候，长时间没有安装系统的我那一刻竟然还有点懵B了，此文主要关注此次故障问题排查和解决记录，作为以后服务器维护 参考之用。\n\n### 网络故障\n\n　　重启之后，首先出现的问题就是网络接口灯直接熄灭，几番周折接上显示器和键盘之后，发现是网卡没有ipv4的地址，反而是ipv6的地址是有的。\n\n```bash\nifconfig -a # 显示所有的网卡，如果网卡无ip，单纯的用ifconfig 是无法显示的所有网卡设备的\n# or\nip a\n```\n\n\n\n　　考虑现今的网络设备ipv6基本属于摆设，所以首先定位的问题是静态ip配置没有生效（此刻持续懵B 3mins，完全忘记如何手工配置 静态ip，某些最基础的操作，还是相当依赖UI），搜索一番得到结果是\n\n```bash\nvi /etc/network/interfaces\n# change\nauto enp1s0 # enp5s0 根据ifconfig 或许实际的网卡编号\niface enp1s0 inet static\n\t\taddress x.x.x.x\n\t\tnetmask 255.255.255.0\n\t\tgateway x.x.x.x\n\t\tdns-nameservers 114.114.114.114 8.8.8.8\n\t\t\nsudo ip a flush enp1s0\nsudo systemctl restart networking.service\n```\n\n　　悲剧的是，最后重启 networking.service 并没有发现这个服务，一度陷入僵局，最后发现 Ubuntu 18.04 LTS 开始启用 netplan 组建作为网络管理器，所以这里应该使用netplan来配置静态ip，又是搜索一番\n\n```bash\nvi /etc/netplan/50-cloud-init.yaml # 对于yaml文件可是深恶痛绝，奇怪冒号后必须接空格 和 不允许 缩进\\t的设定\n# change\n# This file describes the network interfaces available on your system\n# For more information, see netplan(5).\nnetwork:\n  version: 2\n  renderer: networkd\n  ethernets:\n    enp1s0:\n     dhcp4: no\n     addresses: [192.168.1.222/24]\n     gateway4: 192.168.1.1\n     nameservers:\n       addresses: [8.8.8.8,8.8.4.4]\n       \nsudo netplan apply\n# or\nsudo netplan --dubug apply\n```\n\n　　坑爹的是，居然没有 netplan 这个命令，然而却有/etc/netplan/50-cloud-init.yaml这个配置网络的文件，而且里面都是曾经配置完好的文件，应该是上次安装就已经确定下来生成的配置文件，这个就属于很小众的问题了，徜徉 StackOverFlow 和 StackExchange 数小时之后得到答案是：从 Ubuntu 16.04  Upgrade Ubuntu 18.04 会出现 netplan 配置已经安装，但是仍然使用 /etc/network/interfaces 配置ip生效的情况。\n\n```bash\n# netplan 安装 \nsudo apt install netplan.io # 坑爹的软件包命名\n```\n\n\n\n但是在这两个文件都存在，同时都配置的情况下，仍然无法使静态ip的配置生效，最主要的是，这个服务器就是原生安装的Ubuntu 18.04 ，并不存在Upgrade 导致这个问题存在的原因。最后无奈，只能通过命令临时生效的静态ip配置，来勉强达到可以上网的目的，想来毕竟服务器24h不关机，临时配置也算可用。唯一的缺点就是，万一重启，就又不得不去机房，现场维护，颇为不便。\n\n```bash\n# 临时生效的 ip 网关 dns 配置方法\n# 以下所有配置 重启失效\nifconfig enp1s0 x.x.x.x netmask 255.255.255.0\nroute add default gw x.x.x.x\nvi /etc/resolf.conf\n\n```\n\n\n\n### nvidia-smi 以及 docker 故障\n\n　　更坑爹是，总算以为完事了的时候，发现nvidia-smi挂了，docker也挂了\n\n```bash\nnvidia-smi\nNVIDIA-SMI has failed because it couldn’t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n```\n\n### 原因分析　　\n\n　　结合之前网络问题，推测应该是重启导致的系统层面的问题。考虑到此次维护硬件只有内存的变动，通过命令查看内存是正常运作的，判断应该是 Ubuntu 系统在重启的时候发生的变化，导致这个问题发生。以ubuntu重启，nvidia 失效，作为关键词，终于找到本次故障的最终原因：Ubuntu 18.04 会自动更新linux内核，并在重启的时候自动启动最新的内核\n\n\n\n### 故障解决，切换成旧版本内核\n\n　　首先需要确认旧版本内核是否可以解决上述所有问题，最简单的切换就内核的方法是，在 grub 启动菜单里面选择,Advanced options for Ubuntu -> Ubuntu , with linux x.x.x-x-generic ， 终于找到了4.15.0-60-generic 是旧版本内核，并且上述问题全部解决，而4.15.0-122-generic则是产生故障的最新内核版本。\n\n　　那么问题来了，怎么切换内核呢？ 搜索引擎救星又来了\n\n```bash\n# 查看目前系统已安装内核\ndpkg --get-selections |grep linux-image\nlinux-image-4.15.0-122-generic\t\t\tdeinstall\nlinux-image-4.15.0-60-generic\t\t\tinstall\nlinux-image-4.15.0-62-generic\t\t\tdeinstall\n\n# 查看 grub 已经生成 菜单入口名称\ngrep menuentry /boot/grub/grub.cfg\nmenuentry 'Ubuntu, with Linux 4.15.0-60-generic' \n...\n```\n\n#### Solution 1: 修改grub启动配置\n\n```bash\nvi /etc/default/grub\n# change\nGRUB_DEFAULT=“Advanced options for Ubuntu > Ubuntu, with Linux 4.15.0-60-generic”\n# 也可以 用数字标示 0作为第一个菜单\nGRUB_DEFAULT = \"1> 4\" #改成这样\n\nGRUB_TIMEOUT_STYLE=menu # default: hidden\nGRUB_TIMEOUT=3 # default: 0\n\nsudo update-grub\nsudo reboot\n```\n\n#### Solution 2: 删除新内核\n\n```bash\n# 注意 无法删除正在使用的内核\nsudo apt remove linux-image-xxx-xx-generic\n# or\nsudo dpkg --purge linux-image-x.x.x-xx-generic\n# 安装新内核\nsudo apt install linux-headers-x.x.x-x-generic linux-image-x.x.x-x-generic\n# 关闭内核自动更新\nsudo apt-mark hold linux-image-generic linux-headers-generic\n# 开启内核自动更新\nsudo apt-mark unhold linux-image-generic linux-headers-generic\n```\n\n### Conclusion\n\n　　至此，网络问题，nvidia驱动问题，docker问题都得到圆满解决。回顾过程，心态方面还是稍微不够沉着冷静，发现问题根本原因之前，过分关注表象，诸如 netplan 的配置花费的过多时间，反而问题原因没有深入思考，导致东弄西弄一下，试图用碰运气的方式来解决问题方式终究还是有瓶颈的，或许是碰运气的方式曾经取得的成果对现在行为抉择还是产生的一定的影响，其实在陷入僵局之后的思考基本已经锁定了问题，就算是没有解决问题的彼时，心里对故障的排除已经基本有底了，最后的解决也只是水到渠成而已。\n\n\n\nReferences:\n\n[How to configure static IP address on Ubuntu 18.04 ](https://linuxconfig.org/how-to-configure-static-ip-address-on-ubuntu-18-04-bionic-beaver-linux)\n\n[How to enable netplan on ubuntu server upgraded from 16.04 to 18.04](https://askubuntu.com/questions/1034711/how-to-enable-netplan-on-ubuntu-server-upgraded-from-16-04-to-18-04)\n\n[ubuntu18.04 内核自动更新导致驱动掉了](https://blog.csdn.net/qq_43222384/article/details/90314297)","slug":"记ubuntu重启引起的故障排查","published":1,"updated":"2020-12-10T08:01:27.729Z","_id":"ckh35zvbu0000u7282nma27nl","comments":1,"layout":"post","photos":[],"link":"","content":"<p>　　前几天，由于自己服务所在的服务器需要停机维护，运行已有近半年的ubuntu 18.04 LTS终于迎来首次重启，尽管由于预料到长时间的服务器不停机，在关机或者重启阶段会有不可预知的事件发生。但是当服务器真的出现网络无法连接的状况的时候，长时间没有安装系统的我那一刻竟然还有点懵B了，此文主要关注此次故障问题排查和解决记录，作为以后服务器维护 参考之用。</p>\n<h3 id=\"网络故障\"><a href=\"#网络故障\" class=\"headerlink\" title=\"网络故障\"></a>网络故障</h3><p>　　重启之后，首先出现的问题就是网络接口灯直接熄灭，几番周折接上显示器和键盘之后，发现是网卡没有ipv4的地址，反而是ipv6的地址是有的。</p>\n<pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token function\">ifconfig</span> -a <span class=\"token comment\" spellcheck=\"true\"># 显示所有的网卡，如果网卡无ip，单纯的用ifconfig 是无法显示的所有网卡设备的</span>\n<span class=\"token comment\" spellcheck=\"true\"># or</span>\nip a</code></pre>\n<p>　　考虑现今的网络设备ipv6基本属于摆设，所以首先定位的问题是静态ip配置没有生效（此刻持续懵B 3mins，完全忘记如何手工配置 静态ip，某些最基础的操作，还是相当依赖UI），搜索一番得到结果是</p>\n<pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token function\">vi</span> /etc/network/interfaces\n<span class=\"token comment\" spellcheck=\"true\"># change</span>\nauto enp1s0 <span class=\"token comment\" spellcheck=\"true\"># enp5s0 根据ifconfig 或许实际的网卡编号</span>\niface enp1s0 inet static\n        address x.x.x.x\n        netmask 255.255.255.0\n        gateway x.x.x.x\n        dns-nameservers 114.114.114.114 8.8.8.8\n\n<span class=\"token function\">sudo</span> ip a flush enp1s0\n<span class=\"token function\">sudo</span> systemctl restart networking.service</code></pre>\n<p>　　悲剧的是，最后重启 networking.service 并没有发现这个服务，一度陷入僵局，最后发现 Ubuntu 18.04 LTS 开始启用 netplan 组建作为网络管理器，所以这里应该使用netplan来配置静态ip，又是搜索一番</p>\n<pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token function\">vi</span> /etc/netplan/50-cloud-init.yaml <span class=\"token comment\" spellcheck=\"true\"># 对于yaml文件可是深恶痛绝，奇怪冒号后必须接空格 和 不允许 缩进\\t的设定</span>\n<span class=\"token comment\" spellcheck=\"true\"># change</span>\n<span class=\"token comment\" spellcheck=\"true\"># This file describes the network interfaces available on your system</span>\n<span class=\"token comment\" spellcheck=\"true\"># For more information, see netplan(5).</span>\nnetwork:\n  version: 2\n  renderer: networkd\n  ethernets:\n    enp1s0:\n     dhcp4: no\n     addresses: <span class=\"token punctuation\">[</span>192.168.1.222/24<span class=\"token punctuation\">]</span>\n     gateway4: 192.168.1.1\n     nameservers:\n       addresses: <span class=\"token punctuation\">[</span>8.8.8.8,8.8.4.4<span class=\"token punctuation\">]</span>\n\n<span class=\"token function\">sudo</span> netplan apply\n<span class=\"token comment\" spellcheck=\"true\"># or</span>\n<span class=\"token function\">sudo</span> netplan --dubug apply</code></pre>\n<p>　　坑爹的是，居然没有 netplan 这个命令，然而却有/etc/netplan/50-cloud-init.yaml这个配置网络的文件，而且里面都是曾经配置完好的文件，应该是上次安装就已经确定下来生成的配置文件，这个就属于很小众的问题了，徜徉 StackOverFlow 和 StackExchange 数小时之后得到答案是：从 Ubuntu 16.04  Upgrade Ubuntu 18.04 会出现 netplan 配置已经安装，但是仍然使用 /etc/network/interfaces 配置ip生效的情况。</p>\n<pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token comment\" spellcheck=\"true\"># netplan 安装 </span>\n<span class=\"token function\">sudo</span> apt <span class=\"token function\">install</span> netplan.io <span class=\"token comment\" spellcheck=\"true\"># 坑爹的软件包命名</span></code></pre>\n<p>但是在这两个文件都存在，同时都配置的情况下，仍然无法使静态ip的配置生效，最主要的是，这个服务器就是原生安装的Ubuntu 18.04 ，并不存在Upgrade 导致这个问题存在的原因。最后无奈，只能通过命令临时生效的静态ip配置，来勉强达到可以上网的目的，想来毕竟服务器24h不关机，临时配置也算可用。唯一的缺点就是，万一重启，就又不得不去机房，现场维护，颇为不便。</p>\n<pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token comment\" spellcheck=\"true\"># 临时生效的 ip 网关 dns 配置方法</span>\n<span class=\"token comment\" spellcheck=\"true\"># 以下所有配置 重启失效</span>\n<span class=\"token function\">ifconfig</span> enp1s0 x.x.x.x netmask 255.255.255.0\nroute add default gw x.x.x.x\n<span class=\"token function\">vi</span> /etc/resolf.conf\n</code></pre>\n<h3 id=\"nvidia-smi-以及-docker-故障\"><a href=\"#nvidia-smi-以及-docker-故障\" class=\"headerlink\" title=\"nvidia-smi 以及 docker 故障\"></a>nvidia-smi 以及 docker 故障</h3><p>　　更坑爹是，总算以为完事了的时候，发现nvidia-smi挂了，docker也挂了</p>\n<pre class=\" language-bash\"><code class=\"language-bash\">nvidia-smi\nNVIDIA-SMI has failed because it couldn’t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.</code></pre>\n<h3 id=\"原因分析\"><a href=\"#原因分析\" class=\"headerlink\" title=\"原因分析　　\"></a>原因分析　　</h3><p>　　结合之前网络问题，推测应该是重启导致的系统层面的问题。考虑到此次维护硬件只有内存的变动，通过命令查看内存是正常运作的，判断应该是 Ubuntu 系统在重启的时候发生的变化，导致这个问题发生。以ubuntu重启，nvidia 失效，作为关键词，终于找到本次故障的最终原因：Ubuntu 18.04 会自动更新linux内核，并在重启的时候自动启动最新的内核</p>\n<h3 id=\"故障解决，切换成旧版本内核\"><a href=\"#故障解决，切换成旧版本内核\" class=\"headerlink\" title=\"故障解决，切换成旧版本内核\"></a>故障解决，切换成旧版本内核</h3><p>　　首先需要确认旧版本内核是否可以解决上述所有问题，最简单的切换就内核的方法是，在 grub 启动菜单里面选择,Advanced options for Ubuntu -&gt; Ubuntu , with linux x.x.x-x-generic ， 终于找到了4.15.0-60-generic 是旧版本内核，并且上述问题全部解决，而4.15.0-122-generic则是产生故障的最新内核版本。</p>\n<p>　　那么问题来了，怎么切换内核呢？ 搜索引擎救星又来了</p>\n<pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token comment\" spellcheck=\"true\"># 查看目前系统已安装内核</span>\ndpkg --get-selections <span class=\"token operator\">|</span><span class=\"token function\">grep</span> linux-image\nlinux-image-4.15.0-122-generic            deinstall\nlinux-image-4.15.0-60-generic            <span class=\"token function\">install</span>\nlinux-image-4.15.0-62-generic            deinstall\n\n<span class=\"token comment\" spellcheck=\"true\"># 查看 grub 已经生成 菜单入口名称</span>\n<span class=\"token function\">grep</span> menuentry /boot/grub/grub.cfg\nmenuentry <span class=\"token string\">'Ubuntu, with Linux 4.15.0-60-generic'</span> \n<span class=\"token punctuation\">..</span>.</code></pre>\n<h4 id=\"Solution-1-修改grub启动配置\"><a href=\"#Solution-1-修改grub启动配置\" class=\"headerlink\" title=\"Solution 1: 修改grub启动配置\"></a>Solution 1: 修改grub启动配置</h4><pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token function\">vi</span> /etc/default/grub\n<span class=\"token comment\" spellcheck=\"true\"># change</span>\nGRUB_DEFAULT<span class=\"token operator\">=</span>“Advanced options <span class=\"token keyword\">for</span> Ubuntu <span class=\"token operator\">></span> Ubuntu, with Linux 4.15.0-60-generic”\n<span class=\"token comment\" spellcheck=\"true\"># 也可以 用数字标示 0作为第一个菜单</span>\nGRUB_DEFAULT <span class=\"token operator\">=</span> <span class=\"token string\">\"1> 4\"</span> <span class=\"token comment\" spellcheck=\"true\">#改成这样</span>\n\nGRUB_TIMEOUT_STYLE<span class=\"token operator\">=</span>menu <span class=\"token comment\" spellcheck=\"true\"># default: hidden</span>\nGRUB_TIMEOUT<span class=\"token operator\">=</span>3 <span class=\"token comment\" spellcheck=\"true\"># default: 0</span>\n\n<span class=\"token function\">sudo</span> update-grub\n<span class=\"token function\">sudo</span> <span class=\"token function\">reboot</span></code></pre>\n<h4 id=\"Solution-2-删除新内核\"><a href=\"#Solution-2-删除新内核\" class=\"headerlink\" title=\"Solution 2: 删除新内核\"></a>Solution 2: 删除新内核</h4><pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token comment\" spellcheck=\"true\"># 注意 无法删除正在使用的内核</span>\n<span class=\"token function\">sudo</span> apt remove linux-image-xxx-xx-generic\n<span class=\"token comment\" spellcheck=\"true\"># or</span>\n<span class=\"token function\">sudo</span> dpkg --purge linux-image-x.x.x-xx-generic\n<span class=\"token comment\" spellcheck=\"true\"># 安装新内核</span>\n<span class=\"token function\">sudo</span> apt <span class=\"token function\">install</span> linux-headers-x.x.x-x-generic linux-image-x.x.x-x-generic\n<span class=\"token comment\" spellcheck=\"true\"># 关闭内核自动更新</span>\n<span class=\"token function\">sudo</span> apt-mark hold linux-image-generic linux-headers-generic\n<span class=\"token comment\" spellcheck=\"true\"># 开启内核自动更新</span>\n<span class=\"token function\">sudo</span> apt-mark unhold linux-image-generic linux-headers-generic</code></pre>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>　　至此，网络问题，nvidia驱动问题，docker问题都得到圆满解决。回顾过程，心态方面还是稍微不够沉着冷静，发现问题根本原因之前，过分关注表象，诸如 netplan 的配置花费的过多时间，反而问题原因没有深入思考，导致东弄西弄一下，试图用碰运气的方式来解决问题方式终究还是有瓶颈的，或许是碰运气的方式曾经取得的成果对现在行为抉择还是产生的一定的影响，其实在陷入僵局之后的思考基本已经锁定了问题，就算是没有解决问题的彼时，心里对故障的排除已经基本有底了，最后的解决也只是水到渠成而已。</p>\n<p>References:</p>\n<p><a href=\"https://linuxconfig.org/how-to-configure-static-ip-address-on-ubuntu-18-04-bionic-beaver-linux\">How to configure static IP address on Ubuntu 18.04 </a></p>\n<p><a href=\"https://askubuntu.com/questions/1034711/how-to-enable-netplan-on-ubuntu-server-upgraded-from-16-04-to-18-04\">How to enable netplan on ubuntu server upgraded from 16.04 to 18.04</a></p>\n<p><a href=\"https://blog.csdn.net/qq_43222384/article/details/90314297\">ubuntu18.04 内核自动更新导致驱动掉了</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>　　前几天，由于自己服务所在的服务器需要停机维护，运行已有近半年的ubuntu 18.04 LTS终于迎来首次重启，尽管由于预料到长时间的服务器不停机，在关机或者重启阶段会有不可预知的事件发生。但是当服务器真的出现网络无法连接的状况的时候，长时间没有安装系统的我那一刻竟然还有点懵B了，此文主要关注此次故障问题排查和解决记录，作为以后服务器维护 参考之用。</p>\n<h3 id=\"网络故障\"><a href=\"#网络故障\" class=\"headerlink\" title=\"网络故障\"></a>网络故障</h3><p>　　重启之后，首先出现的问题就是网络接口灯直接熄灭，几番周折接上显示器和键盘之后，发现是网卡没有ipv4的地址，反而是ipv6的地址是有的。</p>\n<pre><code class=\"bash\">ifconfig -a # 显示所有的网卡，如果网卡无ip，单纯的用ifconfig 是无法显示的所有网卡设备的\n# or\nip a</code></pre>\n<p>　　考虑现今的网络设备ipv6基本属于摆设，所以首先定位的问题是静态ip配置没有生效（此刻持续懵B 3mins，完全忘记如何手工配置 静态ip，某些最基础的操作，还是相当依赖UI），搜索一番得到结果是</p>\n<pre><code class=\"bash\">vi /etc/network/interfaces\n# change\nauto enp1s0 # enp5s0 根据ifconfig 或许实际的网卡编号\niface enp1s0 inet static\n        address x.x.x.x\n        netmask 255.255.255.0\n        gateway x.x.x.x\n        dns-nameservers 114.114.114.114 8.8.8.8\n\nsudo ip a flush enp1s0\nsudo systemctl restart networking.service</code></pre>\n<p>　　悲剧的是，最后重启 networking.service 并没有发现这个服务，一度陷入僵局，最后发现 Ubuntu 18.04 LTS 开始启用 netplan 组建作为网络管理器，所以这里应该使用netplan来配置静态ip，又是搜索一番</p>\n<pre><code class=\"bash\">vi /etc/netplan/50-cloud-init.yaml # 对于yaml文件可是深恶痛绝，奇怪冒号后必须接空格 和 不允许 缩进\\t的设定\n# change\n# This file describes the network interfaces available on your system\n# For more information, see netplan(5).\nnetwork:\n  version: 2\n  renderer: networkd\n  ethernets:\n    enp1s0:\n     dhcp4: no\n     addresses: [192.168.1.222/24]\n     gateway4: 192.168.1.1\n     nameservers:\n       addresses: [8.8.8.8,8.8.4.4]\n\nsudo netplan apply\n# or\nsudo netplan --dubug apply</code></pre>\n<p>　　坑爹的是，居然没有 netplan 这个命令，然而却有/etc/netplan/50-cloud-init.yaml这个配置网络的文件，而且里面都是曾经配置完好的文件，应该是上次安装就已经确定下来生成的配置文件，这个就属于很小众的问题了，徜徉 StackOverFlow 和 StackExchange 数小时之后得到答案是：从 Ubuntu 16.04  Upgrade Ubuntu 18.04 会出现 netplan 配置已经安装，但是仍然使用 /etc/network/interfaces 配置ip生效的情况。</p>\n<pre><code class=\"bash\"># netplan 安装 \nsudo apt install netplan.io # 坑爹的软件包命名</code></pre>\n<p>但是在这两个文件都存在，同时都配置的情况下，仍然无法使静态ip的配置生效，最主要的是，这个服务器就是原生安装的Ubuntu 18.04 ，并不存在Upgrade 导致这个问题存在的原因。最后无奈，只能通过命令临时生效的静态ip配置，来勉强达到可以上网的目的，想来毕竟服务器24h不关机，临时配置也算可用。唯一的缺点就是，万一重启，就又不得不去机房，现场维护，颇为不便。</p>\n<pre><code class=\"bash\"># 临时生效的 ip 网关 dns 配置方法\n# 以下所有配置 重启失效\nifconfig enp1s0 x.x.x.x netmask 255.255.255.0\nroute add default gw x.x.x.x\nvi /etc/resolf.conf\n</code></pre>\n<h3 id=\"nvidia-smi-以及-docker-故障\"><a href=\"#nvidia-smi-以及-docker-故障\" class=\"headerlink\" title=\"nvidia-smi 以及 docker 故障\"></a>nvidia-smi 以及 docker 故障</h3><p>　　更坑爹是，总算以为完事了的时候，发现nvidia-smi挂了，docker也挂了</p>\n<pre><code class=\"bash\">nvidia-smi\nNVIDIA-SMI has failed because it couldn’t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.</code></pre>\n<h3 id=\"原因分析\"><a href=\"#原因分析\" class=\"headerlink\" title=\"原因分析　　\"></a>原因分析　　</h3><p>　　结合之前网络问题，推测应该是重启导致的系统层面的问题。考虑到此次维护硬件只有内存的变动，通过命令查看内存是正常运作的，判断应该是 Ubuntu 系统在重启的时候发生的变化，导致这个问题发生。以ubuntu重启，nvidia 失效，作为关键词，终于找到本次故障的最终原因：Ubuntu 18.04 会自动更新linux内核，并在重启的时候自动启动最新的内核</p>\n<h3 id=\"故障解决，切换成旧版本内核\"><a href=\"#故障解决，切换成旧版本内核\" class=\"headerlink\" title=\"故障解决，切换成旧版本内核\"></a>故障解决，切换成旧版本内核</h3><p>　　首先需要确认旧版本内核是否可以解决上述所有问题，最简单的切换就内核的方法是，在 grub 启动菜单里面选择,Advanced options for Ubuntu -&gt; Ubuntu , with linux x.x.x-x-generic ， 终于找到了4.15.0-60-generic 是旧版本内核，并且上述问题全部解决，而4.15.0-122-generic则是产生故障的最新内核版本。</p>\n<p>　　那么问题来了，怎么切换内核呢？ 搜索引擎救星又来了</p>\n<pre><code class=\"bash\"># 查看目前系统已安装内核\ndpkg --get-selections |grep linux-image\nlinux-image-4.15.0-122-generic            deinstall\nlinux-image-4.15.0-60-generic            install\nlinux-image-4.15.0-62-generic            deinstall\n\n# 查看 grub 已经生成 菜单入口名称\ngrep menuentry /boot/grub/grub.cfg\nmenuentry &#39;Ubuntu, with Linux 4.15.0-60-generic&#39; \n...</code></pre>\n<h4 id=\"Solution-1-修改grub启动配置\"><a href=\"#Solution-1-修改grub启动配置\" class=\"headerlink\" title=\"Solution 1: 修改grub启动配置\"></a>Solution 1: 修改grub启动配置</h4><pre><code class=\"bash\">vi /etc/default/grub\n# change\nGRUB_DEFAULT=“Advanced options for Ubuntu &gt; Ubuntu, with Linux 4.15.0-60-generic”\n# 也可以 用数字标示 0作为第一个菜单\nGRUB_DEFAULT = &quot;1&gt; 4&quot; #改成这样\n\nGRUB_TIMEOUT_STYLE=menu # default: hidden\nGRUB_TIMEOUT=3 # default: 0\n\nsudo update-grub\nsudo reboot</code></pre>\n<h4 id=\"Solution-2-删除新内核\"><a href=\"#Solution-2-删除新内核\" class=\"headerlink\" title=\"Solution 2: 删除新内核\"></a>Solution 2: 删除新内核</h4><pre><code class=\"bash\"># 注意 无法删除正在使用的内核\nsudo apt remove linux-image-xxx-xx-generic\n# or\nsudo dpkg --purge linux-image-x.x.x-xx-generic\n# 安装新内核\nsudo apt install linux-headers-x.x.x-x-generic linux-image-x.x.x-x-generic\n# 关闭内核自动更新\nsudo apt-mark hold linux-image-generic linux-headers-generic\n# 开启内核自动更新\nsudo apt-mark unhold linux-image-generic linux-headers-generic</code></pre>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>　　至此，网络问题，nvidia驱动问题，docker问题都得到圆满解决。回顾过程，心态方面还是稍微不够沉着冷静，发现问题根本原因之前，过分关注表象，诸如 netplan 的配置花费的过多时间，反而问题原因没有深入思考，导致东弄西弄一下，试图用碰运气的方式来解决问题方式终究还是有瓶颈的，或许是碰运气的方式曾经取得的成果对现在行为抉择还是产生的一定的影响，其实在陷入僵局之后的思考基本已经锁定了问题，就算是没有解决问题的彼时，心里对故障的排除已经基本有底了，最后的解决也只是水到渠成而已。</p>\n<p>References:</p>\n<p><a href=\"https://linuxconfig.org/how-to-configure-static-ip-address-on-ubuntu-18-04-bionic-beaver-linux\">How to configure static IP address on Ubuntu 18.04 </a></p>\n<p><a href=\"https://askubuntu.com/questions/1034711/how-to-enable-netplan-on-ubuntu-server-upgraded-from-16-04-to-18-04\">How to enable netplan on ubuntu server upgraded from 16.04 to 18.04</a></p>\n<p><a href=\"https://blog.csdn.net/qq_43222384/article/details/90314297\">ubuntu18.04 内核自动更新导致驱动掉了</a></p>\n"},{"title":"LeetCode 33. Search in Rotated Sorted Array","toc":true,"mathjax":true,"top":false,"cover":true,"date":"2020-11-17T08:19:58.000Z","updated":"2020-12-10T08:01:36.687Z","_content":"\n### LeetCode 33. Search in Rotated Sorted Array\n\n![](img1.png)\n\n　　应该是二分法的变种优化，但是实际分析的过程中，自己的想法被太多的if条件给弄晕了，虽然勉强解出来了，但是更多是凭借临场发挥来作出的，至于宏观的算法原理，自己仍然理解不够透彻，借助讨论区的总结出几个好玩的解法\n\n#### 解法一\n\n　　最直接的想法是找到数组的偏移量，通过还原数组为排序数组的方式，最终利用二分法搜索target ，但是自己并没有想出可以$O(log(n))$搜索偏移量的方法，所以没有深入思考。借助讨论区的提醒，其实偏移量就是最小值的下标，所以只需要找到一种在$O(log(n))$下找到最小值的方法，这里仍然是用二分法，只不过start, mid, end 之间比较关系是不同的\n\n- start 和 mid:\n\n　mid > start : 最小值 in left\n\n![](img3.jpg)\n\n　mid < start : 最小值仍然 in left:\n\n![](img2.jpg)\n\n- mid 和 end\n\n　　mid < end: 最小值 in left，end = mid\n\n![](img6.jpg)\n\n　　mid > end : 最小值 in right, start = mid\n\n![](img7.jpg)\n\n　　只需要mid 和 end就可以利用二分法搜索最小值，只不过需要注意的一点是，可能会出现死循环\n\n![](img8.jpg)\n\n　　主要原因应该是，当最小值in right 的时候，mid 原地更新，而在in right 的情况，实际上mid所在的value不可能是最小值，所以 start = mid + 1 即可\n\n```python\nwhile start < end:\n\tmid = (start + end ) //2\n\tif nums[start] > nums[end]:\n\t\tstart = mid + 1\n\telse:\n\t\tend = mid\nbias = start\n```\n\n　　最后利用偏移量，转换mid 为 偏移后的mid，二分搜索target\n\n```python\nstart = 0\nend = len(nums) - 1\nwhile start <= end:\n\tmid = (start+end) // 2\n\tmid_pos = (mid + bias) \t% len(nums)\n\tval = nums[mid_pos]\n\tif target == value:\n\t\treturn mid_pos\n\tif target < value:\n\t\tend = mid - 1\n\telse:\n\t\tstart = mid + 1\n    \nreturn -1\n```\n\n\n\nTime Complexity: $O(log(n))$\n\nSpace Complexity: $O(1)$\n\n#### 解法二\n\n　　这里还是将被切分的2段分成2个有序数组来处理，比如,[4 5 6 7 1 2 3 ]分成 [4 5 6 7 ]和[1 2 3 ]，判断target位于其中的哪一段，然后将另一段变成-inf or inf，这样做的目的是可以用正常的二分法来搜索改变后的数组。\n\n[ 4 5 6 7 1 2 3] ，如果 target = 5，那么数组可以看做 [ 4 5 6 7 inf inf inf ]。\n\n[ 4 5 6 7 1 2 3] ，如果 target = 2，那么数组可以看做 [ -inf -inf - inf -inf 1 2 3]。\n\n　　实践阶段，只需要判断nums[mid]的val，如果val跟target同一段，val不变，如果val跟target不通段，就要变成-inf or inf\n\n- nums[mid] 和 target 同一段的条件: \n\n```python\nnums[mid] > nums[0] and target > nums[0]\nor\nnums[mid] < nums[0] and target < nums[0]\n```\n\n\n\n- nums[mid] 和 target 不通段的条件：\n\n```python\nnums[mid] > nums[0] and target < nums[0]\nor\nnums[mid] < nums[0] and target > nums[0]\n```\n\n这样动态的变更nums[mid]的val，保持nums始终与target同段段那部分不变，不同段则变为-inf or inf\n\n```python\nlo = 0\nhi = len(nums) - 1\n\nwhile lo <= hi:\n\tmid = (lo+hi) // 2\n\tval = nums[mid]\n\tif (val > nums[0] ) == (target > nums[0]):\n\t\tpass\n\telse:\n\t\tval =  float('-inf') if target < nums[0] else float('inf')\n   \n\tif val < target:\n\t\tlo = mid + 1\n\telif val > target:\n\t\thi = mid - 1\n\telse:\n\t\treturn mid\n      \n  return -1\n```\n\nTime Complexity: $O(log(n))$\n\nSpace Complexity: $O(1)$\n\n#### 解法三\n\n　　基于一个事实，数组从任意位置劈开后，至少有一半是有序的\n\n　　这里只需要判断二分法的其中一半是否有序， 再根据有序的这一部分判断target是否包含其中\n\n```python\nlo = 0\nhi = len(nums) - 1\n\nwhile lo <= hi:\n\tmid = (lo+hi) // 2\n\tval = nums[mid]\n\tif target == val:\n\t\treturn mid\n\t\t# lo-mid 有序\n\tif nums[lo] <= val:\n\t\tif target >= nums[lo] and target < nums[mid]:\n\t\t\thi = mid - 1\n\t\telse:\n\t\t\tlo = mid + 1\n\telse: # mid-hi 有序\n\t\tif target > nums[mid] and target <= nums[hi]:\n\t\t\tlo = mid + 1\n\t\telse:\n\t\t\thi = mid - 1\n```\n\nTime Complexity: $O(log(n))$\n\nSpace Complexity: $O(1)$\n\n\n\n### Conclusion\n\n　　$log(n)$的时间复杂度，基本都是二分法的变体，平日自己对二分法过于忽视，导致理解不深刻，所以这个题目做起来才比较费劲。除此之外，解法二巧妙的利用了target 和 nums[mid]是否同一个分割排序段的条件，来简化判断，同时动态的更新nums[mid]值的方法很有启发性，比自己笨拙的设置一大堆if条件，结果自己都搞不清楚状况来的优雅许多。解法一二都是利用抽取出的关键信息来还原二分搜索法，解法三，则是试图找到target分段之前，获取排序信息，再以及排序信息搜索target，题目顿时变的简单了许多。\n\n\n\nReference:\n\n[LeetCode 33. Search in Rotated Sorted Array](https://leetcode.wang/leetCode-33-Search-in-Rotated-Sorted-Array.html)\n\n","source":"_posts/LeetCode-33-Search-in-Rotated-Sorted-Array.md","raw":"---\ntitle: LeetCode 33. Search in Rotated Sorted Array\ntoc: true\nmathjax: true\ntop: false\ncover: true\ndate: 2020-11-17 16:19:58\nupdated:\ncategories: Algorithm\ntags:\n\t- Algorithm\n\t- LeetCode\n---\n\n### LeetCode 33. Search in Rotated Sorted Array\n\n![](img1.png)\n\n　　应该是二分法的变种优化，但是实际分析的过程中，自己的想法被太多的if条件给弄晕了，虽然勉强解出来了，但是更多是凭借临场发挥来作出的，至于宏观的算法原理，自己仍然理解不够透彻，借助讨论区的总结出几个好玩的解法\n\n#### 解法一\n\n　　最直接的想法是找到数组的偏移量，通过还原数组为排序数组的方式，最终利用二分法搜索target ，但是自己并没有想出可以$O(log(n))$搜索偏移量的方法，所以没有深入思考。借助讨论区的提醒，其实偏移量就是最小值的下标，所以只需要找到一种在$O(log(n))$下找到最小值的方法，这里仍然是用二分法，只不过start, mid, end 之间比较关系是不同的\n\n- start 和 mid:\n\n　mid > start : 最小值 in left\n\n![](img3.jpg)\n\n　mid < start : 最小值仍然 in left:\n\n![](img2.jpg)\n\n- mid 和 end\n\n　　mid < end: 最小值 in left，end = mid\n\n![](img6.jpg)\n\n　　mid > end : 最小值 in right, start = mid\n\n![](img7.jpg)\n\n　　只需要mid 和 end就可以利用二分法搜索最小值，只不过需要注意的一点是，可能会出现死循环\n\n![](img8.jpg)\n\n　　主要原因应该是，当最小值in right 的时候，mid 原地更新，而在in right 的情况，实际上mid所在的value不可能是最小值，所以 start = mid + 1 即可\n\n```python\nwhile start < end:\n\tmid = (start + end ) //2\n\tif nums[start] > nums[end]:\n\t\tstart = mid + 1\n\telse:\n\t\tend = mid\nbias = start\n```\n\n　　最后利用偏移量，转换mid 为 偏移后的mid，二分搜索target\n\n```python\nstart = 0\nend = len(nums) - 1\nwhile start <= end:\n\tmid = (start+end) // 2\n\tmid_pos = (mid + bias) \t% len(nums)\n\tval = nums[mid_pos]\n\tif target == value:\n\t\treturn mid_pos\n\tif target < value:\n\t\tend = mid - 1\n\telse:\n\t\tstart = mid + 1\n    \nreturn -1\n```\n\n\n\nTime Complexity: $O(log(n))$\n\nSpace Complexity: $O(1)$\n\n#### 解法二\n\n　　这里还是将被切分的2段分成2个有序数组来处理，比如,[4 5 6 7 1 2 3 ]分成 [4 5 6 7 ]和[1 2 3 ]，判断target位于其中的哪一段，然后将另一段变成-inf or inf，这样做的目的是可以用正常的二分法来搜索改变后的数组。\n\n[ 4 5 6 7 1 2 3] ，如果 target = 5，那么数组可以看做 [ 4 5 6 7 inf inf inf ]。\n\n[ 4 5 6 7 1 2 3] ，如果 target = 2，那么数组可以看做 [ -inf -inf - inf -inf 1 2 3]。\n\n　　实践阶段，只需要判断nums[mid]的val，如果val跟target同一段，val不变，如果val跟target不通段，就要变成-inf or inf\n\n- nums[mid] 和 target 同一段的条件: \n\n```python\nnums[mid] > nums[0] and target > nums[0]\nor\nnums[mid] < nums[0] and target < nums[0]\n```\n\n\n\n- nums[mid] 和 target 不通段的条件：\n\n```python\nnums[mid] > nums[0] and target < nums[0]\nor\nnums[mid] < nums[0] and target > nums[0]\n```\n\n这样动态的变更nums[mid]的val，保持nums始终与target同段段那部分不变，不同段则变为-inf or inf\n\n```python\nlo = 0\nhi = len(nums) - 1\n\nwhile lo <= hi:\n\tmid = (lo+hi) // 2\n\tval = nums[mid]\n\tif (val > nums[0] ) == (target > nums[0]):\n\t\tpass\n\telse:\n\t\tval =  float('-inf') if target < nums[0] else float('inf')\n   \n\tif val < target:\n\t\tlo = mid + 1\n\telif val > target:\n\t\thi = mid - 1\n\telse:\n\t\treturn mid\n      \n  return -1\n```\n\nTime Complexity: $O(log(n))$\n\nSpace Complexity: $O(1)$\n\n#### 解法三\n\n　　基于一个事实，数组从任意位置劈开后，至少有一半是有序的\n\n　　这里只需要判断二分法的其中一半是否有序， 再根据有序的这一部分判断target是否包含其中\n\n```python\nlo = 0\nhi = len(nums) - 1\n\nwhile lo <= hi:\n\tmid = (lo+hi) // 2\n\tval = nums[mid]\n\tif target == val:\n\t\treturn mid\n\t\t# lo-mid 有序\n\tif nums[lo] <= val:\n\t\tif target >= nums[lo] and target < nums[mid]:\n\t\t\thi = mid - 1\n\t\telse:\n\t\t\tlo = mid + 1\n\telse: # mid-hi 有序\n\t\tif target > nums[mid] and target <= nums[hi]:\n\t\t\tlo = mid + 1\n\t\telse:\n\t\t\thi = mid - 1\n```\n\nTime Complexity: $O(log(n))$\n\nSpace Complexity: $O(1)$\n\n\n\n### Conclusion\n\n　　$log(n)$的时间复杂度，基本都是二分法的变体，平日自己对二分法过于忽视，导致理解不深刻，所以这个题目做起来才比较费劲。除此之外，解法二巧妙的利用了target 和 nums[mid]是否同一个分割排序段的条件，来简化判断，同时动态的更新nums[mid]值的方法很有启发性，比自己笨拙的设置一大堆if条件，结果自己都搞不清楚状况来的优雅许多。解法一二都是利用抽取出的关键信息来还原二分搜索法，解法三，则是试图找到target分段之前，获取排序信息，再以及排序信息搜索target，题目顿时变的简单了许多。\n\n\n\nReference:\n\n[LeetCode 33. Search in Rotated Sorted Array](https://leetcode.wang/leetCode-33-Search-in-Rotated-Sorted-Array.html)\n\n","slug":"LeetCode-33-Search-in-Rotated-Sorted-Array","published":1,"_id":"ckhlu9lv60000w328appuchpc","comments":1,"layout":"post","photos":[],"link":"","content":"<h3 id=\"LeetCode-33-Search-in-Rotated-Sorted-Array\"><a href=\"#LeetCode-33-Search-in-Rotated-Sorted-Array\" class=\"headerlink\" title=\"LeetCode 33. Search in Rotated Sorted Array\"></a>LeetCode 33. Search in Rotated Sorted Array</h3><p><img src=\"img1.png\"></p>\n<p>　　应该是二分法的变种优化，但是实际分析的过程中，自己的想法被太多的if条件给弄晕了，虽然勉强解出来了，但是更多是凭借临场发挥来作出的，至于宏观的算法原理，自己仍然理解不够透彻，借助讨论区的总结出几个好玩的解法</p>\n<h4 id=\"解法一\"><a href=\"#解法一\" class=\"headerlink\" title=\"解法一\"></a>解法一</h4><p>　　最直接的想法是找到数组的偏移量，通过还原数组为排序数组的方式，最终利用二分法搜索target ，但是自己并没有想出可以$O(log(n))$搜索偏移量的方法，所以没有深入思考。借助讨论区的提醒，其实偏移量就是最小值的下标，所以只需要找到一种在$O(log(n))$下找到最小值的方法，这里仍然是用二分法，只不过start, mid, end 之间比较关系是不同的</p>\n<ul>\n<li>start 和 mid:</li>\n</ul>\n<p>　mid &gt; start : 最小值 in left</p>\n<p><img src=\"img3.jpg\"></p>\n<p>　mid &lt; start : 最小值仍然 in left:</p>\n<p><img src=\"img2.jpg\"></p>\n<ul>\n<li>mid 和 end</li>\n</ul>\n<p>　　mid &lt; end: 最小值 in left，end = mid</p>\n<p><img src=\"img6.jpg\"></p>\n<p>　　mid &gt; end : 最小值 in right, start = mid</p>\n<p><img src=\"img7.jpg\"></p>\n<p>　　只需要mid 和 end就可以利用二分法搜索最小值，只不过需要注意的一点是，可能会出现死循环</p>\n<p><img src=\"img8.jpg\"></p>\n<p>　　主要原因应该是，当最小值in right 的时候，mid 原地更新，而在in right 的情况，实际上mid所在的value不可能是最小值，所以 start = mid + 1 即可</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">while</span> start <span class=\"token operator\">&lt;</span> end<span class=\"token punctuation\">:</span>\n    mid <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>start <span class=\"token operator\">+</span> end <span class=\"token punctuation\">)</span> <span class=\"token operator\">//</span><span class=\"token number\">2</span>\n    <span class=\"token keyword\">if</span> nums<span class=\"token punctuation\">[</span>start<span class=\"token punctuation\">]</span> <span class=\"token operator\">></span> nums<span class=\"token punctuation\">[</span>end<span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n        start <span class=\"token operator\">=</span> mid <span class=\"token operator\">+</span> <span class=\"token number\">1</span>\n    <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n        end <span class=\"token operator\">=</span> mid\nbias <span class=\"token operator\">=</span> start</code></pre>\n<p>　　最后利用偏移量，转换mid 为 偏移后的mid，二分搜索target</p>\n<pre class=\" language-python\"><code class=\"language-python\">start <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\nend <span class=\"token operator\">=</span> len<span class=\"token punctuation\">(</span>nums<span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span> <span class=\"token number\">1</span>\n<span class=\"token keyword\">while</span> start <span class=\"token operator\">&lt;=</span> end<span class=\"token punctuation\">:</span>\n    mid <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>start<span class=\"token operator\">+</span>end<span class=\"token punctuation\">)</span> <span class=\"token operator\">//</span> <span class=\"token number\">2</span>\n    mid_pos <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>mid <span class=\"token operator\">+</span> bias<span class=\"token punctuation\">)</span>     <span class=\"token operator\">%</span> len<span class=\"token punctuation\">(</span>nums<span class=\"token punctuation\">)</span>\n    val <span class=\"token operator\">=</span> nums<span class=\"token punctuation\">[</span>mid_pos<span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">if</span> target <span class=\"token operator\">==</span> value<span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> mid_pos\n    <span class=\"token keyword\">if</span> target <span class=\"token operator\">&lt;</span> value<span class=\"token punctuation\">:</span>\n        end <span class=\"token operator\">=</span> mid <span class=\"token operator\">-</span> <span class=\"token number\">1</span>\n    <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n        start <span class=\"token operator\">=</span> mid <span class=\"token operator\">+</span> <span class=\"token number\">1</span>\n\n<span class=\"token keyword\">return</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span></code></pre>\n<p>Time Complexity: $O(log(n))$</p>\n<p>Space Complexity: $O(1)$</p>\n<h4 id=\"解法二\"><a href=\"#解法二\" class=\"headerlink\" title=\"解法二\"></a>解法二</h4><p>　　这里还是将被切分的2段分成2个有序数组来处理，比如,[4 5 6 7 1 2 3 ]分成 [4 5 6 7 ]和[1 2 3 ]，判断target位于其中的哪一段，然后将另一段变成-inf or inf，这样做的目的是可以用正常的二分法来搜索改变后的数组。</p>\n<p>[ 4 5 6 7 1 2 3] ，如果 target = 5，那么数组可以看做 [ 4 5 6 7 inf inf inf ]。</p>\n<p>[ 4 5 6 7 1 2 3] ，如果 target = 2，那么数组可以看做 [ -inf -inf - inf -inf 1 2 3]。</p>\n<p>　　实践阶段，只需要判断nums[mid]的val，如果val跟target同一段，val不变，如果val跟target不通段，就要变成-inf or inf</p>\n<ul>\n<li>nums[mid] 和 target 同一段的条件: </li>\n</ul>\n<pre class=\" language-python\"><code class=\"language-python\">nums<span class=\"token punctuation\">[</span>mid<span class=\"token punctuation\">]</span> <span class=\"token operator\">></span> nums<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">and</span> target <span class=\"token operator\">></span> nums<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\n<span class=\"token operator\">or</span>\nnums<span class=\"token punctuation\">[</span>mid<span class=\"token punctuation\">]</span> <span class=\"token operator\">&lt;</span> nums<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">and</span> target <span class=\"token operator\">&lt;</span> nums<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span></code></pre>\n<ul>\n<li>nums[mid] 和 target 不通段的条件：</li>\n</ul>\n<pre class=\" language-python\"><code class=\"language-python\">nums<span class=\"token punctuation\">[</span>mid<span class=\"token punctuation\">]</span> <span class=\"token operator\">></span> nums<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">and</span> target <span class=\"token operator\">&lt;</span> nums<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\n<span class=\"token operator\">or</span>\nnums<span class=\"token punctuation\">[</span>mid<span class=\"token punctuation\">]</span> <span class=\"token operator\">&lt;</span> nums<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">and</span> target <span class=\"token operator\">></span> nums<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span></code></pre>\n<p>这样动态的变更nums[mid]的val，保持nums始终与target同段段那部分不变，不同段则变为-inf or inf</p>\n<pre class=\" language-python\"><code class=\"language-python\">lo <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\nhi <span class=\"token operator\">=</span> len<span class=\"token punctuation\">(</span>nums<span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span> <span class=\"token number\">1</span>\n\n<span class=\"token keyword\">while</span> lo <span class=\"token operator\">&lt;=</span> hi<span class=\"token punctuation\">:</span>\n    mid <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>lo<span class=\"token operator\">+</span>hi<span class=\"token punctuation\">)</span> <span class=\"token operator\">//</span> <span class=\"token number\">2</span>\n    val <span class=\"token operator\">=</span> nums<span class=\"token punctuation\">[</span>mid<span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>val <span class=\"token operator\">></span> nums<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token punctuation\">)</span> <span class=\"token operator\">==</span> <span class=\"token punctuation\">(</span>target <span class=\"token operator\">></span> nums<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">pass</span>\n    <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n        val <span class=\"token operator\">=</span>  float<span class=\"token punctuation\">(</span><span class=\"token string\">'-inf'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">if</span> target <span class=\"token operator\">&lt;</span> nums<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token keyword\">else</span> float<span class=\"token punctuation\">(</span><span class=\"token string\">'inf'</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">if</span> val <span class=\"token operator\">&lt;</span> target<span class=\"token punctuation\">:</span>\n        lo <span class=\"token operator\">=</span> mid <span class=\"token operator\">+</span> <span class=\"token number\">1</span>\n    <span class=\"token keyword\">elif</span> val <span class=\"token operator\">></span> target<span class=\"token punctuation\">:</span>\n        hi <span class=\"token operator\">=</span> mid <span class=\"token operator\">-</span> <span class=\"token number\">1</span>\n    <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> mid\n\n  <span class=\"token keyword\">return</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span></code></pre>\n<p>Time Complexity: $O(log(n))$</p>\n<p>Space Complexity: $O(1)$</p>\n<h4 id=\"解法三\"><a href=\"#解法三\" class=\"headerlink\" title=\"解法三\"></a>解法三</h4><p>　　基于一个事实，数组从任意位置劈开后，至少有一半是有序的</p>\n<p>　　这里只需要判断二分法的其中一半是否有序， 再根据有序的这一部分判断target是否包含其中</p>\n<pre class=\" language-python\"><code class=\"language-python\">lo <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\nhi <span class=\"token operator\">=</span> len<span class=\"token punctuation\">(</span>nums<span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span> <span class=\"token number\">1</span>\n\n<span class=\"token keyword\">while</span> lo <span class=\"token operator\">&lt;=</span> hi<span class=\"token punctuation\">:</span>\n    mid <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>lo<span class=\"token operator\">+</span>hi<span class=\"token punctuation\">)</span> <span class=\"token operator\">//</span> <span class=\"token number\">2</span>\n    val <span class=\"token operator\">=</span> nums<span class=\"token punctuation\">[</span>mid<span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">if</span> target <span class=\"token operator\">==</span> val<span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> mid\n        <span class=\"token comment\" spellcheck=\"true\"># lo-mid 有序</span>\n    <span class=\"token keyword\">if</span> nums<span class=\"token punctuation\">[</span>lo<span class=\"token punctuation\">]</span> <span class=\"token operator\">&lt;=</span> val<span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">if</span> target <span class=\"token operator\">>=</span> nums<span class=\"token punctuation\">[</span>lo<span class=\"token punctuation\">]</span> <span class=\"token operator\">and</span> target <span class=\"token operator\">&lt;</span> nums<span class=\"token punctuation\">[</span>mid<span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n            hi <span class=\"token operator\">=</span> mid <span class=\"token operator\">-</span> <span class=\"token number\">1</span>\n        <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n            lo <span class=\"token operator\">=</span> mid <span class=\"token operator\">+</span> <span class=\"token number\">1</span>\n    <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span> <span class=\"token comment\" spellcheck=\"true\"># mid-hi 有序</span>\n        <span class=\"token keyword\">if</span> target <span class=\"token operator\">></span> nums<span class=\"token punctuation\">[</span>mid<span class=\"token punctuation\">]</span> <span class=\"token operator\">and</span> target <span class=\"token operator\">&lt;=</span> nums<span class=\"token punctuation\">[</span>hi<span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n            lo <span class=\"token operator\">=</span> mid <span class=\"token operator\">+</span> <span class=\"token number\">1</span>\n        <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n            hi <span class=\"token operator\">=</span> mid <span class=\"token operator\">-</span> <span class=\"token number\">1</span></code></pre>\n<p>Time Complexity: $O(log(n))$</p>\n<p>Space Complexity: $O(1)$</p>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>　　$log(n)$的时间复杂度，基本都是二分法的变体，平日自己对二分法过于忽视，导致理解不深刻，所以这个题目做起来才比较费劲。除此之外，解法二巧妙的利用了target 和 nums[mid]是否同一个分割排序段的条件，来简化判断，同时动态的更新nums[mid]值的方法很有启发性，比自己笨拙的设置一大堆if条件，结果自己都搞不清楚状况来的优雅许多。解法一二都是利用抽取出的关键信息来还原二分搜索法，解法三，则是试图找到target分段之前，获取排序信息，再以及排序信息搜索target，题目顿时变的简单了许多。</p>\n<p>Reference:</p>\n<p><a href=\"https://leetcode.wang/leetCode-33-Search-in-Rotated-Sorted-Array.html\">LeetCode 33. Search in Rotated Sorted Array</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"LeetCode-33-Search-in-Rotated-Sorted-Array\"><a href=\"#LeetCode-33-Search-in-Rotated-Sorted-Array\" class=\"headerlink\" title=\"LeetCode 33. Search in Rotated Sorted Array\"></a>LeetCode 33. Search in Rotated Sorted Array</h3><p><img src=\"img1.png\"></p>\n<p>　　应该是二分法的变种优化，但是实际分析的过程中，自己的想法被太多的if条件给弄晕了，虽然勉强解出来了，但是更多是凭借临场发挥来作出的，至于宏观的算法原理，自己仍然理解不够透彻，借助讨论区的总结出几个好玩的解法</p>\n<h4 id=\"解法一\"><a href=\"#解法一\" class=\"headerlink\" title=\"解法一\"></a>解法一</h4><p>　　最直接的想法是找到数组的偏移量，通过还原数组为排序数组的方式，最终利用二分法搜索target ，但是自己并没有想出可以$O(log(n))$搜索偏移量的方法，所以没有深入思考。借助讨论区的提醒，其实偏移量就是最小值的下标，所以只需要找到一种在$O(log(n))$下找到最小值的方法，这里仍然是用二分法，只不过start, mid, end 之间比较关系是不同的</p>\n<ul>\n<li>start 和 mid:</li>\n</ul>\n<p>　mid &gt; start : 最小值 in left</p>\n<p><img src=\"img3.jpg\"></p>\n<p>　mid &lt; start : 最小值仍然 in left:</p>\n<p><img src=\"img2.jpg\"></p>\n<ul>\n<li>mid 和 end</li>\n</ul>\n<p>　　mid &lt; end: 最小值 in left，end = mid</p>\n<p><img src=\"img6.jpg\"></p>\n<p>　　mid &gt; end : 最小值 in right, start = mid</p>\n<p><img src=\"img7.jpg\"></p>\n<p>　　只需要mid 和 end就可以利用二分法搜索最小值，只不过需要注意的一点是，可能会出现死循环</p>\n<p><img src=\"img8.jpg\"></p>\n<p>　　主要原因应该是，当最小值in right 的时候，mid 原地更新，而在in right 的情况，实际上mid所在的value不可能是最小值，所以 start = mid + 1 即可</p>\n<pre><code class=\"python\">while start &lt; end:\n    mid = (start + end ) //2\n    if nums[start] &gt; nums[end]:\n        start = mid + 1\n    else:\n        end = mid\nbias = start</code></pre>\n<p>　　最后利用偏移量，转换mid 为 偏移后的mid，二分搜索target</p>\n<pre><code class=\"python\">start = 0\nend = len(nums) - 1\nwhile start &lt;= end:\n    mid = (start+end) // 2\n    mid_pos = (mid + bias)     % len(nums)\n    val = nums[mid_pos]\n    if target == value:\n        return mid_pos\n    if target &lt; value:\n        end = mid - 1\n    else:\n        start = mid + 1\n\nreturn -1</code></pre>\n<p>Time Complexity: $O(log(n))$</p>\n<p>Space Complexity: $O(1)$</p>\n<h4 id=\"解法二\"><a href=\"#解法二\" class=\"headerlink\" title=\"解法二\"></a>解法二</h4><p>　　这里还是将被切分的2段分成2个有序数组来处理，比如,[4 5 6 7 1 2 3 ]分成 [4 5 6 7 ]和[1 2 3 ]，判断target位于其中的哪一段，然后将另一段变成-inf or inf，这样做的目的是可以用正常的二分法来搜索改变后的数组。</p>\n<p>[ 4 5 6 7 1 2 3] ，如果 target = 5，那么数组可以看做 [ 4 5 6 7 inf inf inf ]。</p>\n<p>[ 4 5 6 7 1 2 3] ，如果 target = 2，那么数组可以看做 [ -inf -inf - inf -inf 1 2 3]。</p>\n<p>　　实践阶段，只需要判断nums[mid]的val，如果val跟target同一段，val不变，如果val跟target不通段，就要变成-inf or inf</p>\n<ul>\n<li>nums[mid] 和 target 同一段的条件: </li>\n</ul>\n<pre><code class=\"python\">nums[mid] &gt; nums[0] and target &gt; nums[0]\nor\nnums[mid] &lt; nums[0] and target &lt; nums[0]</code></pre>\n<ul>\n<li>nums[mid] 和 target 不通段的条件：</li>\n</ul>\n<pre><code class=\"python\">nums[mid] &gt; nums[0] and target &lt; nums[0]\nor\nnums[mid] &lt; nums[0] and target &gt; nums[0]</code></pre>\n<p>这样动态的变更nums[mid]的val，保持nums始终与target同段段那部分不变，不同段则变为-inf or inf</p>\n<pre><code class=\"python\">lo = 0\nhi = len(nums) - 1\n\nwhile lo &lt;= hi:\n    mid = (lo+hi) // 2\n    val = nums[mid]\n    if (val &gt; nums[0] ) == (target &gt; nums[0]):\n        pass\n    else:\n        val =  float(&#39;-inf&#39;) if target &lt; nums[0] else float(&#39;inf&#39;)\n\n    if val &lt; target:\n        lo = mid + 1\n    elif val &gt; target:\n        hi = mid - 1\n    else:\n        return mid\n\n  return -1</code></pre>\n<p>Time Complexity: $O(log(n))$</p>\n<p>Space Complexity: $O(1)$</p>\n<h4 id=\"解法三\"><a href=\"#解法三\" class=\"headerlink\" title=\"解法三\"></a>解法三</h4><p>　　基于一个事实，数组从任意位置劈开后，至少有一半是有序的</p>\n<p>　　这里只需要判断二分法的其中一半是否有序， 再根据有序的这一部分判断target是否包含其中</p>\n<pre><code class=\"python\">lo = 0\nhi = len(nums) - 1\n\nwhile lo &lt;= hi:\n    mid = (lo+hi) // 2\n    val = nums[mid]\n    if target == val:\n        return mid\n        # lo-mid 有序\n    if nums[lo] &lt;= val:\n        if target &gt;= nums[lo] and target &lt; nums[mid]:\n            hi = mid - 1\n        else:\n            lo = mid + 1\n    else: # mid-hi 有序\n        if target &gt; nums[mid] and target &lt;= nums[hi]:\n            lo = mid + 1\n        else:\n            hi = mid - 1</code></pre>\n<p>Time Complexity: $O(log(n))$</p>\n<p>Space Complexity: $O(1)$</p>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>　　$log(n)$的时间复杂度，基本都是二分法的变体，平日自己对二分法过于忽视，导致理解不深刻，所以这个题目做起来才比较费劲。除此之外，解法二巧妙的利用了target 和 nums[mid]是否同一个分割排序段的条件，来简化判断，同时动态的更新nums[mid]值的方法很有启发性，比自己笨拙的设置一大堆if条件，结果自己都搞不清楚状况来的优雅许多。解法一二都是利用抽取出的关键信息来还原二分搜索法，解法三，则是试图找到target分段之前，获取排序信息，再以及排序信息搜索target，题目顿时变的简单了许多。</p>\n<p>Reference:</p>\n<p><a href=\"https://leetcode.wang/leetCode-33-Search-in-Rotated-Sorted-Array.html\">LeetCode 33. Search in Rotated Sorted Array</a></p>\n"},{"title":"同步和异步编程(如何并行写同一个日志文件实践)","toc":true,"mathjax":true,"top":false,"cover":true,"date":"2020-11-23T08:27:20.000Z","updated":"2020-12-10T08:01:42.373Z","_content":"\n　　最近遇到如何并行写日志文件的问题，查看之前的代码，发现是利用RabbitMQ 作为消息中转，把并行的日志流通过HTTP 请求整合成串行的方式，然后通过一个单独的 Rabbit-Consumer 来写入文件，这样就成功避免了多个进程同时写一个文件造成的加锁问题，但是我觉得日志这种I/O为主的任务，动用RabbitMQ 有点大材小用的感觉，Producer-Consumer的总体思想可以不变，但是可以用其他更加轻量级的编程模型来实现，也借此重新优化一下日志输出的，毕竟之前的日志输出感觉一片混乱，在自己熟悉代码的情况下，仍然会有些理解困难，这跟当初自己写代码的时候不重视日志输出这一块有关，甚至很多情急之下写下的print打印信息都最后都没有处理。\n\n　　在开始之前，必须要回忆一些似曾相识，却又容易混淆的基本概念，因为在不区分这些基本OS名词之前，根本就完全无法下手，不知道应该如何正确的实践，具体到每个代码跟这些概念是如何结合在一起的，基本上完全不知道，学生时代留下的坏习惯作怪，理论脱离实际，这时候也才意识到书到用时方恨少。\n\n#### 阻塞与非阻塞\n\n　　这个概念有点复杂，来自OS中线程/进程的生命周期，诸如，就绪，运行，阻塞 ，但是如果仅仅按照这个方向理解，那就比较简单，仅仅就是线程/进程调度程序根据一定的策略，把线程/进程制定相应的状态，来安排和调度CPU资源；然而在OS之中，阻塞的原因很多，并非只是单纯的IO，严格上来讲应该是跟IO类似的原因被阻塞，即请求的资源无法得到，所以这里的阻塞调用，则是更加强调的是，单线程/进程在运行时，那些一定会被阻塞的编程模式，最重要的一个就是同步的编程模式，一旦请求的资源不具备，程序立刻陷入等待，这样就会特别容易被OS识别为闲置的线程/进程，并很快被阻塞。相反，异步则是一种将等待的资源交给外部程序和硬件，要么是子线程/进程，要么就是外部设备，总之试图通过改变通信模式的方式，使主程序始终维持运行态，这样就达到不被阻塞的目的。这就对编程模式有比较大的变动，因为主程序需要应对请求资源返回的各种情况需要预先有响应的程序段来处理，也就是常见的编程过程中回调函数的编写。\n\n> 阻塞调用是指调用结果返回之前，调用者会进入阻塞状态等待。只有在得到结果之后才会返回。\n>\n> 非阻塞调用是指在不能立刻得到结果之前，该函数不会阻塞当前线程，而会立刻返回。\n\n阻塞调用：比如 socket 的 recv()，调用这个函数的线程如果没有数据返回，它会一直阻塞着，也就是 recv() 后面的代码都不会执行了，程序就停在 recv() 这里等待，所以一般把 recv() 放在单独的线程里调用。\n\n非阻塞调用：比如非阻塞socket 的 send()，调用这个函数，它只是把待发送的数据复制到TCP输出缓冲区中，就立刻返回了，线程并不会阻塞，数据有没有发出去 send() 是不知道的，不会等待它发出去才返回的。\n\n#### 并发与并行\n\n基本还记得最显著的区别\n\n> 并发是指一个时间段内，有几个程序都在同一个CPU上运行，但任意一个时刻点上只有一个程序在处理机上运行。\n>\n> 并行是指一个时间段内，有几个程序都在几个CPU上运行，任意一个时刻点上，有多个程序在同时运行，并且多道程序之间互不干扰。 两者区别如下图\n\n![](1.jpeg)\n\n![](2.jpeg)\n\n#### 同步与异步\n\n这个有点基本印象，经过很多次反复理解，依然还是没有很清晰掌握\n\n> 同步：在发出一个同步调用时，在没有得到结果之前，该调用就不返回\n>\n> 异步：在发出一个异步调用后，调用者不会立刻得到结果，该调用就返回了\n\n　　同步和阻塞， 异步和非阻塞概念非常类似，但是所涉及的范围还是有所不同，总的来讲应该是2个独立的概念体系。\n\n　　同步和异步，应该是编程模型中，约定好的一种线程/进程间互相通信的方式，同步则是比较自然的一种程序运作方式，就是自顶向下，资源缺失，主程序等待资源准备，资源就绪继续运行；然而在OS系统调度的指挥下，同步很明显的缺点就是主程序无差别的等待所有资源，很容易程序会因为一些无关紧要的情况而一直处于等待，这其中就有比较大的优化空间；所以根据这种情况一种自然的解决办法是，让程序对各种资源缺失的情况能有对应的方案，最终形成了异步的通信策略，相比于定义上的，调用之后立刻返回这样不知所云的解释，我的理解是，程序是制定了一系列某些资源缺失或者不可用时，程序应该如何运作的逻辑，并且为了保持主程序运行，把程序调用之后的所有职责都交给了子程序，即子程序调用之后，应该由子程序来负责通知主程序，而不是跟同步那样，主程序持续等待子程序准备就绪，而原地踏步；这里的主要区别就是异步时候，主程序调用之后立刻转入下一步不需要这个资源的逻辑，有点类似于，乐观锁和悲观锁的性质，不加判断的断定后续逻辑分支，直接运行。缺点是，感觉异步编程为了解决临界区问题和通信问题，有很多复杂的管控逻辑需要解决，简单来说就是很多坑。好处是，如果这些坑都能解决，程序运行效率会有比较大的提升。\n\n　　如果同步异步是编程模式的话，那么阻塞和非阻塞相对来说我认为是一个并非严格的概念，如果调用必定会导致主程序闲置，进而被置入阻塞状态，那么就是阻塞，如果调用不会导致主程序闲置，那么就是非阻塞。这其中的关键点就是OS的进程/线程调度，而不是调用的问题，然而阻塞的定义中常常会加入调用是否立刻返回的解释，常常让我跟同步异步的概念分不清楚，诚然调用不返回在当今的OS下一定会引起主程序闲置并阻塞，但是主因必然是主程序闲置等待，如果异步通信，但是仍然没有后续程序逻辑，显然异步程序也会有阻塞的风险。所以，我觉得阻塞这个更应该是一个OS的结果，而不是原因。\n\n> 阻塞和非阻塞关注的是程序等待调用结果时的状态，是闲置还是持续运行\n\n线程与进程\n\n　　曾经考试的时候定义背的滚瓜烂熟，然而却完全没有代码经验，感觉彼时都到门口了，缺乏临门一脚，直到现在又千辛万苦来回到这里，重新完成当初没有做到的事情。\n\n　　这里我在实践中，线程跟并发 搞混了，以至于以为线程之间必然是并发，所以有可能分时共享cuda 的引用，认为利用多线程就可以优化GPU利用率。最后实践中，怎么都达不到预想的目的。仔细想想，尽管同一个进程的资源可以被进程内的所有线程共享，这样如果多个线程是分时共享的，那么cuda引用作为一个共享资源，就可以达到并发使用的目的；但是这个前提是，线程必须是并发的，那么线程真的是并发的吗？如果是曾经的单核单线程CPU，答案是肯定的，现在的基本都是多核多线程CPU了，所以线程级别肯定是并行的，那么cuda引用则必然作为一个临界共享资源，被多个线程抢占使用，多线程并行退化成多线程并发，虽然预想的目的似乎达到了，但是貌似并没有达到优化的目的，至少跟之前编写的单线程基本性能是一样的。\n\n　　其次是Python的GIL锁, Global Interpreter Lock，即任何Python 线程执行，只有GIL锁的线程才可以运行，也就是说一个进程下的多线程在Python下，永远只有一个线程在运行\n\n```python\nimport threading, multiprocessing\n\ndef loop():\n\t\tx = 0\n\t\twhile True:\n\t\t\t\tx = x^1\n\t\t\nfor i in range(multiprocessing.cpu_count()):\n\t\tt = threading.Thread(target = loop)\n\t\tt.start()\n```\n\n上述程序在多核CPU下占有率只有100%，但是C++/Java改写的死循环，则可以每个核心都跑完，即4核心400%\n\n更坑的是，GIL锁每个线程只有执行100条字节码的机会，那么有效的利用cuda资源则成为空谈，因为频繁的显存数据进出，比较浪费时间。所以python下一般都是利用多个进程来实现并行，因为多个进程有独立的GIL锁，互不影响。\n\n　　惭愧的是，自己写了这么久的Python，居然才第一次面对并行化问题，才知道GIL锁有这么多繁琐的坑。理论上证实这样的不可行之后，有点失望，但是也还是保留机会来完成并行写日志这个小任务的，毕竟通过HTTP接口，再怎么并发，框架还是很好的完成的这些繁琐的坑，只需要在producer-consumer 框架下，实现并行接受，串行消费就可以了，这还是比较基础的一个改写，对自己还是有信心的。\n\n　　线程安全，进程安全，意思就是在多线程或者多进程下，可以在代码中不加改变的直接使用，库和框架会自动解决并发和并行问题。其中 queue.Queue 就是线程安全的，下面就是利用Queue来实现进程间通信的producer-consumer\n\n```python\nfrom queue import Queue\nfrom threading import Thread\n\ndef producer(out_q):\n\twhile True:\n\t\tout_q.put(data)\n        \ndef consumer(in_q):\n\twhile True:\n\t\tdata = in_q.get()\n\t\t\t...\n    \t# 通过q通知 任务完成\n      in_q.task_done()\n        \nq = Queue()\nt1 = Thread(target=consumer, args=(q,))\nt2 = Thread(taget=producer, args=(q,))\nt1.start()\nt2.start()\n\n# wait for all produced items to be consumed\nq.join()\n```\n\n\n\n　　优化点，队列的流量控制，q需要指定大小，必要时候阻塞Queue，以避免连锁效应导致程序运行失常，同时阻塞的队列也会形成死锁，通过非阻塞和设定超时时间，来处理队列满或者超时的情况，避免阻塞\n\n```python\nq = Queue(100)\n\ntry:\n\tdata = q.get(block=False)\nexcept queue.Empty:\n\tpass\n    \ntry:\n\tq.put(item, timeout=5.0)\nexcept queue.Full:\n\tlog.warning('queued item %r discarded!', item)\n```\n\n\n\n### 最终方案\n\n　　利用tornado接受并发的http请求，解析request_body之后，直接入列队伍q.put()，这里利用了tornado 的异步接受并发的特点，直接简化producer过程。接着只需要编写consumer，取出数据，写到文件即可\n\n```python\nfrom eliot import log_call, start_action, to_file\nclass LogHandler(tornado.web.RequestHandler):\n\t# eliot 直接利用装饰器来添加日志功能，只记录input, output\n\t@log_call\n\tdef post(self, *args, **kwargs):\n\t\ttry:\n\t\t\tself.timestamp = time.ctime()\n\t\t\tself.recv_json = json.loads(self.request.body, strict=False)\n\t\t\tq.put((self.timestamp, self.recv_json ))\n\t\t\toutput_dic = {\n                'status': '1',\n                'result': 'success'\n      }\n\t\texcept Exception as e:\n\t\t\tprint('LogHandler Error: ', e)\n\t\t\treturn\n          \n\n# eliot.start_action 通过 with 来记录函数段，有点繁琐，但是对于某段代码的日志记录很方便\ndef consume_msg(queue):\n\twhile True:\n\t\twith start_action(action_type='consume_msg', lenghth = queue.qsize()):\n\t\t\ttry:\n\t\t\t\ttimestamp, msg= q.get()\n\t\t\t\tstart_time = time.time()\n\t\t\t\twith start_action(action_type='save_request_body', timestamp=timestamp,msg=msg):\n\t\t\t\t\tsave_body(msg)\n\t\t\texcept Exception as e:\n\t\t\t\twith start_action(action_type='consume_msg exceptiosn', timestamp=timestamp,e=e, msg=msg):\n\t\t\t\t\tprint('consume_mgs exception : '  , e)\n                    \nconsumer = Process(target=consume_msg, args=(q, ))\nconsumer.daemon = True # 守护进程\nconsumer.start()\n```\n\n\n\n　　至此，多进程的日志记录完成，同时也用上了之前一直想要用eliot日志系统，让日志看起来清爽了许多。\n\n### RabbiMQ\n\nRabbiMQ 的方案也写进来，毕竟原理大同小异，只是初始化和连接的部分需要重新封装一下，而且适用性更广。\n\n```python\n# rabbitmq init\ncredentials = pika.PlainCredentials(RabbitMQ.USERNAME, RabbitMQ.USERPWD)\nconnection = pika.BlockingConnection(\n                pika.ConnectionParameters(RabbitMQ.HOST, RabbitMQ.PORT,\n                                          credentials=credentials)\n    )\n# channel init and declare\nchannel = connection.channel()\nchannel.exchange_declare(exchange=RabbitMQ.EXCHANGE,\n                             exchange_type=RabbitMQ.EXCHANGE_TYPE,\n                             durable=True)\nchannel.queue_declare(queue=RabbitMQ.QUEUE_NAME_ALG, durable=True)\nchannel.queue_bind(exchange=RabbitMQ.EXCHANGE,\n                       queue=RabbitMQ.QUEUE_NAME_ALG,\n                       routing_key=RabbitMQ.ROUTING_KEY)\nchannel.basic_qos(prefetch_count=1)    # 能者多劳\nchannel.basic_consume(on_message_callback=consumer,\n                          queue=RabbitMQ.QUEUE_NAME_ALG, auto_ack=False)\nprint(' [*] Waiting for messages...')\nchannel.start_consuming()\n\ndef consumer(ch, method, properties, body):\n\tbody = json.loads(body, strict=False)\n\tflag = body['flag']\n\t# process body\n  \n\t# Ack manually\n\tch.basic_ack(delivery_tag=method.delivery_tag)\n  \n\n  \n# producer.py\ntry:\n\tcredentials = pika.PlainCredentials(\n                RabbitMQ.USERNAME, RabbitMQ.USERPWD)\n\tconnection = pika.BlockingConnection(\n                pika.ConnectionParameters(RabbitMQ.HOST, RabbitMQ.PORT,\n                                          credentials=credentials)\n            )\n\tchannel = self.connection.channel()\n\tchannel.exchange_declare(exchange=RabbitMQ.EXCHANGE,                                      \t\t\t\t\t\t\t\t\t\t\t\t\t\texchange_type=RabbitMQ.EXCHANGE_TYPE,\n                                          durable=True)\nexcept Exception as e:\n\tpass\n        \nsent_msg = {...}\nbody = json.dumps(sent_msg)\nchannel.basic_publish(exchange=RabbitMQ.EXCHANGE, \t\t\t\t\n                      routing_key=RabbitMQ.ROUTING_KEY,\n                      body=body, \n                      properties=pika.BasicProperties(delivery_mode=2))\n\nchannel.close()\nconnection.close()\n```\n\n\n\n### 计算密集型 和 IO密集型的思考\n\n　　当初写论文的时候，就在机器上跑了很多类型的负载任务，最直观的感受就是，计算密集型的负载比较难找，相对于纯粹的计算任务，很多实际的任务都会因为千奇百怪的原因被消耗在IO上，导致运行效率低下。\n\n> 计算密集型，代码运行效率为主，因为主要时间都消耗在CPU上，Python的脚本语言不适合这类任务，最好用C++/Java实现\n>\n> IO密集型，大部分时间消耗在IO上，C++/Java无法发挥优势，代码量少开发效率最高的Python具备优势\n\n　　所以，Python最好最为原型开发，保持逻辑流程通顺的前提下，可以用其他高效的语言如C++/Java改写最耗时的模块，迭代优化开发。\n\n### 协程：单线程异步\n\n　　虽然Python无法利用多线程的并行优势，但是还是有优化的手段的。如果把条件限制到单线程，那么有一个很自然的优化方式就是单线程异步，也就是只用单线程的情况下，实现程序内部的并发，这个并发是程序可控的，而不是由操作系统来根据一定策略加锁。\n\n　　Python下有协程库asyncio内置了异步IO的支持。编程模型是一个消息循环，从asyncio获取EventLoop引用，把需要执行的协程作为参数传递给EventLoop，实现异步IO,下面直接给出协程版本的producer-consumer\n\n```python\nimport asyncio\nimport random\n\nasync def producer(queue, n):\n\tfor x in range(1, n+1):\n\t\tprint('producing : ', x)    \n\t\t# simulate io job\n\t\tawait asyncio.sleep(random.random())\n\t\titem = str(x)\n\t\tawait queue.put(item)\n        \nasync def consume(queue):\n\twhile True:\n\t\titem = await queue.get()\n    # process item\n    print('consuming : ', item)\n    # simulate io \n    await asyncio.sleep(random.random())\n    # notify queue that the item has been processed\n    queue.task_done()\n        \nasync def run(n):\n\tqueue = asyncio.Queue()\n  # schedule consumer\n  consumer= asyncio.ensure_future(consume(queue))\n  await producer(queue, n)\n\t# wait until consumer processed all items\n  await queue.join()\n  # consumer is still awaiting item, cancel it\n  consumer.cancel()\n    \nloop = asyncio.get_event_loop()\nloop.run_until_complete(run(10))\nloop.close()\n```\n\n\n\n### Conclusion\n\n　　至此，关于同步异步的一些知识点和实践问题梳理完毕，协程的实践经验还是有点欠缺，以后看框架源码的时候，需要多学习和思考。\n\n\n\nReferences:\n\n[Producer/consumer-examlpe_asyncio](https://asyncio.readthedocs.io/en/latest/producer_consumer.html)\n\n[协程-廖雪峰](https://www.liaoxuefeng.com/wiki/1016959663602400/1017968846697824)\n\n[进程-线程-廖雪峰](https://www.liaoxuefeng.com/wiki/1016959663602400/1017631469467456)\n\n[线程间通信-Python_CookBook](https://python3-cookbook.readthedocs.io/zh_CN/latest/c12/p03_communicating_between_threads.html)\n\n[Python下的多进程日志记录方案](https://juejin.cn/post/6844904039210024967#heading-3)\n\n","source":"_posts/同步和异步编程-如何并行写同一个日志文件实践.md","raw":"---\ntitle: 同步和异步编程(如何并行写同一个日志文件实践)\ntoc: true\nmathjax: true\ntop: false\ncover: true\ndate: 2020-11-23 16:27:20\nupdated:\ncategories: Algorithms\ntags: \n\t- Algorithms\n\t- OS\n---\n\n　　最近遇到如何并行写日志文件的问题，查看之前的代码，发现是利用RabbitMQ 作为消息中转，把并行的日志流通过HTTP 请求整合成串行的方式，然后通过一个单独的 Rabbit-Consumer 来写入文件，这样就成功避免了多个进程同时写一个文件造成的加锁问题，但是我觉得日志这种I/O为主的任务，动用RabbitMQ 有点大材小用的感觉，Producer-Consumer的总体思想可以不变，但是可以用其他更加轻量级的编程模型来实现，也借此重新优化一下日志输出的，毕竟之前的日志输出感觉一片混乱，在自己熟悉代码的情况下，仍然会有些理解困难，这跟当初自己写代码的时候不重视日志输出这一块有关，甚至很多情急之下写下的print打印信息都最后都没有处理。\n\n　　在开始之前，必须要回忆一些似曾相识，却又容易混淆的基本概念，因为在不区分这些基本OS名词之前，根本就完全无法下手，不知道应该如何正确的实践，具体到每个代码跟这些概念是如何结合在一起的，基本上完全不知道，学生时代留下的坏习惯作怪，理论脱离实际，这时候也才意识到书到用时方恨少。\n\n#### 阻塞与非阻塞\n\n　　这个概念有点复杂，来自OS中线程/进程的生命周期，诸如，就绪，运行，阻塞 ，但是如果仅仅按照这个方向理解，那就比较简单，仅仅就是线程/进程调度程序根据一定的策略，把线程/进程制定相应的状态，来安排和调度CPU资源；然而在OS之中，阻塞的原因很多，并非只是单纯的IO，严格上来讲应该是跟IO类似的原因被阻塞，即请求的资源无法得到，所以这里的阻塞调用，则是更加强调的是，单线程/进程在运行时，那些一定会被阻塞的编程模式，最重要的一个就是同步的编程模式，一旦请求的资源不具备，程序立刻陷入等待，这样就会特别容易被OS识别为闲置的线程/进程，并很快被阻塞。相反，异步则是一种将等待的资源交给外部程序和硬件，要么是子线程/进程，要么就是外部设备，总之试图通过改变通信模式的方式，使主程序始终维持运行态，这样就达到不被阻塞的目的。这就对编程模式有比较大的变动，因为主程序需要应对请求资源返回的各种情况需要预先有响应的程序段来处理，也就是常见的编程过程中回调函数的编写。\n\n> 阻塞调用是指调用结果返回之前，调用者会进入阻塞状态等待。只有在得到结果之后才会返回。\n>\n> 非阻塞调用是指在不能立刻得到结果之前，该函数不会阻塞当前线程，而会立刻返回。\n\n阻塞调用：比如 socket 的 recv()，调用这个函数的线程如果没有数据返回，它会一直阻塞着，也就是 recv() 后面的代码都不会执行了，程序就停在 recv() 这里等待，所以一般把 recv() 放在单独的线程里调用。\n\n非阻塞调用：比如非阻塞socket 的 send()，调用这个函数，它只是把待发送的数据复制到TCP输出缓冲区中，就立刻返回了，线程并不会阻塞，数据有没有发出去 send() 是不知道的，不会等待它发出去才返回的。\n\n#### 并发与并行\n\n基本还记得最显著的区别\n\n> 并发是指一个时间段内，有几个程序都在同一个CPU上运行，但任意一个时刻点上只有一个程序在处理机上运行。\n>\n> 并行是指一个时间段内，有几个程序都在几个CPU上运行，任意一个时刻点上，有多个程序在同时运行，并且多道程序之间互不干扰。 两者区别如下图\n\n![](1.jpeg)\n\n![](2.jpeg)\n\n#### 同步与异步\n\n这个有点基本印象，经过很多次反复理解，依然还是没有很清晰掌握\n\n> 同步：在发出一个同步调用时，在没有得到结果之前，该调用就不返回\n>\n> 异步：在发出一个异步调用后，调用者不会立刻得到结果，该调用就返回了\n\n　　同步和阻塞， 异步和非阻塞概念非常类似，但是所涉及的范围还是有所不同，总的来讲应该是2个独立的概念体系。\n\n　　同步和异步，应该是编程模型中，约定好的一种线程/进程间互相通信的方式，同步则是比较自然的一种程序运作方式，就是自顶向下，资源缺失，主程序等待资源准备，资源就绪继续运行；然而在OS系统调度的指挥下，同步很明显的缺点就是主程序无差别的等待所有资源，很容易程序会因为一些无关紧要的情况而一直处于等待，这其中就有比较大的优化空间；所以根据这种情况一种自然的解决办法是，让程序对各种资源缺失的情况能有对应的方案，最终形成了异步的通信策略，相比于定义上的，调用之后立刻返回这样不知所云的解释，我的理解是，程序是制定了一系列某些资源缺失或者不可用时，程序应该如何运作的逻辑，并且为了保持主程序运行，把程序调用之后的所有职责都交给了子程序，即子程序调用之后，应该由子程序来负责通知主程序，而不是跟同步那样，主程序持续等待子程序准备就绪，而原地踏步；这里的主要区别就是异步时候，主程序调用之后立刻转入下一步不需要这个资源的逻辑，有点类似于，乐观锁和悲观锁的性质，不加判断的断定后续逻辑分支，直接运行。缺点是，感觉异步编程为了解决临界区问题和通信问题，有很多复杂的管控逻辑需要解决，简单来说就是很多坑。好处是，如果这些坑都能解决，程序运行效率会有比较大的提升。\n\n　　如果同步异步是编程模式的话，那么阻塞和非阻塞相对来说我认为是一个并非严格的概念，如果调用必定会导致主程序闲置，进而被置入阻塞状态，那么就是阻塞，如果调用不会导致主程序闲置，那么就是非阻塞。这其中的关键点就是OS的进程/线程调度，而不是调用的问题，然而阻塞的定义中常常会加入调用是否立刻返回的解释，常常让我跟同步异步的概念分不清楚，诚然调用不返回在当今的OS下一定会引起主程序闲置并阻塞，但是主因必然是主程序闲置等待，如果异步通信，但是仍然没有后续程序逻辑，显然异步程序也会有阻塞的风险。所以，我觉得阻塞这个更应该是一个OS的结果，而不是原因。\n\n> 阻塞和非阻塞关注的是程序等待调用结果时的状态，是闲置还是持续运行\n\n线程与进程\n\n　　曾经考试的时候定义背的滚瓜烂熟，然而却完全没有代码经验，感觉彼时都到门口了，缺乏临门一脚，直到现在又千辛万苦来回到这里，重新完成当初没有做到的事情。\n\n　　这里我在实践中，线程跟并发 搞混了，以至于以为线程之间必然是并发，所以有可能分时共享cuda 的引用，认为利用多线程就可以优化GPU利用率。最后实践中，怎么都达不到预想的目的。仔细想想，尽管同一个进程的资源可以被进程内的所有线程共享，这样如果多个线程是分时共享的，那么cuda引用作为一个共享资源，就可以达到并发使用的目的；但是这个前提是，线程必须是并发的，那么线程真的是并发的吗？如果是曾经的单核单线程CPU，答案是肯定的，现在的基本都是多核多线程CPU了，所以线程级别肯定是并行的，那么cuda引用则必然作为一个临界共享资源，被多个线程抢占使用，多线程并行退化成多线程并发，虽然预想的目的似乎达到了，但是貌似并没有达到优化的目的，至少跟之前编写的单线程基本性能是一样的。\n\n　　其次是Python的GIL锁, Global Interpreter Lock，即任何Python 线程执行，只有GIL锁的线程才可以运行，也就是说一个进程下的多线程在Python下，永远只有一个线程在运行\n\n```python\nimport threading, multiprocessing\n\ndef loop():\n\t\tx = 0\n\t\twhile True:\n\t\t\t\tx = x^1\n\t\t\nfor i in range(multiprocessing.cpu_count()):\n\t\tt = threading.Thread(target = loop)\n\t\tt.start()\n```\n\n上述程序在多核CPU下占有率只有100%，但是C++/Java改写的死循环，则可以每个核心都跑完，即4核心400%\n\n更坑的是，GIL锁每个线程只有执行100条字节码的机会，那么有效的利用cuda资源则成为空谈，因为频繁的显存数据进出，比较浪费时间。所以python下一般都是利用多个进程来实现并行，因为多个进程有独立的GIL锁，互不影响。\n\n　　惭愧的是，自己写了这么久的Python，居然才第一次面对并行化问题，才知道GIL锁有这么多繁琐的坑。理论上证实这样的不可行之后，有点失望，但是也还是保留机会来完成并行写日志这个小任务的，毕竟通过HTTP接口，再怎么并发，框架还是很好的完成的这些繁琐的坑，只需要在producer-consumer 框架下，实现并行接受，串行消费就可以了，这还是比较基础的一个改写，对自己还是有信心的。\n\n　　线程安全，进程安全，意思就是在多线程或者多进程下，可以在代码中不加改变的直接使用，库和框架会自动解决并发和并行问题。其中 queue.Queue 就是线程安全的，下面就是利用Queue来实现进程间通信的producer-consumer\n\n```python\nfrom queue import Queue\nfrom threading import Thread\n\ndef producer(out_q):\n\twhile True:\n\t\tout_q.put(data)\n        \ndef consumer(in_q):\n\twhile True:\n\t\tdata = in_q.get()\n\t\t\t...\n    \t# 通过q通知 任务完成\n      in_q.task_done()\n        \nq = Queue()\nt1 = Thread(target=consumer, args=(q,))\nt2 = Thread(taget=producer, args=(q,))\nt1.start()\nt2.start()\n\n# wait for all produced items to be consumed\nq.join()\n```\n\n\n\n　　优化点，队列的流量控制，q需要指定大小，必要时候阻塞Queue，以避免连锁效应导致程序运行失常，同时阻塞的队列也会形成死锁，通过非阻塞和设定超时时间，来处理队列满或者超时的情况，避免阻塞\n\n```python\nq = Queue(100)\n\ntry:\n\tdata = q.get(block=False)\nexcept queue.Empty:\n\tpass\n    \ntry:\n\tq.put(item, timeout=5.0)\nexcept queue.Full:\n\tlog.warning('queued item %r discarded!', item)\n```\n\n\n\n### 最终方案\n\n　　利用tornado接受并发的http请求，解析request_body之后，直接入列队伍q.put()，这里利用了tornado 的异步接受并发的特点，直接简化producer过程。接着只需要编写consumer，取出数据，写到文件即可\n\n```python\nfrom eliot import log_call, start_action, to_file\nclass LogHandler(tornado.web.RequestHandler):\n\t# eliot 直接利用装饰器来添加日志功能，只记录input, output\n\t@log_call\n\tdef post(self, *args, **kwargs):\n\t\ttry:\n\t\t\tself.timestamp = time.ctime()\n\t\t\tself.recv_json = json.loads(self.request.body, strict=False)\n\t\t\tq.put((self.timestamp, self.recv_json ))\n\t\t\toutput_dic = {\n                'status': '1',\n                'result': 'success'\n      }\n\t\texcept Exception as e:\n\t\t\tprint('LogHandler Error: ', e)\n\t\t\treturn\n          \n\n# eliot.start_action 通过 with 来记录函数段，有点繁琐，但是对于某段代码的日志记录很方便\ndef consume_msg(queue):\n\twhile True:\n\t\twith start_action(action_type='consume_msg', lenghth = queue.qsize()):\n\t\t\ttry:\n\t\t\t\ttimestamp, msg= q.get()\n\t\t\t\tstart_time = time.time()\n\t\t\t\twith start_action(action_type='save_request_body', timestamp=timestamp,msg=msg):\n\t\t\t\t\tsave_body(msg)\n\t\t\texcept Exception as e:\n\t\t\t\twith start_action(action_type='consume_msg exceptiosn', timestamp=timestamp,e=e, msg=msg):\n\t\t\t\t\tprint('consume_mgs exception : '  , e)\n                    \nconsumer = Process(target=consume_msg, args=(q, ))\nconsumer.daemon = True # 守护进程\nconsumer.start()\n```\n\n\n\n　　至此，多进程的日志记录完成，同时也用上了之前一直想要用eliot日志系统，让日志看起来清爽了许多。\n\n### RabbiMQ\n\nRabbiMQ 的方案也写进来，毕竟原理大同小异，只是初始化和连接的部分需要重新封装一下，而且适用性更广。\n\n```python\n# rabbitmq init\ncredentials = pika.PlainCredentials(RabbitMQ.USERNAME, RabbitMQ.USERPWD)\nconnection = pika.BlockingConnection(\n                pika.ConnectionParameters(RabbitMQ.HOST, RabbitMQ.PORT,\n                                          credentials=credentials)\n    )\n# channel init and declare\nchannel = connection.channel()\nchannel.exchange_declare(exchange=RabbitMQ.EXCHANGE,\n                             exchange_type=RabbitMQ.EXCHANGE_TYPE,\n                             durable=True)\nchannel.queue_declare(queue=RabbitMQ.QUEUE_NAME_ALG, durable=True)\nchannel.queue_bind(exchange=RabbitMQ.EXCHANGE,\n                       queue=RabbitMQ.QUEUE_NAME_ALG,\n                       routing_key=RabbitMQ.ROUTING_KEY)\nchannel.basic_qos(prefetch_count=1)    # 能者多劳\nchannel.basic_consume(on_message_callback=consumer,\n                          queue=RabbitMQ.QUEUE_NAME_ALG, auto_ack=False)\nprint(' [*] Waiting for messages...')\nchannel.start_consuming()\n\ndef consumer(ch, method, properties, body):\n\tbody = json.loads(body, strict=False)\n\tflag = body['flag']\n\t# process body\n  \n\t# Ack manually\n\tch.basic_ack(delivery_tag=method.delivery_tag)\n  \n\n  \n# producer.py\ntry:\n\tcredentials = pika.PlainCredentials(\n                RabbitMQ.USERNAME, RabbitMQ.USERPWD)\n\tconnection = pika.BlockingConnection(\n                pika.ConnectionParameters(RabbitMQ.HOST, RabbitMQ.PORT,\n                                          credentials=credentials)\n            )\n\tchannel = self.connection.channel()\n\tchannel.exchange_declare(exchange=RabbitMQ.EXCHANGE,                                      \t\t\t\t\t\t\t\t\t\t\t\t\t\texchange_type=RabbitMQ.EXCHANGE_TYPE,\n                                          durable=True)\nexcept Exception as e:\n\tpass\n        \nsent_msg = {...}\nbody = json.dumps(sent_msg)\nchannel.basic_publish(exchange=RabbitMQ.EXCHANGE, \t\t\t\t\n                      routing_key=RabbitMQ.ROUTING_KEY,\n                      body=body, \n                      properties=pika.BasicProperties(delivery_mode=2))\n\nchannel.close()\nconnection.close()\n```\n\n\n\n### 计算密集型 和 IO密集型的思考\n\n　　当初写论文的时候，就在机器上跑了很多类型的负载任务，最直观的感受就是，计算密集型的负载比较难找，相对于纯粹的计算任务，很多实际的任务都会因为千奇百怪的原因被消耗在IO上，导致运行效率低下。\n\n> 计算密集型，代码运行效率为主，因为主要时间都消耗在CPU上，Python的脚本语言不适合这类任务，最好用C++/Java实现\n>\n> IO密集型，大部分时间消耗在IO上，C++/Java无法发挥优势，代码量少开发效率最高的Python具备优势\n\n　　所以，Python最好最为原型开发，保持逻辑流程通顺的前提下，可以用其他高效的语言如C++/Java改写最耗时的模块，迭代优化开发。\n\n### 协程：单线程异步\n\n　　虽然Python无法利用多线程的并行优势，但是还是有优化的手段的。如果把条件限制到单线程，那么有一个很自然的优化方式就是单线程异步，也就是只用单线程的情况下，实现程序内部的并发，这个并发是程序可控的，而不是由操作系统来根据一定策略加锁。\n\n　　Python下有协程库asyncio内置了异步IO的支持。编程模型是一个消息循环，从asyncio获取EventLoop引用，把需要执行的协程作为参数传递给EventLoop，实现异步IO,下面直接给出协程版本的producer-consumer\n\n```python\nimport asyncio\nimport random\n\nasync def producer(queue, n):\n\tfor x in range(1, n+1):\n\t\tprint('producing : ', x)    \n\t\t# simulate io job\n\t\tawait asyncio.sleep(random.random())\n\t\titem = str(x)\n\t\tawait queue.put(item)\n        \nasync def consume(queue):\n\twhile True:\n\t\titem = await queue.get()\n    # process item\n    print('consuming : ', item)\n    # simulate io \n    await asyncio.sleep(random.random())\n    # notify queue that the item has been processed\n    queue.task_done()\n        \nasync def run(n):\n\tqueue = asyncio.Queue()\n  # schedule consumer\n  consumer= asyncio.ensure_future(consume(queue))\n  await producer(queue, n)\n\t# wait until consumer processed all items\n  await queue.join()\n  # consumer is still awaiting item, cancel it\n  consumer.cancel()\n    \nloop = asyncio.get_event_loop()\nloop.run_until_complete(run(10))\nloop.close()\n```\n\n\n\n### Conclusion\n\n　　至此，关于同步异步的一些知识点和实践问题梳理完毕，协程的实践经验还是有点欠缺，以后看框架源码的时候，需要多学习和思考。\n\n\n\nReferences:\n\n[Producer/consumer-examlpe_asyncio](https://asyncio.readthedocs.io/en/latest/producer_consumer.html)\n\n[协程-廖雪峰](https://www.liaoxuefeng.com/wiki/1016959663602400/1017968846697824)\n\n[进程-线程-廖雪峰](https://www.liaoxuefeng.com/wiki/1016959663602400/1017631469467456)\n\n[线程间通信-Python_CookBook](https://python3-cookbook.readthedocs.io/zh_CN/latest/c12/p03_communicating_between_threads.html)\n\n[Python下的多进程日志记录方案](https://juejin.cn/post/6844904039210024967#heading-3)\n\n","slug":"同步和异步编程-如何并行写同一个日志文件实践","published":1,"_id":"ckhvqydbn00006z28dot614mu","comments":1,"layout":"post","photos":[],"link":"","content":"<p>　　最近遇到如何并行写日志文件的问题，查看之前的代码，发现是利用RabbitMQ 作为消息中转，把并行的日志流通过HTTP 请求整合成串行的方式，然后通过一个单独的 Rabbit-Consumer 来写入文件，这样就成功避免了多个进程同时写一个文件造成的加锁问题，但是我觉得日志这种I/O为主的任务，动用RabbitMQ 有点大材小用的感觉，Producer-Consumer的总体思想可以不变，但是可以用其他更加轻量级的编程模型来实现，也借此重新优化一下日志输出的，毕竟之前的日志输出感觉一片混乱，在自己熟悉代码的情况下，仍然会有些理解困难，这跟当初自己写代码的时候不重视日志输出这一块有关，甚至很多情急之下写下的print打印信息都最后都没有处理。</p>\n<p>　　在开始之前，必须要回忆一些似曾相识，却又容易混淆的基本概念，因为在不区分这些基本OS名词之前，根本就完全无法下手，不知道应该如何正确的实践，具体到每个代码跟这些概念是如何结合在一起的，基本上完全不知道，学生时代留下的坏习惯作怪，理论脱离实际，这时候也才意识到书到用时方恨少。</p>\n<h4 id=\"阻塞与非阻塞\"><a href=\"#阻塞与非阻塞\" class=\"headerlink\" title=\"阻塞与非阻塞\"></a>阻塞与非阻塞</h4><p>　　这个概念有点复杂，来自OS中线程/进程的生命周期，诸如，就绪，运行，阻塞 ，但是如果仅仅按照这个方向理解，那就比较简单，仅仅就是线程/进程调度程序根据一定的策略，把线程/进程制定相应的状态，来安排和调度CPU资源；然而在OS之中，阻塞的原因很多，并非只是单纯的IO，严格上来讲应该是跟IO类似的原因被阻塞，即请求的资源无法得到，所以这里的阻塞调用，则是更加强调的是，单线程/进程在运行时，那些一定会被阻塞的编程模式，最重要的一个就是同步的编程模式，一旦请求的资源不具备，程序立刻陷入等待，这样就会特别容易被OS识别为闲置的线程/进程，并很快被阻塞。相反，异步则是一种将等待的资源交给外部程序和硬件，要么是子线程/进程，要么就是外部设备，总之试图通过改变通信模式的方式，使主程序始终维持运行态，这样就达到不被阻塞的目的。这就对编程模式有比较大的变动，因为主程序需要应对请求资源返回的各种情况需要预先有响应的程序段来处理，也就是常见的编程过程中回调函数的编写。</p>\n<blockquote>\n<p>阻塞调用是指调用结果返回之前，调用者会进入阻塞状态等待。只有在得到结果之后才会返回。</p>\n<p>非阻塞调用是指在不能立刻得到结果之前，该函数不会阻塞当前线程，而会立刻返回。</p>\n</blockquote>\n<p>阻塞调用：比如 socket 的 recv()，调用这个函数的线程如果没有数据返回，它会一直阻塞着，也就是 recv() 后面的代码都不会执行了，程序就停在 recv() 这里等待，所以一般把 recv() 放在单独的线程里调用。</p>\n<p>非阻塞调用：比如非阻塞socket 的 send()，调用这个函数，它只是把待发送的数据复制到TCP输出缓冲区中，就立刻返回了，线程并不会阻塞，数据有没有发出去 send() 是不知道的，不会等待它发出去才返回的。</p>\n<h4 id=\"并发与并行\"><a href=\"#并发与并行\" class=\"headerlink\" title=\"并发与并行\"></a>并发与并行</h4><p>基本还记得最显著的区别</p>\n<blockquote>\n<p>并发是指一个时间段内，有几个程序都在同一个CPU上运行，但任意一个时刻点上只有一个程序在处理机上运行。</p>\n<p>并行是指一个时间段内，有几个程序都在几个CPU上运行，任意一个时刻点上，有多个程序在同时运行，并且多道程序之间互不干扰。 两者区别如下图</p>\n</blockquote>\n<p><img src=\"1.jpeg\"></p>\n<p><img src=\"2.jpeg\"></p>\n<h4 id=\"同步与异步\"><a href=\"#同步与异步\" class=\"headerlink\" title=\"同步与异步\"></a>同步与异步</h4><p>这个有点基本印象，经过很多次反复理解，依然还是没有很清晰掌握</p>\n<blockquote>\n<p>同步：在发出一个同步调用时，在没有得到结果之前，该调用就不返回</p>\n<p>异步：在发出一个异步调用后，调用者不会立刻得到结果，该调用就返回了</p>\n</blockquote>\n<p>　　同步和阻塞， 异步和非阻塞概念非常类似，但是所涉及的范围还是有所不同，总的来讲应该是2个独立的概念体系。</p>\n<p>　　同步和异步，应该是编程模型中，约定好的一种线程/进程间互相通信的方式，同步则是比较自然的一种程序运作方式，就是自顶向下，资源缺失，主程序等待资源准备，资源就绪继续运行；然而在OS系统调度的指挥下，同步很明显的缺点就是主程序无差别的等待所有资源，很容易程序会因为一些无关紧要的情况而一直处于等待，这其中就有比较大的优化空间；所以根据这种情况一种自然的解决办法是，让程序对各种资源缺失的情况能有对应的方案，最终形成了异步的通信策略，相比于定义上的，调用之后立刻返回这样不知所云的解释，我的理解是，程序是制定了一系列某些资源缺失或者不可用时，程序应该如何运作的逻辑，并且为了保持主程序运行，把程序调用之后的所有职责都交给了子程序，即子程序调用之后，应该由子程序来负责通知主程序，而不是跟同步那样，主程序持续等待子程序准备就绪，而原地踏步；这里的主要区别就是异步时候，主程序调用之后立刻转入下一步不需要这个资源的逻辑，有点类似于，乐观锁和悲观锁的性质，不加判断的断定后续逻辑分支，直接运行。缺点是，感觉异步编程为了解决临界区问题和通信问题，有很多复杂的管控逻辑需要解决，简单来说就是很多坑。好处是，如果这些坑都能解决，程序运行效率会有比较大的提升。</p>\n<p>　　如果同步异步是编程模式的话，那么阻塞和非阻塞相对来说我认为是一个并非严格的概念，如果调用必定会导致主程序闲置，进而被置入阻塞状态，那么就是阻塞，如果调用不会导致主程序闲置，那么就是非阻塞。这其中的关键点就是OS的进程/线程调度，而不是调用的问题，然而阻塞的定义中常常会加入调用是否立刻返回的解释，常常让我跟同步异步的概念分不清楚，诚然调用不返回在当今的OS下一定会引起主程序闲置并阻塞，但是主因必然是主程序闲置等待，如果异步通信，但是仍然没有后续程序逻辑，显然异步程序也会有阻塞的风险。所以，我觉得阻塞这个更应该是一个OS的结果，而不是原因。</p>\n<blockquote>\n<p>阻塞和非阻塞关注的是程序等待调用结果时的状态，是闲置还是持续运行</p>\n</blockquote>\n<p>线程与进程</p>\n<p>　　曾经考试的时候定义背的滚瓜烂熟，然而却完全没有代码经验，感觉彼时都到门口了，缺乏临门一脚，直到现在又千辛万苦来回到这里，重新完成当初没有做到的事情。</p>\n<p>　　这里我在实践中，线程跟并发 搞混了，以至于以为线程之间必然是并发，所以有可能分时共享cuda 的引用，认为利用多线程就可以优化GPU利用率。最后实践中，怎么都达不到预想的目的。仔细想想，尽管同一个进程的资源可以被进程内的所有线程共享，这样如果多个线程是分时共享的，那么cuda引用作为一个共享资源，就可以达到并发使用的目的；但是这个前提是，线程必须是并发的，那么线程真的是并发的吗？如果是曾经的单核单线程CPU，答案是肯定的，现在的基本都是多核多线程CPU了，所以线程级别肯定是并行的，那么cuda引用则必然作为一个临界共享资源，被多个线程抢占使用，多线程并行退化成多线程并发，虽然预想的目的似乎达到了，但是貌似并没有达到优化的目的，至少跟之前编写的单线程基本性能是一样的。</p>\n<p>　　其次是Python的GIL锁, Global Interpreter Lock，即任何Python 线程执行，只有GIL锁的线程才可以运行，也就是说一个进程下的多线程在Python下，永远只有一个线程在运行</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> threading<span class=\"token punctuation\">,</span> multiprocessing\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">loop</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        x <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n        <span class=\"token keyword\">while</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">:</span>\n                x <span class=\"token operator\">=</span> x<span class=\"token operator\">^</span><span class=\"token number\">1</span>\n\n<span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span>multiprocessing<span class=\"token punctuation\">.</span>cpu_count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        t <span class=\"token operator\">=</span> threading<span class=\"token punctuation\">.</span>Thread<span class=\"token punctuation\">(</span>target <span class=\"token operator\">=</span> loop<span class=\"token punctuation\">)</span>\n        t<span class=\"token punctuation\">.</span>start<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre>\n<p>上述程序在多核CPU下占有率只有100%，但是C++/Java改写的死循环，则可以每个核心都跑完，即4核心400%</p>\n<p>更坑的是，GIL锁每个线程只有执行100条字节码的机会，那么有效的利用cuda资源则成为空谈，因为频繁的显存数据进出，比较浪费时间。所以python下一般都是利用多个进程来实现并行，因为多个进程有独立的GIL锁，互不影响。</p>\n<p>　　惭愧的是，自己写了这么久的Python，居然才第一次面对并行化问题，才知道GIL锁有这么多繁琐的坑。理论上证实这样的不可行之后，有点失望，但是也还是保留机会来完成并行写日志这个小任务的，毕竟通过HTTP接口，再怎么并发，框架还是很好的完成的这些繁琐的坑，只需要在producer-consumer 框架下，实现并行接受，串行消费就可以了，这还是比较基础的一个改写，对自己还是有信心的。</p>\n<p>　　线程安全，进程安全，意思就是在多线程或者多进程下，可以在代码中不加改变的直接使用，库和框架会自动解决并发和并行问题。其中 queue.Queue 就是线程安全的，下面就是利用Queue来实现进程间通信的producer-consumer</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> queue <span class=\"token keyword\">import</span> Queue\n<span class=\"token keyword\">from</span> threading <span class=\"token keyword\">import</span> Thread\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">producer</span><span class=\"token punctuation\">(</span>out_q<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">while</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">:</span>\n        out_q<span class=\"token punctuation\">.</span>put<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">consumer</span><span class=\"token punctuation\">(</span>in_q<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">while</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">:</span>\n        data <span class=\"token operator\">=</span> in_q<span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n            <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>\n        <span class=\"token comment\" spellcheck=\"true\"># 通过q通知 任务完成</span>\n      in_q<span class=\"token punctuation\">.</span>task_done<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\nq <span class=\"token operator\">=</span> Queue<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nt1 <span class=\"token operator\">=</span> Thread<span class=\"token punctuation\">(</span>target<span class=\"token operator\">=</span>consumer<span class=\"token punctuation\">,</span> args<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>q<span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nt2 <span class=\"token operator\">=</span> Thread<span class=\"token punctuation\">(</span>taget<span class=\"token operator\">=</span>producer<span class=\"token punctuation\">,</span> args<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>q<span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nt1<span class=\"token punctuation\">.</span>start<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nt2<span class=\"token punctuation\">.</span>start<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># wait for all produced items to be consumed</span>\nq<span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre>\n<p>　　优化点，队列的流量控制，q需要指定大小，必要时候阻塞Queue，以避免连锁效应导致程序运行失常，同时阻塞的队列也会形成死锁，通过非阻塞和设定超时时间，来处理队列满或者超时的情况，避免阻塞</p>\n<pre class=\" language-python\"><code class=\"language-python\">q <span class=\"token operator\">=</span> Queue<span class=\"token punctuation\">(</span><span class=\"token number\">100</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">try</span><span class=\"token punctuation\">:</span>\n    data <span class=\"token operator\">=</span> q<span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span>block<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">except</span> queue<span class=\"token punctuation\">.</span>Empty<span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">pass</span>\n\n<span class=\"token keyword\">try</span><span class=\"token punctuation\">:</span>\n    q<span class=\"token punctuation\">.</span>put<span class=\"token punctuation\">(</span>item<span class=\"token punctuation\">,</span> timeout<span class=\"token operator\">=</span><span class=\"token number\">5.0</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">except</span> queue<span class=\"token punctuation\">.</span>Full<span class=\"token punctuation\">:</span>\n    log<span class=\"token punctuation\">.</span>warning<span class=\"token punctuation\">(</span><span class=\"token string\">'queued item %r discarded!'</span><span class=\"token punctuation\">,</span> item<span class=\"token punctuation\">)</span></code></pre>\n<h3 id=\"最终方案\"><a href=\"#最终方案\" class=\"headerlink\" title=\"最终方案\"></a>最终方案</h3><p>　　利用tornado接受并发的http请求，解析request_body之后，直接入列队伍q.put()，这里利用了tornado 的异步接受并发的特点，直接简化producer过程。接着只需要编写consumer，取出数据，写到文件即可</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> eliot <span class=\"token keyword\">import</span> log_call<span class=\"token punctuation\">,</span> start_action<span class=\"token punctuation\">,</span> to_file\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">LogHandler</span><span class=\"token punctuation\">(</span>tornado<span class=\"token punctuation\">.</span>web<span class=\"token punctuation\">.</span>RequestHandler<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\" spellcheck=\"true\"># eliot 直接利用装饰器来添加日志功能，只记录input, output</span>\n    @log_call\n    <span class=\"token keyword\">def</span> <span class=\"token function\">post</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> <span class=\"token operator\">*</span>args<span class=\"token punctuation\">,</span> <span class=\"token operator\">**</span>kwargs<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">try</span><span class=\"token punctuation\">:</span>\n            self<span class=\"token punctuation\">.</span>timestamp <span class=\"token operator\">=</span> time<span class=\"token punctuation\">.</span>ctime<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n            self<span class=\"token punctuation\">.</span>recv_json <span class=\"token operator\">=</span> json<span class=\"token punctuation\">.</span>loads<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>request<span class=\"token punctuation\">.</span>body<span class=\"token punctuation\">,</span> strict<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span>\n            q<span class=\"token punctuation\">.</span>put<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>timestamp<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>recv_json <span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n            output_dic <span class=\"token operator\">=</span> <span class=\"token operator\">&amp;</span><span class=\"token comment\" spellcheck=\"true\">#123;</span>\n                <span class=\"token string\">'status'</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'1'</span><span class=\"token punctuation\">,</span>\n                <span class=\"token string\">'result'</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'success'</span>\n      <span class=\"token operator\">&amp;</span><span class=\"token comment\" spellcheck=\"true\">#125;</span>\n        <span class=\"token keyword\">except</span> Exception <span class=\"token keyword\">as</span> e<span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'LogHandler Error: '</span><span class=\"token punctuation\">,</span> e<span class=\"token punctuation\">)</span>\n            <span class=\"token keyword\">return</span>\n\n\n<span class=\"token comment\" spellcheck=\"true\"># eliot.start_action 通过 with 来记录函数段，有点繁琐，但是对于某段代码的日志记录很方便</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">consume_msg</span><span class=\"token punctuation\">(</span>queue<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">while</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">with</span> start_action<span class=\"token punctuation\">(</span>action_type<span class=\"token operator\">=</span><span class=\"token string\">'consume_msg'</span><span class=\"token punctuation\">,</span> lenghth <span class=\"token operator\">=</span> queue<span class=\"token punctuation\">.</span>qsize<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">try</span><span class=\"token punctuation\">:</span>\n                timestamp<span class=\"token punctuation\">,</span> msg<span class=\"token operator\">=</span> q<span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n                start_time <span class=\"token operator\">=</span> time<span class=\"token punctuation\">.</span>time<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n                <span class=\"token keyword\">with</span> start_action<span class=\"token punctuation\">(</span>action_type<span class=\"token operator\">=</span><span class=\"token string\">'save_request_body'</span><span class=\"token punctuation\">,</span> timestamp<span class=\"token operator\">=</span>timestamp<span class=\"token punctuation\">,</span>msg<span class=\"token operator\">=</span>msg<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n                    save_body<span class=\"token punctuation\">(</span>msg<span class=\"token punctuation\">)</span>\n            <span class=\"token keyword\">except</span> Exception <span class=\"token keyword\">as</span> e<span class=\"token punctuation\">:</span>\n                <span class=\"token keyword\">with</span> start_action<span class=\"token punctuation\">(</span>action_type<span class=\"token operator\">=</span><span class=\"token string\">'consume_msg exceptiosn'</span><span class=\"token punctuation\">,</span> timestamp<span class=\"token operator\">=</span>timestamp<span class=\"token punctuation\">,</span>e<span class=\"token operator\">=</span>e<span class=\"token punctuation\">,</span> msg<span class=\"token operator\">=</span>msg<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n                    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'consume_mgs exception : '</span>  <span class=\"token punctuation\">,</span> e<span class=\"token punctuation\">)</span>\n\nconsumer <span class=\"token operator\">=</span> Process<span class=\"token punctuation\">(</span>target<span class=\"token operator\">=</span>consume_msg<span class=\"token punctuation\">,</span> args<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>q<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nconsumer<span class=\"token punctuation\">.</span>daemon <span class=\"token operator\">=</span> <span class=\"token boolean\">True</span> <span class=\"token comment\" spellcheck=\"true\"># 守护进程</span>\nconsumer<span class=\"token punctuation\">.</span>start<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre>\n<p>　　至此，多进程的日志记录完成，同时也用上了之前一直想要用eliot日志系统，让日志看起来清爽了许多。</p>\n<h3 id=\"RabbiMQ\"><a href=\"#RabbiMQ\" class=\"headerlink\" title=\"RabbiMQ\"></a>RabbiMQ</h3><p>RabbiMQ 的方案也写进来，毕竟原理大同小异，只是初始化和连接的部分需要重新封装一下，而且适用性更广。</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token comment\" spellcheck=\"true\"># rabbitmq init</span>\ncredentials <span class=\"token operator\">=</span> pika<span class=\"token punctuation\">.</span>PlainCredentials<span class=\"token punctuation\">(</span>RabbitMQ<span class=\"token punctuation\">.</span>USERNAME<span class=\"token punctuation\">,</span> RabbitMQ<span class=\"token punctuation\">.</span>USERPWD<span class=\"token punctuation\">)</span>\nconnection <span class=\"token operator\">=</span> pika<span class=\"token punctuation\">.</span>BlockingConnection<span class=\"token punctuation\">(</span>\n                pika<span class=\"token punctuation\">.</span>ConnectionParameters<span class=\"token punctuation\">(</span>RabbitMQ<span class=\"token punctuation\">.</span>HOST<span class=\"token punctuation\">,</span> RabbitMQ<span class=\"token punctuation\">.</span>PORT<span class=\"token punctuation\">,</span>\n                                          credentials<span class=\"token operator\">=</span>credentials<span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">)</span>\n<span class=\"token comment\" spellcheck=\"true\"># channel init and declare</span>\nchannel <span class=\"token operator\">=</span> connection<span class=\"token punctuation\">.</span>channel<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nchannel<span class=\"token punctuation\">.</span>exchange_declare<span class=\"token punctuation\">(</span>exchange<span class=\"token operator\">=</span>RabbitMQ<span class=\"token punctuation\">.</span>EXCHANGE<span class=\"token punctuation\">,</span>\n                             exchange_type<span class=\"token operator\">=</span>RabbitMQ<span class=\"token punctuation\">.</span>EXCHANGE_TYPE<span class=\"token punctuation\">,</span>\n                             durable<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\nchannel<span class=\"token punctuation\">.</span>queue_declare<span class=\"token punctuation\">(</span>queue<span class=\"token operator\">=</span>RabbitMQ<span class=\"token punctuation\">.</span>QUEUE_NAME_ALG<span class=\"token punctuation\">,</span> durable<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\nchannel<span class=\"token punctuation\">.</span>queue_bind<span class=\"token punctuation\">(</span>exchange<span class=\"token operator\">=</span>RabbitMQ<span class=\"token punctuation\">.</span>EXCHANGE<span class=\"token punctuation\">,</span>\n                       queue<span class=\"token operator\">=</span>RabbitMQ<span class=\"token punctuation\">.</span>QUEUE_NAME_ALG<span class=\"token punctuation\">,</span>\n                       routing_key<span class=\"token operator\">=</span>RabbitMQ<span class=\"token punctuation\">.</span>ROUTING_KEY<span class=\"token punctuation\">)</span>\nchannel<span class=\"token punctuation\">.</span>basic_qos<span class=\"token punctuation\">(</span>prefetch_count<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>    <span class=\"token comment\" spellcheck=\"true\"># 能者多劳</span>\nchannel<span class=\"token punctuation\">.</span>basic_consume<span class=\"token punctuation\">(</span>on_message_callback<span class=\"token operator\">=</span>consumer<span class=\"token punctuation\">,</span>\n                          queue<span class=\"token operator\">=</span>RabbitMQ<span class=\"token punctuation\">.</span>QUEUE_NAME_ALG<span class=\"token punctuation\">,</span> auto_ack<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">' [*] Waiting for messages...'</span><span class=\"token punctuation\">)</span>\nchannel<span class=\"token punctuation\">.</span>start_consuming<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">consumer</span><span class=\"token punctuation\">(</span>ch<span class=\"token punctuation\">,</span> method<span class=\"token punctuation\">,</span> properties<span class=\"token punctuation\">,</span> body<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    body <span class=\"token operator\">=</span> json<span class=\"token punctuation\">.</span>loads<span class=\"token punctuation\">(</span>body<span class=\"token punctuation\">,</span> strict<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span>\n    flag <span class=\"token operator\">=</span> body<span class=\"token punctuation\">[</span><span class=\"token string\">'flag'</span><span class=\"token punctuation\">]</span>\n    <span class=\"token comment\" spellcheck=\"true\"># process body</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># Ack manually</span>\n    ch<span class=\"token punctuation\">.</span>basic_ack<span class=\"token punctuation\">(</span>delivery_tag<span class=\"token operator\">=</span>method<span class=\"token punctuation\">.</span>delivery_tag<span class=\"token punctuation\">)</span>\n\n\n\n<span class=\"token comment\" spellcheck=\"true\"># producer.py</span>\n<span class=\"token keyword\">try</span><span class=\"token punctuation\">:</span>\n    credentials <span class=\"token operator\">=</span> pika<span class=\"token punctuation\">.</span>PlainCredentials<span class=\"token punctuation\">(</span>\n                RabbitMQ<span class=\"token punctuation\">.</span>USERNAME<span class=\"token punctuation\">,</span> RabbitMQ<span class=\"token punctuation\">.</span>USERPWD<span class=\"token punctuation\">)</span>\n    connection <span class=\"token operator\">=</span> pika<span class=\"token punctuation\">.</span>BlockingConnection<span class=\"token punctuation\">(</span>\n                pika<span class=\"token punctuation\">.</span>ConnectionParameters<span class=\"token punctuation\">(</span>RabbitMQ<span class=\"token punctuation\">.</span>HOST<span class=\"token punctuation\">,</span> RabbitMQ<span class=\"token punctuation\">.</span>PORT<span class=\"token punctuation\">,</span>\n                                          credentials<span class=\"token operator\">=</span>credentials<span class=\"token punctuation\">)</span>\n            <span class=\"token punctuation\">)</span>\n    channel <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>connection<span class=\"token punctuation\">.</span>channel<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    channel<span class=\"token punctuation\">.</span>exchange_declare<span class=\"token punctuation\">(</span>exchange<span class=\"token operator\">=</span>RabbitMQ<span class=\"token punctuation\">.</span>EXCHANGE<span class=\"token punctuation\">,</span>                                                                                              exchange_type<span class=\"token operator\">=</span>RabbitMQ<span class=\"token punctuation\">.</span>EXCHANGE_TYPE<span class=\"token punctuation\">,</span>\n                                          durable<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">except</span> Exception <span class=\"token keyword\">as</span> e<span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">pass</span>\n\nsent_msg <span class=\"token operator\">=</span> <span class=\"token operator\">&amp;</span><span class=\"token comment\" spellcheck=\"true\">#123;...&amp;#125;</span>\nbody <span class=\"token operator\">=</span> json<span class=\"token punctuation\">.</span>dumps<span class=\"token punctuation\">(</span>sent_msg<span class=\"token punctuation\">)</span>\nchannel<span class=\"token punctuation\">.</span>basic_publish<span class=\"token punctuation\">(</span>exchange<span class=\"token operator\">=</span>RabbitMQ<span class=\"token punctuation\">.</span>EXCHANGE<span class=\"token punctuation\">,</span>                 \n                      routing_key<span class=\"token operator\">=</span>RabbitMQ<span class=\"token punctuation\">.</span>ROUTING_KEY<span class=\"token punctuation\">,</span>\n                      body<span class=\"token operator\">=</span>body<span class=\"token punctuation\">,</span> \n                      properties<span class=\"token operator\">=</span>pika<span class=\"token punctuation\">.</span>BasicProperties<span class=\"token punctuation\">(</span>delivery_mode<span class=\"token operator\">=</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\nchannel<span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nconnection<span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre>\n<h3 id=\"计算密集型-和-IO密集型的思考\"><a href=\"#计算密集型-和-IO密集型的思考\" class=\"headerlink\" title=\"计算密集型 和 IO密集型的思考\"></a>计算密集型 和 IO密集型的思考</h3><p>　　当初写论文的时候，就在机器上跑了很多类型的负载任务，最直观的感受就是，计算密集型的负载比较难找，相对于纯粹的计算任务，很多实际的任务都会因为千奇百怪的原因被消耗在IO上，导致运行效率低下。</p>\n<blockquote>\n<p>计算密集型，代码运行效率为主，因为主要时间都消耗在CPU上，Python的脚本语言不适合这类任务，最好用C++/Java实现</p>\n<p>IO密集型，大部分时间消耗在IO上，C++/Java无法发挥优势，代码量少开发效率最高的Python具备优势</p>\n</blockquote>\n<p>　　所以，Python最好最为原型开发，保持逻辑流程通顺的前提下，可以用其他高效的语言如C++/Java改写最耗时的模块，迭代优化开发。</p>\n<h3 id=\"协程：单线程异步\"><a href=\"#协程：单线程异步\" class=\"headerlink\" title=\"协程：单线程异步\"></a>协程：单线程异步</h3><p>　　虽然Python无法利用多线程的并行优势，但是还是有优化的手段的。如果把条件限制到单线程，那么有一个很自然的优化方式就是单线程异步，也就是只用单线程的情况下，实现程序内部的并发，这个并发是程序可控的，而不是由操作系统来根据一定策略加锁。</p>\n<p>　　Python下有协程库asyncio内置了异步IO的支持。编程模型是一个消息循环，从asyncio获取EventLoop引用，把需要执行的协程作为参数传递给EventLoop，实现异步IO,下面直接给出协程版本的producer-consumer</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> asyncio\n<span class=\"token keyword\">import</span> random\n\n<span class=\"token keyword\">async</span> <span class=\"token keyword\">def</span> <span class=\"token function\">producer</span><span class=\"token punctuation\">(</span>queue<span class=\"token punctuation\">,</span> n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> n<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'producing : '</span><span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span>    \n        <span class=\"token comment\" spellcheck=\"true\"># simulate io job</span>\n        <span class=\"token keyword\">await</span> asyncio<span class=\"token punctuation\">.</span>sleep<span class=\"token punctuation\">(</span>random<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        item <span class=\"token operator\">=</span> str<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">await</span> queue<span class=\"token punctuation\">.</span>put<span class=\"token punctuation\">(</span>item<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">async</span> <span class=\"token keyword\">def</span> <span class=\"token function\">consume</span><span class=\"token punctuation\">(</span>queue<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">while</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">:</span>\n        item <span class=\"token operator\">=</span> <span class=\"token keyword\">await</span> queue<span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># process item</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'consuming : '</span><span class=\"token punctuation\">,</span> item<span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># simulate io </span>\n    <span class=\"token keyword\">await</span> asyncio<span class=\"token punctuation\">.</span>sleep<span class=\"token punctuation\">(</span>random<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># notify queue that the item has been processed</span>\n    queue<span class=\"token punctuation\">.</span>task_done<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">async</span> <span class=\"token keyword\">def</span> <span class=\"token function\">run</span><span class=\"token punctuation\">(</span>n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    queue <span class=\"token operator\">=</span> asyncio<span class=\"token punctuation\">.</span>Queue<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n  <span class=\"token comment\" spellcheck=\"true\"># schedule consumer</span>\n  consumer<span class=\"token operator\">=</span> asyncio<span class=\"token punctuation\">.</span>ensure_future<span class=\"token punctuation\">(</span>consume<span class=\"token punctuation\">(</span>queue<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n  <span class=\"token keyword\">await</span> producer<span class=\"token punctuation\">(</span>queue<span class=\"token punctuation\">,</span> n<span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># wait until consumer processed all items</span>\n  <span class=\"token keyword\">await</span> queue<span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n  <span class=\"token comment\" spellcheck=\"true\"># consumer is still awaiting item, cancel it</span>\n  consumer<span class=\"token punctuation\">.</span>cancel<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\nloop <span class=\"token operator\">=</span> asyncio<span class=\"token punctuation\">.</span>get_event_loop<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nloop<span class=\"token punctuation\">.</span>run_until_complete<span class=\"token punctuation\">(</span>run<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nloop<span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>　　至此，关于同步异步的一些知识点和实践问题梳理完毕，协程的实践经验还是有点欠缺，以后看框架源码的时候，需要多学习和思考。</p>\n<p>References:</p>\n<p><a href=\"https://asyncio.readthedocs.io/en/latest/producer_consumer.html\">Producer/consumer-examlpe_asyncio</a></p>\n<p><a href=\"https://www.liaoxuefeng.com/wiki/1016959663602400/1017968846697824\">协程-廖雪峰</a></p>\n<p><a href=\"https://www.liaoxuefeng.com/wiki/1016959663602400/1017631469467456\">进程-线程-廖雪峰</a></p>\n<p><a href=\"https://python3-cookbook.readthedocs.io/zh_CN/latest/c12/p03_communicating_between_threads.html\">线程间通信-Python_CookBook</a></p>\n<p><a href=\"https://juejin.cn/post/6844904039210024967#heading-3\">Python下的多进程日志记录方案</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>　　最近遇到如何并行写日志文件的问题，查看之前的代码，发现是利用RabbitMQ 作为消息中转，把并行的日志流通过HTTP 请求整合成串行的方式，然后通过一个单独的 Rabbit-Consumer 来写入文件，这样就成功避免了多个进程同时写一个文件造成的加锁问题，但是我觉得日志这种I/O为主的任务，动用RabbitMQ 有点大材小用的感觉，Producer-Consumer的总体思想可以不变，但是可以用其他更加轻量级的编程模型来实现，也借此重新优化一下日志输出的，毕竟之前的日志输出感觉一片混乱，在自己熟悉代码的情况下，仍然会有些理解困难，这跟当初自己写代码的时候不重视日志输出这一块有关，甚至很多情急之下写下的print打印信息都最后都没有处理。</p>\n<p>　　在开始之前，必须要回忆一些似曾相识，却又容易混淆的基本概念，因为在不区分这些基本OS名词之前，根本就完全无法下手，不知道应该如何正确的实践，具体到每个代码跟这些概念是如何结合在一起的，基本上完全不知道，学生时代留下的坏习惯作怪，理论脱离实际，这时候也才意识到书到用时方恨少。</p>\n<h4 id=\"阻塞与非阻塞\"><a href=\"#阻塞与非阻塞\" class=\"headerlink\" title=\"阻塞与非阻塞\"></a>阻塞与非阻塞</h4><p>　　这个概念有点复杂，来自OS中线程/进程的生命周期，诸如，就绪，运行，阻塞 ，但是如果仅仅按照这个方向理解，那就比较简单，仅仅就是线程/进程调度程序根据一定的策略，把线程/进程制定相应的状态，来安排和调度CPU资源；然而在OS之中，阻塞的原因很多，并非只是单纯的IO，严格上来讲应该是跟IO类似的原因被阻塞，即请求的资源无法得到，所以这里的阻塞调用，则是更加强调的是，单线程/进程在运行时，那些一定会被阻塞的编程模式，最重要的一个就是同步的编程模式，一旦请求的资源不具备，程序立刻陷入等待，这样就会特别容易被OS识别为闲置的线程/进程，并很快被阻塞。相反，异步则是一种将等待的资源交给外部程序和硬件，要么是子线程/进程，要么就是外部设备，总之试图通过改变通信模式的方式，使主程序始终维持运行态，这样就达到不被阻塞的目的。这就对编程模式有比较大的变动，因为主程序需要应对请求资源返回的各种情况需要预先有响应的程序段来处理，也就是常见的编程过程中回调函数的编写。</p>\n<blockquote>\n<p>阻塞调用是指调用结果返回之前，调用者会进入阻塞状态等待。只有在得到结果之后才会返回。</p>\n<p>非阻塞调用是指在不能立刻得到结果之前，该函数不会阻塞当前线程，而会立刻返回。</p>\n</blockquote>\n<p>阻塞调用：比如 socket 的 recv()，调用这个函数的线程如果没有数据返回，它会一直阻塞着，也就是 recv() 后面的代码都不会执行了，程序就停在 recv() 这里等待，所以一般把 recv() 放在单独的线程里调用。</p>\n<p>非阻塞调用：比如非阻塞socket 的 send()，调用这个函数，它只是把待发送的数据复制到TCP输出缓冲区中，就立刻返回了，线程并不会阻塞，数据有没有发出去 send() 是不知道的，不会等待它发出去才返回的。</p>\n<h4 id=\"并发与并行\"><a href=\"#并发与并行\" class=\"headerlink\" title=\"并发与并行\"></a>并发与并行</h4><p>基本还记得最显著的区别</p>\n<blockquote>\n<p>并发是指一个时间段内，有几个程序都在同一个CPU上运行，但任意一个时刻点上只有一个程序在处理机上运行。</p>\n<p>并行是指一个时间段内，有几个程序都在几个CPU上运行，任意一个时刻点上，有多个程序在同时运行，并且多道程序之间互不干扰。 两者区别如下图</p>\n</blockquote>\n<p><img src=\"1.jpeg\"></p>\n<p><img src=\"2.jpeg\"></p>\n<h4 id=\"同步与异步\"><a href=\"#同步与异步\" class=\"headerlink\" title=\"同步与异步\"></a>同步与异步</h4><p>这个有点基本印象，经过很多次反复理解，依然还是没有很清晰掌握</p>\n<blockquote>\n<p>同步：在发出一个同步调用时，在没有得到结果之前，该调用就不返回</p>\n<p>异步：在发出一个异步调用后，调用者不会立刻得到结果，该调用就返回了</p>\n</blockquote>\n<p>　　同步和阻塞， 异步和非阻塞概念非常类似，但是所涉及的范围还是有所不同，总的来讲应该是2个独立的概念体系。</p>\n<p>　　同步和异步，应该是编程模型中，约定好的一种线程/进程间互相通信的方式，同步则是比较自然的一种程序运作方式，就是自顶向下，资源缺失，主程序等待资源准备，资源就绪继续运行；然而在OS系统调度的指挥下，同步很明显的缺点就是主程序无差别的等待所有资源，很容易程序会因为一些无关紧要的情况而一直处于等待，这其中就有比较大的优化空间；所以根据这种情况一种自然的解决办法是，让程序对各种资源缺失的情况能有对应的方案，最终形成了异步的通信策略，相比于定义上的，调用之后立刻返回这样不知所云的解释，我的理解是，程序是制定了一系列某些资源缺失或者不可用时，程序应该如何运作的逻辑，并且为了保持主程序运行，把程序调用之后的所有职责都交给了子程序，即子程序调用之后，应该由子程序来负责通知主程序，而不是跟同步那样，主程序持续等待子程序准备就绪，而原地踏步；这里的主要区别就是异步时候，主程序调用之后立刻转入下一步不需要这个资源的逻辑，有点类似于，乐观锁和悲观锁的性质，不加判断的断定后续逻辑分支，直接运行。缺点是，感觉异步编程为了解决临界区问题和通信问题，有很多复杂的管控逻辑需要解决，简单来说就是很多坑。好处是，如果这些坑都能解决，程序运行效率会有比较大的提升。</p>\n<p>　　如果同步异步是编程模式的话，那么阻塞和非阻塞相对来说我认为是一个并非严格的概念，如果调用必定会导致主程序闲置，进而被置入阻塞状态，那么就是阻塞，如果调用不会导致主程序闲置，那么就是非阻塞。这其中的关键点就是OS的进程/线程调度，而不是调用的问题，然而阻塞的定义中常常会加入调用是否立刻返回的解释，常常让我跟同步异步的概念分不清楚，诚然调用不返回在当今的OS下一定会引起主程序闲置并阻塞，但是主因必然是主程序闲置等待，如果异步通信，但是仍然没有后续程序逻辑，显然异步程序也会有阻塞的风险。所以，我觉得阻塞这个更应该是一个OS的结果，而不是原因。</p>\n<blockquote>\n<p>阻塞和非阻塞关注的是程序等待调用结果时的状态，是闲置还是持续运行</p>\n</blockquote>\n<p>线程与进程</p>\n<p>　　曾经考试的时候定义背的滚瓜烂熟，然而却完全没有代码经验，感觉彼时都到门口了，缺乏临门一脚，直到现在又千辛万苦来回到这里，重新完成当初没有做到的事情。</p>\n<p>　　这里我在实践中，线程跟并发 搞混了，以至于以为线程之间必然是并发，所以有可能分时共享cuda 的引用，认为利用多线程就可以优化GPU利用率。最后实践中，怎么都达不到预想的目的。仔细想想，尽管同一个进程的资源可以被进程内的所有线程共享，这样如果多个线程是分时共享的，那么cuda引用作为一个共享资源，就可以达到并发使用的目的；但是这个前提是，线程必须是并发的，那么线程真的是并发的吗？如果是曾经的单核单线程CPU，答案是肯定的，现在的基本都是多核多线程CPU了，所以线程级别肯定是并行的，那么cuda引用则必然作为一个临界共享资源，被多个线程抢占使用，多线程并行退化成多线程并发，虽然预想的目的似乎达到了，但是貌似并没有达到优化的目的，至少跟之前编写的单线程基本性能是一样的。</p>\n<p>　　其次是Python的GIL锁, Global Interpreter Lock，即任何Python 线程执行，只有GIL锁的线程才可以运行，也就是说一个进程下的多线程在Python下，永远只有一个线程在运行</p>\n<pre><code class=\"python\">import threading, multiprocessing\n\ndef loop():\n        x = 0\n        while True:\n                x = x^1\n\nfor i in range(multiprocessing.cpu_count()):\n        t = threading.Thread(target = loop)\n        t.start()</code></pre>\n<p>上述程序在多核CPU下占有率只有100%，但是C++/Java改写的死循环，则可以每个核心都跑完，即4核心400%</p>\n<p>更坑的是，GIL锁每个线程只有执行100条字节码的机会，那么有效的利用cuda资源则成为空谈，因为频繁的显存数据进出，比较浪费时间。所以python下一般都是利用多个进程来实现并行，因为多个进程有独立的GIL锁，互不影响。</p>\n<p>　　惭愧的是，自己写了这么久的Python，居然才第一次面对并行化问题，才知道GIL锁有这么多繁琐的坑。理论上证实这样的不可行之后，有点失望，但是也还是保留机会来完成并行写日志这个小任务的，毕竟通过HTTP接口，再怎么并发，框架还是很好的完成的这些繁琐的坑，只需要在producer-consumer 框架下，实现并行接受，串行消费就可以了，这还是比较基础的一个改写，对自己还是有信心的。</p>\n<p>　　线程安全，进程安全，意思就是在多线程或者多进程下，可以在代码中不加改变的直接使用，库和框架会自动解决并发和并行问题。其中 queue.Queue 就是线程安全的，下面就是利用Queue来实现进程间通信的producer-consumer</p>\n<pre><code class=\"python\">from queue import Queue\nfrom threading import Thread\n\ndef producer(out_q):\n    while True:\n        out_q.put(data)\n\ndef consumer(in_q):\n    while True:\n        data = in_q.get()\n            ...\n        # 通过q通知 任务完成\n      in_q.task_done()\n\nq = Queue()\nt1 = Thread(target=consumer, args=(q,))\nt2 = Thread(taget=producer, args=(q,))\nt1.start()\nt2.start()\n\n# wait for all produced items to be consumed\nq.join()</code></pre>\n<p>　　优化点，队列的流量控制，q需要指定大小，必要时候阻塞Queue，以避免连锁效应导致程序运行失常，同时阻塞的队列也会形成死锁，通过非阻塞和设定超时时间，来处理队列满或者超时的情况，避免阻塞</p>\n<pre><code class=\"python\">q = Queue(100)\n\ntry:\n    data = q.get(block=False)\nexcept queue.Empty:\n    pass\n\ntry:\n    q.put(item, timeout=5.0)\nexcept queue.Full:\n    log.warning(&#39;queued item %r discarded!&#39;, item)</code></pre>\n<h3 id=\"最终方案\"><a href=\"#最终方案\" class=\"headerlink\" title=\"最终方案\"></a>最终方案</h3><p>　　利用tornado接受并发的http请求，解析request_body之后，直接入列队伍q.put()，这里利用了tornado 的异步接受并发的特点，直接简化producer过程。接着只需要编写consumer，取出数据，写到文件即可</p>\n<pre><code class=\"python\">from eliot import log_call, start_action, to_file\nclass LogHandler(tornado.web.RequestHandler):\n    # eliot 直接利用装饰器来添加日志功能，只记录input, output\n    @log_call\n    def post(self, *args, **kwargs):\n        try:\n            self.timestamp = time.ctime()\n            self.recv_json = json.loads(self.request.body, strict=False)\n            q.put((self.timestamp, self.recv_json ))\n            output_dic = &#123;\n                &#39;status&#39;: &#39;1&#39;,\n                &#39;result&#39;: &#39;success&#39;\n      &#125;\n        except Exception as e:\n            print(&#39;LogHandler Error: &#39;, e)\n            return\n\n\n# eliot.start_action 通过 with 来记录函数段，有点繁琐，但是对于某段代码的日志记录很方便\ndef consume_msg(queue):\n    while True:\n        with start_action(action_type=&#39;consume_msg&#39;, lenghth = queue.qsize()):\n            try:\n                timestamp, msg= q.get()\n                start_time = time.time()\n                with start_action(action_type=&#39;save_request_body&#39;, timestamp=timestamp,msg=msg):\n                    save_body(msg)\n            except Exception as e:\n                with start_action(action_type=&#39;consume_msg exceptiosn&#39;, timestamp=timestamp,e=e, msg=msg):\n                    print(&#39;consume_mgs exception : &#39;  , e)\n\nconsumer = Process(target=consume_msg, args=(q, ))\nconsumer.daemon = True # 守护进程\nconsumer.start()</code></pre>\n<p>　　至此，多进程的日志记录完成，同时也用上了之前一直想要用eliot日志系统，让日志看起来清爽了许多。</p>\n<h3 id=\"RabbiMQ\"><a href=\"#RabbiMQ\" class=\"headerlink\" title=\"RabbiMQ\"></a>RabbiMQ</h3><p>RabbiMQ 的方案也写进来，毕竟原理大同小异，只是初始化和连接的部分需要重新封装一下，而且适用性更广。</p>\n<pre><code class=\"python\"># rabbitmq init\ncredentials = pika.PlainCredentials(RabbitMQ.USERNAME, RabbitMQ.USERPWD)\nconnection = pika.BlockingConnection(\n                pika.ConnectionParameters(RabbitMQ.HOST, RabbitMQ.PORT,\n                                          credentials=credentials)\n    )\n# channel init and declare\nchannel = connection.channel()\nchannel.exchange_declare(exchange=RabbitMQ.EXCHANGE,\n                             exchange_type=RabbitMQ.EXCHANGE_TYPE,\n                             durable=True)\nchannel.queue_declare(queue=RabbitMQ.QUEUE_NAME_ALG, durable=True)\nchannel.queue_bind(exchange=RabbitMQ.EXCHANGE,\n                       queue=RabbitMQ.QUEUE_NAME_ALG,\n                       routing_key=RabbitMQ.ROUTING_KEY)\nchannel.basic_qos(prefetch_count=1)    # 能者多劳\nchannel.basic_consume(on_message_callback=consumer,\n                          queue=RabbitMQ.QUEUE_NAME_ALG, auto_ack=False)\nprint(&#39; [*] Waiting for messages...&#39;)\nchannel.start_consuming()\n\ndef consumer(ch, method, properties, body):\n    body = json.loads(body, strict=False)\n    flag = body[&#39;flag&#39;]\n    # process body\n\n    # Ack manually\n    ch.basic_ack(delivery_tag=method.delivery_tag)\n\n\n\n# producer.py\ntry:\n    credentials = pika.PlainCredentials(\n                RabbitMQ.USERNAME, RabbitMQ.USERPWD)\n    connection = pika.BlockingConnection(\n                pika.ConnectionParameters(RabbitMQ.HOST, RabbitMQ.PORT,\n                                          credentials=credentials)\n            )\n    channel = self.connection.channel()\n    channel.exchange_declare(exchange=RabbitMQ.EXCHANGE,                                                                                              exchange_type=RabbitMQ.EXCHANGE_TYPE,\n                                          durable=True)\nexcept Exception as e:\n    pass\n\nsent_msg = &#123;...&#125;\nbody = json.dumps(sent_msg)\nchannel.basic_publish(exchange=RabbitMQ.EXCHANGE,                 \n                      routing_key=RabbitMQ.ROUTING_KEY,\n                      body=body, \n                      properties=pika.BasicProperties(delivery_mode=2))\n\nchannel.close()\nconnection.close()</code></pre>\n<h3 id=\"计算密集型-和-IO密集型的思考\"><a href=\"#计算密集型-和-IO密集型的思考\" class=\"headerlink\" title=\"计算密集型 和 IO密集型的思考\"></a>计算密集型 和 IO密集型的思考</h3><p>　　当初写论文的时候，就在机器上跑了很多类型的负载任务，最直观的感受就是，计算密集型的负载比较难找，相对于纯粹的计算任务，很多实际的任务都会因为千奇百怪的原因被消耗在IO上，导致运行效率低下。</p>\n<blockquote>\n<p>计算密集型，代码运行效率为主，因为主要时间都消耗在CPU上，Python的脚本语言不适合这类任务，最好用C++/Java实现</p>\n<p>IO密集型，大部分时间消耗在IO上，C++/Java无法发挥优势，代码量少开发效率最高的Python具备优势</p>\n</blockquote>\n<p>　　所以，Python最好最为原型开发，保持逻辑流程通顺的前提下，可以用其他高效的语言如C++/Java改写最耗时的模块，迭代优化开发。</p>\n<h3 id=\"协程：单线程异步\"><a href=\"#协程：单线程异步\" class=\"headerlink\" title=\"协程：单线程异步\"></a>协程：单线程异步</h3><p>　　虽然Python无法利用多线程的并行优势，但是还是有优化的手段的。如果把条件限制到单线程，那么有一个很自然的优化方式就是单线程异步，也就是只用单线程的情况下，实现程序内部的并发，这个并发是程序可控的，而不是由操作系统来根据一定策略加锁。</p>\n<p>　　Python下有协程库asyncio内置了异步IO的支持。编程模型是一个消息循环，从asyncio获取EventLoop引用，把需要执行的协程作为参数传递给EventLoop，实现异步IO,下面直接给出协程版本的producer-consumer</p>\n<pre><code class=\"python\">import asyncio\nimport random\n\nasync def producer(queue, n):\n    for x in range(1, n+1):\n        print(&#39;producing : &#39;, x)    \n        # simulate io job\n        await asyncio.sleep(random.random())\n        item = str(x)\n        await queue.put(item)\n\nasync def consume(queue):\n    while True:\n        item = await queue.get()\n    # process item\n    print(&#39;consuming : &#39;, item)\n    # simulate io \n    await asyncio.sleep(random.random())\n    # notify queue that the item has been processed\n    queue.task_done()\n\nasync def run(n):\n    queue = asyncio.Queue()\n  # schedule consumer\n  consumer= asyncio.ensure_future(consume(queue))\n  await producer(queue, n)\n    # wait until consumer processed all items\n  await queue.join()\n  # consumer is still awaiting item, cancel it\n  consumer.cancel()\n\nloop = asyncio.get_event_loop()\nloop.run_until_complete(run(10))\nloop.close()</code></pre>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>　　至此，关于同步异步的一些知识点和实践问题梳理完毕，协程的实践经验还是有点欠缺，以后看框架源码的时候，需要多学习和思考。</p>\n<p>References:</p>\n<p><a href=\"https://asyncio.readthedocs.io/en/latest/producer_consumer.html\">Producer/consumer-examlpe_asyncio</a></p>\n<p><a href=\"https://www.liaoxuefeng.com/wiki/1016959663602400/1017968846697824\">协程-廖雪峰</a></p>\n<p><a href=\"https://www.liaoxuefeng.com/wiki/1016959663602400/1017631469467456\">进程-线程-廖雪峰</a></p>\n<p><a href=\"https://python3-cookbook.readthedocs.io/zh_CN/latest/c12/p03_communicating_between_threads.html\">线程间通信-Python_CookBook</a></p>\n<p><a href=\"https://juejin.cn/post/6844904039210024967#heading-3\">Python下的多进程日志记录方案</a></p>\n"},{"title":"GPU in Pytorch  并行和分布式实践","toc":true,"mathjax":true,"top":false,"cover":true,"date":"2020-11-25T07:07:07.000Z","updated":"2020-12-10T08:34:40.141Z","_content":"\n　　虽然还没有机会用到CUDA集群，但是前段时间对协程和并行化的研究，让我忍不住想要探索一下如何在多个GPU下利用Pytorch加快训练的实践方法，算是为之后并行训练优化一个理论参考吧！\n\n　　Pytorch 大体上有3种实现并行的接口（另外还有一种不利用接口的拼接模型的技巧，之后再单独讨论），分别是：torch.multiprocessing, nn.DataParallel, nn.parallel.DistributedDataParallel，如果是是GPU多卡运行，最佳实践是 nn.parallle.DistributedDataParallel,官方文档 [CUDA SEMANTIC](https://pytorch.org/docs/stable/notes/cuda.html#cuda-nn-ddp-instead) 是这么描述的：\n\n> **Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel**\n>\n> Most use cases involving batched inputs and multiple GPUs should default to using [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel) to utilize more than one GPU.\n>\n> There are significant caveats to using CUDA models with [`multiprocessing`](https://pytorch.org/docs/stable/multiprocessing.html#module-torch.multiprocessing); unless care is taken to meet the data handling requirements exactly, it is likely that your program will have incorrect or undefined behavior.\n>\n> It is recommended to use [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel), instead of [`DataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel) to do multi-GPU training, even if there is only a single node.\n>\n> The difference between [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel) and [`DataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel) is: [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel) uses multiprocessing where a process is created for each GPU, while [`DataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel) uses multithreading. By using multiprocessing, each GPU has its dedicated process, this avoids the performance overhead caused by GIL of Python interpreter.\n>\n> If you use [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel), you could use torch.distributed.launch utility to launch your program, see [Third-party backends](https://pytorch.org/docs/stable/distributed.html#distributed-launch).\n\n\n\n### torch.multiprocessing\n\n　　torch.multiprocessing 是 Python下 multiprocessing 的替代品，接口基本一致，并根据情况进行扩张，建议使用 python: multiprocessing.Queue 在进程中传递Pytorch对象。除此之外，还有很多坑，详见官方笔记 [MULTIPROCESSING BEST PRACTICES](https://pytorch.org/docs/stable/notes/multiprocessing.html), [并行处理最佳实践](https://pytorch.apachecn.org/docs/1.4/64.html)\n\n```python\nimport torch.multiprocessing as mp\nfrom model import MyModel\n\ndef train(model):\n    # Construct data_loader, optimizer, etc.\n    for data, labels in data_loader:\n        optimizer.zero_grad()\n        loss_fn(model(data), labels).backward()\n        optimizer.step()  # This will update the shared parameters\n\nif __name__ == '__main__':\n    num_processes = 4\n    model = MyModel()\n    # NOTE: this is required for the ``fork`` method to work\n    model.share_memory()\n    processes = []\n    for rank in range(num_processes):\n        p = mp.Process(target=train, args=(model,))\n        p.start()\n        processes.append(p)\n    for p in processes:\n        p.join()\n```\n\n\n\n### DataParallel\n\n　　DataParallel原理是，把model 副本copy到所有GPU上，其中每个GPU消耗数据的不同分区，相当于SIMD，把数据条目根据GPU数量重新分配\n\n#### 单机DataParallel并行\n\n```python\nmodel = nn.DataParallel(model)\n```\n\n　　代码验证 outside model 数据维度  和 inside model 维度\n\n```python\nclass Model(nn.Module):\n    # Our model\n\n    def __init__(self, input_size, output_size):\n        super(Model, self).__init__()\n        self.fc = nn.Linear(input_size, output_size)\n\n    def forward(self, input):\n        output = self.fc(input)\n        print(\"\\tIn Model: input size\", input.size(),\n              \"output size\", output.size())\n\n        return output\n        \n\nmodel = Model(input_size, output_size)\nif torch.cuda.device_count() > 1:\n  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n  model = nn.DataParallel(model)\n\nmodel.to(device)\n\nfor data in rand_loader:\n    input = data.to(device)\n    output = model(input)\n    print(\"Outside: input size\", input.size(),\n          \"output_size\", output.size())\n    \n# 2 GPUs\n# on 2 GPUs\nLet's use 2 GPUs!\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\nOutside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n```\n\n优点是，通过消耗输入数据不同分区的方式加快训练过程，缺点是，如果单个模型太大，无法放入单个GPU，就无法运行，这时有个Trick可以把模型分段载入不同GPU，利用拼接的方式完成并行训练，该模型将单个模型拆分到不同的 GPU 上，而不是在每个 GPU 上复制整个模型(具体来说， 假设模型`m`包含 10 层：使用`DataParallel`时，每个 GPU 都具有这 10 层中每个层的副本，而当在两个 GPU 上并行使用模型时，每个 GPU 可以承载 5 层）\n\n#### 单机模型拼接并行\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = torch.nn.Linear(10, 10).to('cuda:0')\n        self.relu = torch.nn.ReLU()\n        self.net2 = torch.nn.Linear(10, 5).to('cuda:1')\n\n    def forward(self, x):\n        x = self.relu(self.net1(x.to('cuda:0')))\n        return self.net2(x.to('cuda:1'))\n```\n\n对于模型太大而无法放入单个 GPU 的情况，上述实现解决了该问题。 但是，分析原理的时候可能已经注意到，如果模型单个GPU可以载入，它将比在单个 GPU 上运行它要慢。 这是因为在任何时间点，两个 GPU 中只有一个在工作，而另一个在那儿什么也没做。 由于中间输出需要在`layer2`和`layer3`之间从`cuda:0`复制到`cuda:1`，因此性能进一步恶化。\n\n除此之外，还有通过异步方式构建流水线的手段加速，这个Trick比较精巧和优雅，思想可以学习一下\n\n#### 单机Pipeline 并行\n\n```python\nclass PipelineParallelResNet50(ModelParallelResNet50):\n    def __init__(self, split_size=20, *args, **kwargs):\n        super(PipelineParallelResNet50, self).__init__(*args, **kwargs)\n        self.split_size = split_size\n\n    def forward(self, x):\n        splits = iter(x.split(self.split_size, dim=0))\n        s_next = next(splits)\n        s_prev = self.seq1(s_next).to('cuda:1')\n        ret = []\n\n        for s_next in splits:\n            # A. s_prev runs on cuda:1\n            s_prev = self.seq2(s_prev)\n            ret.append(self.fc(s_prev.view(s_prev.size(0), -1)))\n\n            # B. s_next runs on cuda:0, which can run concurrently with A\n            s_prev = self.seq1(s_next).to('cuda:1')\n\n        s_prev = self.seq2(s_prev)\n        ret.append(self.fc(s_prev.view(s_prev.size(0), -1)))\n\n        return torch.cat(ret)\n\nsetup = \"model = PipelineParallelResNet50()\"\npp_run_times = timeit.repeat(\n    stmt, setup, number=1, repeat=num_repeat, globals=globals())\npp_mean, pp_std = np.mean(pp_run_times), np.std(pp_run_times)\n\nplot([mp_mean, rn_mean, pp_mean],\n     [mp_std, rn_std, pp_std],\n     ['Model Parallel', 'Single GPU', 'Pipelining Model Parallel'],\n     'mp_vs_rn_vs_pp.png')\n```\n\n在2 GPUs上能有50%的提速，离100%还是有点差距，加速对比如下\n\n###![1](1.png) \n\n### \n\n依次是 2-GPU并行， 1-GPU运行， 2-GPU Pipelining 并行 的运行时间对比\n\n\n\n### nn.parallel.DistributedDataParallel\n\n　　基于torch.distributed 的分布式数据并行，原理和DataParallel类似，但是是跨机器和跨设备级别的数据并行，在每台机器和每台设备上复制，并且每个这样的副本处理一部分输入， 在向后传递过程中，将平均每个节点的梯度，总之，是按批处理维度分块指定设备之间的输入来并行化给定模块的应用程序。\n\n`DistributedDataParallel`可以通过以下两种方式使用：\n\n#### 单线程多GPU\n\n```python\ntorch.distributed.init_process_group(backend=\"nccl\")\n# device_ids will include all GPU devices by default\nmodel = DistributedDataParallel(model) \n\n```\n\n#### 多线程多GPU\n\n　　强烈推荐的使用方式，在单机多GPU的情况下，单进程很容易由于GIL出现利用率不足的问题，这时候多进程就是唯一解决办法。最佳实践是，将DDP(DistributedDataParallel)配合多进程一起使用，每个GPU分配一个进程，会比torch.nn.DataParallel快得多，也是目前Pytorch最快的训练方法。\n\n使用步骤：\n\n1. 在N个GPU的单机上生成N个进程，这个过程可以交给torch.distributed.launch完成\n\n```bash\npython -m torch.distributed.launch --nproc_per_node=n distributed_data_parallel.py\n```\n\n2. 在代码中绑定GPU 编号，同时并行化model\n\n```python\nparser = argparse.ArgumentParser()\n# FOR DISTRIBUTED:  Parse for the local_rank argument, which will be supplied\n# automatically by torch.distributed.launch.\nparser.add_argument(\"--local_rank\", default=0, type=int)\nargs = parser.parse_args()\n\n# 进程内绑定 GPU rank id\ntorch.cuda.set_device(args.local_rank)\n# 构造model\ntorch.distributed.init_process_group(backend='nccl', world_size=n, init_method='env://')\nmodel = torch.nn.parallel.DistributedDataParallel(\n  \t\t\t\t\t\t\t\t\tmodel,\n\t\t\t\t\t\t\t\t\t\tdevice_ids=[args.local_rank],\n\t\t\t\t\t\t\t\t\t\toutput_device=args.local_rank)\n```\n\nNote:\n\n1. nccl 后端是分布式训练使用的推荐的最快后端，适用于单节点和多节点分布式训练\n2. nccl同时支持混合精度分布式训练\n3. no_sync 用于禁用DDP进程之间的梯度同步，直到退出此上下文区域的第一个梯度Forward-Backward中进行梯度同步\n\n```python\nddp = torch.nn.DistributedDataParallel(model, pg)\nwith ddp.no_sync():\nfor input in inputs:\n\tddp(input).backward()  # no synchronization, accumulate grads\nddp(another_input).backward()  # synchronize grads\n```\n\n\n\n### apex.parallel.DistributedDataParallel\n\n　　基本上是torch.nn.parallel.DistributedDataParallel的wrapper，同时调用的时候优化了NCCL的使用和简化了参数\n\n``` bash\n# 调用的时候，注意 n <= 每个节点的GPU数量 同时默认 1个GPU对应1进程\ntorch.distributed.launch --nproc_per_node=n distributed_data_parallel.py\n# 会自动提供的参数目前已知的是:\n# args.local_rank\n# os.environ['WORLD_SIZE']\n\n```\n\n简化的要点：\n\n1. model = DDP(model) 即可，无需再传递 devices_ids output_device\n\n2. init_process_group 中的 init_method='env://'\n\n   ``` python\n   torch.distributed.init_process_group(backend='nccl',init_method='env://')\n   ```\n\n直接给出示例代码\n\n```python\n# distributed_data_parallel.py\nimport torch\nimport argparse\nimport os\nfrom apex import amp\n# FOR DISTRIBUTED: (can also use torch.nn.parallel.DistributedDataParallel instead)\nfrom apex.parallel import DistributedDataParallel\n\nparser = argparse.ArgumentParser()\n# FOR DISTRIBUTED:  Parse for the local_rank argument, which will be supplied\n# automatically by torch.distributed.launch.\nparser.add_argument(\"--local_rank\", default=0, type=int)\nargs = parser.parse_args()\n\n# FOR DISTRIBUTED:  If we are running under torch.distributed.launch,\n# the 'WORLD_SIZE' environment variable will also be set automatically.\nargs.distributed = False\nif 'WORLD_SIZE' in os.environ:\n    args.distributed = int(os.environ['WORLD_SIZE']) > 1\n\nif args.distributed:\n    # FOR DISTRIBUTED:  Set the device according to local_rank.\n    torch.cuda.set_device(args.local_rank)\n\n    # FOR DISTRIBUTED:  Initialize the backend.  torch.distributed.launch will provide\n    # environment variables, and requires that you use init_method=`env://`.\n    torch.distributed.init_process_group(backend='nccl',\n                                         init_method='env://')\n\ntorch.backends.cudnn.benchmark = True\n\nN, D_in, D_out = 64, 1024, 16\n\n# Each process receives its own batch of \"fake input data\" and \"fake target data.\"\n# The \"training loop\" in each process just uses this fake batch over and over.\n# https://github.com/NVIDIA/apex/tree/master/examples/imagenet provides a more realistic\n# example of distributed data sampling for both training and validation.\nx = torch.randn(N, D_in, device='cuda')\ny = torch.randn(N, D_out, device='cuda')\n\nmodel = torch.nn.Linear(D_in, D_out).cuda()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n\nmodel, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n\nif args.distributed:\n    # FOR DISTRIBUTED:  After amp.initialize, wrap the model with\n    # apex.parallel.DistributedDataParallel.\n    model = DistributedDataParallel(model)\n    # torch.nn.parallel.DistributedDataParallel is also fine, with some added args:\n    # model = torch.nn.parallel.DistributedDataParallel(model,\n    #                                                   device_ids=[args.local_rank],\n    #                                                   output_device=args.local_rank)\n\nloss_fn = torch.nn.MSELoss()\n\nfor t in range(500):\n    optimizer.zero_grad()\n    y_pred = model(x)\n    loss = loss_fn(y_pred, y)\n    with amp.scale_loss(loss, optimizer) as scaled_loss:\n        scaled_loss.backward()\n    optimizer.step()\n\nif args.local_rank == 0:\n    print(\"final loss = \", loss)\n```\n\n更复杂的多精度调用见 [mixed precision training with DDP](https://github.com/NVIDIA/apex/tree/master/examples/imagenet)\n\n\n\n### DDP 保存和加载检查点\n\n　　一般是使用torch.save 和 torch.load 来完成，但是在多进程下，优化方法是，仅在一个进程中保存，然后在其他所有进程中加载。\n\n``` python\nimport os\nimport tempfile\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\ndef demo_checkpoint(rank, world_size):\n    setup(rank, world_size)\n\n    # setup devices for this process, rank 1 uses GPUs [0, 1, 2, 3] and\n    # rank 2 uses GPUs [4, 5, 6, 7].\n    n = torch.cuda.device_count() // world_size\n    device_ids = list(range(rank * n, (rank + 1) * n))\n\n    model = ToyModel().to(device_ids[0])\n    # output_device defaults to device_ids[0]\n    ddp_model = DDP(model, device_ids=device_ids)\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    CHECKPOINT_PATH = tempfile.gettempdir() + \"/model.checkpoint\"\n    if rank == 0:\n        # All processes should see same parameters as they all start from same\n        # random parameters and gradients are synchronized in backward passes.\n        # Therefore, saving it in one process is sufficient.\n        torch.save(ddp_model.state_dict(), CHECKPOINT_PATH)\n\n    # Use a barrier() to make sure that process 1 loads the model after process\n    # 0 saves it.\n    dist.barrier()\n    # configure map_location properly\n    rank0_devices = [x - rank * len(device_ids) for x in device_ids]\n    device_pairs = zip(rank0_devices, device_ids)\n    map_location = {'cuda:%d' % x: 'cuda:%d' % y for x, y in device_pairs}\n    ddp_model.load_state_dict(\n        torch.load(CHECKPOINT_PATH, map_location=map_location))\n\n    optimizer.zero_grad()\n    outputs = ddp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(device_ids[0])\n    loss_fn = nn.MSELoss()\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    # Use a barrier() to make sure that all processes have finished reading the\n    # checkpoint\n    dist.barrier()\n\n    if rank == 0:\n        os.remove(CHECKPOINT_PATH)\n\n    cleanup()\n```\n\n\n\n### DDP 与模型拼接并行\n\n　　DDP 还可以与多 GPU 模型一起使用，但是不支持进程内的复制。 您需要为每个模块副本创建一个进程，与每个进程的多个副本相比，通常可以提高性能。 当训练具有大量数据的大型模型时，DDP 包装多 GPU 模型特别有用。 使用此功能时，需要小心地实现多 GPU 模型，以避免使用硬编码的设备，因为会将不同的模型副本放置到不同的设备上。\n\n``` python\nclass ToyMpModel(nn.Module):\n    def __init__(self, dev0, dev1):\n        super(ToyMpModel, self).__init__()\n        self.dev0 = dev0\n        self.dev1 = dev1\n        self.net1 = torch.nn.Linear(10, 10).to(dev0)\n        self.relu = torch.nn.ReLU()\n        self.net2 = torch.nn.Linear(10, 5).to(dev1)\n\n    def forward(self, x):\n        x = x.to(self.dev0)\n        x = self.relu(self.net1(x))\n        x = x.to(self.dev1)\n        return self.net2(x)\n```\n\n将多 GPU 模型传递给 DDP 时，不得设置`device_ids`和`output_device`。 输入和输出数据将通过应用程序或模型`forward()`方法放置在适当的设备中。\n\n``` python\ndef demo_model_parallel(rank, world_size):\n    setup(rank, world_size)\n\n    # setup mp_model and devices for this process\n    dev0 = rank * 2\n    dev1 = rank * 2 + 1\n    mp_model = ToyMpModel(dev0, dev1)\n    ddp_mp_model = DDP(mp_model)\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_mp_model.parameters(), lr=0.001)\n\n    optimizer.zero_grad()\n    # outputs will be on dev1\n    outputs = ddp_mp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(dev1)\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    cleanup()\n\nif __name__ == \"__main__\":\n    run_demo(demo_basic, 2)\n    run_demo(demo_checkpoint, 2)\n\n    if torch.cuda.device_count() >= 8:\n        run_demo(demo_model_parallel, 4)\n```\n\n注意上述setup 是通过 torch.multiprocessing.spawn 来完成多线程启动的，所以初始化方法也需要通过setup单独来指定\n\n```python\nimport os\nimport tempfile\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n\n    # initialize the process group\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n\n    # Explicitly setting seed to make sure that models created in two processes\n    # start from same random weights and biases.\n    torch.manual_seed(42)\n\ndef cleanup():\n    dist.destroy_process_group()\n    \nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(10, 10)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(10, 5)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\ndef demo_basic(rank, world_size):\n    setup(rank, world_size)\n\n    # setup devices for this process, rank 1 uses GPUs [0, 1, 2, 3] and\n    # rank 2 uses GPUs [4, 5, 6, 7].\n    n = torch.cuda.device_count() // world_size\n    device_ids = list(range(rank * n, (rank + 1) * n))\n\n    # create model and move it to device_ids[0]\n    model = ToyModel().to(device_ids[0])\n    # output_device defaults to device_ids[0]\n    ddp_model = DDP(model, device_ids=device_ids)\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    optimizer.zero_grad()\n    outputs = ddp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(device_ids[0])\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    cleanup()\n\ndef run_demo(demo_fn, world_size):\n    mp.spawn(demo_fn,\n             args=(world_size,),\n             nprocs=world_size,\n             join=True)\n```\n\n\n\n### Conclusion\n\n　　至此基本分析完所有Pytorch下的多GPU使用技巧，大体方案还是利用多进程的方式来规避GIL来提升性能，只需要用DDP 来包装model ，同时进行分布式对应的初始化，然后多进程启动即可加速训练，基本没有大的变动。\n\n\n\nReferences:\n\n[torch.nn](https://pytorch.apachecn.org/docs/1.4/75.html)\n\n[DistributedDataParallel API](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html?highlight=distributeddataparallel#torch.nn.parallel.DistributedDataParallel)\n\n[CUDA Semantics](https://pytorch.org/docs/stable/notes/cuda.html#cuda-nn-ddp-instead)\n\n[分布式数据并行入门](https://pytorch.apachecn.org/docs/1.4/34.html)\n\n[用Pytorch编写分布式应用程序](https://pytorch.apachecn.org/docs/1.4/35.html)\n\n[apex.parallel](https://nvidia.github.io/apex/parallel.html)\n\n","source":"_posts/GPU-in-Pytorch-并行和分布式实践.md","raw":"---\ntitle: GPU in Pytorch  并行和分布式实践\ntoc: true\nmathjax: true\ntop: false\ncover: true\ndate: 2020-11-25 15:07:07\nupdated:\ncategories: Pytorch\ntags:\n\t- Pytorch\n\t- CUDA\n\t- GPU\n---\n\n　　虽然还没有机会用到CUDA集群，但是前段时间对协程和并行化的研究，让我忍不住想要探索一下如何在多个GPU下利用Pytorch加快训练的实践方法，算是为之后并行训练优化一个理论参考吧！\n\n　　Pytorch 大体上有3种实现并行的接口（另外还有一种不利用接口的拼接模型的技巧，之后再单独讨论），分别是：torch.multiprocessing, nn.DataParallel, nn.parallel.DistributedDataParallel，如果是是GPU多卡运行，最佳实践是 nn.parallle.DistributedDataParallel,官方文档 [CUDA SEMANTIC](https://pytorch.org/docs/stable/notes/cuda.html#cuda-nn-ddp-instead) 是这么描述的：\n\n> **Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel**\n>\n> Most use cases involving batched inputs and multiple GPUs should default to using [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel) to utilize more than one GPU.\n>\n> There are significant caveats to using CUDA models with [`multiprocessing`](https://pytorch.org/docs/stable/multiprocessing.html#module-torch.multiprocessing); unless care is taken to meet the data handling requirements exactly, it is likely that your program will have incorrect or undefined behavior.\n>\n> It is recommended to use [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel), instead of [`DataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel) to do multi-GPU training, even if there is only a single node.\n>\n> The difference between [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel) and [`DataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel) is: [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel) uses multiprocessing where a process is created for each GPU, while [`DataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel) uses multithreading. By using multiprocessing, each GPU has its dedicated process, this avoids the performance overhead caused by GIL of Python interpreter.\n>\n> If you use [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel), you could use torch.distributed.launch utility to launch your program, see [Third-party backends](https://pytorch.org/docs/stable/distributed.html#distributed-launch).\n\n\n\n### torch.multiprocessing\n\n　　torch.multiprocessing 是 Python下 multiprocessing 的替代品，接口基本一致，并根据情况进行扩张，建议使用 python: multiprocessing.Queue 在进程中传递Pytorch对象。除此之外，还有很多坑，详见官方笔记 [MULTIPROCESSING BEST PRACTICES](https://pytorch.org/docs/stable/notes/multiprocessing.html), [并行处理最佳实践](https://pytorch.apachecn.org/docs/1.4/64.html)\n\n```python\nimport torch.multiprocessing as mp\nfrom model import MyModel\n\ndef train(model):\n    # Construct data_loader, optimizer, etc.\n    for data, labels in data_loader:\n        optimizer.zero_grad()\n        loss_fn(model(data), labels).backward()\n        optimizer.step()  # This will update the shared parameters\n\nif __name__ == '__main__':\n    num_processes = 4\n    model = MyModel()\n    # NOTE: this is required for the ``fork`` method to work\n    model.share_memory()\n    processes = []\n    for rank in range(num_processes):\n        p = mp.Process(target=train, args=(model,))\n        p.start()\n        processes.append(p)\n    for p in processes:\n        p.join()\n```\n\n\n\n### DataParallel\n\n　　DataParallel原理是，把model 副本copy到所有GPU上，其中每个GPU消耗数据的不同分区，相当于SIMD，把数据条目根据GPU数量重新分配\n\n#### 单机DataParallel并行\n\n```python\nmodel = nn.DataParallel(model)\n```\n\n　　代码验证 outside model 数据维度  和 inside model 维度\n\n```python\nclass Model(nn.Module):\n    # Our model\n\n    def __init__(self, input_size, output_size):\n        super(Model, self).__init__()\n        self.fc = nn.Linear(input_size, output_size)\n\n    def forward(self, input):\n        output = self.fc(input)\n        print(\"\\tIn Model: input size\", input.size(),\n              \"output size\", output.size())\n\n        return output\n        \n\nmodel = Model(input_size, output_size)\nif torch.cuda.device_count() > 1:\n  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n  model = nn.DataParallel(model)\n\nmodel.to(device)\n\nfor data in rand_loader:\n    input = data.to(device)\n    output = model(input)\n    print(\"Outside: input size\", input.size(),\n          \"output_size\", output.size())\n    \n# 2 GPUs\n# on 2 GPUs\nLet's use 2 GPUs!\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\nOutside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n```\n\n优点是，通过消耗输入数据不同分区的方式加快训练过程，缺点是，如果单个模型太大，无法放入单个GPU，就无法运行，这时有个Trick可以把模型分段载入不同GPU，利用拼接的方式完成并行训练，该模型将单个模型拆分到不同的 GPU 上，而不是在每个 GPU 上复制整个模型(具体来说， 假设模型`m`包含 10 层：使用`DataParallel`时，每个 GPU 都具有这 10 层中每个层的副本，而当在两个 GPU 上并行使用模型时，每个 GPU 可以承载 5 层）\n\n#### 单机模型拼接并行\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = torch.nn.Linear(10, 10).to('cuda:0')\n        self.relu = torch.nn.ReLU()\n        self.net2 = torch.nn.Linear(10, 5).to('cuda:1')\n\n    def forward(self, x):\n        x = self.relu(self.net1(x.to('cuda:0')))\n        return self.net2(x.to('cuda:1'))\n```\n\n对于模型太大而无法放入单个 GPU 的情况，上述实现解决了该问题。 但是，分析原理的时候可能已经注意到，如果模型单个GPU可以载入，它将比在单个 GPU 上运行它要慢。 这是因为在任何时间点，两个 GPU 中只有一个在工作，而另一个在那儿什么也没做。 由于中间输出需要在`layer2`和`layer3`之间从`cuda:0`复制到`cuda:1`，因此性能进一步恶化。\n\n除此之外，还有通过异步方式构建流水线的手段加速，这个Trick比较精巧和优雅，思想可以学习一下\n\n#### 单机Pipeline 并行\n\n```python\nclass PipelineParallelResNet50(ModelParallelResNet50):\n    def __init__(self, split_size=20, *args, **kwargs):\n        super(PipelineParallelResNet50, self).__init__(*args, **kwargs)\n        self.split_size = split_size\n\n    def forward(self, x):\n        splits = iter(x.split(self.split_size, dim=0))\n        s_next = next(splits)\n        s_prev = self.seq1(s_next).to('cuda:1')\n        ret = []\n\n        for s_next in splits:\n            # A. s_prev runs on cuda:1\n            s_prev = self.seq2(s_prev)\n            ret.append(self.fc(s_prev.view(s_prev.size(0), -1)))\n\n            # B. s_next runs on cuda:0, which can run concurrently with A\n            s_prev = self.seq1(s_next).to('cuda:1')\n\n        s_prev = self.seq2(s_prev)\n        ret.append(self.fc(s_prev.view(s_prev.size(0), -1)))\n\n        return torch.cat(ret)\n\nsetup = \"model = PipelineParallelResNet50()\"\npp_run_times = timeit.repeat(\n    stmt, setup, number=1, repeat=num_repeat, globals=globals())\npp_mean, pp_std = np.mean(pp_run_times), np.std(pp_run_times)\n\nplot([mp_mean, rn_mean, pp_mean],\n     [mp_std, rn_std, pp_std],\n     ['Model Parallel', 'Single GPU', 'Pipelining Model Parallel'],\n     'mp_vs_rn_vs_pp.png')\n```\n\n在2 GPUs上能有50%的提速，离100%还是有点差距，加速对比如下\n\n###![1](1.png) \n\n### \n\n依次是 2-GPU并行， 1-GPU运行， 2-GPU Pipelining 并行 的运行时间对比\n\n\n\n### nn.parallel.DistributedDataParallel\n\n　　基于torch.distributed 的分布式数据并行，原理和DataParallel类似，但是是跨机器和跨设备级别的数据并行，在每台机器和每台设备上复制，并且每个这样的副本处理一部分输入， 在向后传递过程中，将平均每个节点的梯度，总之，是按批处理维度分块指定设备之间的输入来并行化给定模块的应用程序。\n\n`DistributedDataParallel`可以通过以下两种方式使用：\n\n#### 单线程多GPU\n\n```python\ntorch.distributed.init_process_group(backend=\"nccl\")\n# device_ids will include all GPU devices by default\nmodel = DistributedDataParallel(model) \n\n```\n\n#### 多线程多GPU\n\n　　强烈推荐的使用方式，在单机多GPU的情况下，单进程很容易由于GIL出现利用率不足的问题，这时候多进程就是唯一解决办法。最佳实践是，将DDP(DistributedDataParallel)配合多进程一起使用，每个GPU分配一个进程，会比torch.nn.DataParallel快得多，也是目前Pytorch最快的训练方法。\n\n使用步骤：\n\n1. 在N个GPU的单机上生成N个进程，这个过程可以交给torch.distributed.launch完成\n\n```bash\npython -m torch.distributed.launch --nproc_per_node=n distributed_data_parallel.py\n```\n\n2. 在代码中绑定GPU 编号，同时并行化model\n\n```python\nparser = argparse.ArgumentParser()\n# FOR DISTRIBUTED:  Parse for the local_rank argument, which will be supplied\n# automatically by torch.distributed.launch.\nparser.add_argument(\"--local_rank\", default=0, type=int)\nargs = parser.parse_args()\n\n# 进程内绑定 GPU rank id\ntorch.cuda.set_device(args.local_rank)\n# 构造model\ntorch.distributed.init_process_group(backend='nccl', world_size=n, init_method='env://')\nmodel = torch.nn.parallel.DistributedDataParallel(\n  \t\t\t\t\t\t\t\t\tmodel,\n\t\t\t\t\t\t\t\t\t\tdevice_ids=[args.local_rank],\n\t\t\t\t\t\t\t\t\t\toutput_device=args.local_rank)\n```\n\nNote:\n\n1. nccl 后端是分布式训练使用的推荐的最快后端，适用于单节点和多节点分布式训练\n2. nccl同时支持混合精度分布式训练\n3. no_sync 用于禁用DDP进程之间的梯度同步，直到退出此上下文区域的第一个梯度Forward-Backward中进行梯度同步\n\n```python\nddp = torch.nn.DistributedDataParallel(model, pg)\nwith ddp.no_sync():\nfor input in inputs:\n\tddp(input).backward()  # no synchronization, accumulate grads\nddp(another_input).backward()  # synchronize grads\n```\n\n\n\n### apex.parallel.DistributedDataParallel\n\n　　基本上是torch.nn.parallel.DistributedDataParallel的wrapper，同时调用的时候优化了NCCL的使用和简化了参数\n\n``` bash\n# 调用的时候，注意 n <= 每个节点的GPU数量 同时默认 1个GPU对应1进程\ntorch.distributed.launch --nproc_per_node=n distributed_data_parallel.py\n# 会自动提供的参数目前已知的是:\n# args.local_rank\n# os.environ['WORLD_SIZE']\n\n```\n\n简化的要点：\n\n1. model = DDP(model) 即可，无需再传递 devices_ids output_device\n\n2. init_process_group 中的 init_method='env://'\n\n   ``` python\n   torch.distributed.init_process_group(backend='nccl',init_method='env://')\n   ```\n\n直接给出示例代码\n\n```python\n# distributed_data_parallel.py\nimport torch\nimport argparse\nimport os\nfrom apex import amp\n# FOR DISTRIBUTED: (can also use torch.nn.parallel.DistributedDataParallel instead)\nfrom apex.parallel import DistributedDataParallel\n\nparser = argparse.ArgumentParser()\n# FOR DISTRIBUTED:  Parse for the local_rank argument, which will be supplied\n# automatically by torch.distributed.launch.\nparser.add_argument(\"--local_rank\", default=0, type=int)\nargs = parser.parse_args()\n\n# FOR DISTRIBUTED:  If we are running under torch.distributed.launch,\n# the 'WORLD_SIZE' environment variable will also be set automatically.\nargs.distributed = False\nif 'WORLD_SIZE' in os.environ:\n    args.distributed = int(os.environ['WORLD_SIZE']) > 1\n\nif args.distributed:\n    # FOR DISTRIBUTED:  Set the device according to local_rank.\n    torch.cuda.set_device(args.local_rank)\n\n    # FOR DISTRIBUTED:  Initialize the backend.  torch.distributed.launch will provide\n    # environment variables, and requires that you use init_method=`env://`.\n    torch.distributed.init_process_group(backend='nccl',\n                                         init_method='env://')\n\ntorch.backends.cudnn.benchmark = True\n\nN, D_in, D_out = 64, 1024, 16\n\n# Each process receives its own batch of \"fake input data\" and \"fake target data.\"\n# The \"training loop\" in each process just uses this fake batch over and over.\n# https://github.com/NVIDIA/apex/tree/master/examples/imagenet provides a more realistic\n# example of distributed data sampling for both training and validation.\nx = torch.randn(N, D_in, device='cuda')\ny = torch.randn(N, D_out, device='cuda')\n\nmodel = torch.nn.Linear(D_in, D_out).cuda()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n\nmodel, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n\nif args.distributed:\n    # FOR DISTRIBUTED:  After amp.initialize, wrap the model with\n    # apex.parallel.DistributedDataParallel.\n    model = DistributedDataParallel(model)\n    # torch.nn.parallel.DistributedDataParallel is also fine, with some added args:\n    # model = torch.nn.parallel.DistributedDataParallel(model,\n    #                                                   device_ids=[args.local_rank],\n    #                                                   output_device=args.local_rank)\n\nloss_fn = torch.nn.MSELoss()\n\nfor t in range(500):\n    optimizer.zero_grad()\n    y_pred = model(x)\n    loss = loss_fn(y_pred, y)\n    with amp.scale_loss(loss, optimizer) as scaled_loss:\n        scaled_loss.backward()\n    optimizer.step()\n\nif args.local_rank == 0:\n    print(\"final loss = \", loss)\n```\n\n更复杂的多精度调用见 [mixed precision training with DDP](https://github.com/NVIDIA/apex/tree/master/examples/imagenet)\n\n\n\n### DDP 保存和加载检查点\n\n　　一般是使用torch.save 和 torch.load 来完成，但是在多进程下，优化方法是，仅在一个进程中保存，然后在其他所有进程中加载。\n\n``` python\nimport os\nimport tempfile\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\ndef demo_checkpoint(rank, world_size):\n    setup(rank, world_size)\n\n    # setup devices for this process, rank 1 uses GPUs [0, 1, 2, 3] and\n    # rank 2 uses GPUs [4, 5, 6, 7].\n    n = torch.cuda.device_count() // world_size\n    device_ids = list(range(rank * n, (rank + 1) * n))\n\n    model = ToyModel().to(device_ids[0])\n    # output_device defaults to device_ids[0]\n    ddp_model = DDP(model, device_ids=device_ids)\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    CHECKPOINT_PATH = tempfile.gettempdir() + \"/model.checkpoint\"\n    if rank == 0:\n        # All processes should see same parameters as they all start from same\n        # random parameters and gradients are synchronized in backward passes.\n        # Therefore, saving it in one process is sufficient.\n        torch.save(ddp_model.state_dict(), CHECKPOINT_PATH)\n\n    # Use a barrier() to make sure that process 1 loads the model after process\n    # 0 saves it.\n    dist.barrier()\n    # configure map_location properly\n    rank0_devices = [x - rank * len(device_ids) for x in device_ids]\n    device_pairs = zip(rank0_devices, device_ids)\n    map_location = {'cuda:%d' % x: 'cuda:%d' % y for x, y in device_pairs}\n    ddp_model.load_state_dict(\n        torch.load(CHECKPOINT_PATH, map_location=map_location))\n\n    optimizer.zero_grad()\n    outputs = ddp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(device_ids[0])\n    loss_fn = nn.MSELoss()\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    # Use a barrier() to make sure that all processes have finished reading the\n    # checkpoint\n    dist.barrier()\n\n    if rank == 0:\n        os.remove(CHECKPOINT_PATH)\n\n    cleanup()\n```\n\n\n\n### DDP 与模型拼接并行\n\n　　DDP 还可以与多 GPU 模型一起使用，但是不支持进程内的复制。 您需要为每个模块副本创建一个进程，与每个进程的多个副本相比，通常可以提高性能。 当训练具有大量数据的大型模型时，DDP 包装多 GPU 模型特别有用。 使用此功能时，需要小心地实现多 GPU 模型，以避免使用硬编码的设备，因为会将不同的模型副本放置到不同的设备上。\n\n``` python\nclass ToyMpModel(nn.Module):\n    def __init__(self, dev0, dev1):\n        super(ToyMpModel, self).__init__()\n        self.dev0 = dev0\n        self.dev1 = dev1\n        self.net1 = torch.nn.Linear(10, 10).to(dev0)\n        self.relu = torch.nn.ReLU()\n        self.net2 = torch.nn.Linear(10, 5).to(dev1)\n\n    def forward(self, x):\n        x = x.to(self.dev0)\n        x = self.relu(self.net1(x))\n        x = x.to(self.dev1)\n        return self.net2(x)\n```\n\n将多 GPU 模型传递给 DDP 时，不得设置`device_ids`和`output_device`。 输入和输出数据将通过应用程序或模型`forward()`方法放置在适当的设备中。\n\n``` python\ndef demo_model_parallel(rank, world_size):\n    setup(rank, world_size)\n\n    # setup mp_model and devices for this process\n    dev0 = rank * 2\n    dev1 = rank * 2 + 1\n    mp_model = ToyMpModel(dev0, dev1)\n    ddp_mp_model = DDP(mp_model)\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_mp_model.parameters(), lr=0.001)\n\n    optimizer.zero_grad()\n    # outputs will be on dev1\n    outputs = ddp_mp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(dev1)\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    cleanup()\n\nif __name__ == \"__main__\":\n    run_demo(demo_basic, 2)\n    run_demo(demo_checkpoint, 2)\n\n    if torch.cuda.device_count() >= 8:\n        run_demo(demo_model_parallel, 4)\n```\n\n注意上述setup 是通过 torch.multiprocessing.spawn 来完成多线程启动的，所以初始化方法也需要通过setup单独来指定\n\n```python\nimport os\nimport tempfile\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n\n    # initialize the process group\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n\n    # Explicitly setting seed to make sure that models created in two processes\n    # start from same random weights and biases.\n    torch.manual_seed(42)\n\ndef cleanup():\n    dist.destroy_process_group()\n    \nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(10, 10)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(10, 5)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\ndef demo_basic(rank, world_size):\n    setup(rank, world_size)\n\n    # setup devices for this process, rank 1 uses GPUs [0, 1, 2, 3] and\n    # rank 2 uses GPUs [4, 5, 6, 7].\n    n = torch.cuda.device_count() // world_size\n    device_ids = list(range(rank * n, (rank + 1) * n))\n\n    # create model and move it to device_ids[0]\n    model = ToyModel().to(device_ids[0])\n    # output_device defaults to device_ids[0]\n    ddp_model = DDP(model, device_ids=device_ids)\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    optimizer.zero_grad()\n    outputs = ddp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(device_ids[0])\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    cleanup()\n\ndef run_demo(demo_fn, world_size):\n    mp.spawn(demo_fn,\n             args=(world_size,),\n             nprocs=world_size,\n             join=True)\n```\n\n\n\n### Conclusion\n\n　　至此基本分析完所有Pytorch下的多GPU使用技巧，大体方案还是利用多进程的方式来规避GIL来提升性能，只需要用DDP 来包装model ，同时进行分布式对应的初始化，然后多进程启动即可加速训练，基本没有大的变动。\n\n\n\nReferences:\n\n[torch.nn](https://pytorch.apachecn.org/docs/1.4/75.html)\n\n[DistributedDataParallel API](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html?highlight=distributeddataparallel#torch.nn.parallel.DistributedDataParallel)\n\n[CUDA Semantics](https://pytorch.org/docs/stable/notes/cuda.html#cuda-nn-ddp-instead)\n\n[分布式数据并行入门](https://pytorch.apachecn.org/docs/1.4/34.html)\n\n[用Pytorch编写分布式应用程序](https://pytorch.apachecn.org/docs/1.4/35.html)\n\n[apex.parallel](https://nvidia.github.io/apex/parallel.html)\n\n","slug":"GPU-in-Pytorch-并行和分布式实践","published":1,"_id":"ckhyotdja0000l9289pxq20m8","comments":1,"layout":"post","photos":[],"link":"","content":"<p>　　虽然还没有机会用到CUDA集群，但是前段时间对协程和并行化的研究，让我忍不住想要探索一下如何在多个GPU下利用Pytorch加快训练的实践方法，算是为之后并行训练优化一个理论参考吧！</p>\n<p>　　Pytorch 大体上有3种实现并行的接口（另外还有一种不利用接口的拼接模型的技巧，之后再单独讨论），分别是：torch.multiprocessing, nn.DataParallel, nn.parallel.DistributedDataParallel，如果是是GPU多卡运行，最佳实践是 nn.parallle.DistributedDataParallel,官方文档 <a href=\"https://pytorch.org/docs/stable/notes/cuda.html#cuda-nn-ddp-instead\">CUDA SEMANTIC</a> 是这么描述的：</p>\n<blockquote>\n<p><strong>Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel</strong></p>\n<p>Most use cases involving batched inputs and multiple GPUs should default to using <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\"><code>DistributedDataParallel</code></a> to utilize more than one GPU.</p>\n<p>There are significant caveats to using CUDA models with <a href=\"https://pytorch.org/docs/stable/multiprocessing.html#module-torch.multiprocessing\"><code>multiprocessing</code></a>; unless care is taken to meet the data handling requirements exactly, it is likely that your program will have incorrect or undefined behavior.</p>\n<p>It is recommended to use <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\"><code>DistributedDataParallel</code></a>, instead of <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel\"><code>DataParallel</code></a> to do multi-GPU training, even if there is only a single node.</p>\n<p>The difference between <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\"><code>DistributedDataParallel</code></a> and <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel\"><code>DataParallel</code></a> is: <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\"><code>DistributedDataParallel</code></a> uses multiprocessing where a process is created for each GPU, while <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel\"><code>DataParallel</code></a> uses multithreading. By using multiprocessing, each GPU has its dedicated process, this avoids the performance overhead caused by GIL of Python interpreter.</p>\n<p>If you use <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\"><code>DistributedDataParallel</code></a>, you could use torch.distributed.launch utility to launch your program, see <a href=\"https://pytorch.org/docs/stable/distributed.html#distributed-launch\">Third-party backends</a>.</p>\n</blockquote>\n<h3 id=\"torch-multiprocessing\"><a href=\"#torch-multiprocessing\" class=\"headerlink\" title=\"torch.multiprocessing\"></a>torch.multiprocessing</h3><p>　　torch.multiprocessing 是 Python下 multiprocessing 的替代品，接口基本一致，并根据情况进行扩张，建议使用 python: multiprocessing.Queue 在进程中传递Pytorch对象。除此之外，还有很多坑，详见官方笔记 <a href=\"https://pytorch.org/docs/stable/notes/multiprocessing.html\">MULTIPROCESSING BEST PRACTICES</a>, <a href=\"https://pytorch.apachecn.org/docs/1.4/64.html\">并行处理最佳实践</a></p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>multiprocessing <span class=\"token keyword\">as</span> mp\n<span class=\"token keyword\">from</span> model <span class=\"token keyword\">import</span> MyModel\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">train</span><span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\" spellcheck=\"true\"># Construct data_loader, optimizer, etc.</span>\n    <span class=\"token keyword\">for</span> data<span class=\"token punctuation\">,</span> labels <span class=\"token keyword\">in</span> data_loader<span class=\"token punctuation\">:</span>\n        optimizer<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        loss_fn<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> labels<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        optimizer<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\" spellcheck=\"true\"># This will update the shared parameters</span>\n\n<span class=\"token keyword\">if</span> __name__ <span class=\"token operator\">==</span> <span class=\"token string\">'__main__'</span><span class=\"token punctuation\">:</span>\n    num_processes <span class=\"token operator\">=</span> <span class=\"token number\">4</span>\n    model <span class=\"token operator\">=</span> MyModel<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># NOTE: this is required for the ``fork`` method to work</span>\n    model<span class=\"token punctuation\">.</span>share_memory<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    processes <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">for</span> rank <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span>num_processes<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        p <span class=\"token operator\">=</span> mp<span class=\"token punctuation\">.</span>Process<span class=\"token punctuation\">(</span>target<span class=\"token operator\">=</span>train<span class=\"token punctuation\">,</span> args<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        p<span class=\"token punctuation\">.</span>start<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        processes<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>p<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">for</span> p <span class=\"token keyword\">in</span> processes<span class=\"token punctuation\">:</span>\n        p<span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre>\n<h3 id=\"DataParallel\"><a href=\"#DataParallel\" class=\"headerlink\" title=\"DataParallel\"></a>DataParallel</h3><p>　　DataParallel原理是，把model 副本copy到所有GPU上，其中每个GPU消耗数据的不同分区，相当于SIMD，把数据条目根据GPU数量重新分配</p>\n<h4 id=\"单机DataParallel并行\"><a href=\"#单机DataParallel并行\" class=\"headerlink\" title=\"单机DataParallel并行\"></a>单机DataParallel并行</h4><pre class=\" language-python\"><code class=\"language-python\">model <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>DataParallel<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">)</span></code></pre>\n<p>　　代码验证 outside model 数据维度  和 inside model 维度</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">Model</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\" spellcheck=\"true\"># Our model</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> input_size<span class=\"token punctuation\">,</span> output_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        super<span class=\"token punctuation\">(</span>Model<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>fc <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>input_size<span class=\"token punctuation\">,</span> output_size<span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> input<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        output <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>fc<span class=\"token punctuation\">(</span>input<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"\\tIn Model: input size\"</span><span class=\"token punctuation\">,</span> input<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n              <span class=\"token string\">\"output size\"</span><span class=\"token punctuation\">,</span> output<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n        <span class=\"token keyword\">return</span> output\n\n\nmodel <span class=\"token operator\">=</span> Model<span class=\"token punctuation\">(</span>input_size<span class=\"token punctuation\">,</span> output_size<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">if</span> torch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>device_count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">></span> <span class=\"token number\">1</span><span class=\"token punctuation\">:</span>\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Let's use\"</span><span class=\"token punctuation\">,</span> torch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>device_count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"GPUs!\"</span><span class=\"token punctuation\">)</span>\n  <span class=\"token comment\" spellcheck=\"true\"># dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs</span>\n  model <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>DataParallel<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">)</span>\n\nmodel<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">for</span> data <span class=\"token keyword\">in</span> rand_loader<span class=\"token punctuation\">:</span>\n    input <span class=\"token operator\">=</span> data<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device<span class=\"token punctuation\">)</span>\n    output <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>input<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Outside: input size\"</span><span class=\"token punctuation\">,</span> input<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n          <span class=\"token string\">\"output_size\"</span><span class=\"token punctuation\">,</span> output<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># 2 GPUs</span>\n<span class=\"token comment\" spellcheck=\"true\"># on 2 GPUs</span>\nLet's use <span class=\"token number\">2</span> GPUs!\n    In Model<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    In Model<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nOutside<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">30</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output_size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">30</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    In Model<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    In Model<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nOutside<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">30</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output_size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">30</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    In Model<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    In Model<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nOutside<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">30</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output_size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">30</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    In Model<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    In Model<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nOutside<span class=\"token punctuation\">:</span> input size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> output_size torch<span class=\"token punctuation\">.</span>Size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre>\n<p>优点是，通过消耗输入数据不同分区的方式加快训练过程，缺点是，如果单个模型太大，无法放入单个GPU，就无法运行，这时有个Trick可以把模型分段载入不同GPU，利用拼接的方式完成并行训练，该模型将单个模型拆分到不同的 GPU 上，而不是在每个 GPU 上复制整个模型(具体来说， 假设模型<code>m</code>包含 10 层：使用<code>DataParallel</code>时，每个 GPU 都具有这 10 层中每个层的副本，而当在两个 GPU 上并行使用模型时，每个 GPU 可以承载 5 层）</p>\n<h4 id=\"单机模型拼接并行\"><a href=\"#单机模型拼接并行\" class=\"headerlink\" title=\"单机模型拼接并行\"></a>单机模型拼接并行</h4><pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>nn <span class=\"token keyword\">as</span> nn\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>optim <span class=\"token keyword\">as</span> optim\n\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">ToyModel</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        super<span class=\"token punctuation\">(</span>ToyModel<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>net1 <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span><span class=\"token string\">'cuda:0'</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>relu <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>ReLU<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>net2 <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span><span class=\"token string\">'cuda:1'</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        x <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>relu<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>net1<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span><span class=\"token string\">'cuda:0'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> self<span class=\"token punctuation\">.</span>net2<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span><span class=\"token string\">'cuda:1'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre>\n<p>对于模型太大而无法放入单个 GPU 的情况，上述实现解决了该问题。 但是，分析原理的时候可能已经注意到，如果模型单个GPU可以载入，它将比在单个 GPU 上运行它要慢。 这是因为在任何时间点，两个 GPU 中只有一个在工作，而另一个在那儿什么也没做。 由于中间输出需要在<code>layer2</code>和<code>layer3</code>之间从<code>cuda:0</code>复制到<code>cuda:1</code>，因此性能进一步恶化。</p>\n<p>除此之外，还有通过异步方式构建流水线的手段加速，这个Trick比较精巧和优雅，思想可以学习一下</p>\n<h4 id=\"单机Pipeline-并行\"><a href=\"#单机Pipeline-并行\" class=\"headerlink\" title=\"单机Pipeline 并行\"></a>单机Pipeline 并行</h4><pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">PipelineParallelResNet50</span><span class=\"token punctuation\">(</span>ModelParallelResNet50<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> split_size<span class=\"token operator\">=</span><span class=\"token number\">20</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">*</span>args<span class=\"token punctuation\">,</span> <span class=\"token operator\">**</span>kwargs<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        super<span class=\"token punctuation\">(</span>PipelineParallelResNet50<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token operator\">*</span>args<span class=\"token punctuation\">,</span> <span class=\"token operator\">**</span>kwargs<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>split_size <span class=\"token operator\">=</span> split_size\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        splits <span class=\"token operator\">=</span> iter<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>split_size<span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        s_next <span class=\"token operator\">=</span> next<span class=\"token punctuation\">(</span>splits<span class=\"token punctuation\">)</span>\n        s_prev <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>seq1<span class=\"token punctuation\">(</span>s_next<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span><span class=\"token string\">'cuda:1'</span><span class=\"token punctuation\">)</span>\n        ret <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n\n        <span class=\"token keyword\">for</span> s_next <span class=\"token keyword\">in</span> splits<span class=\"token punctuation\">:</span>\n            <span class=\"token comment\" spellcheck=\"true\"># A. s_prev runs on cuda:1</span>\n            s_prev <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>seq2<span class=\"token punctuation\">(</span>s_prev<span class=\"token punctuation\">)</span>\n            ret<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>fc<span class=\"token punctuation\">(</span>s_prev<span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span>s_prev<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n            <span class=\"token comment\" spellcheck=\"true\"># B. s_next runs on cuda:0, which can run concurrently with A</span>\n            s_prev <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>seq1<span class=\"token punctuation\">(</span>s_next<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span><span class=\"token string\">'cuda:1'</span><span class=\"token punctuation\">)</span>\n\n        s_prev <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>seq2<span class=\"token punctuation\">(</span>s_prev<span class=\"token punctuation\">)</span>\n        ret<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>fc<span class=\"token punctuation\">(</span>s_prev<span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span>s_prev<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n        <span class=\"token keyword\">return</span> torch<span class=\"token punctuation\">.</span>cat<span class=\"token punctuation\">(</span>ret<span class=\"token punctuation\">)</span>\n\nsetup <span class=\"token operator\">=</span> <span class=\"token string\">\"model = PipelineParallelResNet50()\"</span>\npp_run_times <span class=\"token operator\">=</span> timeit<span class=\"token punctuation\">.</span>repeat<span class=\"token punctuation\">(</span>\n    stmt<span class=\"token punctuation\">,</span> setup<span class=\"token punctuation\">,</span> number<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> repeat<span class=\"token operator\">=</span>num_repeat<span class=\"token punctuation\">,</span> globals<span class=\"token operator\">=</span>globals<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\npp_mean<span class=\"token punctuation\">,</span> pp_std <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>mean<span class=\"token punctuation\">(</span>pp_run_times<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> np<span class=\"token punctuation\">.</span>std<span class=\"token punctuation\">(</span>pp_run_times<span class=\"token punctuation\">)</span>\n\nplot<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>mp_mean<span class=\"token punctuation\">,</span> rn_mean<span class=\"token punctuation\">,</span> pp_mean<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n     <span class=\"token punctuation\">[</span>mp_std<span class=\"token punctuation\">,</span> rn_std<span class=\"token punctuation\">,</span> pp_std<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n     <span class=\"token punctuation\">[</span><span class=\"token string\">'Model Parallel'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Single GPU'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'Pipelining Model Parallel'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n     <span class=\"token string\">'mp_vs_rn_vs_pp.png'</span><span class=\"token punctuation\">)</span></code></pre>\n<p>在2 GPUs上能有50%的提速，离100%还是有点差距，加速对比如下</p>\n<p>###<img src=\"1.png\" alt=\"1\"> </p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>依次是 2-GPU并行， 1-GPU运行， 2-GPU Pipelining 并行 的运行时间对比</p>\n<h3 id=\"nn-parallel-DistributedDataParallel\"><a href=\"#nn-parallel-DistributedDataParallel\" class=\"headerlink\" title=\"nn.parallel.DistributedDataParallel\"></a>nn.parallel.DistributedDataParallel</h3><p>　　基于torch.distributed 的分布式数据并行，原理和DataParallel类似，但是是跨机器和跨设备级别的数据并行，在每台机器和每台设备上复制，并且每个这样的副本处理一部分输入， 在向后传递过程中，将平均每个节点的梯度，总之，是按批处理维度分块指定设备之间的输入来并行化给定模块的应用程序。</p>\n<p><code>DistributedDataParallel</code>可以通过以下两种方式使用：</p>\n<h4 id=\"单线程多GPU\"><a href=\"#单线程多GPU\" class=\"headerlink\" title=\"单线程多GPU\"></a>单线程多GPU</h4><pre class=\" language-python\"><code class=\"language-python\">torch<span class=\"token punctuation\">.</span>distributed<span class=\"token punctuation\">.</span>init_process_group<span class=\"token punctuation\">(</span>backend<span class=\"token operator\">=</span><span class=\"token string\">\"nccl\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\" spellcheck=\"true\"># device_ids will include all GPU devices by default</span>\nmodel <span class=\"token operator\">=</span> DistributedDataParallel<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">)</span> \n</code></pre>\n<h4 id=\"多线程多GPU\"><a href=\"#多线程多GPU\" class=\"headerlink\" title=\"多线程多GPU\"></a>多线程多GPU</h4><p>　　强烈推荐的使用方式，在单机多GPU的情况下，单进程很容易由于GIL出现利用率不足的问题，这时候多进程就是唯一解决办法。最佳实践是，将DDP(DistributedDataParallel)配合多进程一起使用，每个GPU分配一个进程，会比torch.nn.DataParallel快得多，也是目前Pytorch最快的训练方法。</p>\n<p>使用步骤：</p>\n<ol>\n<li>在N个GPU的单机上生成N个进程，这个过程可以交给torch.distributed.launch完成</li>\n</ol>\n<pre class=\" language-bash\"><code class=\"language-bash\">python -m torch.distributed.launch --nproc_per_node<span class=\"token operator\">=</span>n distributed_data_parallel.py</code></pre>\n<ol start=\"2\">\n<li>在代码中绑定GPU 编号，同时并行化model</li>\n</ol>\n<pre class=\" language-python\"><code class=\"language-python\">parser <span class=\"token operator\">=</span> argparse<span class=\"token punctuation\">.</span>ArgumentParser<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\" spellcheck=\"true\"># FOR DISTRIBUTED:  Parse for the local_rank argument, which will be supplied</span>\n<span class=\"token comment\" spellcheck=\"true\"># automatically by torch.distributed.launch.</span>\nparser<span class=\"token punctuation\">.</span>add_argument<span class=\"token punctuation\">(</span><span class=\"token string\">\"--local_rank\"</span><span class=\"token punctuation\">,</span> default<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> type<span class=\"token operator\">=</span>int<span class=\"token punctuation\">)</span>\nargs <span class=\"token operator\">=</span> parser<span class=\"token punctuation\">.</span>parse_args<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># 进程内绑定 GPU rank id</span>\ntorch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>set_device<span class=\"token punctuation\">(</span>args<span class=\"token punctuation\">.</span>local_rank<span class=\"token punctuation\">)</span>\n<span class=\"token comment\" spellcheck=\"true\"># 构造model</span>\ntorch<span class=\"token punctuation\">.</span>distributed<span class=\"token punctuation\">.</span>init_process_group<span class=\"token punctuation\">(</span>backend<span class=\"token operator\">=</span><span class=\"token string\">'nccl'</span><span class=\"token punctuation\">,</span> world_size<span class=\"token operator\">=</span>n<span class=\"token punctuation\">,</span> init_method<span class=\"token operator\">=</span><span class=\"token string\">'env://'</span><span class=\"token punctuation\">)</span>\nmodel <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>parallel<span class=\"token punctuation\">.</span>DistributedDataParallel<span class=\"token punctuation\">(</span>\n                                      model<span class=\"token punctuation\">,</span>\n                                        device_ids<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span>args<span class=\"token punctuation\">.</span>local_rank<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n                                        output_device<span class=\"token operator\">=</span>args<span class=\"token punctuation\">.</span>local_rank<span class=\"token punctuation\">)</span></code></pre>\n<p>Note:</p>\n<ol>\n<li>nccl 后端是分布式训练使用的推荐的最快后端，适用于单节点和多节点分布式训练</li>\n<li>nccl同时支持混合精度分布式训练</li>\n<li>no_sync 用于禁用DDP进程之间的梯度同步，直到退出此上下文区域的第一个梯度Forward-Backward中进行梯度同步</li>\n</ol>\n<pre class=\" language-python\"><code class=\"language-python\">ddp <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>DistributedDataParallel<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span> pg<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">with</span> ddp<span class=\"token punctuation\">.</span>no_sync<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n<span class=\"token keyword\">for</span> input <span class=\"token keyword\">in</span> inputs<span class=\"token punctuation\">:</span>\n    ddp<span class=\"token punctuation\">(</span>input<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\" spellcheck=\"true\"># no synchronization, accumulate grads</span>\nddp<span class=\"token punctuation\">(</span>another_input<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\" spellcheck=\"true\"># synchronize grads</span></code></pre>\n<h3 id=\"apex-parallel-DistributedDataParallel\"><a href=\"#apex-parallel-DistributedDataParallel\" class=\"headerlink\" title=\"apex.parallel.DistributedDataParallel\"></a>apex.parallel.DistributedDataParallel</h3><p>　　基本上是torch.nn.parallel.DistributedDataParallel的wrapper，同时调用的时候优化了NCCL的使用和简化了参数</p>\n<pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token comment\" spellcheck=\"true\"># 调用的时候，注意 n &lt;= 每个节点的GPU数量 同时默认 1个GPU对应1进程</span>\ntorch.distributed.launch --nproc_per_node<span class=\"token operator\">=</span>n distributed_data_parallel.py\n<span class=\"token comment\" spellcheck=\"true\"># 会自动提供的参数目前已知的是:</span>\n<span class=\"token comment\" spellcheck=\"true\"># args.local_rank</span>\n<span class=\"token comment\" spellcheck=\"true\"># os.environ['WORLD_SIZE']</span>\n</code></pre>\n<p>简化的要点：</p>\n<ol>\n<li><p>model = DDP(model) 即可，无需再传递 devices_ids output_device</p>\n</li>\n<li><p>init_process_group 中的 init_method=’env://‘</p>\n<pre class=\" language-python\"><code class=\"language-python\">torch<span class=\"token punctuation\">.</span>distributed<span class=\"token punctuation\">.</span>init_process_group<span class=\"token punctuation\">(</span>backend<span class=\"token operator\">=</span><span class=\"token string\">'nccl'</span><span class=\"token punctuation\">,</span>init_method<span class=\"token operator\">=</span><span class=\"token string\">'env://'</span><span class=\"token punctuation\">)</span></code></pre>\n</li>\n</ol>\n<p>直接给出示例代码</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token comment\" spellcheck=\"true\"># distributed_data_parallel.py</span>\n<span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">import</span> argparse\n<span class=\"token keyword\">import</span> os\n<span class=\"token keyword\">from</span> apex <span class=\"token keyword\">import</span> amp\n<span class=\"token comment\" spellcheck=\"true\"># FOR DISTRIBUTED: (can also use torch.nn.parallel.DistributedDataParallel instead)</span>\n<span class=\"token keyword\">from</span> apex<span class=\"token punctuation\">.</span>parallel <span class=\"token keyword\">import</span> DistributedDataParallel\n\nparser <span class=\"token operator\">=</span> argparse<span class=\"token punctuation\">.</span>ArgumentParser<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\" spellcheck=\"true\"># FOR DISTRIBUTED:  Parse for the local_rank argument, which will be supplied</span>\n<span class=\"token comment\" spellcheck=\"true\"># automatically by torch.distributed.launch.</span>\nparser<span class=\"token punctuation\">.</span>add_argument<span class=\"token punctuation\">(</span><span class=\"token string\">\"--local_rank\"</span><span class=\"token punctuation\">,</span> default<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> type<span class=\"token operator\">=</span>int<span class=\"token punctuation\">)</span>\nargs <span class=\"token operator\">=</span> parser<span class=\"token punctuation\">.</span>parse_args<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># FOR DISTRIBUTED:  If we are running under torch.distributed.launch,</span>\n<span class=\"token comment\" spellcheck=\"true\"># the 'WORLD_SIZE' environment variable will also be set automatically.</span>\nargs<span class=\"token punctuation\">.</span>distributed <span class=\"token operator\">=</span> <span class=\"token boolean\">False</span>\n<span class=\"token keyword\">if</span> <span class=\"token string\">'WORLD_SIZE'</span> <span class=\"token keyword\">in</span> os<span class=\"token punctuation\">.</span>environ<span class=\"token punctuation\">:</span>\n    args<span class=\"token punctuation\">.</span>distributed <span class=\"token operator\">=</span> int<span class=\"token punctuation\">(</span>os<span class=\"token punctuation\">.</span>environ<span class=\"token punctuation\">[</span><span class=\"token string\">'WORLD_SIZE'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">></span> <span class=\"token number\">1</span>\n\n<span class=\"token keyword\">if</span> args<span class=\"token punctuation\">.</span>distributed<span class=\"token punctuation\">:</span>\n    <span class=\"token comment\" spellcheck=\"true\"># FOR DISTRIBUTED:  Set the device according to local_rank.</span>\n    torch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>set_device<span class=\"token punctuation\">(</span>args<span class=\"token punctuation\">.</span>local_rank<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># FOR DISTRIBUTED:  Initialize the backend.  torch.distributed.launch will provide</span>\n    <span class=\"token comment\" spellcheck=\"true\"># environment variables, and requires that you use init_method=`env://`.</span>\n    torch<span class=\"token punctuation\">.</span>distributed<span class=\"token punctuation\">.</span>init_process_group<span class=\"token punctuation\">(</span>backend<span class=\"token operator\">=</span><span class=\"token string\">'nccl'</span><span class=\"token punctuation\">,</span>\n                                         init_method<span class=\"token operator\">=</span><span class=\"token string\">'env://'</span><span class=\"token punctuation\">)</span>\n\ntorch<span class=\"token punctuation\">.</span>backends<span class=\"token punctuation\">.</span>cudnn<span class=\"token punctuation\">.</span>benchmark <span class=\"token operator\">=</span> <span class=\"token boolean\">True</span>\n\nN<span class=\"token punctuation\">,</span> D_in<span class=\"token punctuation\">,</span> D_out <span class=\"token operator\">=</span> <span class=\"token number\">64</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1024</span><span class=\"token punctuation\">,</span> <span class=\"token number\">16</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># Each process receives its own batch of \"fake input data\" and \"fake target data.\"</span>\n<span class=\"token comment\" spellcheck=\"true\"># The \"training loop\" in each process just uses this fake batch over and over.</span>\n<span class=\"token comment\" spellcheck=\"true\"># https://github.com/NVIDIA/apex/tree/master/examples/imagenet provides a more realistic</span>\n<span class=\"token comment\" spellcheck=\"true\"># example of distributed data sampling for both training and validation.</span>\nx <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span>N<span class=\"token punctuation\">,</span> D_in<span class=\"token punctuation\">,</span> device<span class=\"token operator\">=</span><span class=\"token string\">'cuda'</span><span class=\"token punctuation\">)</span>\ny <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span>N<span class=\"token punctuation\">,</span> D_out<span class=\"token punctuation\">,</span> device<span class=\"token operator\">=</span><span class=\"token string\">'cuda'</span><span class=\"token punctuation\">)</span>\n\nmodel <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>D_in<span class=\"token punctuation\">,</span> D_out<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\noptimizer <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>optim<span class=\"token punctuation\">.</span>SGD<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> lr<span class=\"token operator\">=</span><span class=\"token number\">1e</span><span class=\"token operator\">-</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span>\n\nmodel<span class=\"token punctuation\">,</span> optimizer <span class=\"token operator\">=</span> amp<span class=\"token punctuation\">.</span>initialize<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span> optimizer<span class=\"token punctuation\">,</span> opt_level<span class=\"token operator\">=</span><span class=\"token string\">\"O1\"</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">if</span> args<span class=\"token punctuation\">.</span>distributed<span class=\"token punctuation\">:</span>\n    <span class=\"token comment\" spellcheck=\"true\"># FOR DISTRIBUTED:  After amp.initialize, wrap the model with</span>\n    <span class=\"token comment\" spellcheck=\"true\"># apex.parallel.DistributedDataParallel.</span>\n    model <span class=\"token operator\">=</span> DistributedDataParallel<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># torch.nn.parallel.DistributedDataParallel is also fine, with some added args:</span>\n    <span class=\"token comment\" spellcheck=\"true\"># model = torch.nn.parallel.DistributedDataParallel(model,</span>\n    <span class=\"token comment\" spellcheck=\"true\">#                                                   device_ids=[args.local_rank],</span>\n    <span class=\"token comment\" spellcheck=\"true\">#                                                   output_device=args.local_rank)</span>\n\nloss_fn <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>MSELoss<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">for</span> t <span class=\"token keyword\">in</span> range<span class=\"token punctuation\">(</span><span class=\"token number\">500</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    optimizer<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    y_pred <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n    loss <span class=\"token operator\">=</span> loss_fn<span class=\"token punctuation\">(</span>y_pred<span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">with</span> amp<span class=\"token punctuation\">.</span>scale_loss<span class=\"token punctuation\">(</span>loss<span class=\"token punctuation\">,</span> optimizer<span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> scaled_loss<span class=\"token punctuation\">:</span>\n        scaled_loss<span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    optimizer<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">if</span> args<span class=\"token punctuation\">.</span>local_rank <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"final loss = \"</span><span class=\"token punctuation\">,</span> loss<span class=\"token punctuation\">)</span></code></pre>\n<p>更复杂的多精度调用见 <a href=\"https://github.com/NVIDIA/apex/tree/master/examples/imagenet\">mixed precision training with DDP</a></p>\n<h3 id=\"DDP-保存和加载检查点\"><a href=\"#DDP-保存和加载检查点\" class=\"headerlink\" title=\"DDP 保存和加载检查点\"></a>DDP 保存和加载检查点</h3><p>　　一般是使用torch.save 和 torch.load 来完成，但是在多进程下，优化方法是，仅在一个进程中保存，然后在其他所有进程中加载。</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> os\n<span class=\"token keyword\">import</span> tempfile\n<span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>distributed <span class=\"token keyword\">as</span> dist\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>nn <span class=\"token keyword\">as</span> nn\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>optim <span class=\"token keyword\">as</span> optim\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>multiprocessing <span class=\"token keyword\">as</span> mp\n\n<span class=\"token keyword\">from</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>parallel <span class=\"token keyword\">import</span> DistributedDataParallel <span class=\"token keyword\">as</span> DDP\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">demo_checkpoint</span><span class=\"token punctuation\">(</span>rank<span class=\"token punctuation\">,</span> world_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    setup<span class=\"token punctuation\">(</span>rank<span class=\"token punctuation\">,</span> world_size<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># setup devices for this process, rank 1 uses GPUs [0, 1, 2, 3] and</span>\n    <span class=\"token comment\" spellcheck=\"true\"># rank 2 uses GPUs [4, 5, 6, 7].</span>\n    n <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>device_count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">//</span> world_size\n    device_ids <span class=\"token operator\">=</span> list<span class=\"token punctuation\">(</span>range<span class=\"token punctuation\">(</span>rank <span class=\"token operator\">*</span> n<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>rank <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    model <span class=\"token operator\">=</span> ToyModel<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device_ids<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># output_device defaults to device_ids[0]</span>\n    ddp_model <span class=\"token operator\">=</span> DDP<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span> device_ids<span class=\"token operator\">=</span>device_ids<span class=\"token punctuation\">)</span>\n\n    loss_fn <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>MSELoss<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    optimizer <span class=\"token operator\">=</span> optim<span class=\"token punctuation\">.</span>SGD<span class=\"token punctuation\">(</span>ddp_model<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> lr<span class=\"token operator\">=</span><span class=\"token number\">0.001</span><span class=\"token punctuation\">)</span>\n\n    CHECKPOINT_PATH <span class=\"token operator\">=</span> tempfile<span class=\"token punctuation\">.</span>gettempdir<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> <span class=\"token string\">\"/model.checkpoint\"</span>\n    <span class=\"token keyword\">if</span> rank <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n        <span class=\"token comment\" spellcheck=\"true\"># All processes should see same parameters as they all start from same</span>\n        <span class=\"token comment\" spellcheck=\"true\"># random parameters and gradients are synchronized in backward passes.</span>\n        <span class=\"token comment\" spellcheck=\"true\"># Therefore, saving it in one process is sufficient.</span>\n        torch<span class=\"token punctuation\">.</span>save<span class=\"token punctuation\">(</span>ddp_model<span class=\"token punctuation\">.</span>state_dict<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> CHECKPOINT_PATH<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># Use a barrier() to make sure that process 1 loads the model after process</span>\n    <span class=\"token comment\" spellcheck=\"true\"># 0 saves it.</span>\n    dist<span class=\"token punctuation\">.</span>barrier<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># configure map_location properly</span>\n    rank0_devices <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>x <span class=\"token operator\">-</span> rank <span class=\"token operator\">*</span> len<span class=\"token punctuation\">(</span>device_ids<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> device_ids<span class=\"token punctuation\">]</span>\n    device_pairs <span class=\"token operator\">=</span> zip<span class=\"token punctuation\">(</span>rank0_devices<span class=\"token punctuation\">,</span> device_ids<span class=\"token punctuation\">)</span>\n    map_location <span class=\"token operator\">=</span> <span class=\"token operator\">&amp;</span><span class=\"token comment\" spellcheck=\"true\">#123;'cuda:%d' % x: 'cuda:%d' % y for x, y in device_pairs&amp;#125;</span>\n    ddp_model<span class=\"token punctuation\">.</span>load_state_dict<span class=\"token punctuation\">(</span>\n        torch<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span>CHECKPOINT_PATH<span class=\"token punctuation\">,</span> map_location<span class=\"token operator\">=</span>map_location<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    optimizer<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    outputs <span class=\"token operator\">=</span> ddp_model<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span><span class=\"token number\">20</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    labels <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span><span class=\"token number\">20</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device_ids<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    loss_fn <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>MSELoss<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    loss_fn<span class=\"token punctuation\">(</span>outputs<span class=\"token punctuation\">,</span> labels<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    optimizer<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># Use a barrier() to make sure that all processes have finished reading the</span>\n    <span class=\"token comment\" spellcheck=\"true\"># checkpoint</span>\n    dist<span class=\"token punctuation\">.</span>barrier<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">if</span> rank <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n        os<span class=\"token punctuation\">.</span>remove<span class=\"token punctuation\">(</span>CHECKPOINT_PATH<span class=\"token punctuation\">)</span>\n\n    cleanup<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre>\n<h3 id=\"DDP-与模型拼接并行\"><a href=\"#DDP-与模型拼接并行\" class=\"headerlink\" title=\"DDP 与模型拼接并行\"></a>DDP 与模型拼接并行</h3><p>　　DDP 还可以与多 GPU 模型一起使用，但是不支持进程内的复制。 您需要为每个模块副本创建一个进程，与每个进程的多个副本相比，通常可以提高性能。 当训练具有大量数据的大型模型时，DDP 包装多 GPU 模型特别有用。 使用此功能时，需要小心地实现多 GPU 模型，以避免使用硬编码的设备，因为会将不同的模型副本放置到不同的设备上。</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">ToyMpModel</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> dev0<span class=\"token punctuation\">,</span> dev1<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        super<span class=\"token punctuation\">(</span>ToyMpModel<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>dev0 <span class=\"token operator\">=</span> dev0\n        self<span class=\"token punctuation\">.</span>dev1 <span class=\"token operator\">=</span> dev1\n        self<span class=\"token punctuation\">.</span>net1 <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>dev0<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>relu <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>ReLU<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>net2 <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>dev1<span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        x <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>dev0<span class=\"token punctuation\">)</span>\n        x <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>relu<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>net1<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        x <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>dev1<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> self<span class=\"token punctuation\">.</span>net2<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span></code></pre>\n<p>将多 GPU 模型传递给 DDP 时，不得设置<code>device_ids</code>和<code>output_device</code>。 输入和输出数据将通过应用程序或模型<code>forward()</code>方法放置在适当的设备中。</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">demo_model_parallel</span><span class=\"token punctuation\">(</span>rank<span class=\"token punctuation\">,</span> world_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    setup<span class=\"token punctuation\">(</span>rank<span class=\"token punctuation\">,</span> world_size<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># setup mp_model and devices for this process</span>\n    dev0 <span class=\"token operator\">=</span> rank <span class=\"token operator\">*</span> <span class=\"token number\">2</span>\n    dev1 <span class=\"token operator\">=</span> rank <span class=\"token operator\">*</span> <span class=\"token number\">2</span> <span class=\"token operator\">+</span> <span class=\"token number\">1</span>\n    mp_model <span class=\"token operator\">=</span> ToyMpModel<span class=\"token punctuation\">(</span>dev0<span class=\"token punctuation\">,</span> dev1<span class=\"token punctuation\">)</span>\n    ddp_mp_model <span class=\"token operator\">=</span> DDP<span class=\"token punctuation\">(</span>mp_model<span class=\"token punctuation\">)</span>\n\n    loss_fn <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>MSELoss<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    optimizer <span class=\"token operator\">=</span> optim<span class=\"token punctuation\">.</span>SGD<span class=\"token punctuation\">(</span>ddp_mp_model<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> lr<span class=\"token operator\">=</span><span class=\"token number\">0.001</span><span class=\"token punctuation\">)</span>\n\n    optimizer<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># outputs will be on dev1</span>\n    outputs <span class=\"token operator\">=</span> ddp_mp_model<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span><span class=\"token number\">20</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    labels <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span><span class=\"token number\">20</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>dev1<span class=\"token punctuation\">)</span>\n    loss_fn<span class=\"token punctuation\">(</span>outputs<span class=\"token punctuation\">,</span> labels<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    optimizer<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    cleanup<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">if</span> __name__ <span class=\"token operator\">==</span> <span class=\"token string\">\"__main__\"</span><span class=\"token punctuation\">:</span>\n    run_demo<span class=\"token punctuation\">(</span>demo_basic<span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n    run_demo<span class=\"token punctuation\">(</span>demo_checkpoint<span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">if</span> torch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>device_count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">>=</span> <span class=\"token number\">8</span><span class=\"token punctuation\">:</span>\n        run_demo<span class=\"token punctuation\">(</span>demo_model_parallel<span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token punctuation\">)</span></code></pre>\n<p>注意上述setup 是通过 torch.multiprocessing.spawn 来完成多线程启动的，所以初始化方法也需要通过setup单独来指定</p>\n<pre class=\" language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> os\n<span class=\"token keyword\">import</span> tempfile\n<span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>distributed <span class=\"token keyword\">as</span> dist\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>nn <span class=\"token keyword\">as</span> nn\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>optim <span class=\"token keyword\">as</span> optim\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>multiprocessing <span class=\"token keyword\">as</span> mp\n\n<span class=\"token keyword\">from</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>parallel <span class=\"token keyword\">import</span> DistributedDataParallel <span class=\"token keyword\">as</span> DDP\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">setup</span><span class=\"token punctuation\">(</span>rank<span class=\"token punctuation\">,</span> world_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    os<span class=\"token punctuation\">.</span>environ<span class=\"token punctuation\">[</span><span class=\"token string\">'MASTER_ADDR'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token string\">'localhost'</span>\n    os<span class=\"token punctuation\">.</span>environ<span class=\"token punctuation\">[</span><span class=\"token string\">'MASTER_PORT'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token string\">'12355'</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># initialize the process group</span>\n    dist<span class=\"token punctuation\">.</span>init_process_group<span class=\"token punctuation\">(</span><span class=\"token string\">\"gloo\"</span><span class=\"token punctuation\">,</span> rank<span class=\"token operator\">=</span>rank<span class=\"token punctuation\">,</span> world_size<span class=\"token operator\">=</span>world_size<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># Explicitly setting seed to make sure that models created in two processes</span>\n    <span class=\"token comment\" spellcheck=\"true\"># start from same random weights and biases.</span>\n    torch<span class=\"token punctuation\">.</span>manual_seed<span class=\"token punctuation\">(</span><span class=\"token number\">42</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">cleanup</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    dist<span class=\"token punctuation\">.</span>destroy_process_group<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">ToyModel</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        super<span class=\"token punctuation\">(</span>ToyModel<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>net1 <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>relu <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>ReLU<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>net2 <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> self<span class=\"token punctuation\">.</span>net2<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>relu<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>net1<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">demo_basic</span><span class=\"token punctuation\">(</span>rank<span class=\"token punctuation\">,</span> world_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    setup<span class=\"token punctuation\">(</span>rank<span class=\"token punctuation\">,</span> world_size<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># setup devices for this process, rank 1 uses GPUs [0, 1, 2, 3] and</span>\n    <span class=\"token comment\" spellcheck=\"true\"># rank 2 uses GPUs [4, 5, 6, 7].</span>\n    n <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>device_count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">//</span> world_size\n    device_ids <span class=\"token operator\">=</span> list<span class=\"token punctuation\">(</span>range<span class=\"token punctuation\">(</span>rank <span class=\"token operator\">*</span> n<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>rank <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># create model and move it to device_ids[0]</span>\n    model <span class=\"token operator\">=</span> ToyModel<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device_ids<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\" spellcheck=\"true\"># output_device defaults to device_ids[0]</span>\n    ddp_model <span class=\"token operator\">=</span> DDP<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span> device_ids<span class=\"token operator\">=</span>device_ids<span class=\"token punctuation\">)</span>\n\n    loss_fn <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>MSELoss<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    optimizer <span class=\"token operator\">=</span> optim<span class=\"token punctuation\">.</span>SGD<span class=\"token punctuation\">(</span>ddp_model<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> lr<span class=\"token operator\">=</span><span class=\"token number\">0.001</span><span class=\"token punctuation\">)</span>\n\n    optimizer<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    outputs <span class=\"token operator\">=</span> ddp_model<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span><span class=\"token number\">20</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    labels <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span><span class=\"token number\">20</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device_ids<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    loss_fn<span class=\"token punctuation\">(</span>outputs<span class=\"token punctuation\">,</span> labels<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    optimizer<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    cleanup<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">run_demo</span><span class=\"token punctuation\">(</span>demo_fn<span class=\"token punctuation\">,</span> world_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    mp<span class=\"token punctuation\">.</span>spawn<span class=\"token punctuation\">(</span>demo_fn<span class=\"token punctuation\">,</span>\n             args<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>world_size<span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n             nprocs<span class=\"token operator\">=</span>world_size<span class=\"token punctuation\">,</span>\n             join<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span></code></pre>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>　　至此基本分析完所有Pytorch下的多GPU使用技巧，大体方案还是利用多进程的方式来规避GIL来提升性能，只需要用DDP 来包装model ，同时进行分布式对应的初始化，然后多进程启动即可加速训练，基本没有大的变动。</p>\n<p>References:</p>\n<p><a href=\"https://pytorch.apachecn.org/docs/1.4/75.html\">torch.nn</a></p>\n<p><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html?highlight=distributeddataparallel#torch.nn.parallel.DistributedDataParallel\">DistributedDataParallel API</a></p>\n<p><a href=\"https://pytorch.org/docs/stable/notes/cuda.html#cuda-nn-ddp-instead\">CUDA Semantics</a></p>\n<p><a href=\"https://pytorch.apachecn.org/docs/1.4/34.html\">分布式数据并行入门</a></p>\n<p><a href=\"https://pytorch.apachecn.org/docs/1.4/35.html\">用Pytorch编写分布式应用程序</a></p>\n<p><a href=\"https://nvidia.github.io/apex/parallel.html\">apex.parallel</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>　　虽然还没有机会用到CUDA集群，但是前段时间对协程和并行化的研究，让我忍不住想要探索一下如何在多个GPU下利用Pytorch加快训练的实践方法，算是为之后并行训练优化一个理论参考吧！</p>\n<p>　　Pytorch 大体上有3种实现并行的接口（另外还有一种不利用接口的拼接模型的技巧，之后再单独讨论），分别是：torch.multiprocessing, nn.DataParallel, nn.parallel.DistributedDataParallel，如果是是GPU多卡运行，最佳实践是 nn.parallle.DistributedDataParallel,官方文档 <a href=\"https://pytorch.org/docs/stable/notes/cuda.html#cuda-nn-ddp-instead\">CUDA SEMANTIC</a> 是这么描述的：</p>\n<blockquote>\n<p><strong>Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel</strong></p>\n<p>Most use cases involving batched inputs and multiple GPUs should default to using <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\"><code>DistributedDataParallel</code></a> to utilize more than one GPU.</p>\n<p>There are significant caveats to using CUDA models with <a href=\"https://pytorch.org/docs/stable/multiprocessing.html#module-torch.multiprocessing\"><code>multiprocessing</code></a>; unless care is taken to meet the data handling requirements exactly, it is likely that your program will have incorrect or undefined behavior.</p>\n<p>It is recommended to use <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\"><code>DistributedDataParallel</code></a>, instead of <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel\"><code>DataParallel</code></a> to do multi-GPU training, even if there is only a single node.</p>\n<p>The difference between <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\"><code>DistributedDataParallel</code></a> and <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel\"><code>DataParallel</code></a> is: <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\"><code>DistributedDataParallel</code></a> uses multiprocessing where a process is created for each GPU, while <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel\"><code>DataParallel</code></a> uses multithreading. By using multiprocessing, each GPU has its dedicated process, this avoids the performance overhead caused by GIL of Python interpreter.</p>\n<p>If you use <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\"><code>DistributedDataParallel</code></a>, you could use torch.distributed.launch utility to launch your program, see <a href=\"https://pytorch.org/docs/stable/distributed.html#distributed-launch\">Third-party backends</a>.</p>\n</blockquote>\n<h3 id=\"torch-multiprocessing\"><a href=\"#torch-multiprocessing\" class=\"headerlink\" title=\"torch.multiprocessing\"></a>torch.multiprocessing</h3><p>　　torch.multiprocessing 是 Python下 multiprocessing 的替代品，接口基本一致，并根据情况进行扩张，建议使用 python: multiprocessing.Queue 在进程中传递Pytorch对象。除此之外，还有很多坑，详见官方笔记 <a href=\"https://pytorch.org/docs/stable/notes/multiprocessing.html\">MULTIPROCESSING BEST PRACTICES</a>, <a href=\"https://pytorch.apachecn.org/docs/1.4/64.html\">并行处理最佳实践</a></p>\n<pre><code class=\"python\">import torch.multiprocessing as mp\nfrom model import MyModel\n\ndef train(model):\n    # Construct data_loader, optimizer, etc.\n    for data, labels in data_loader:\n        optimizer.zero_grad()\n        loss_fn(model(data), labels).backward()\n        optimizer.step()  # This will update the shared parameters\n\nif __name__ == &#39;__main__&#39;:\n    num_processes = 4\n    model = MyModel()\n    # NOTE: this is required for the ``fork`` method to work\n    model.share_memory()\n    processes = []\n    for rank in range(num_processes):\n        p = mp.Process(target=train, args=(model,))\n        p.start()\n        processes.append(p)\n    for p in processes:\n        p.join()</code></pre>\n<h3 id=\"DataParallel\"><a href=\"#DataParallel\" class=\"headerlink\" title=\"DataParallel\"></a>DataParallel</h3><p>　　DataParallel原理是，把model 副本copy到所有GPU上，其中每个GPU消耗数据的不同分区，相当于SIMD，把数据条目根据GPU数量重新分配</p>\n<h4 id=\"单机DataParallel并行\"><a href=\"#单机DataParallel并行\" class=\"headerlink\" title=\"单机DataParallel并行\"></a>单机DataParallel并行</h4><pre><code class=\"python\">model = nn.DataParallel(model)</code></pre>\n<p>　　代码验证 outside model 数据维度  和 inside model 维度</p>\n<pre><code class=\"python\">class Model(nn.Module):\n    # Our model\n\n    def __init__(self, input_size, output_size):\n        super(Model, self).__init__()\n        self.fc = nn.Linear(input_size, output_size)\n\n    def forward(self, input):\n        output = self.fc(input)\n        print(&quot;\\tIn Model: input size&quot;, input.size(),\n              &quot;output size&quot;, output.size())\n\n        return output\n\n\nmodel = Model(input_size, output_size)\nif torch.cuda.device_count() &gt; 1:\n  print(&quot;Let&#39;s use&quot;, torch.cuda.device_count(), &quot;GPUs!&quot;)\n  # dim = 0 [30, xxx] -&gt; [10, ...], [10, ...], [10, ...] on 3 GPUs\n  model = nn.DataParallel(model)\n\nmodel.to(device)\n\nfor data in rand_loader:\n    input = data.to(device)\n    output = model(input)\n    print(&quot;Outside: input size&quot;, input.size(),\n          &quot;output_size&quot;, output.size())\n\n# 2 GPUs\n# on 2 GPUs\nLet&#39;s use 2 GPUs!\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\nOutside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</code></pre>\n<p>优点是，通过消耗输入数据不同分区的方式加快训练过程，缺点是，如果单个模型太大，无法放入单个GPU，就无法运行，这时有个Trick可以把模型分段载入不同GPU，利用拼接的方式完成并行训练，该模型将单个模型拆分到不同的 GPU 上，而不是在每个 GPU 上复制整个模型(具体来说， 假设模型<code>m</code>包含 10 层：使用<code>DataParallel</code>时，每个 GPU 都具有这 10 层中每个层的副本，而当在两个 GPU 上并行使用模型时，每个 GPU 可以承载 5 层）</p>\n<h4 id=\"单机模型拼接并行\"><a href=\"#单机模型拼接并行\" class=\"headerlink\" title=\"单机模型拼接并行\"></a>单机模型拼接并行</h4><pre><code class=\"python\">import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = torch.nn.Linear(10, 10).to(&#39;cuda:0&#39;)\n        self.relu = torch.nn.ReLU()\n        self.net2 = torch.nn.Linear(10, 5).to(&#39;cuda:1&#39;)\n\n    def forward(self, x):\n        x = self.relu(self.net1(x.to(&#39;cuda:0&#39;)))\n        return self.net2(x.to(&#39;cuda:1&#39;))</code></pre>\n<p>对于模型太大而无法放入单个 GPU 的情况，上述实现解决了该问题。 但是，分析原理的时候可能已经注意到，如果模型单个GPU可以载入，它将比在单个 GPU 上运行它要慢。 这是因为在任何时间点，两个 GPU 中只有一个在工作，而另一个在那儿什么也没做。 由于中间输出需要在<code>layer2</code>和<code>layer3</code>之间从<code>cuda:0</code>复制到<code>cuda:1</code>，因此性能进一步恶化。</p>\n<p>除此之外，还有通过异步方式构建流水线的手段加速，这个Trick比较精巧和优雅，思想可以学习一下</p>\n<h4 id=\"单机Pipeline-并行\"><a href=\"#单机Pipeline-并行\" class=\"headerlink\" title=\"单机Pipeline 并行\"></a>单机Pipeline 并行</h4><pre><code class=\"python\">class PipelineParallelResNet50(ModelParallelResNet50):\n    def __init__(self, split_size=20, *args, **kwargs):\n        super(PipelineParallelResNet50, self).__init__(*args, **kwargs)\n        self.split_size = split_size\n\n    def forward(self, x):\n        splits = iter(x.split(self.split_size, dim=0))\n        s_next = next(splits)\n        s_prev = self.seq1(s_next).to(&#39;cuda:1&#39;)\n        ret = []\n\n        for s_next in splits:\n            # A. s_prev runs on cuda:1\n            s_prev = self.seq2(s_prev)\n            ret.append(self.fc(s_prev.view(s_prev.size(0), -1)))\n\n            # B. s_next runs on cuda:0, which can run concurrently with A\n            s_prev = self.seq1(s_next).to(&#39;cuda:1&#39;)\n\n        s_prev = self.seq2(s_prev)\n        ret.append(self.fc(s_prev.view(s_prev.size(0), -1)))\n\n        return torch.cat(ret)\n\nsetup = &quot;model = PipelineParallelResNet50()&quot;\npp_run_times = timeit.repeat(\n    stmt, setup, number=1, repeat=num_repeat, globals=globals())\npp_mean, pp_std = np.mean(pp_run_times), np.std(pp_run_times)\n\nplot([mp_mean, rn_mean, pp_mean],\n     [mp_std, rn_std, pp_std],\n     [&#39;Model Parallel&#39;, &#39;Single GPU&#39;, &#39;Pipelining Model Parallel&#39;],\n     &#39;mp_vs_rn_vs_pp.png&#39;)</code></pre>\n<p>在2 GPUs上能有50%的提速，离100%还是有点差距，加速对比如下</p>\n<p>###<img src=\"1.png\" alt=\"1\"> </p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p>依次是 2-GPU并行， 1-GPU运行， 2-GPU Pipelining 并行 的运行时间对比</p>\n<h3 id=\"nn-parallel-DistributedDataParallel\"><a href=\"#nn-parallel-DistributedDataParallel\" class=\"headerlink\" title=\"nn.parallel.DistributedDataParallel\"></a>nn.parallel.DistributedDataParallel</h3><p>　　基于torch.distributed 的分布式数据并行，原理和DataParallel类似，但是是跨机器和跨设备级别的数据并行，在每台机器和每台设备上复制，并且每个这样的副本处理一部分输入， 在向后传递过程中，将平均每个节点的梯度，总之，是按批处理维度分块指定设备之间的输入来并行化给定模块的应用程序。</p>\n<p><code>DistributedDataParallel</code>可以通过以下两种方式使用：</p>\n<h4 id=\"单线程多GPU\"><a href=\"#单线程多GPU\" class=\"headerlink\" title=\"单线程多GPU\"></a>单线程多GPU</h4><pre><code class=\"python\">torch.distributed.init_process_group(backend=&quot;nccl&quot;)\n# device_ids will include all GPU devices by default\nmodel = DistributedDataParallel(model) \n</code></pre>\n<h4 id=\"多线程多GPU\"><a href=\"#多线程多GPU\" class=\"headerlink\" title=\"多线程多GPU\"></a>多线程多GPU</h4><p>　　强烈推荐的使用方式，在单机多GPU的情况下，单进程很容易由于GIL出现利用率不足的问题，这时候多进程就是唯一解决办法。最佳实践是，将DDP(DistributedDataParallel)配合多进程一起使用，每个GPU分配一个进程，会比torch.nn.DataParallel快得多，也是目前Pytorch最快的训练方法。</p>\n<p>使用步骤：</p>\n<ol>\n<li>在N个GPU的单机上生成N个进程，这个过程可以交给torch.distributed.launch完成</li>\n</ol>\n<pre><code class=\"bash\">python -m torch.distributed.launch --nproc_per_node=n distributed_data_parallel.py</code></pre>\n<ol start=\"2\">\n<li>在代码中绑定GPU 编号，同时并行化model</li>\n</ol>\n<pre><code class=\"python\">parser = argparse.ArgumentParser()\n# FOR DISTRIBUTED:  Parse for the local_rank argument, which will be supplied\n# automatically by torch.distributed.launch.\nparser.add_argument(&quot;--local_rank&quot;, default=0, type=int)\nargs = parser.parse_args()\n\n# 进程内绑定 GPU rank id\ntorch.cuda.set_device(args.local_rank)\n# 构造model\ntorch.distributed.init_process_group(backend=&#39;nccl&#39;, world_size=n, init_method=&#39;env://&#39;)\nmodel = torch.nn.parallel.DistributedDataParallel(\n                                      model,\n                                        device_ids=[args.local_rank],\n                                        output_device=args.local_rank)</code></pre>\n<p>Note:</p>\n<ol>\n<li>nccl 后端是分布式训练使用的推荐的最快后端，适用于单节点和多节点分布式训练</li>\n<li>nccl同时支持混合精度分布式训练</li>\n<li>no_sync 用于禁用DDP进程之间的梯度同步，直到退出此上下文区域的第一个梯度Forward-Backward中进行梯度同步</li>\n</ol>\n<pre><code class=\"python\">ddp = torch.nn.DistributedDataParallel(model, pg)\nwith ddp.no_sync():\nfor input in inputs:\n    ddp(input).backward()  # no synchronization, accumulate grads\nddp(another_input).backward()  # synchronize grads</code></pre>\n<h3 id=\"apex-parallel-DistributedDataParallel\"><a href=\"#apex-parallel-DistributedDataParallel\" class=\"headerlink\" title=\"apex.parallel.DistributedDataParallel\"></a>apex.parallel.DistributedDataParallel</h3><p>　　基本上是torch.nn.parallel.DistributedDataParallel的wrapper，同时调用的时候优化了NCCL的使用和简化了参数</p>\n<pre><code class=\"bash\"># 调用的时候，注意 n &lt;= 每个节点的GPU数量 同时默认 1个GPU对应1进程\ntorch.distributed.launch --nproc_per_node=n distributed_data_parallel.py\n# 会自动提供的参数目前已知的是:\n# args.local_rank\n# os.environ[&#39;WORLD_SIZE&#39;]\n</code></pre>\n<p>简化的要点：</p>\n<ol>\n<li><p>model = DDP(model) 即可，无需再传递 devices_ids output_device</p>\n</li>\n<li><p>init_process_group 中的 init_method=’env://‘</p>\n<pre><code class=\"python\">torch.distributed.init_process_group(backend=&#39;nccl&#39;,init_method=&#39;env://&#39;)</code></pre>\n</li>\n</ol>\n<p>直接给出示例代码</p>\n<pre><code class=\"python\"># distributed_data_parallel.py\nimport torch\nimport argparse\nimport os\nfrom apex import amp\n# FOR DISTRIBUTED: (can also use torch.nn.parallel.DistributedDataParallel instead)\nfrom apex.parallel import DistributedDataParallel\n\nparser = argparse.ArgumentParser()\n# FOR DISTRIBUTED:  Parse for the local_rank argument, which will be supplied\n# automatically by torch.distributed.launch.\nparser.add_argument(&quot;--local_rank&quot;, default=0, type=int)\nargs = parser.parse_args()\n\n# FOR DISTRIBUTED:  If we are running under torch.distributed.launch,\n# the &#39;WORLD_SIZE&#39; environment variable will also be set automatically.\nargs.distributed = False\nif &#39;WORLD_SIZE&#39; in os.environ:\n    args.distributed = int(os.environ[&#39;WORLD_SIZE&#39;]) &gt; 1\n\nif args.distributed:\n    # FOR DISTRIBUTED:  Set the device according to local_rank.\n    torch.cuda.set_device(args.local_rank)\n\n    # FOR DISTRIBUTED:  Initialize the backend.  torch.distributed.launch will provide\n    # environment variables, and requires that you use init_method=`env://`.\n    torch.distributed.init_process_group(backend=&#39;nccl&#39;,\n                                         init_method=&#39;env://&#39;)\n\ntorch.backends.cudnn.benchmark = True\n\nN, D_in, D_out = 64, 1024, 16\n\n# Each process receives its own batch of &quot;fake input data&quot; and &quot;fake target data.&quot;\n# The &quot;training loop&quot; in each process just uses this fake batch over and over.\n# https://github.com/NVIDIA/apex/tree/master/examples/imagenet provides a more realistic\n# example of distributed data sampling for both training and validation.\nx = torch.randn(N, D_in, device=&#39;cuda&#39;)\ny = torch.randn(N, D_out, device=&#39;cuda&#39;)\n\nmodel = torch.nn.Linear(D_in, D_out).cuda()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n\nmodel, optimizer = amp.initialize(model, optimizer, opt_level=&quot;O1&quot;)\n\nif args.distributed:\n    # FOR DISTRIBUTED:  After amp.initialize, wrap the model with\n    # apex.parallel.DistributedDataParallel.\n    model = DistributedDataParallel(model)\n    # torch.nn.parallel.DistributedDataParallel is also fine, with some added args:\n    # model = torch.nn.parallel.DistributedDataParallel(model,\n    #                                                   device_ids=[args.local_rank],\n    #                                                   output_device=args.local_rank)\n\nloss_fn = torch.nn.MSELoss()\n\nfor t in range(500):\n    optimizer.zero_grad()\n    y_pred = model(x)\n    loss = loss_fn(y_pred, y)\n    with amp.scale_loss(loss, optimizer) as scaled_loss:\n        scaled_loss.backward()\n    optimizer.step()\n\nif args.local_rank == 0:\n    print(&quot;final loss = &quot;, loss)</code></pre>\n<p>更复杂的多精度调用见 <a href=\"https://github.com/NVIDIA/apex/tree/master/examples/imagenet\">mixed precision training with DDP</a></p>\n<h3 id=\"DDP-保存和加载检查点\"><a href=\"#DDP-保存和加载检查点\" class=\"headerlink\" title=\"DDP 保存和加载检查点\"></a>DDP 保存和加载检查点</h3><p>　　一般是使用torch.save 和 torch.load 来完成，但是在多进程下，优化方法是，仅在一个进程中保存，然后在其他所有进程中加载。</p>\n<pre><code class=\"python\">import os\nimport tempfile\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\ndef demo_checkpoint(rank, world_size):\n    setup(rank, world_size)\n\n    # setup devices for this process, rank 1 uses GPUs [0, 1, 2, 3] and\n    # rank 2 uses GPUs [4, 5, 6, 7].\n    n = torch.cuda.device_count() // world_size\n    device_ids = list(range(rank * n, (rank + 1) * n))\n\n    model = ToyModel().to(device_ids[0])\n    # output_device defaults to device_ids[0]\n    ddp_model = DDP(model, device_ids=device_ids)\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    CHECKPOINT_PATH = tempfile.gettempdir() + &quot;/model.checkpoint&quot;\n    if rank == 0:\n        # All processes should see same parameters as they all start from same\n        # random parameters and gradients are synchronized in backward passes.\n        # Therefore, saving it in one process is sufficient.\n        torch.save(ddp_model.state_dict(), CHECKPOINT_PATH)\n\n    # Use a barrier() to make sure that process 1 loads the model after process\n    # 0 saves it.\n    dist.barrier()\n    # configure map_location properly\n    rank0_devices = [x - rank * len(device_ids) for x in device_ids]\n    device_pairs = zip(rank0_devices, device_ids)\n    map_location = &#123;&#39;cuda:%d&#39; % x: &#39;cuda:%d&#39; % y for x, y in device_pairs&#125;\n    ddp_model.load_state_dict(\n        torch.load(CHECKPOINT_PATH, map_location=map_location))\n\n    optimizer.zero_grad()\n    outputs = ddp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(device_ids[0])\n    loss_fn = nn.MSELoss()\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    # Use a barrier() to make sure that all processes have finished reading the\n    # checkpoint\n    dist.barrier()\n\n    if rank == 0:\n        os.remove(CHECKPOINT_PATH)\n\n    cleanup()</code></pre>\n<h3 id=\"DDP-与模型拼接并行\"><a href=\"#DDP-与模型拼接并行\" class=\"headerlink\" title=\"DDP 与模型拼接并行\"></a>DDP 与模型拼接并行</h3><p>　　DDP 还可以与多 GPU 模型一起使用，但是不支持进程内的复制。 您需要为每个模块副本创建一个进程，与每个进程的多个副本相比，通常可以提高性能。 当训练具有大量数据的大型模型时，DDP 包装多 GPU 模型特别有用。 使用此功能时，需要小心地实现多 GPU 模型，以避免使用硬编码的设备，因为会将不同的模型副本放置到不同的设备上。</p>\n<pre><code class=\"python\">class ToyMpModel(nn.Module):\n    def __init__(self, dev0, dev1):\n        super(ToyMpModel, self).__init__()\n        self.dev0 = dev0\n        self.dev1 = dev1\n        self.net1 = torch.nn.Linear(10, 10).to(dev0)\n        self.relu = torch.nn.ReLU()\n        self.net2 = torch.nn.Linear(10, 5).to(dev1)\n\n    def forward(self, x):\n        x = x.to(self.dev0)\n        x = self.relu(self.net1(x))\n        x = x.to(self.dev1)\n        return self.net2(x)</code></pre>\n<p>将多 GPU 模型传递给 DDP 时，不得设置<code>device_ids</code>和<code>output_device</code>。 输入和输出数据将通过应用程序或模型<code>forward()</code>方法放置在适当的设备中。</p>\n<pre><code class=\"python\">def demo_model_parallel(rank, world_size):\n    setup(rank, world_size)\n\n    # setup mp_model and devices for this process\n    dev0 = rank * 2\n    dev1 = rank * 2 + 1\n    mp_model = ToyMpModel(dev0, dev1)\n    ddp_mp_model = DDP(mp_model)\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_mp_model.parameters(), lr=0.001)\n\n    optimizer.zero_grad()\n    # outputs will be on dev1\n    outputs = ddp_mp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(dev1)\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    cleanup()\n\nif __name__ == &quot;__main__&quot;:\n    run_demo(demo_basic, 2)\n    run_demo(demo_checkpoint, 2)\n\n    if torch.cuda.device_count() &gt;= 8:\n        run_demo(demo_model_parallel, 4)</code></pre>\n<p>注意上述setup 是通过 torch.multiprocessing.spawn 来完成多线程启动的，所以初始化方法也需要通过setup单独来指定</p>\n<pre><code class=\"python\">import os\nimport tempfile\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\ndef setup(rank, world_size):\n    os.environ[&#39;MASTER_ADDR&#39;] = &#39;localhost&#39;\n    os.environ[&#39;MASTER_PORT&#39;] = &#39;12355&#39;\n\n    # initialize the process group\n    dist.init_process_group(&quot;gloo&quot;, rank=rank, world_size=world_size)\n\n    # Explicitly setting seed to make sure that models created in two processes\n    # start from same random weights and biases.\n    torch.manual_seed(42)\n\ndef cleanup():\n    dist.destroy_process_group()\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(10, 10)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(10, 5)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\ndef demo_basic(rank, world_size):\n    setup(rank, world_size)\n\n    # setup devices for this process, rank 1 uses GPUs [0, 1, 2, 3] and\n    # rank 2 uses GPUs [4, 5, 6, 7].\n    n = torch.cuda.device_count() // world_size\n    device_ids = list(range(rank * n, (rank + 1) * n))\n\n    # create model and move it to device_ids[0]\n    model = ToyModel().to(device_ids[0])\n    # output_device defaults to device_ids[0]\n    ddp_model = DDP(model, device_ids=device_ids)\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    optimizer.zero_grad()\n    outputs = ddp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(device_ids[0])\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    cleanup()\n\ndef run_demo(demo_fn, world_size):\n    mp.spawn(demo_fn,\n             args=(world_size,),\n             nprocs=world_size,\n             join=True)</code></pre>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>　　至此基本分析完所有Pytorch下的多GPU使用技巧，大体方案还是利用多进程的方式来规避GIL来提升性能，只需要用DDP 来包装model ，同时进行分布式对应的初始化，然后多进程启动即可加速训练，基本没有大的变动。</p>\n<p>References:</p>\n<p><a href=\"https://pytorch.apachecn.org/docs/1.4/75.html\">torch.nn</a></p>\n<p><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html?highlight=distributeddataparallel#torch.nn.parallel.DistributedDataParallel\">DistributedDataParallel API</a></p>\n<p><a href=\"https://pytorch.org/docs/stable/notes/cuda.html#cuda-nn-ddp-instead\">CUDA Semantics</a></p>\n<p><a href=\"https://pytorch.apachecn.org/docs/1.4/34.html\">分布式数据并行入门</a></p>\n<p><a href=\"https://pytorch.apachecn.org/docs/1.4/35.html\">用Pytorch编写分布式应用程序</a></p>\n<p><a href=\"https://nvidia.github.io/apex/parallel.html\">apex.parallel</a></p>\n"},{"title":"LeetCode 1-100 刷题有感","toc":true,"mathjax":true,"top":false,"cover":true,"date":"2020-12-07T03:06:05.000Z","updated":"2020-12-10T08:01:47.037Z","_content":"\n　　跌跌撞撞终于在前几天完成的LeetCode 100题，刷题过程印证了之前对自我的一个论断，起了个大早，干了个晚集。 早在2012年，自己就开始了解到LeetCode 刷完前100题的神迹，这时LeetCode应该还没创立多久吧！当时国内只有北大的POJ可用，那曲高和寡的界面和难度，着实有点劝退！自己的编程学习之路从来没有一个类似引路人之类的角色，全凭自学，再加上我三分钟热度的神奇特性，所以对ACM之类的算法竞赛全然不知也就毫无意外。彼时，复现数据结构的排序等级实践水平就足以让自己沾沾自喜半天了，毕竟身边有大把无法做到这一点的人，而且还是靠背代码的方式，二叉树倒是无论是理论学习阶段还是代码实现阶段都没有遇到过太大的挑战，图论则是看算法（第四版）就收获了巨大的理论学习上的满足感，听信了据说实践基本不会出现这种级别的题目的话，就止步于此，毕竟图的数据结构确实有点繁琐，代码复杂度也有点棘手，但是在已经完全理解算法理论的基础上，实现起来也至多花一点时间而已。再加上当时眼界受限，以为排序，搜索以及图论之类就已经是算法的全部了。现在的我才真正意识到，理解算法理论和实现算法代码，在学习上是截然不同的2种境界，所幸自己就算没有在电脑上实现代码，也还是保持了记笔记的好习惯，很多自己觉得精妙和优雅的算法，忍不住在笔记本下写下来了，也算是另一种维度的代码实践吧！\n\n　　回到了LeetCode上来，2012年记得是硕士复试需要准备OJ，当时觉得还是颇为新奇，所以比较重视，刷了很多OJ题目，其中就有LeetCode，不过当时光理解题目意思都有点费劲，刷了不到10题的Easy难度的题目就劝退了；复试结束之后，习惯性的放弃了，直到看完算法（第四版），重新理解了Java之后，忍不住想要做点什么，就又开始重新开始刷LeetCode，用Java 勉强刷了几十道medium和easy的题目，虽然大部分都还是做不出来，不清楚常规解题套路，Hard题目一想就是一整天都没什么头绪，最后还是看Discuss才能找到一点感觉，甚至很多时候光理解代码就需要大半天时间。由于LeetCode OJ系统的易用性，免去了调试环境的很多烦恼，能专注于算法编码和Debug，其次社区讨论都非常活跃，遇到不会做的题目或者是思路受阻，基本都能在Discuss下面找到解答，当时虽然抱着模仿性质的想法，试图想要刷完LeetCode 100题，但是止步于好几个Hard题目，记得有一道计算城市天际线面积的题目，整整想了一周，总是感觉算法上，似乎已经找到解决算法，但是编码时候总是磕磕碰碰，自己的想法还没到验证的阶段，就被茫茫然的细节问题给淹没了。最后不得不求助社区，最后理解完正确的代码之后，就这么弃置不顾。完全放弃了在LeetCode中寻找乐趣的想法了。\n\n　　以现在的角度回顾分析之前为什么放弃 LeetCode 刷题，或许会有不错的收获！首先，也是老生常谈的问题，就是自己三分钟热度的问题，也不过多展开了，直接给出结论就是，中立的来看，自己既然有这样不算好也不算坏的特质，别像以前那样，说到三分钟热度，大部分时候都是带着点贬义和自我批判意味。再说，三分钟热度起码给了自己一种不断变换角度来看周遭和自己的机会，抱有对世界最单纯的好奇心。缺点自然也很明显，也就是浅尝辄止。最后的一句话点醒了梦中人，**有三分钟热度，就有三分钟收获**。其次，就是自己刷题时候的无知者无畏的盲目自信，想要试图跳级做题，而不是惯常的循序渐进，徒增难度，却发现自己完全没有做好准备，浪费了很多时间，还严重打击了自信，于是便开始自暴自弃，顺带连之前积累的理论知识都被荒废了。这可能是学生思维的弊端，不够谦卑，更不够自信，这其中的度，没有拿捏得当。诚然，数据结构和算法的理论支撑下，很多算法题都会变的比较容易用常规的套路解答，特别是很多算法笔试和考试中，都已经把最难点给精确的给出来了，照本宣科就可以了。那么，这是否意味着解决了核心难点之后，其他方面是否不值一提呢？真实情况并不是这样的，很多时候并没有意识到这个难点为什么是难点，只是被动的接受知识而已，这理所当然的导致另一个重要的能力的缺失，学会如何灵活的运用这些算法理论，也就是融会贯通。只有当无论什么算法题目，都能很准确的分析出问题本质，根据分析结果找到相应的数据结构工具和手段解决问题，才能完成编码的核心，就是算法伪代码的初步构思；但是是否真的能在OJ中AC，只有代码才能验证。这跟数学证明题一样，在证明过程顺利铺开之前，一切构思都只是可能的解决方案而已，能不能达到预想的目的，还需要证明过程来验证，况且，这个构思可能会被证明过程自我否定掉。这些内容总结起来，其实就是**执行力**。回顾自己大部分的学生时代，遗憾的发现，自己执行力可能是属于非常拉胯的级别。也就勉强仗着的天赋好，和某些时候的灵光乍现以及为数不多的树立并确信要去达成的目标的时候，才能走到现在。近年来，终于能心平气和的分析自己的优缺点的时候，才意识到，循序渐进的构思-行动，小步快走的达成目标的重要性和难能可贵。这一点可能是当初年轻气盛的时候不具备的。再则，其实也是上一个原因派生出来的问题，就是Java本身的标准库和数据结构的不熟悉，谁能想到，自学了Java这么久，居然还是懒得看Java的标准文档，尽管算法（第四版）实现了简化版的，堆栈，队列，二叉树等数据结构，文件I/O也大幅简化，增强了易用性，也让自己缺乏去看繁杂的Java标准实现的动力，所以很多时候都是现学现用，这当然无可厚非，因为熟记这些库是没有必要的，只需要知道是堆栈等这些数据结构的接口就可以了，用的时候再去查就行了，无非就是低效一点，并不至于无法实现的地步，但是结果就是不用就不学，导致直到现在对Java的标准库还是一知半解，前段时间打包个Jar还老费劲了。这样做的当时可能觉得没啥，但是一旦实践起来，就很容易出现问题，在做LeetCode OJ的时候，就集中爆发出来了，就是自己是算法构思，迟迟没办法得到代码验证，常常会卡在Java的实现细节上，当时也没有对构思和实践分的这么开，觉得有个构思好像能解决问题，就硬着头强上，以至于最后败兴而归都没能整明白，究竟是算法构思的问题，还是代码实现的问题；以至于虽然刷了很久的题目，收获不大，反倒是积累了很多Java的问题没有解决。以上的一些做的不够好的地方，基本解释了当初为什么没能坚持刷LeetCode ？ \n\n　　还有一个重要的需要解释的问题就是，为什么刷过的LeetCode 过段时间回头来看，仍然解不出来或者是一知半解？譬如说，现在回去做城市天际线的题目，自己似乎没有自信能100%解出来。主要原因应该是自己并没有规划性的做复习工作，很多算法题做完，并没有留下一个可供复习的笔记和资料为后续复习做准备，也就是在之前Blog做反思的过程中提到的，没有系统行的整理和复习自己的学习笔记和资料，导致每次复习的时候，都不得不重头开始做题，把之前踩过的坑又重新踩了一遍，这在几年后，也就是最近几年开始用Python重做这些题的时候都渐渐体现出来了，这其实完全没有必要，因为发生这样的事情主要原因仅仅只是遗忘了而已，思维过程其实还是在没有复习的情况仍然保有印象。所以，定期的记录刷题方法并整理和复习在这个阶段就变的很重要了，这也是自己强调过很多次的问题了，也是努力在改进，就不多细说了。\n\n　　用Python重刷题的时候，就显得有效率了很多，一来很多题目都比较熟悉了，只需要用Python相对应的数据结构重新实现即可，这个过程其实也加深了很多经常用的数据结构的理解，不常用的类和编程模式也熟悉了不少，也算是一个不错的收获吧，毕竟曾经对实践编码的忽视缺失导致自己眼高手低，失去了很多提升编码能力的机会，编码之后，才真实的认识到编程原本就是一项工程实践，自己怎么会天真到仅仅满足于算法构思，而不是脚踏实地的实现出来；再则，其实也算是一种复习，对之前理解不深刻，一知半解的方法做一个完整的梳理。 这也解释为什么，上周某天下午半天不到的时间，就把最后20题，一次性全部刷完，高效的自己都觉得不可思议。其中甚至包含2道Hard，自己原以为需要借助一些提示才做的出来的题目，也比较顺利的完成了。 刷完100题的时候，感觉真的很兴奋，曾经以为很厉害，遥不可及的神迹，现在自己也站在这里达成了这个目标，虽然耗时比较久，过程断断续续，其实过程基本是大部分时间没怎么刷，突然心血来潮就刷很多题那样，也算是达成一个不大不小的成就吧！\n\n　　接下来该怎么做呢？ 这也是最最重要和需要关注的问题，毕竟尽管自己可以一下午刷几十道题，停不下来，但是我还是会开个小差，摸鱼一整天，更是停不下来的那种。我已经渐渐摸索出应对这种情况的解决办法，就是合理的做规划，给出目标，不多不少，不偏不倚。因为从刷题过程来看，我还是非常享受刷题带来的乐趣的，特别是那些觉得有点困难的题目，AC的时候，可以开心好一阵子了，以至于都忘记要整理自己的代码和笔记了。设想中的刷完100题之后，应该是需要整理一下100题的笔记和要点，作为刷题总结的方式，一篇一篇的成为Blog的主要内容来源，现在看来，似乎这项工作并没有开展起来，因为很多注释都写在代码里面了，现在回过头来一个一个的去复习和整理，觉得有点没必要，似乎只需要关注最主要和核心的题目整理出笔记即可，更主要的原因可能是担心这项工作可能会延误自己刷100-200题的进度，毕竟比起回头复习旧题目，我似乎更喜欢做新题。现在回想一下，其实很多代码注释的内容，都足以作为Blog的文字内容存在了，只需要稍加排版即可成为一份笔记，所以对于刷题的笔记模版，应该无需过于担忧。更值得关注的应该是要求自己刷100之后的新题的时候，务必要求自己必须整理出一份代码说明出来，定期排版成Blog，将定期刷题变成一个习惯，这样就有精力去迎接下一个挑战了，毕竟对自己来说是一件很开心的事情。\n\n　　LeetCode 刷完100题有感就这么多，接下来还是希望自己能贯彻自己的日常目标，现阶段坚持刷题和背单词，再坚持锻炼身体，不要避重就轻的断断续续的逃避，相信有了这几项准备和积累，面对以后的工作和生活的挑战，自己能应对的更加自如，处理的更加高效！","source":"_posts/LeetCode-1-100-刷题有感.md","raw":"---\ntitle: LeetCode 1-100 刷题有感\ntoc: true\nmathjax: true\ntop: false\ncover: true\ndate: 2020-12-07 11:06:05\nupdated:\ncategories: 随笔\ntags:\n\t- Algorithms\n\t- LeetCode\n\t- 随笔\n---\n\n　　跌跌撞撞终于在前几天完成的LeetCode 100题，刷题过程印证了之前对自我的一个论断，起了个大早，干了个晚集。 早在2012年，自己就开始了解到LeetCode 刷完前100题的神迹，这时LeetCode应该还没创立多久吧！当时国内只有北大的POJ可用，那曲高和寡的界面和难度，着实有点劝退！自己的编程学习之路从来没有一个类似引路人之类的角色，全凭自学，再加上我三分钟热度的神奇特性，所以对ACM之类的算法竞赛全然不知也就毫无意外。彼时，复现数据结构的排序等级实践水平就足以让自己沾沾自喜半天了，毕竟身边有大把无法做到这一点的人，而且还是靠背代码的方式，二叉树倒是无论是理论学习阶段还是代码实现阶段都没有遇到过太大的挑战，图论则是看算法（第四版）就收获了巨大的理论学习上的满足感，听信了据说实践基本不会出现这种级别的题目的话，就止步于此，毕竟图的数据结构确实有点繁琐，代码复杂度也有点棘手，但是在已经完全理解算法理论的基础上，实现起来也至多花一点时间而已。再加上当时眼界受限，以为排序，搜索以及图论之类就已经是算法的全部了。现在的我才真正意识到，理解算法理论和实现算法代码，在学习上是截然不同的2种境界，所幸自己就算没有在电脑上实现代码，也还是保持了记笔记的好习惯，很多自己觉得精妙和优雅的算法，忍不住在笔记本下写下来了，也算是另一种维度的代码实践吧！\n\n　　回到了LeetCode上来，2012年记得是硕士复试需要准备OJ，当时觉得还是颇为新奇，所以比较重视，刷了很多OJ题目，其中就有LeetCode，不过当时光理解题目意思都有点费劲，刷了不到10题的Easy难度的题目就劝退了；复试结束之后，习惯性的放弃了，直到看完算法（第四版），重新理解了Java之后，忍不住想要做点什么，就又开始重新开始刷LeetCode，用Java 勉强刷了几十道medium和easy的题目，虽然大部分都还是做不出来，不清楚常规解题套路，Hard题目一想就是一整天都没什么头绪，最后还是看Discuss才能找到一点感觉，甚至很多时候光理解代码就需要大半天时间。由于LeetCode OJ系统的易用性，免去了调试环境的很多烦恼，能专注于算法编码和Debug，其次社区讨论都非常活跃，遇到不会做的题目或者是思路受阻，基本都能在Discuss下面找到解答，当时虽然抱着模仿性质的想法，试图想要刷完LeetCode 100题，但是止步于好几个Hard题目，记得有一道计算城市天际线面积的题目，整整想了一周，总是感觉算法上，似乎已经找到解决算法，但是编码时候总是磕磕碰碰，自己的想法还没到验证的阶段，就被茫茫然的细节问题给淹没了。最后不得不求助社区，最后理解完正确的代码之后，就这么弃置不顾。完全放弃了在LeetCode中寻找乐趣的想法了。\n\n　　以现在的角度回顾分析之前为什么放弃 LeetCode 刷题，或许会有不错的收获！首先，也是老生常谈的问题，就是自己三分钟热度的问题，也不过多展开了，直接给出结论就是，中立的来看，自己既然有这样不算好也不算坏的特质，别像以前那样，说到三分钟热度，大部分时候都是带着点贬义和自我批判意味。再说，三分钟热度起码给了自己一种不断变换角度来看周遭和自己的机会，抱有对世界最单纯的好奇心。缺点自然也很明显，也就是浅尝辄止。最后的一句话点醒了梦中人，**有三分钟热度，就有三分钟收获**。其次，就是自己刷题时候的无知者无畏的盲目自信，想要试图跳级做题，而不是惯常的循序渐进，徒增难度，却发现自己完全没有做好准备，浪费了很多时间，还严重打击了自信，于是便开始自暴自弃，顺带连之前积累的理论知识都被荒废了。这可能是学生思维的弊端，不够谦卑，更不够自信，这其中的度，没有拿捏得当。诚然，数据结构和算法的理论支撑下，很多算法题都会变的比较容易用常规的套路解答，特别是很多算法笔试和考试中，都已经把最难点给精确的给出来了，照本宣科就可以了。那么，这是否意味着解决了核心难点之后，其他方面是否不值一提呢？真实情况并不是这样的，很多时候并没有意识到这个难点为什么是难点，只是被动的接受知识而已，这理所当然的导致另一个重要的能力的缺失，学会如何灵活的运用这些算法理论，也就是融会贯通。只有当无论什么算法题目，都能很准确的分析出问题本质，根据分析结果找到相应的数据结构工具和手段解决问题，才能完成编码的核心，就是算法伪代码的初步构思；但是是否真的能在OJ中AC，只有代码才能验证。这跟数学证明题一样，在证明过程顺利铺开之前，一切构思都只是可能的解决方案而已，能不能达到预想的目的，还需要证明过程来验证，况且，这个构思可能会被证明过程自我否定掉。这些内容总结起来，其实就是**执行力**。回顾自己大部分的学生时代，遗憾的发现，自己执行力可能是属于非常拉胯的级别。也就勉强仗着的天赋好，和某些时候的灵光乍现以及为数不多的树立并确信要去达成的目标的时候，才能走到现在。近年来，终于能心平气和的分析自己的优缺点的时候，才意识到，循序渐进的构思-行动，小步快走的达成目标的重要性和难能可贵。这一点可能是当初年轻气盛的时候不具备的。再则，其实也是上一个原因派生出来的问题，就是Java本身的标准库和数据结构的不熟悉，谁能想到，自学了Java这么久，居然还是懒得看Java的标准文档，尽管算法（第四版）实现了简化版的，堆栈，队列，二叉树等数据结构，文件I/O也大幅简化，增强了易用性，也让自己缺乏去看繁杂的Java标准实现的动力，所以很多时候都是现学现用，这当然无可厚非，因为熟记这些库是没有必要的，只需要知道是堆栈等这些数据结构的接口就可以了，用的时候再去查就行了，无非就是低效一点，并不至于无法实现的地步，但是结果就是不用就不学，导致直到现在对Java的标准库还是一知半解，前段时间打包个Jar还老费劲了。这样做的当时可能觉得没啥，但是一旦实践起来，就很容易出现问题，在做LeetCode OJ的时候，就集中爆发出来了，就是自己是算法构思，迟迟没办法得到代码验证，常常会卡在Java的实现细节上，当时也没有对构思和实践分的这么开，觉得有个构思好像能解决问题，就硬着头强上，以至于最后败兴而归都没能整明白，究竟是算法构思的问题，还是代码实现的问题；以至于虽然刷了很久的题目，收获不大，反倒是积累了很多Java的问题没有解决。以上的一些做的不够好的地方，基本解释了当初为什么没能坚持刷LeetCode ？ \n\n　　还有一个重要的需要解释的问题就是，为什么刷过的LeetCode 过段时间回头来看，仍然解不出来或者是一知半解？譬如说，现在回去做城市天际线的题目，自己似乎没有自信能100%解出来。主要原因应该是自己并没有规划性的做复习工作，很多算法题做完，并没有留下一个可供复习的笔记和资料为后续复习做准备，也就是在之前Blog做反思的过程中提到的，没有系统行的整理和复习自己的学习笔记和资料，导致每次复习的时候，都不得不重头开始做题，把之前踩过的坑又重新踩了一遍，这在几年后，也就是最近几年开始用Python重做这些题的时候都渐渐体现出来了，这其实完全没有必要，因为发生这样的事情主要原因仅仅只是遗忘了而已，思维过程其实还是在没有复习的情况仍然保有印象。所以，定期的记录刷题方法并整理和复习在这个阶段就变的很重要了，这也是自己强调过很多次的问题了，也是努力在改进，就不多细说了。\n\n　　用Python重刷题的时候，就显得有效率了很多，一来很多题目都比较熟悉了，只需要用Python相对应的数据结构重新实现即可，这个过程其实也加深了很多经常用的数据结构的理解，不常用的类和编程模式也熟悉了不少，也算是一个不错的收获吧，毕竟曾经对实践编码的忽视缺失导致自己眼高手低，失去了很多提升编码能力的机会，编码之后，才真实的认识到编程原本就是一项工程实践，自己怎么会天真到仅仅满足于算法构思，而不是脚踏实地的实现出来；再则，其实也算是一种复习，对之前理解不深刻，一知半解的方法做一个完整的梳理。 这也解释为什么，上周某天下午半天不到的时间，就把最后20题，一次性全部刷完，高效的自己都觉得不可思议。其中甚至包含2道Hard，自己原以为需要借助一些提示才做的出来的题目，也比较顺利的完成了。 刷完100题的时候，感觉真的很兴奋，曾经以为很厉害，遥不可及的神迹，现在自己也站在这里达成了这个目标，虽然耗时比较久，过程断断续续，其实过程基本是大部分时间没怎么刷，突然心血来潮就刷很多题那样，也算是达成一个不大不小的成就吧！\n\n　　接下来该怎么做呢？ 这也是最最重要和需要关注的问题，毕竟尽管自己可以一下午刷几十道题，停不下来，但是我还是会开个小差，摸鱼一整天，更是停不下来的那种。我已经渐渐摸索出应对这种情况的解决办法，就是合理的做规划，给出目标，不多不少，不偏不倚。因为从刷题过程来看，我还是非常享受刷题带来的乐趣的，特别是那些觉得有点困难的题目，AC的时候，可以开心好一阵子了，以至于都忘记要整理自己的代码和笔记了。设想中的刷完100题之后，应该是需要整理一下100题的笔记和要点，作为刷题总结的方式，一篇一篇的成为Blog的主要内容来源，现在看来，似乎这项工作并没有开展起来，因为很多注释都写在代码里面了，现在回过头来一个一个的去复习和整理，觉得有点没必要，似乎只需要关注最主要和核心的题目整理出笔记即可，更主要的原因可能是担心这项工作可能会延误自己刷100-200题的进度，毕竟比起回头复习旧题目，我似乎更喜欢做新题。现在回想一下，其实很多代码注释的内容，都足以作为Blog的文字内容存在了，只需要稍加排版即可成为一份笔记，所以对于刷题的笔记模版，应该无需过于担忧。更值得关注的应该是要求自己刷100之后的新题的时候，务必要求自己必须整理出一份代码说明出来，定期排版成Blog，将定期刷题变成一个习惯，这样就有精力去迎接下一个挑战了，毕竟对自己来说是一件很开心的事情。\n\n　　LeetCode 刷完100题有感就这么多，接下来还是希望自己能贯彻自己的日常目标，现阶段坚持刷题和背单词，再坚持锻炼身体，不要避重就轻的断断续续的逃避，相信有了这几项准备和积累，面对以后的工作和生活的挑战，自己能应对的更加自如，处理的更加高效！","slug":"LeetCode-1-100-刷题有感","published":1,"_id":"ckiedp6df0000ue287z2u4kkg","comments":1,"layout":"post","photos":[],"link":"","content":"<p>　　跌跌撞撞终于在前几天完成的LeetCode 100题，刷题过程印证了之前对自我的一个论断，起了个大早，干了个晚集。 早在2012年，自己就开始了解到LeetCode 刷完前100题的神迹，这时LeetCode应该还没创立多久吧！当时国内只有北大的POJ可用，那曲高和寡的界面和难度，着实有点劝退！自己的编程学习之路从来没有一个类似引路人之类的角色，全凭自学，再加上我三分钟热度的神奇特性，所以对ACM之类的算法竞赛全然不知也就毫无意外。彼时，复现数据结构的排序等级实践水平就足以让自己沾沾自喜半天了，毕竟身边有大把无法做到这一点的人，而且还是靠背代码的方式，二叉树倒是无论是理论学习阶段还是代码实现阶段都没有遇到过太大的挑战，图论则是看算法（第四版）就收获了巨大的理论学习上的满足感，听信了据说实践基本不会出现这种级别的题目的话，就止步于此，毕竟图的数据结构确实有点繁琐，代码复杂度也有点棘手，但是在已经完全理解算法理论的基础上，实现起来也至多花一点时间而已。再加上当时眼界受限，以为排序，搜索以及图论之类就已经是算法的全部了。现在的我才真正意识到，理解算法理论和实现算法代码，在学习上是截然不同的2种境界，所幸自己就算没有在电脑上实现代码，也还是保持了记笔记的好习惯，很多自己觉得精妙和优雅的算法，忍不住在笔记本下写下来了，也算是另一种维度的代码实践吧！</p>\n<p>　　回到了LeetCode上来，2012年记得是硕士复试需要准备OJ，当时觉得还是颇为新奇，所以比较重视，刷了很多OJ题目，其中就有LeetCode，不过当时光理解题目意思都有点费劲，刷了不到10题的Easy难度的题目就劝退了；复试结束之后，习惯性的放弃了，直到看完算法（第四版），重新理解了Java之后，忍不住想要做点什么，就又开始重新开始刷LeetCode，用Java 勉强刷了几十道medium和easy的题目，虽然大部分都还是做不出来，不清楚常规解题套路，Hard题目一想就是一整天都没什么头绪，最后还是看Discuss才能找到一点感觉，甚至很多时候光理解代码就需要大半天时间。由于LeetCode OJ系统的易用性，免去了调试环境的很多烦恼，能专注于算法编码和Debug，其次社区讨论都非常活跃，遇到不会做的题目或者是思路受阻，基本都能在Discuss下面找到解答，当时虽然抱着模仿性质的想法，试图想要刷完LeetCode 100题，但是止步于好几个Hard题目，记得有一道计算城市天际线面积的题目，整整想了一周，总是感觉算法上，似乎已经找到解决算法，但是编码时候总是磕磕碰碰，自己的想法还没到验证的阶段，就被茫茫然的细节问题给淹没了。最后不得不求助社区，最后理解完正确的代码之后，就这么弃置不顾。完全放弃了在LeetCode中寻找乐趣的想法了。</p>\n<p>　　以现在的角度回顾分析之前为什么放弃 LeetCode 刷题，或许会有不错的收获！首先，也是老生常谈的问题，就是自己三分钟热度的问题，也不过多展开了，直接给出结论就是，中立的来看，自己既然有这样不算好也不算坏的特质，别像以前那样，说到三分钟热度，大部分时候都是带着点贬义和自我批判意味。再说，三分钟热度起码给了自己一种不断变换角度来看周遭和自己的机会，抱有对世界最单纯的好奇心。缺点自然也很明显，也就是浅尝辄止。最后的一句话点醒了梦中人，<strong>有三分钟热度，就有三分钟收获</strong>。其次，就是自己刷题时候的无知者无畏的盲目自信，想要试图跳级做题，而不是惯常的循序渐进，徒增难度，却发现自己完全没有做好准备，浪费了很多时间，还严重打击了自信，于是便开始自暴自弃，顺带连之前积累的理论知识都被荒废了。这可能是学生思维的弊端，不够谦卑，更不够自信，这其中的度，没有拿捏得当。诚然，数据结构和算法的理论支撑下，很多算法题都会变的比较容易用常规的套路解答，特别是很多算法笔试和考试中，都已经把最难点给精确的给出来了，照本宣科就可以了。那么，这是否意味着解决了核心难点之后，其他方面是否不值一提呢？真实情况并不是这样的，很多时候并没有意识到这个难点为什么是难点，只是被动的接受知识而已，这理所当然的导致另一个重要的能力的缺失，学会如何灵活的运用这些算法理论，也就是融会贯通。只有当无论什么算法题目，都能很准确的分析出问题本质，根据分析结果找到相应的数据结构工具和手段解决问题，才能完成编码的核心，就是算法伪代码的初步构思；但是是否真的能在OJ中AC，只有代码才能验证。这跟数学证明题一样，在证明过程顺利铺开之前，一切构思都只是可能的解决方案而已，能不能达到预想的目的，还需要证明过程来验证，况且，这个构思可能会被证明过程自我否定掉。这些内容总结起来，其实就是<strong>执行力</strong>。回顾自己大部分的学生时代，遗憾的发现，自己执行力可能是属于非常拉胯的级别。也就勉强仗着的天赋好，和某些时候的灵光乍现以及为数不多的树立并确信要去达成的目标的时候，才能走到现在。近年来，终于能心平气和的分析自己的优缺点的时候，才意识到，循序渐进的构思-行动，小步快走的达成目标的重要性和难能可贵。这一点可能是当初年轻气盛的时候不具备的。再则，其实也是上一个原因派生出来的问题，就是Java本身的标准库和数据结构的不熟悉，谁能想到，自学了Java这么久，居然还是懒得看Java的标准文档，尽管算法（第四版）实现了简化版的，堆栈，队列，二叉树等数据结构，文件I/O也大幅简化，增强了易用性，也让自己缺乏去看繁杂的Java标准实现的动力，所以很多时候都是现学现用，这当然无可厚非，因为熟记这些库是没有必要的，只需要知道是堆栈等这些数据结构的接口就可以了，用的时候再去查就行了，无非就是低效一点，并不至于无法实现的地步，但是结果就是不用就不学，导致直到现在对Java的标准库还是一知半解，前段时间打包个Jar还老费劲了。这样做的当时可能觉得没啥，但是一旦实践起来，就很容易出现问题，在做LeetCode OJ的时候，就集中爆发出来了，就是自己是算法构思，迟迟没办法得到代码验证，常常会卡在Java的实现细节上，当时也没有对构思和实践分的这么开，觉得有个构思好像能解决问题，就硬着头强上，以至于最后败兴而归都没能整明白，究竟是算法构思的问题，还是代码实现的问题；以至于虽然刷了很久的题目，收获不大，反倒是积累了很多Java的问题没有解决。以上的一些做的不够好的地方，基本解释了当初为什么没能坚持刷LeetCode ？ </p>\n<p>　　还有一个重要的需要解释的问题就是，为什么刷过的LeetCode 过段时间回头来看，仍然解不出来或者是一知半解？譬如说，现在回去做城市天际线的题目，自己似乎没有自信能100%解出来。主要原因应该是自己并没有规划性的做复习工作，很多算法题做完，并没有留下一个可供复习的笔记和资料为后续复习做准备，也就是在之前Blog做反思的过程中提到的，没有系统行的整理和复习自己的学习笔记和资料，导致每次复习的时候，都不得不重头开始做题，把之前踩过的坑又重新踩了一遍，这在几年后，也就是最近几年开始用Python重做这些题的时候都渐渐体现出来了，这其实完全没有必要，因为发生这样的事情主要原因仅仅只是遗忘了而已，思维过程其实还是在没有复习的情况仍然保有印象。所以，定期的记录刷题方法并整理和复习在这个阶段就变的很重要了，这也是自己强调过很多次的问题了，也是努力在改进，就不多细说了。</p>\n<p>　　用Python重刷题的时候，就显得有效率了很多，一来很多题目都比较熟悉了，只需要用Python相对应的数据结构重新实现即可，这个过程其实也加深了很多经常用的数据结构的理解，不常用的类和编程模式也熟悉了不少，也算是一个不错的收获吧，毕竟曾经对实践编码的忽视缺失导致自己眼高手低，失去了很多提升编码能力的机会，编码之后，才真实的认识到编程原本就是一项工程实践，自己怎么会天真到仅仅满足于算法构思，而不是脚踏实地的实现出来；再则，其实也算是一种复习，对之前理解不深刻，一知半解的方法做一个完整的梳理。 这也解释为什么，上周某天下午半天不到的时间，就把最后20题，一次性全部刷完，高效的自己都觉得不可思议。其中甚至包含2道Hard，自己原以为需要借助一些提示才做的出来的题目，也比较顺利的完成了。 刷完100题的时候，感觉真的很兴奋，曾经以为很厉害，遥不可及的神迹，现在自己也站在这里达成了这个目标，虽然耗时比较久，过程断断续续，其实过程基本是大部分时间没怎么刷，突然心血来潮就刷很多题那样，也算是达成一个不大不小的成就吧！</p>\n<p>　　接下来该怎么做呢？ 这也是最最重要和需要关注的问题，毕竟尽管自己可以一下午刷几十道题，停不下来，但是我还是会开个小差，摸鱼一整天，更是停不下来的那种。我已经渐渐摸索出应对这种情况的解决办法，就是合理的做规划，给出目标，不多不少，不偏不倚。因为从刷题过程来看，我还是非常享受刷题带来的乐趣的，特别是那些觉得有点困难的题目，AC的时候，可以开心好一阵子了，以至于都忘记要整理自己的代码和笔记了。设想中的刷完100题之后，应该是需要整理一下100题的笔记和要点，作为刷题总结的方式，一篇一篇的成为Blog的主要内容来源，现在看来，似乎这项工作并没有开展起来，因为很多注释都写在代码里面了，现在回过头来一个一个的去复习和整理，觉得有点没必要，似乎只需要关注最主要和核心的题目整理出笔记即可，更主要的原因可能是担心这项工作可能会延误自己刷100-200题的进度，毕竟比起回头复习旧题目，我似乎更喜欢做新题。现在回想一下，其实很多代码注释的内容，都足以作为Blog的文字内容存在了，只需要稍加排版即可成为一份笔记，所以对于刷题的笔记模版，应该无需过于担忧。更值得关注的应该是要求自己刷100之后的新题的时候，务必要求自己必须整理出一份代码说明出来，定期排版成Blog，将定期刷题变成一个习惯，这样就有精力去迎接下一个挑战了，毕竟对自己来说是一件很开心的事情。</p>\n<p>　　LeetCode 刷完100题有感就这么多，接下来还是希望自己能贯彻自己的日常目标，现阶段坚持刷题和背单词，再坚持锻炼身体，不要避重就轻的断断续续的逃避，相信有了这几项准备和积累，面对以后的工作和生活的挑战，自己能应对的更加自如，处理的更加高效！</p>\n","site":{"data":{}},"excerpt":"","more":"<p>　　跌跌撞撞终于在前几天完成的LeetCode 100题，刷题过程印证了之前对自我的一个论断，起了个大早，干了个晚集。 早在2012年，自己就开始了解到LeetCode 刷完前100题的神迹，这时LeetCode应该还没创立多久吧！当时国内只有北大的POJ可用，那曲高和寡的界面和难度，着实有点劝退！自己的编程学习之路从来没有一个类似引路人之类的角色，全凭自学，再加上我三分钟热度的神奇特性，所以对ACM之类的算法竞赛全然不知也就毫无意外。彼时，复现数据结构的排序等级实践水平就足以让自己沾沾自喜半天了，毕竟身边有大把无法做到这一点的人，而且还是靠背代码的方式，二叉树倒是无论是理论学习阶段还是代码实现阶段都没有遇到过太大的挑战，图论则是看算法（第四版）就收获了巨大的理论学习上的满足感，听信了据说实践基本不会出现这种级别的题目的话，就止步于此，毕竟图的数据结构确实有点繁琐，代码复杂度也有点棘手，但是在已经完全理解算法理论的基础上，实现起来也至多花一点时间而已。再加上当时眼界受限，以为排序，搜索以及图论之类就已经是算法的全部了。现在的我才真正意识到，理解算法理论和实现算法代码，在学习上是截然不同的2种境界，所幸自己就算没有在电脑上实现代码，也还是保持了记笔记的好习惯，很多自己觉得精妙和优雅的算法，忍不住在笔记本下写下来了，也算是另一种维度的代码实践吧！</p>\n<p>　　回到了LeetCode上来，2012年记得是硕士复试需要准备OJ，当时觉得还是颇为新奇，所以比较重视，刷了很多OJ题目，其中就有LeetCode，不过当时光理解题目意思都有点费劲，刷了不到10题的Easy难度的题目就劝退了；复试结束之后，习惯性的放弃了，直到看完算法（第四版），重新理解了Java之后，忍不住想要做点什么，就又开始重新开始刷LeetCode，用Java 勉强刷了几十道medium和easy的题目，虽然大部分都还是做不出来，不清楚常规解题套路，Hard题目一想就是一整天都没什么头绪，最后还是看Discuss才能找到一点感觉，甚至很多时候光理解代码就需要大半天时间。由于LeetCode OJ系统的易用性，免去了调试环境的很多烦恼，能专注于算法编码和Debug，其次社区讨论都非常活跃，遇到不会做的题目或者是思路受阻，基本都能在Discuss下面找到解答，当时虽然抱着模仿性质的想法，试图想要刷完LeetCode 100题，但是止步于好几个Hard题目，记得有一道计算城市天际线面积的题目，整整想了一周，总是感觉算法上，似乎已经找到解决算法，但是编码时候总是磕磕碰碰，自己的想法还没到验证的阶段，就被茫茫然的细节问题给淹没了。最后不得不求助社区，最后理解完正确的代码之后，就这么弃置不顾。完全放弃了在LeetCode中寻找乐趣的想法了。</p>\n<p>　　以现在的角度回顾分析之前为什么放弃 LeetCode 刷题，或许会有不错的收获！首先，也是老生常谈的问题，就是自己三分钟热度的问题，也不过多展开了，直接给出结论就是，中立的来看，自己既然有这样不算好也不算坏的特质，别像以前那样，说到三分钟热度，大部分时候都是带着点贬义和自我批判意味。再说，三分钟热度起码给了自己一种不断变换角度来看周遭和自己的机会，抱有对世界最单纯的好奇心。缺点自然也很明显，也就是浅尝辄止。最后的一句话点醒了梦中人，<strong>有三分钟热度，就有三分钟收获</strong>。其次，就是自己刷题时候的无知者无畏的盲目自信，想要试图跳级做题，而不是惯常的循序渐进，徒增难度，却发现自己完全没有做好准备，浪费了很多时间，还严重打击了自信，于是便开始自暴自弃，顺带连之前积累的理论知识都被荒废了。这可能是学生思维的弊端，不够谦卑，更不够自信，这其中的度，没有拿捏得当。诚然，数据结构和算法的理论支撑下，很多算法题都会变的比较容易用常规的套路解答，特别是很多算法笔试和考试中，都已经把最难点给精确的给出来了，照本宣科就可以了。那么，这是否意味着解决了核心难点之后，其他方面是否不值一提呢？真实情况并不是这样的，很多时候并没有意识到这个难点为什么是难点，只是被动的接受知识而已，这理所当然的导致另一个重要的能力的缺失，学会如何灵活的运用这些算法理论，也就是融会贯通。只有当无论什么算法题目，都能很准确的分析出问题本质，根据分析结果找到相应的数据结构工具和手段解决问题，才能完成编码的核心，就是算法伪代码的初步构思；但是是否真的能在OJ中AC，只有代码才能验证。这跟数学证明题一样，在证明过程顺利铺开之前，一切构思都只是可能的解决方案而已，能不能达到预想的目的，还需要证明过程来验证，况且，这个构思可能会被证明过程自我否定掉。这些内容总结起来，其实就是<strong>执行力</strong>。回顾自己大部分的学生时代，遗憾的发现，自己执行力可能是属于非常拉胯的级别。也就勉强仗着的天赋好，和某些时候的灵光乍现以及为数不多的树立并确信要去达成的目标的时候，才能走到现在。近年来，终于能心平气和的分析自己的优缺点的时候，才意识到，循序渐进的构思-行动，小步快走的达成目标的重要性和难能可贵。这一点可能是当初年轻气盛的时候不具备的。再则，其实也是上一个原因派生出来的问题，就是Java本身的标准库和数据结构的不熟悉，谁能想到，自学了Java这么久，居然还是懒得看Java的标准文档，尽管算法（第四版）实现了简化版的，堆栈，队列，二叉树等数据结构，文件I/O也大幅简化，增强了易用性，也让自己缺乏去看繁杂的Java标准实现的动力，所以很多时候都是现学现用，这当然无可厚非，因为熟记这些库是没有必要的，只需要知道是堆栈等这些数据结构的接口就可以了，用的时候再去查就行了，无非就是低效一点，并不至于无法实现的地步，但是结果就是不用就不学，导致直到现在对Java的标准库还是一知半解，前段时间打包个Jar还老费劲了。这样做的当时可能觉得没啥，但是一旦实践起来，就很容易出现问题，在做LeetCode OJ的时候，就集中爆发出来了，就是自己是算法构思，迟迟没办法得到代码验证，常常会卡在Java的实现细节上，当时也没有对构思和实践分的这么开，觉得有个构思好像能解决问题，就硬着头强上，以至于最后败兴而归都没能整明白，究竟是算法构思的问题，还是代码实现的问题；以至于虽然刷了很久的题目，收获不大，反倒是积累了很多Java的问题没有解决。以上的一些做的不够好的地方，基本解释了当初为什么没能坚持刷LeetCode ？ </p>\n<p>　　还有一个重要的需要解释的问题就是，为什么刷过的LeetCode 过段时间回头来看，仍然解不出来或者是一知半解？譬如说，现在回去做城市天际线的题目，自己似乎没有自信能100%解出来。主要原因应该是自己并没有规划性的做复习工作，很多算法题做完，并没有留下一个可供复习的笔记和资料为后续复习做准备，也就是在之前Blog做反思的过程中提到的，没有系统行的整理和复习自己的学习笔记和资料，导致每次复习的时候，都不得不重头开始做题，把之前踩过的坑又重新踩了一遍，这在几年后，也就是最近几年开始用Python重做这些题的时候都渐渐体现出来了，这其实完全没有必要，因为发生这样的事情主要原因仅仅只是遗忘了而已，思维过程其实还是在没有复习的情况仍然保有印象。所以，定期的记录刷题方法并整理和复习在这个阶段就变的很重要了，这也是自己强调过很多次的问题了，也是努力在改进，就不多细说了。</p>\n<p>　　用Python重刷题的时候，就显得有效率了很多，一来很多题目都比较熟悉了，只需要用Python相对应的数据结构重新实现即可，这个过程其实也加深了很多经常用的数据结构的理解，不常用的类和编程模式也熟悉了不少，也算是一个不错的收获吧，毕竟曾经对实践编码的忽视缺失导致自己眼高手低，失去了很多提升编码能力的机会，编码之后，才真实的认识到编程原本就是一项工程实践，自己怎么会天真到仅仅满足于算法构思，而不是脚踏实地的实现出来；再则，其实也算是一种复习，对之前理解不深刻，一知半解的方法做一个完整的梳理。 这也解释为什么，上周某天下午半天不到的时间，就把最后20题，一次性全部刷完，高效的自己都觉得不可思议。其中甚至包含2道Hard，自己原以为需要借助一些提示才做的出来的题目，也比较顺利的完成了。 刷完100题的时候，感觉真的很兴奋，曾经以为很厉害，遥不可及的神迹，现在自己也站在这里达成了这个目标，虽然耗时比较久，过程断断续续，其实过程基本是大部分时间没怎么刷，突然心血来潮就刷很多题那样，也算是达成一个不大不小的成就吧！</p>\n<p>　　接下来该怎么做呢？ 这也是最最重要和需要关注的问题，毕竟尽管自己可以一下午刷几十道题，停不下来，但是我还是会开个小差，摸鱼一整天，更是停不下来的那种。我已经渐渐摸索出应对这种情况的解决办法，就是合理的做规划，给出目标，不多不少，不偏不倚。因为从刷题过程来看，我还是非常享受刷题带来的乐趣的，特别是那些觉得有点困难的题目，AC的时候，可以开心好一阵子了，以至于都忘记要整理自己的代码和笔记了。设想中的刷完100题之后，应该是需要整理一下100题的笔记和要点，作为刷题总结的方式，一篇一篇的成为Blog的主要内容来源，现在看来，似乎这项工作并没有开展起来，因为很多注释都写在代码里面了，现在回过头来一个一个的去复习和整理，觉得有点没必要，似乎只需要关注最主要和核心的题目整理出笔记即可，更主要的原因可能是担心这项工作可能会延误自己刷100-200题的进度，毕竟比起回头复习旧题目，我似乎更喜欢做新题。现在回想一下，其实很多代码注释的内容，都足以作为Blog的文字内容存在了，只需要稍加排版即可成为一份笔记，所以对于刷题的笔记模版，应该无需过于担忧。更值得关注的应该是要求自己刷100之后的新题的时候，务必要求自己必须整理出一份代码说明出来，定期排版成Blog，将定期刷题变成一个习惯，这样就有精力去迎接下一个挑战了，毕竟对自己来说是一件很开心的事情。</p>\n<p>　　LeetCode 刷完100题有感就这么多，接下来还是希望自己能贯彻自己的日常目标，现阶段坚持刷题和背单词，再坚持锻炼身体，不要避重就轻的断断续续的逃避，相信有了这几项准备和积累，面对以后的工作和生活的挑战，自己能应对的更加自如，处理的更加高效！</p>\n"},{"title":"LeetCode 115. Distinct Subsequences","toc":true,"mathjax":true,"top":false,"cover":true,"date":"2020-12-09T09:52:16.000Z","updated":"2020-12-09T10:17:52.722Z","_content":"\n> Given two strings `s` and `t`, return *the number of distinct subsequences of `s` which equals `t`*.\n>\n> A string's **subsequence** is a new string formed from the original string by deleting some (can be none) of the characters without disturbing the relative positions of the remaining characters. (i.e., `\"ACE\"` is a subsequence of `\"ABCDE\"` while `\"AEC\"` is not).\n>\n> It's guaranteed the answer fits on a 32-bit signed integer.\n>\n>  \n>\n> **Example 1:**\n>\n> ```\n> Input: s = \"rabbbit\", t = \"rabbit\"\n> Output: 3\n> Explanation:\n> As shown below, there are 3 ways you can generate \"rabbit\" from S.\n> rabbbit\n> rabbbit\n> rabbbit\n> ```\n>\n> **Example 2:**\n>\n> ```\n> Input: s = \"babgbag\", t = \"bag\"\n> Output: 5\n> Explanation:\n> As shown below, there are 5 ways you can generate \"bag\" from S.\n> babgbag\n> babgbag\n> babgbag\n> babgbag\n> babgbag\n> ```\n>\n>  \n>\n> **Constraints:**\n>\n> - `0 <= s.length, t.length <= 1000`\n> - `s` and `t` consist of English letters.\n\n\n\n　　或许是由于之前做过这个题目的原因，所以面对这道Hard题目，心态比较冷静；再加上100题前后的几乎都是二叉树类别的题目，自己面对链表和树类别的题目基本没碰到困难，所以DFS用的手软，到这里有点停不下来的感觉。所以此时第一想法就想到用DFS，虽然预感对于字符串匹配类别的题目，DFS肯定会出现性能问题，但是作为验证算法思想的先导思路，还是打算先实现出来再看，毕竟用DFS写代码，代码真的很短，AC的时候更是有种 吊炸天的 爽快感；结果还是差一点AC；其次，可能是之前做过的原因，虽然第一时间没写出dp递推公式，但是写出了递推矩阵，演算了一番，没想到居然是对的，很容易的转换成了算法代码，结果干净利落的AC了，甚至比之前的AC的时候还要快一倍以上。虽然这不是自己第一次独立解决Hard题目，但是这题没有借鉴之前的代码，而且从构思算法道实践成功基本没遇到什么障碍，还是值得自己兴奋一阵子了。\n\n### 解法一：DFS\n\n　　","source":"_posts/LeetCode-115-Distinct-Subsequences.md","raw":"---\ntitle: LeetCode 115. Distinct Subsequences\ntoc: true\nmathjax: true\ntop: false\ncover: true\ndate: 2020-12-09 17:52:16\nupdated:\ncategories:\ntags:\n---\n\n> Given two strings `s` and `t`, return *the number of distinct subsequences of `s` which equals `t`*.\n>\n> A string's **subsequence** is a new string formed from the original string by deleting some (can be none) of the characters without disturbing the relative positions of the remaining characters. (i.e., `\"ACE\"` is a subsequence of `\"ABCDE\"` while `\"AEC\"` is not).\n>\n> It's guaranteed the answer fits on a 32-bit signed integer.\n>\n>  \n>\n> **Example 1:**\n>\n> ```\n> Input: s = \"rabbbit\", t = \"rabbit\"\n> Output: 3\n> Explanation:\n> As shown below, there are 3 ways you can generate \"rabbit\" from S.\n> rabbbit\n> rabbbit\n> rabbbit\n> ```\n>\n> **Example 2:**\n>\n> ```\n> Input: s = \"babgbag\", t = \"bag\"\n> Output: 5\n> Explanation:\n> As shown below, there are 5 ways you can generate \"bag\" from S.\n> babgbag\n> babgbag\n> babgbag\n> babgbag\n> babgbag\n> ```\n>\n>  \n>\n> **Constraints:**\n>\n> - `0 <= s.length, t.length <= 1000`\n> - `s` and `t` consist of English letters.\n\n\n\n　　或许是由于之前做过这个题目的原因，所以面对这道Hard题目，心态比较冷静；再加上100题前后的几乎都是二叉树类别的题目，自己面对链表和树类别的题目基本没碰到困难，所以DFS用的手软，到这里有点停不下来的感觉。所以此时第一想法就想到用DFS，虽然预感对于字符串匹配类别的题目，DFS肯定会出现性能问题，但是作为验证算法思想的先导思路，还是打算先实现出来再看，毕竟用DFS写代码，代码真的很短，AC的时候更是有种 吊炸天的 爽快感；结果还是差一点AC；其次，可能是之前做过的原因，虽然第一时间没写出dp递推公式，但是写出了递推矩阵，演算了一番，没想到居然是对的，很容易的转换成了算法代码，结果干净利落的AC了，甚至比之前的AC的时候还要快一倍以上。虽然这不是自己第一次独立解决Hard题目，但是这题没有借鉴之前的代码，而且从构思算法道实践成功基本没遇到什么障碍，还是值得自己兴奋一阵子了。\n\n### 解法一：DFS\n\n　　","slug":"LeetCode-115-Distinct-Subsequences","published":1,"_id":"ckiijvpg20000ce28c4jz8x92","comments":1,"layout":"post","photos":[],"link":"","content":"<blockquote>\n<p>Given two strings <code>s</code> and <code>t</code>, return <em>the number of distinct subsequences of <code>s</code> which equals <code>t</code></em>.</p>\n<p>A string’s <strong>subsequence</strong> is a new string formed from the original string by deleting some (can be none) of the characters without disturbing the relative positions of the remaining characters. (i.e., <code>&quot;ACE&quot;</code> is a subsequence of <code>&quot;ABCDE&quot;</code> while <code>&quot;AEC&quot;</code> is not).</p>\n<p>It’s guaranteed the answer fits on a 32-bit signed integer.</p>\n<p><strong>Example 1:</strong></p>\n<pre><code>Input: s = &quot;rabbbit&quot;, t = &quot;rabbit&quot;\nOutput: 3\nExplanation:\nAs shown below, there are 3 ways you can generate &quot;rabbit&quot; from S.\nrabbbit\nrabbbit\nrabbbit</code></pre>\n<p><strong>Example 2:</strong></p>\n<pre><code>Input: s = &quot;babgbag&quot;, t = &quot;bag&quot;\nOutput: 5\nExplanation:\nAs shown below, there are 5 ways you can generate &quot;bag&quot; from S.\nbabgbag\nbabgbag\nbabgbag\nbabgbag\nbabgbag</code></pre>\n<p><strong>Constraints:</strong></p>\n<ul>\n<li><code>0 &lt;= s.length, t.length &lt;= 1000</code></li>\n<li><code>s</code> and <code>t</code> consist of English letters.</li>\n</ul>\n</blockquote>\n<p>　　或许是由于之前做过这个题目的原因，所以面对这道Hard题目，心态比较冷静；再加上100题前后的几乎都是二叉树类别的题目，自己面对链表和树类别的题目基本没碰到困难，所以DFS用的手软，到这里有点停不下来的感觉。所以此时第一想法就想到用DFS，虽然预感对于字符串匹配类别的题目，DFS肯定会出现性能问题，但是作为验证算法思想的先导思路，还是打算先实现出来再看，毕竟用DFS写代码，代码真的很短，AC的时候更是有种 吊炸天的 爽快感；结果还是差一点AC；其次，可能是之前做过的原因，虽然第一时间没写出dp递推公式，但是写出了递推矩阵，演算了一番，没想到居然是对的，很容易的转换成了算法代码，结果干净利落的AC了，甚至比之前的AC的时候还要快一倍以上。虽然这不是自己第一次独立解决Hard题目，但是这题没有借鉴之前的代码，而且从构思算法道实践成功基本没遇到什么障碍，还是值得自己兴奋一阵子了。</p>\n<h3 id=\"解法一：DFS\"><a href=\"#解法一：DFS\" class=\"headerlink\" title=\"解法一：DFS\"></a>解法一：DFS</h3><p>　　</p>\n","site":{"data":{}},"excerpt":"","more":"<blockquote>\n<p>Given two strings <code>s</code> and <code>t</code>, return <em>the number of distinct subsequences of <code>s</code> which equals <code>t</code></em>.</p>\n<p>A string’s <strong>subsequence</strong> is a new string formed from the original string by deleting some (can be none) of the characters without disturbing the relative positions of the remaining characters. (i.e., <code>&quot;ACE&quot;</code> is a subsequence of <code>&quot;ABCDE&quot;</code> while <code>&quot;AEC&quot;</code> is not).</p>\n<p>It’s guaranteed the answer fits on a 32-bit signed integer.</p>\n<p><strong>Example 1:</strong></p>\n<pre><code>Input: s = &quot;rabbbit&quot;, t = &quot;rabbit&quot;\nOutput: 3\nExplanation:\nAs shown below, there are 3 ways you can generate &quot;rabbit&quot; from S.\nrabbbit\nrabbbit\nrabbbit</code></pre>\n<p><strong>Example 2:</strong></p>\n<pre><code>Input: s = &quot;babgbag&quot;, t = &quot;bag&quot;\nOutput: 5\nExplanation:\nAs shown below, there are 5 ways you can generate &quot;bag&quot; from S.\nbabgbag\nbabgbag\nbabgbag\nbabgbag\nbabgbag</code></pre>\n<p><strong>Constraints:</strong></p>\n<ul>\n<li><code>0 &lt;= s.length, t.length &lt;= 1000</code></li>\n<li><code>s</code> and <code>t</code> consist of English letters.</li>\n</ul>\n</blockquote>\n<p>　　或许是由于之前做过这个题目的原因，所以面对这道Hard题目，心态比较冷静；再加上100题前后的几乎都是二叉树类别的题目，自己面对链表和树类别的题目基本没碰到困难，所以DFS用的手软，到这里有点停不下来的感觉。所以此时第一想法就想到用DFS，虽然预感对于字符串匹配类别的题目，DFS肯定会出现性能问题，但是作为验证算法思想的先导思路，还是打算先实现出来再看，毕竟用DFS写代码，代码真的很短，AC的时候更是有种 吊炸天的 爽快感；结果还是差一点AC；其次，可能是之前做过的原因，虽然第一时间没写出dp递推公式，但是写出了递推矩阵，演算了一番，没想到居然是对的，很容易的转换成了算法代码，结果干净利落的AC了，甚至比之前的AC的时候还要快一倍以上。虽然这不是自己第一次独立解决Hard题目，但是这题没有借鉴之前的代码，而且从构思算法道实践成功基本没遇到什么障碍，还是值得自己兴奋一阵子了。</p>\n<h3 id=\"解法一：DFS\"><a href=\"#解法一：DFS\" class=\"headerlink\" title=\"解法一：DFS\"></a>解法一：DFS</h3><p>　　</p>\n"}],"PostAsset":[{"_id":"source/_posts/my-first-blog/Euphonium_Movie_2nd_KV.jpg","slug":"Euphonium_Movie_2nd_KV.jpg","post":"ckg7qjivj00037j28eqt5h8o6","modified":0,"renderable":0},{"_id":"source/_posts/my-first-blog/Euphonium_Movie_Finale_KV2.jpg","slug":"Euphonium_Movie_Finale_KV2.jpg","post":"ckg7qjivj00037j28eqt5h8o6","modified":0,"renderable":0},{"_id":"source/_posts/my-first-blog/relife-1.png","slug":"relife-1.png","post":"ckg7qjivj00037j28eqt5h8o6","modified":0,"renderable":0},{"_id":"source/_posts/my-first-blog/relife-2.jpg","slug":"relife-2.jpg","post":"ckg7qjivj00037j28eqt5h8o6","modified":0,"renderable":0},{"_id":"source/_posts/改变与懒惰/2020-07-31.jpg","slug":"2020-07-31.jpg","post":"ckgt6w3by0000wx280fap2hbl","modified":0,"renderable":0},{"_id":"source/_posts/改变与懒惰/IMG_9373.jpeg","slug":"IMG_9373.jpeg","post":"ckgt6w3by0000wx280fap2hbl","modified":0,"renderable":0},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img1.png","slug":"img1.png","post":"ckhlu9lv60000w328appuchpc","modified":0,"renderable":0},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img2.jpg","slug":"img2.jpg","post":"ckhlu9lv60000w328appuchpc","modified":0,"renderable":0},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img3.jpg","slug":"img3.jpg","post":"ckhlu9lv60000w328appuchpc","modified":0,"renderable":0},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img6.jpg","slug":"img6.jpg","post":"ckhlu9lv60000w328appuchpc","modified":0,"renderable":0},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img7.jpg","slug":"img7.jpg","post":"ckhlu9lv60000w328appuchpc","modified":0,"renderable":0},{"_id":"source/_posts/LeetCode-33-Search-in-Rotated-Sorted-Array/img8.jpg","slug":"img8.jpg","post":"ckhlu9lv60000w328appuchpc","modified":0,"renderable":0},{"_id":"source/_posts/同步和异步编程-如何并行写同一个日志文件实践/1.jpeg","slug":"1.jpeg","post":"ckhvqydbn00006z28dot614mu","modified":0,"renderable":0},{"_id":"source/_posts/同步和异步编程-如何并行写同一个日志文件实践/2.jpeg","slug":"2.jpeg","post":"ckhvqydbn00006z28dot614mu","modified":0,"renderable":0},{"_id":"source/_posts/GPU-in-Pytorch-并行和分布式实践/1.png","slug":"1.png","post":"ckhyotdja0000l9289pxq20m8","modified":0,"renderable":0}],"PostCategory":[{"post_id":"ckg7lu78w0001bz2813ba5wh9","category_id":"ckg7piwa500006a2813an1zrb","_id":"ckg7piwa700016a285sew2411"},{"post_id":"ckg7qjivj00037j28eqt5h8o6","category_id":"ckg7qrrd000047j280hei7e4u","_id":"ckg7qrrd200077j2815sh1w8k"},{"post_id":"ckgt6w3by0000wx280fap2hbl","category_id":"ckg7qrrd000047j280hei7e4u","_id":"ckgt6w3c20002wx28byyididl"},{"post_id":"ckh35zvbu0000u7282nma27nl","category_id":"ckh35zvby0001u72804px1qo7","_id":"ckh35zvc10004u7286ara9hbw"},{"post_id":"ckhlu9lv60000w328appuchpc","category_id":"ckhlubjrc0000y9282so58qf8","_id":"ckhludw3200011e288weserfg"},{"post_id":"ckhvqydbn00006z28dot614mu","category_id":"ckhvqydbp00016z281mnxdhf3","_id":"ckhvqydbs00046z28hojpcuoy"},{"post_id":"ckhyotdja0000l9289pxq20m8","category_id":"ckhyotdje0002l928gyqa3gxs","_id":"ckhyotdji0006l928chmj50lm"},{"post_id":"ckiedp6df0000ue287z2u4kkg","category_id":"ckg7qrrd000047j280hei7e4u","_id":"ckiedp6dh0003ue28bxwgcd1k"}],"PostTag":[{"post_id":"ckg7lu78w0001bz2813ba5wh9","tag_id":"ckg7piagr00003v286tc5gsox","_id":"ckg7piagv00013v284nc01o56"},{"post_id":"ckg7qjivj00037j28eqt5h8o6","tag_id":"ckg7qrrd100057j289azi5q0k","_id":"ckg7qrrd100067j285lt81jqx"},{"post_id":"ckgt6w3by0000wx280fap2hbl","tag_id":"ckg7qrrd100057j289azi5q0k","_id":"ckgt6w3c10001wx28eo595uf9"},{"post_id":"ckh35zvbu0000u7282nma27nl","tag_id":"ckh35zvc00002u728a8q656mh","_id":"ckh35zvc20007u728fve34h0a"},{"post_id":"ckh35zvbu0000u7282nma27nl","tag_id":"ckh35zvc10003u7282616cu2s","_id":"ckh35zvc20008u7288k0q907w"},{"post_id":"ckh35zvbu0000u7282nma27nl","tag_id":"ckh35zvc10005u7281p3i2ffq","_id":"ckh35zvc20009u728720xdpaf"},{"post_id":"ckh35zvbu0000u7282nma27nl","tag_id":"ckh35zvc20006u7284k31d8oe","_id":"ckh35zvc2000au728gaocav7r"},{"post_id":"ckhlu9lv60000w328appuchpc","tag_id":"ckhlubjrc0001y928d4gv9u9l","_id":"ckhlubjrd0002y928dmdedxmp"},{"post_id":"ckhlu9lv60000w328appuchpc","tag_id":"ckhlue30200021e28486k1p79","_id":"ckhlue30300031e285vhycy46"},{"post_id":"ckhvqydbn00006z28dot614mu","tag_id":"ckhvqydbr00026z288h0ue571","_id":"ckhvqydbs00056z28fvjve3ff"},{"post_id":"ckhvqydbn00006z28dot614mu","tag_id":"ckhvqydbr00036z28g66r7hsq","_id":"ckhvqydbs00066z287q3d8g0m"},{"post_id":"ckhyotdja0000l9289pxq20m8","tag_id":"ckhyotdjg0003l928djuaby6g","_id":"ckhyotdjj000al928fghx5blk"},{"post_id":"ckhyotdja0000l9289pxq20m8","tag_id":"ckhyotdjh0005l928hv1rcwfl","_id":"ckhyotdjj000bl9280y7t97t2"},{"post_id":"ckhyotdja0000l9289pxq20m8","tag_id":"ckhyotdjj0007l928cxr4e0vg","_id":"ckhyotdjk000dl9284vfag7vy"},{"post_id":"ckiedp6df0000ue287z2u4kkg","tag_id":"ckhvqydbr00026z288h0ue571","_id":"ckiedp6dh0001ue28glzp1ror"},{"post_id":"ckiedp6df0000ue287z2u4kkg","tag_id":"ckhlue30200021e28486k1p79","_id":"ckiedp6dh0002ue28cbft5tkf"},{"post_id":"ckiedp6df0000ue287z2u4kkg","tag_id":"ckg7qrrd100057j289azi5q0k","_id":"ckiedp6dh0004ue2829v986c9"}],"Tag":[{"name":"test","_id":"ckg7piagr00003v286tc5gsox"},{"name":"随笔","_id":"ckg7qrrd100057j289azi5q0k"},{"name":"ubuntu","_id":"ckh35zvc00002u728a8q656mh"},{"name":"故障排查","_id":"ckh35zvc10003u7282616cu2s"},{"name":"linux内核","_id":"ckh35zvc10005u7281p3i2ffq"},{"name":"grub","_id":"ckh35zvc20006u7284k31d8oe"},{"name":"-Algorithm","_id":"ckhlu9lvk0002w328hz4x9glf"},{"name":"Algorithm","_id":"ckhlubjrc0001y928d4gv9u9l"},{"name":"LeetCode","_id":"ckhlue30200021e28486k1p79"},{"name":"Algorithms","_id":"ckhvqydbr00026z288h0ue571"},{"name":"OS","_id":"ckhvqydbr00036z28g66r7hsq"},{"name":"Pytorch","_id":"ckhyotdjg0003l928djuaby6g"},{"name":"CUDA","_id":"ckhyotdjh0005l928hv1rcwfl"},{"name":"GPU","_id":"ckhyotdjj0007l928cxr4e0vg"},{"name":"NLP","_id":"ckhyotdjj000cl928cahc99hr"}]}}